[
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tScraping a website means extracting data from a website in a usable way.\nThe ultimate goal when scraping a website is to use the extracted data to build something else.\nIn this article, I will show you how to extract the content of all existing articles of Theodo\u2019s blog with Scrapy, an easy-to-learn, open source Python library used to do data scraping.\nI personally used this data to train a machine learning model that generates new blog articles based on all previous articles (with relative success)!\nImportant:\nBefore we start, you must remember to always read the terms and conditions of a website before you scrape it as the website may have some requirements on how you can legally use its data (usually not for commercial use).\nYou should also make sure that you are not scraping the website too aggressively (sending too many requests in a short period of time) as it may have an impact on the scraped website.\nScrapy\nScrapy is an open source and collaborative framework for extracting data from websites.\nScrapy creates new classes called Spider that define how a website will be scraped by providing the starting URLs and what to do on each crawled page.\nI invite you to read the documentation on Spiders if you want to better understand how scraping is done when using Scrapy\u2019s Spiders.\nScrapy is a Python library that is available with pip.\nTo install it, simply run pip install scrapy.\nYou are now ready to start the tutorial, let\u2019s get to it!\nExtracting all the content of our blog\nYou can find all the code used in this article in the accompanying repository.\nGet the content of a single article\nFirst, what we want to do is retrieve all the content of a single article.\nLet\u2019s create our first Spider. To do that, you can create an article_spider.py file with the following code:\nimport scrapy\r\n\r\n\r\nclass ArticleSpider(scrapy.Spider):\r\n    name = \"article\"\r\n    start_urls = ['http://blog.theodo.fr/2018/02/scrape-websites-5-minutes-scrapy']\r\n\r\n    def parse(self, response):\r\n        content = response.xpath(\".//div[@class='entry-content']/descendant::text()\").extract()\r\n        yield {'article': ''.join(content)}\r\n\nLet\u2019s break everything down!\nFirst we import the scrapy library and we define a new class ArticleSpider derived from the Spider class from scrapy.\nWe define the name of our Spider and the start_urls, the URLs that our Spider will visit (in our case, only the URL of this blog post).\nFinally, we define a parse method that will be executed on each page crawled by our spider.\nIf you inspect the HTML of this page, you will see that all the content of the article is contained in a div of class entry-content.\nScrapy provides an xpath method on the response object (the content of the crawled page) that creates a Selector object useful to select parts of the page.\nIn the xpath method, it will create a Selector based on the xpath language.\nUsing this method, we find the list of the text of all the descendants (divs, spans\u2026) contained in the entry-content block.\nWe then return a dictionary with the content of the article by concatenating this list of text.\nNow, if you want to see the result of this Spider, you can run the command scrapy runspider article_spider.py -o article.json.\nWhen you run this command, Scrapy looks for a Spider definition inside the file and runs it through its crawling engine.\nThe -o flag (or --output) will put the content of our Spider in the article.json file, you can open it and see that we indeed retrieved all the content of the article!\nNavigate through all the articles of a page\nWe now know how to extract the content of an article.\nBut how can we extract the content of all articles contained on a page ?\nTo do this, we need to identify the URLs of each article and use what we learned in the previous section to extract the content of each article.\nWe could use the same Spider as the last section and give all the URLs to the start_urls attribute but that would take a lot of manual time to retrieve all the URLs.\nIn a blog page like this one, you can go to an article by clicking on the title of the article.\nWe must thus find a way to visit all of the articles by clicking on each titles.\nScrapy provides another method on the response object, the css method that also creates a Selector object, but this time based on the CSS language (which is easier to use than xpath).\nIf you inspect the title of an article, you can see that it is a link with a a tag contained in a div of class entry-title.\nSo, to extract all the links of a page, we can use the selector with response.css('.entry-title a ::attr(\"href\")').extract().\nNow let\u2019s put two and two together and create a page_spider.py file with this code:\nimport scrapy\r\n\r\n\r\nclass PageSpider(scrapy.Spider):\r\n    name = \"page\"\r\n    start_urls = ['http://blog.theodo.fr/']\r\n\r\n    def parse(self, response):\r\n        for article_url in response.css('.entry-title a ::attr(\"href\")').extract():\r\n            yield response.follow(article_url, callback=self.parse_article)\r\n\r\n    def parse_article(self, response):\r\n        content = response.xpath(\".//div[@class='entry-content']/descendant::text()\").extract()\r\n        yield {'article': ''.join(content)}\r\n\nWhat our PageSpider is doing here is start on the homepage of our blog and identify the URLs of each article in the page using the css method.\nThen, we use the follow method on each URL to extract the content of each article using the parse_article callback (directly inspired from the first part).\nThe follow method allow us to do a new request and apply a callback on it, this is really useful to do a Spider that navigates through multiple pages.\nIf you run the command scrapy runspider page_spider.py -o page.json, you will see in the page.json output that we retrieved the content of each article of the homepage.\nYou may notice one of the main advantages about Scrapy: requests are scheduled and processed asynchronously.\nThis means that Scrapy doesn\u2019t need to wait for a request to be finished and processed, it can send another request or do other things in the meantime.\nNavigate through all the pages of the blog\nNow that we know how to extract the content of all articles in a page, let\u2019s extract all the content of the blog by going through all the pages of the blog.\nOn each page of the blog, at the bottom of the page, you can see an \u201cOlder posts\u201d button that links to the previous page of the blog.\nTherefore, if we want to visit all pages of the blog, we can start from the first page and click on \u201cOlder posts\u201d until we reach the last page (obviously, the last page of the blog does not contain the button).\nThe \u201cOlder posts\u201d button can be easily identified using the same css method as the previous section with response.css('.nav-previous a ::attr(\"href\")').extract_first().\nNow let\u2019s retrieve all the content of our blog with a blog_spider.py:\nimport scrapy\r\n\r\n\r\nclass BlogSpider(scrapy.Spider):\r\n    name = \"blog\"\r\n    start_urls = ['http://blog.theodo.fr/']\r\n\r\n    def parse(self, response):\r\n        for article_url in response.css('.entry-title a ::attr(\"href\")').extract():\r\n            yield response.follow(article_url, callback=self.parse_article)\r\n        older_posts = response.css('.nav-previous a ::attr(\"href\")').extract_first()\r\n        if older_posts is not None:\r\n            yield response.follow(older_posts, callback=self.parse)\r\n\r\n    def parse_article(self, response):\r\n        content = response.xpath(\".//div[@class='entry-content']/descendant::text()\").extract()\r\n        yield {'article': ''.join(content)}\r\n\nNow, our BlogSpider extracts all URLs of articles and calls the parse_article callback and then extracts the URL of the \u201cOlder posts\u201d button.\nIt then follows the URL and applies the parse callback on the previous page of our blog.\nIf you run the command scrapy runspider blog_spider.py -o blog.json, you will see that our Spider will visit every article page and retrieve the content of each article since the beginning of our blog!\nGoing further\n\nWe could have also used a CrawlSpider, another Scrapy class that provides a dedicated mechanism for following links by defining a set of rules directly in the class.\nYou can look at the documentation here.\nIn the parse_article function, we retrieved all the text content but that also includes the aside where we have the author of each article.\nTo remove it from the output, you can change the xpath by response.xpath(\".//div[@class='entry-content']/descendant::text()[not(ancestor::aside)]\").\nYou can see that the output of the scraper is not perfect as we see some unorthodox characters like \\r, \\n or \\t.\nTo exploit the data, we would first need to clean what we retrieved by removing unwanted characters.\nFor example, we could replace \\t characters to a space with a simple content.replace('\\r', ' ').\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tThomas Mollard\r\n  \t\t\t\r\n  \t\t\t\tWeb Developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"}
]